{
    "2": [
        "The novel methods of manual specification, contextual diffusion, Google n-grams, and Word-Net all outperform in general the original assumption H = I.",
        "The two reduction methods of PCA and t-SNE represent a popular classical technique and a recently proposed technique that outperforms other recent competitors (LLE, Isomap, MVU, CCA, Laplacian eigenmaps).",
        "Our experiments demonstrate that different domain knowledge methods perform best in different situations.",
        "As a generalization, however, the contextual diffusion and Google n-gram methods had the strongest performance.",
        "Combining different types of domain knowledge provides increased effectiveness, and such combinations may be found without the use of labeled data."
    ],
    "6": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptive Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The proposed parsing algorithm performs discourse parsing in the PDTB representation and is the first end-to-end discourse parser that can parse any unrestricted text into its discourse structure in the PDTB style.",
        "The system evaluated both component-wise as well as in an end-to-end fashion with cascaded errors, and achieved overall system F 1 scores for partial matching of 46.80% with gold standard parses and 38.18% with full automation.",
        "The discourse parser is useful in downstream applications such as text summarization and question answering (QA), as it can recognize updates and redundancy, and answer why-questions using causal relations."
    ],
    "7": [
        "The study demonstrates new state-of-the-art performance on the FewRel 2.0 dataset using the proposed approach.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The Cosine Annealing Strategy combines the two methods to improve domain adaptation performance.",
        "Pseudo-labeled target-domain data is used to train the few-shot classifier, leading to improved performance.",
        "The approach achieves state-of-the-art performance on the FewRel 2.0 dataset without requiring labeled source-domain data.",
        "The study demonstrates the effectiveness of using gaze features and part-of-speech information for disambiguating categories.",
        "Late gaze features are the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The model based on gaze features and part-of-speech information achieves accuracy similar to that of linguistic-based models and state-of-the-art systems without the need for text processing."
    ],
    "15": [
        "The four dimensions of linguistic variability are recognized as markers of all five personality traits by both language communities.",
        "There are qualitative differences in the perception of openness, conscientiousness, agreeableness, and naturalness across language communities.",
        "The results can be used to adapt natural language generation and interpretation to native speakers of American English or Arabic.",
        "The study supports the feasibility of the crowdsourcing approach to validate the linguistic devices that realize rich points-behaviors that signal differences across languages and cultures.",
        "Future work shall evaluate effects of regional dialects and address the issue of particular wording choices by using multiple stimuli per condition."
    ],
    "18": [
        "The authors propose a new approach to assessing sentence formality using a five-point Likert scale.",
        "The authors obtained better and consistent agreement values on a set of 500 sentences using this approach.",
        "Sentences from different categories (blog, forum, news, and paper) were found to follow different formality rating distributions.",
        "The authors performed a difficulty analysis to identify problematic sentences.",
        "As a by-product of their study, the authors obtained a seed set of human-annotated sentences that can be used in evaluating an automatic scoring mechanism for sentence-level formality."
    ],
    "23": [
        "Approaches based on machine learning need to be able (a) to handle biased data, and (b) to adjust to the rapidly changing vocabulary to prevent a flood of false positives when new topics trend.",
        "Future work will compare keyword classifiers against more conceptual approaches such as [12] and also compare the performance characteristics of change point detection algorithms.",
        "Our experimental application called DIZIE samples Twitter messages originating in major world cities and automatically classifies them according to syndromes.",
        "Based on the outcome of our follow-up study, we intend to integrate DIZIE's output with our event-based surveillance system BioCaster which is currently used by the international public health community."
    ],
    "25": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our proposed framework is effective and suitable for collaborative development efforts, as well as useful in a teaching environment.",
        "Our results provide improvements an order of magnitude greater than our 2010 attempt (Yu et al., 2010)."
    ],
    "26": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Using this, we have generated a TIMEX3 resource with an order of magnitude more annotations than all previous resources put together.",
        "The resource contains new information about temporal expressions, and is helpful for training automatic timex annotation systems."
    ],
    "48": [
        "DNNs can extract more invariant and discriminative features at the higher layers.",
        "The features learned by DNNs are less sensitive to small perturbations in the input features.",
        "DNNs generalize better than shallow networks.",
        "CD-DNN-HMMs perform speech recognition in a manner that is more robust to mismatches in speaker, environment, or bandwidth.",
        "DNNs require seeing representative samples to perform well.",
        "A multi-style training strategy and letting DNNs to generalize to similar patterns can equal the best result ever reported on the Aurora 4 noise robustness benchmark task without the need for multiple recognition passes and model adaptation."
    ],
    "51": [
        "The proposed technique can lead to interesting results, especially when compared to previous results achieved with a linguistically rich framework that required several months of skilled labor to build.",
        "The accuracy obtained for the Cinema corpus is higher than previous results, and the NLU module can be easily developed.",
        "The approach has some limitations, such as dependence on words used during training and difficulty in detecting paraphrases.",
        "Adding extra weight to certain words or adding synonyms to the training utterances file could help improve the system.",
        "The actual model does not include any history of interactions, and the behavior of the system should be carefully analyzed as the classification process becomes more complex with an increasing number of interactions."
    ],
    "53": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our proposed recurrent chunking mechanisms outperform benchmark models across different datasets.",
        "We have presented the first probabilistic approach to frame induction and achieved state-of-the-art results on end-to-end entity extraction in standard MUC and TAC data sets.",
        "Our model is inspired by recent advances in unsupervised semantic induction and in content modeling in summarization, and is easy to extend.",
        "We would like to further investigate frame induction evaluation, for example to evaluate event clustering in addition to the slots and entities."
    ],
    "54": [
        "Clustering the topics distributions of the global context of polysemous words in the topic space to induce their sense is cheap as it does not require any annotated data and is language-independent.",
        "Our clustering carried some different senses, even though it did not fully conform with the set of senses given by the GS classes.",
        "In one case, a GS sense was not captured by the topic model, and instead, other cues from its instances context were used to cluster them accordingly.",
        "The induced clustering had some noise though.",
        "This simple WSI approach can be used for cheap sense induction or for languages for which no POS tagger has been created yet.",
        "This system which had the second highest V-measure score in SemEval-2 WSI task achieves a good trade-off between performance and cost."
    ],
    "56": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Existing datasets and the proposed method can be combined with distant supervision and reinforcement learning to achieve better performance without requiring a large, high-quality annotated corpus for causality extraction.",
        "The embeddings of GloVe, ELMo, and BERT contain gender, religion, and nationality biases.",
        "A projection-based method can be used to attenuate these biases in the embeddings.",
        "The method works for static GloVe embeddings and can effectively attenuate bias in contextualized embeddings without loss of entailment accuracy.",
        "The circumplex in Figure 2 illustrates how different cultures conceptualize emotions differently, with significant differences in the valence and arousal dimensions.",
        "Certain emotions, such as depression, are conceptualized differently by different cultures, with Asians finding it more negative than other cultures and control groups.",
        "The emotions happy and calm are found to be more positive by Europeans and Asians than by North Americans and all control groups.",
        "Interest is found to be a very positive and aroused emotion for Asians, compared to North Americans who conceptualize it as negative and disengaged."
    ],
    "57": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The Cosine Annealing Strategy combines two methods to improve domain adaptation performance.",
        "The approach utilizes pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "The proposed method improves efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Book summarization remains a challenging task, but improved book/summary alignments hold intrinsic value in shedding light on what features of a work are deemed \"summarizable\" by human editors."
    ],
    "58": [
        "The beam problem in neural machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our proposed method for causality extraction using a self-attentive BiLSTM-CRF-based solution is effective, but the performance is limited by the insufficiency of high-quality annotated data.",
        "The performance of our proposed method for predicting deleted messages on Twitter can be very high for certain groups of users."
    ],
    "67": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptive mechanism of Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The structural models investigated in the paper can effectively preserve salient source relations in summaries.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "A sizable percentage of nouns, verbs, adjectives, and adverbs are emotive, with trust and joy being the most common emotions associated with terms.",
        "Adjectives and adverbs are more commonly associated with emotions than nouns and verbs.",
        "The EmoLex-WAL rows serve to determine how much the Turker annotations match annotations in the Wordnet Affect Lexicon (WAL).",
        "Many terms marked as anger terms in WAL are not truly associated with anger, and some terms are associated with both anger and joy.",
        "The Turkers marked some terms as being associated with both anger and joy.",
        "The EmoLex-WAL rows also indicate which emotions tend to be jointly associated with a term, such as anger terms being associated with disgust and joy terms being associated with trust."
    ],
    "76": [
        "Fine-grained emotion features provide statistically significant gains over the baseline.",
        "Coarse affect features and specificity features do not provide significant improvements.",
        "Fine affect categories contain useful discriminating information not present in coarse affect categories or simple specificity features.",
        "The top ten emotion categories with the highest gain for each of the five personality dimensions are indicative of either end of the personality trait.",
        "Some emotions are very close to the basic emotions of happiness and sadness.",
        "The terms in the #possessive category tend to be used more often by extroverts, while the terms in the #apart category tend to be associated more with introverts."
    ],
    "77": [
        "Our method outperforms two solid baselines substantially, especially in precision.",
        "The extracted DSSWs outperform general sentiment lexicon and baseline DSSWs in sentiment classification task.",
        "With the increasing number of patterns and general sentiment words, the F-value increases obviously.",
        "Our method is useful for extracting domain-specific sentiment words.",
        "In future work, we intend to explore hidden targets to improve the recall of our method.",
        "We plan to rank the extracted patterns to increase the accuracy."
    ],
    "78": [
        "Our method, learned over bitext alone, can rival performance of supervised models trained with thousands of labeled examples.",
        "Applying our method in a setting where all labeled examples are available also shows improvements over state-of-the-art supervised methods.",
        "Soft expectation projection is more favorable to hard projection.",
        "This technique can be generalized to all sequence labeling tasks, and can be extended to include more complex constraints.",
        "We plan to apply this method to more language pairs and examine the formal properties of the model."
    ],
    "82": [
        "Accuracies up to 67% were obtained for ensemble averages of 100 models, trained on the best parameter choices, with individual model accuracies rising as high as 69%.",
        "Finding the best models is an arduous task.",
        "In order to obtain a good fit, several training parameters must be explored: the thresholding of word-counts into bins, and the runtime dynamical feature-selection size.",
        "The most interesting result is that word-pairs can be used to build more accurate models than single words alone.",
        "However, in order for this to work well, a number of data cuts must be applied: word pairs with low mutual information scores should be discarded; infrequently occurring pairs and words should be discarded, and, most important of all, word-pairs that don't contain 'significant' words should be discarded as well."
    ],
    "92": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "We have demonstrated that appealing word embeddings can be obtained by computing a Hellinger PCA of the word co-occurence matrix.",
        "A neural network language model can be painful and long to train, but our method gives an interesting and practical alternative to generating word embeddings.",
        "Deep-learning is an interesting framework to finetune embeddings over specific NLP tasks."
    ],
    "94": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our novel constituent hierarchy predictor based on recurrent neural networks captures global sentential information.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our power low rank ensembles technique generalizes existing n-gram smoothing techniques to non-integer n.",
        "Our method captures both the fine-grained and coarse structures in word sequences.",
        "Our discounting strategy preserves the marginal constraint and thus generalizes Kneser Ney, and under slight changes can also extend other smoothing methods such as deleted-interpolation/Jelinek-Mercer smoothing.",
        "Experimentally, PLRE convincingly outperforms Kneser-Ney smoothing as well as class-based baselines."
    ],
    "95": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed zero-shot learning framework for SUC learns a knowledge-base using deep networks trained on large amounts of search engine query log data.",
        "The novel way to learn embeddings without access to labelled data is effective.",
        "The methods are experimentally shown to be effective."
    ],
    "97": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The model can be applied to other learning problems.",
        "Our system can be fed with a large number of articles per day and sensitive data without including third parties in the translation process.",
        "Performance and translation time vary according to the number and complexity of sentences and language pairs.",
        "The domain of news articles dynamically changes according to the main events in the world, while existing parallel data is static and usually associated to governmental domains.",
        "It is our intention to investigate how to adapt our translation system updating the language model with the English articles of the day."
    ],
    "100": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Approaching lexical entailment as a supervised learning problem of semantic relation classification is a promising approach.",
        "The results indicate that this is a promising approach to lexical entailment."
    ],
    "104": [
        "The extension to the PYP-HMM part-of-speech model incorporating a sparse prior on the lexicon and an SMC based inference algorithm provides a more plausible model of part-of-speech induction.",
        "The model is able to meet or exceed the performance of the previous state-of-the-art across a range of language families, as evidenced by empirical evaluation.",
        "The model learns ambiguity classes that are often quite similar to those in the gold standard, as indicated by analysis.",
        "Further improvements in both the structure of the lexicon prior and the inference algorithm will lead to additional performance gains, such as better modelling the relationship between a word's morphology and its ambiguity class.",
        "The model could be improved by applying it to recent semi-supervised approaches which induce partial tag dictionaries from parallel language data or Wiktionary.",
        "The Lex-HMM models ambiguity classes to focus the sampler on the most likely parts-of-speech for a given word-type, matching or improving accuracy while running much faster."
    ],
    "107": [
        "Our modification to the standard projected subgradient dual decomposition algorithm for performing MAP inference subject to hard constraints leads to significant gains in accuracy.",
        "Using soft constraints and selecting which constraints to use with our penalty-learning procedure can lead to significant gains in accuracy.",
        "We achieve a 17% gain in accuracy over a chain-structured CRF model while only needing to run MAP in the CRF an average of less than 2 times per example.",
        "Our algorithm obtains certificates of optimality on 100% of our test examples in practice, suggesting its usefulness for large-scale applications.",
        "We encourage further use of our Soft-DD procedure for other structured prediction problems."
    ],
    "109": [
        "Inferring high-level topics helps ensure consistency across sentences in multi-sentence video descriptions.",
        "Hand centric features improve visual recognition of manipulated objects, leading to improved sentence production.",
        "Using probabilistic input in SMT improves the quality of sentences produced for individual video snippets.",
        "Producing video descriptions at multiple levels of detail is a previously unexplored task that can be addressed by analyzing human descriptions of different lengths.",
        "The language used to describe videos can be compressed according to the topic of the video, allowing for the extraction of most relevant segments.",
        "Using a language model targeted at the type of description improves over using one learned from descriptions of another level of detail."
    ],
    "113": [
        "The gap between the best phrase-structure parsing and direct dependency parsing methods has narrowed due to developments in dependency parsing.",
        "Of-speech tagging has a notable effect on the gap between phrase-structure parsers and direct dependency parsing.",
        "Targeted part-of-speech representations for dependencies can improve joint part-of-speech/dependency analysis.",
        "An alternative, more syntax-focused dependency representation can be beneficial for dependency parsing."
    ],
    "114": [
        "The proposed framework can successfully train open question answering models with very little supervision, using embeddings as its core.",
        "The approach significantly outperforms previous work for answering simple factual questions.",
        "The fine-tuning method introduced in the paper can be used to solve optimization problems that cannot be completely solved.",
        "Despite the promising results, there are still challenges to be addressed, such as scaling up the model to answer more complex questions.",
        "The current model can only answer simple factual questions satisfactorily due to the very low supervision signal, and does not consider the word ordering when modeling them.",
        "Much more work needs to be carried out to encode the semantics of more complex questions into the embedding space."
    ],
    "115": [
        "The proposed method outperforms the state-of-the-art on the task of cross-lingual document classification.",
        "The approach extends the distributional hypothesis to multilingual joint-space representations.",
        "Simple composition functions can be used to improve the performance of the model.",
        "Bilingual signals are a useful tool for learning distributed representations and enabling models to abstract away from mono-lingual surface realisations into a deeper semantic space.",
        "The approach allows lexicality and syntax to interact with each other in the joint search process, improving the accuracy of tagging and parsing.",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality."
    ],
    "116": [
        "Utilizing radical information for Chinese character learning can significantly improve the performance of Chinese computational processing.",
        "Our proposed method, which integrates radical information into a dedicated neural architecture with a hybrid loss function, is effective in capturing semantic connections between characters from both syntactic contexts and radical information.",
        "Our radical-enhanced model outperforms two widely-accepted embedding learning algorithms that do not utilize radical information on Chinese character similarity judgement and Chinese word segmentation tasks.",
        "The effectiveness of our method has been verified through experiment results on both tasks."
    ],
    "120": [
        "We have shown that a statistically significant improvement in the performance of NER system for a given language can be obtained when the training data is supplemented with word clusters from a secondary language(s) which is written using the same alphabet.",
        "The amount of help provided by this secondary language depends on how similar the secondary language is to the given language phylogenetically and also on the domain of the data from which the word clusters are obtained.",
        "Many of the NEs, specially, names of persons and locations remain the same when used in a different language and hence the word class information of such an OOV word is helpful in predicting its NE class.",
        "The use of word clusters from a secondary language can improve the performance of a NER system for a given language.",
        "The similarity between the secondary language and the target language affects the amount of help provided by the word clusters.",
        "The domain of the data from which the word clusters are obtained can also impact the amount of help provided."
    ],
    "122": [
        "The distributed word alignment model (DWA) learns both word representations and alignments simultaneously, and is able to learn alignments on par with the FASTALIGN alignment model.",
        "The DWA model can effectively project documents from one language to another using the learned word representations.",
        "The word representations learned by the DWA model are semantically plausible and useful for cross-lingual document classification tasks, outperforming prior work and achieving state-of-the-art results.",
        "The probabilistic account of word representations across multiple languages provided by the DWA model can be applied to machine translation and related tasks, where previous approaches are less suited.",
        "Combining the DWA model with monolingual language models, particularly in the context of semantic transfer into resource-poor languages, is a potential avenue for further study."
    ],
    "123": [
        "The proposed speech enhancement front-end consists of a cascaded robust AEC, a residual echo power estimator based on a double-talk probability estimator, and a novel quasi-binary masking that utilizes the classical MMSE-based method at very low SNRs.",
        "The optimized front-end is then tested in realistic environments for the remote control of a music playback device with a limited-sized command dictionary.",
        "The result shows a fairly high recognition rate for voice commands at a speech-to-music ratio as low as -35 dB, scenarios hardly seen through the literature.",
        "The tuning improves the speech recognition rate substantially on the TIMIT database."
    ],
    "124": [
        "The D-Bees algorithm has been introduced, a novel knowledge-based unsupervised method for solving the problem of WSD inspired by bee colony optimization.",
        "The experiments on the standard dataset SemEval 2007 coarse-grained English all-words task corpus have shown that D-Bees achieves promising results and is competitive to other methods in this field.",
        "The use of bee colony optimization in the D-Bees algorithm inspires further research on related algorithms."
    ],
    "134": [
        "The proposed embedding model can learn to perform open QA using training data made of questions paired with their answers and a KB, achieving promising performance on the competitive benchmark WebQuestions.",
        "The model learns to perform open QA using training data, which provides a structure among answers.",
        "The proposed approach can achieve promising performance on the competitive benchmark WebQuestions.",
        "The model uses training data made of questions paired with their answers and a KB to provide a structure among answers.",
        "The embedding model learns to perform open QA using training data, which is a structured representation of answers."
    ],
    "139": [
        "We have developed an evaluation technique to automatically measure how well a video summary retains the semantic information in the original video.",
        "Our approach is based on generating a text representation of the video summary, and measuring the semantic distance of the text to ground-truth text summaries written by humans.",
        "Our experiments show that this approach correlates well with human judgment, and outperforms pixel-based distance measures.",
        "In addition, our framework can be extended to evaluate any type of video summary, and can accommodate future extensions to our semantic distance metric."
    ],
    "141": [
        "SCODE word embeddings surpassed prior state-of-the-art methods for all tasks.",
        "Using SCODE word embeddings as additional features in dependency parsing led to successful results, compared to other word embeddings or achieving better results.",
        "SCODE word embeddings are consistent in improving the baseline systems in multilingual settings.",
        "Other word embeddings have not been studied in multilingual settings yet, making SCODE word embeddings a unique and valuable contribution to the field.",
        "The code used in generating SCODE embeddings is publicly available, making it accessible for further research and development."
    ],
    "146": [
        "Using a heuristic optimization algorithm to tune the combination of components had a positive effect on the performance of our system.",
        "The overall performance is also in line with previous evaluations on the same dataset.",
        "Our methodology could be used to evaluate the added value of each component, which could be useful in future work.",
        "We can easily add new components to the system to make it more competitive in the future, as only the basic components have been integrated so far.",
        "Some effort may be required to understand why some of our components do not bring so much added value, and modify them to address this situation.",
        "Different optimization methods could also be implemented."
    ],
    "147": [
        "The decoding algorithm enables first-pass LVCSR with a language model for CTC-trained neural networks, removing the dependence on HMM-based systems found in previous work.",
        "First-pass decoding demonstrates the capabilities of a CTC-trained system without the confounding factor of potential effects from pruning the search space via a provided lattice.",
        "The CTC-based speech recognition systems demonstrate the promise of high-quality LVCSR without the complexity of HMM-based infrastructure.",
        "The BRDNN is a less complex architecture than LSTMs and can relatively easily be made to run on GPUs, simplifying the infrastructure needed for CTC-based speech recognition systems.",
        "Recurrent connections are critical for good performance in CTC-based speech recognition systems, and bi-directional recurrence helps beyond single direction recurrence but could be sacrificed in cases that require low-latency, online speech recognition."
    ],
    "148": [
        "The use of a costsensitive classification approach can effectively lower the number of false positives in spammer detection while compromising on spammer detection efficacy.",
        "Simple text normalization and substring clustering can generate efficient and agile models suitable for real-time filtering.",
        "The use of these techniques can provide a positive but modest contribution to pushing the F1 of message classification further.",
        "There is still room for improvements in this area that will require a detailed case-by-case analysis, which will be addressed in future work.",
        "Obvious advantages are present over standard tokenization approaches, but they can also generate false positives, which will require further work to identify potentially problematic cases."
    ],
    "151": [
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We plan to include probabilistic and distributional features from a top-down incremental parser e.g. Roark et al. (2009) , and use STIR's distributional features to classify repair type."
    ],
    "161": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.",
        "Adding excitation and prosodic information to the networks still fits within the method.",
        "Finding the right stable hardware combination that offers the most efficient training platform is an investigatory task."
    ],
    "162": [
        "We propose a multimodal Recurrent Neural Network (m-RNN) framework that performs at the state-of-the-art in three tasks: sentence generation, sentence retrieval given query image, and image retrieval given query sentence.",
        "Our m-RNN can be extended to use more complex image features (e.g., object detection features) and more sophisticated language models.",
        "Our framework achieves state-of-the-art performance in sentence generation, sentence retrieval given query image, and image retrieval given query sentence.",
        "We extend our m-RNN to use more complex image features and more sophisticated language models, which allows for further improvement in performance."
    ],
    "170": [
        "The best RMSE value achieved with stepwise linear regression is around 11 at 1-32 scale.\" (related to the effectiveness of the NLP methods used)",
        "Suicidal ideation is predictable.\" (related to the predictive accuracy of the models)",
        "Innovatively, we attempt topics inferred from a model trained by high suicidal ideation group.\" (related to the use of topics inferred from a model trained on a high-risk group)",
        "The same method is worth trying on other predict tasks.\" (related to the potential applicability of the method to other predictive tasks)",
        "Some conclusions made by previous works are confirmed in our study.\" (related to the validation of previous findings)"
    ],
    "172": [
        "We have presented a general and scalable framework to learn from unlabeled examples for structured prediction.",
        "The technique allows features with global scope in observed variables with favorable asymptotic inference runtime.",
        "We achieve this by embedding a CRF as the encoding model in the input layer of an autoencoder, and reconstructing a transformation of the input at the output layer using simple categorical distributions.",
        "The key advantages of the proposed model are scalability and modeling flexibility.",
        "We applied the model to POS induction and bitext word alignment, obtaining results that are competitive with the state of the art on both tasks.",
        "We consider two baselines: \u2022 hmm: a standard first-order hidden Markov model learned with EM; 13\u2022 fhmm: the directed alternative discussed in the main paper, as implemented by Berg-Kirkpatrick et al. [3] , with the feature set from Smith and Eisner [38] .",
        "The fhmm baseline also uses a squared L 2 regularizer for the log-linear parameters.",
        "The hyperparameters of our model, as well as baseline models, were tuned to maximize many-to-one accuracy for The English PennTreebank."
    ],
    "175": [
        "We investigate the effects of entity representations by incorporating unsupervised vectors pre-trained on extra textual resources.",
        "Our results show several interesting findings, enabling the design of a simple embedding model that achieves the new state-of-the-art performance on a popular knowledge base completion task evaluated on Freebase.",
        "Given the recent successes of deep learning in various applications, our future work will aim to exploit deep structure including possibly tensor construct in computing the neural embedding vectors.",
        "This will extend the current multi-relational neural embedding model to a deep version that is potentially capable of capturing hierarchical structure hidden in the input data."
    ],
    "176": [
        "Retrofitting improves word vector quality and is more modular than other approaches that use semantic information while training.",
        "Retrofitting outperforms existing alternatives.",
        "Retrofitting can be applied to vectors obtained from any word vector training method."
    ],
    "179": [
        "The system trains on images and corresponding captions, and learns to extract nouns, verbs, and adjectives from regions in the image.",
        "The detected words guide a language model to generate text that reads well and includes the detected words.",
        "The global deep multimodal similarity model introduced in this paper is used to re-rank candidate captions.",
        "The system is state-of-the-art on all 14 official metrics of the COCO image captioning task, and equal to or exceeding human performance on 12 out of the 14 official metrics.",
        "Our generated captions have been judged by humans (Mechanical Turk workers) to be equal to or better than human-written captions 34% of the time."
    ],
    "181": [
        "Our protocol enables an objective comparison of machine generation approaches based on their 'human-likeness'.",
        "We introduce an annotation modality for measuring consensus, a metric CIDEr for automatically computing consensus.",
        "We demonstrate CIDEr has improved accuracy over existing metrics for measuring consensus.",
        "Our protocol does not rely on arbitrary calls on weighing content, grammar, saliency, etc. with respect to each other.",
        "The proposed evaluation protocol is effective in comparing machine generation approaches based on their 'human-likeness'."
    ],
    "182": [
        "We present a compositional method for inducing distributional representations not only of discourse arguments, but also of the entities that thread through the discourse.",
        "In this approach, semantic composition is applied up the syntactic parse tree to induce the argument-level representation, and then down the parse tree to induce representations of entity spans.",
        "Discourse arguments can then be compared in terms of their overall distributional representation, as well as by the representations of coreferent entity mentions.",
        "This approach outperforms previous work on classification of implicit discourse relations in the Penn Discourse Treebank.",
        "Future work may consider joint models of discourse structure and coreference, as well as representations for other discourse elements, such as event coreference and shallow semantics."
    ],
    "183": [
        "The experimental results show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The difference in performance of the two methods underlines the need to find the appropriate sentence spaces for particular tasks.",
        "This preliminary study indicates that plausibility training may be better suited for disambiguation.",
        "Further work will consist of more in-depth analysis and optimization of the training procedure, as well as investigation into ways of low-cost learning of task-specific sentence spaces."
    ],
    "184": [
        "CNN provides an alternative mechanism for effective use of word order for text categorization.",
        "With the parallel CNN framework, several types of embedding can be learned and combined so that they can complement each other for higher accuracy.",
        "State-of-the-art performances on sentiment classification and topic classification were achieved using this approach.",
        "The traditional bag-of-ngram approach or word-vector CNN is not the only way to use word order for text categorization.",
        "Learning multiple types of embedding and combining them can lead to higher accuracy in text categorization."
    ],
    "188": [
        "The proposed method considers both co-occurrences and semantic relations to learn word representations.",
        "The method can be applied to manually created relational graphs, such as ontologies, as well as automatically extracted relational graphs from text corpora.",
        "The learnt word representations are useful for answering semantic word analogy questions.",
        "Lexical patterns are particularly useful for learning good word representations, outperforming several baseline methods.",
        "The work inspires future research in word representation learning to exploit the rich semantic relations that exist between words, extending beyond simple co-occurrences."
    ],
    "189": [
        "The proposed framework of incorporating \"weight tuning\" of SVM into DL architectures leads to better higher-level representations and generates better performance against standard neural models in multiple tasks.",
        "While it still underperforms bag-of-word models in some cases, and the newly proposed paragraph vector approach, it provides an alternative to existing recursive neural models for representation learning.",
        "The idea of weight tuning in WNN and BENN, which associates nodes in neural models with additional weighed variables, is a general one and can be extended to many other deep learning models with minor adjustment."
    ],
    "196": [
        "Our end-to-end deep learning-based speech system outperforms existing state-of-the-art recognition pipelines in two challenging scenarios: clear, conversational speech and speech in noisy environments.",
        "Our approach is enabled by multi-GPU training and data collection and synthesis strategies to build large training sets exhibiting the distortions our system must handle (such as background noise and Lombard effect).",
        "Our data-driven speech system is better performing than existing methods while no longer relying on the complex processing stages that had stymied further progress.",
        "We believe this approach will continue to improve as we capitalize on increased computing power and dataset sizes in the future."
    ],
    "206": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "We introduced a ConvNet model for documents with an architecture designed to support introspection of the document structure.",
        "We demonstrated this technique by extracting sentiment-relevant sentences from movie reviews.",
        "Our evaluation shows that our model extracts more task-relevant information than the baseline methods, even when the total number of extracted sentences is very small."
    ],
    "213": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism, which uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.",
        "We introduce two multimodal extensions of SKIP-GRAM, MMSKIP-GRAM-A and MMSKIP-GRAM-B, which integrate and propagate visual information in word representations.",
        "The models can be used as input in systems benefiting from prior visual knowledge, such as caption generation, metaphor detection, or retrieving/generating pictures of abstract concepts.",
        "Their incremental nature makes them well-suited for cognitive simulations of grounded language acquisition, an avenue of research we plan to explore further."
    ],
    "220": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The proposed approach of joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The proposed TF normalization method using a partially-axiomatic approach is better than previous smoothing methods, especially for verbose type of query.",
        "JMV2 significantly improved JM for all type of queries, and DirV eliminated the limitation of Dir by providing the robustness of performances for verbose type of query, as well as improving precisions (Pr@5 or Pr@10) for keyword type of query.",
        "Passage-based retrieval could be applied to handle long-length documents, but it requires additional processes such as indexing of position information, pre-segmenting individual passages, and more importantly the additional overhead at online retrieval time.",
        "The proposed approach handles multi-topical documents in a simplified manner by investigating a more accurate TF normalization without additional cost of efficiency."
    ],
    "223": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The problem of sentence generation can be effectively achieved without the use of complex recurrent networks.",
        "Our algorithm achieves similar results on this task as state-of-the-art models, despite being simpler.",
        "Our model generates new sentences which are not generally present in the training set.",
        "Future research directions will go towards leveraging unsupervised data and more complex language models to improve sentence generation.",
        "Assessing the impact of visually grounded phrase representations into existing natural language processing systems is an interesting research direction."
    ],
    "231": [
        "We proposed a reduction technique that allows to implement a constituent parser when only a dependency parser is given.",
        "The technique is applicable to any dependency parser, regardless of its nature or kind.",
        "The reduction was accomplished by endowing dependency trees with a weak order relation, and showing that the resulting class has desirable properties."
    ],
    "232": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4)."
    ],
    "234": [
        "Our method improves the baseline system even with the same feature input, in a large-scale Chinese-English machine translation task.",
        "We use a single two-layer neural network to model the translation hypothesis, and our method performs training independently of any previous linear training methods.",
        "Our method does not require pre-training or post-training procedures, and it uses heuristics to reduce the complexity of network structures and obtain extra advantages over standard networks.",
        "The non-linear modeling in our method is important for improving the performance of a machine translation system, and integrating more features into our learning framework is a future work.",
        "Our method shows that heuristics and intuitions of the data and features are still important to a machine translation system, even with the advancement of non-linear modeling."
    ],
    "236": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Seemingly mundane representation choices can raise the performance of simple linear models to be comparable with much more sophisticated models.",
        "Achieving these results is not a matter of deep expertise about the domain or engineering skill; the choices can be automated.",
        "Our approach is a first step towards black-box NLP systems that work with raw text and do not require manual tuning."
    ],
    "240": [
        "The proposed discriminative model for unsupervised morphological segmentation seamlessly integrates orthographic and semantic properties of words.",
        "The model consistently equals or outperforms five state-of-the-art systems on Arabic, English, and Turkish.",
        "Future directions of work include using better neighborhood functions for contrastive estimation, exploring other views of the data that could be incorporated, examining better prediction schemes, and employing morphological chains in other applications in NLP."
    ],
    "244": [
        "Our proposed model for matching two short-texts outperforms existing models with large margins, as shown by empirical study.",
        "The tree-mining algorithm in our model discovers a vast amount of matching patterns, which are then used by a DNN to perform the task.",
        "Our model relies on a generic approach that can be applied to any short-text pair, making it a versatile and effective solution for tweet and response matching.",
        "The empirical study on tweet and response matching shows that our model significantly outperforms competitors, demonstrating its effectiveness in this task."
    ],
    "246": [
        "Our proposed deep convolutional architectures for matching natural language sentences can nicely combine the hierarchical modeling of individual sentences and the patterns of their matching, outperforming competitors on a variety of matching tasks.",
        "We propose a chunking policy network for machine reading comprehension that enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning, and add a recurrent mechanism to allow information to flow across segments.",
        "Our approach outperforms benchmark models across different datasets, as demonstrated by extensive experiments on three public datasets of machine reading comprehension.",
        "We first train a representation extractor with the Clustering Promotion Mechanism, which uses Similarity Entropy Minimization to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains, and combine it with the proposed Cosine Annealing Strategy.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset, as demonstrated by experimental results."
    ],
    "251": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; more general globally-normalized models can be trained in a similarly inexpensive way.",
        "Our model can significantly improve upon state-of-the-arts in sentence generation, perplexity, and n-best re-ranking for machine translation."
    ],
    "254": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our cross-lingual projection method utilizes OLLIE and GOOGLE TRANSLATE to extract relations in the language of interest, without relying on the availability of linguistic resources such as POS-taggers or dependency parsers in the target language.",
        "Our approach does not rely on the availability of linguistic resources such as POS-taggers or dependency parsers in the target language and can thus be extended to multiple languages supported by a machine translation system.",
        "We are releasing the manually annotated judgements for open relations in the three languages and the open relations extracted over the entire Wikipedia corpus in 61 languages."
    ],
    "255": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The dataset of 69,907 news produced by four major global media corporations was gathered during a minimum of eight consecutive months in 2014.",
        "The methodology used sentiment analysis to understand intrinsic aspects of headlines and infer news popularity based on a URL shortening service.",
        "The results unveil interesting findings regarding sentiment of news headlines and their use in online news.",
        "The dataset built by the authors is now available for other researchers to facilitate different research efforts in this field."
    ],
    "257": [
        "Simply training RNN or BOW NLMs on six dictionaries yields a reverse dictionary that performs comparably to the leading commercial system, even with access to much less dictionary data.",
        "The embedding models consistently return syntactically and semantically plausible responses, which are generally part of a more coherent and homogeneous set of candidates than those produced by the commercial systems.",
        "We also showed how the architecture can be easily extended to produce bilingual versions of the same model.",
        "In the analyses performed thus far, we only test the dictionary embedding approach on tasks that it was trained to accomplish (mapping definitions or descriptions to words).",
        "We have shown how these lexical resources can constitute valuable data for training the latest neural language models."
    ],
    "260": [
        "Our proposed neural discriminative sentence model based on sentence parsing structures achieves high performance in sentiment analysis and question classification tasks.",
        "The dependency tree-based convolution (d-TBCNN) variant of our model outperforms the constituency tree-based convolution (c-TBCNN) variant and previous state-of-the-art results in both tasks.",
        "The d-TBCNN variant is slightly better than c-TBCNN in our experiments, and has outperformed previous state-of-the-art results in both tasks.",
        "Tree-based convolution can capture sentences' structural information effectively, which is useful for sentence modeling.",
        "The use of dependency trees in our model allows for better visualization and understanding of the meaning of every node."
    ],
    "266": [
        "EVENTs or broad clinical concept categories (i.e., Problem, Treatment, Test) can be automatically extracted (using CRF) with comparable scores to human benchmark.",
        "negation of concepts can be automatically determined (using ConText negation tool with minor 'tailoring') with comparable accuracy to human benchmark.",
        "temporal entity identification can be automatically extracted with comparable score to human benchmarks.",
        "temporal entity normalisation is comparably challenging (even for humans).",
        "determining the value (ISO-8601) was harder than type identification.",
        "TLINK extraction is overall an open research problem.",
        "DocTimeRel or SECTIME links can be extracted with good scores (93%) while intraand inter-sentence links are notably more challenging to extract.",
        "incorporating lexical variant generation for EVENT extraction\" and \"expanding the TE normalisation component\" are potential future research directions."
    ],
    "268": [
        "The proposed approach, AdaSent, demonstrates effectiveness and robustness in short sequence modeling.",
        "AdaSent explores a new direction to represent a sequence by a multi-scale hierarchy instead of a flat, fixed-length, continuous vector representation.",
        "AdaSent can learn to represent input sequences depending on the task at hand."
    ],
    "270": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses a combination of Clustering Promotion Mechanism, Similarity Entropy Minimization, and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The proposed Cosine Annealing Strategy combines the two methods to improve domain adaptation performance.",
        "The approach uses pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "The density and kernel-based method called support vector clustering (SVC) is improved and applied to extract clusters more efficiently.",
        "SVC can be seen as an efficient cluster extraction method if clusters are separable in a 2-D map.",
        "The representation for term clustering using a mixed Jaccard-Radial base kernel is proven to be efficient with SVC for term clustering in a natural language processing task.",
        "The approach aims to investigate the extraction of clusters over more than 2 dimensions and test the robustness for non-separable data in future work."
    ],
    "271": [
        "The authors define a new state-of-the-art for the SemEval-2010 Task 8 dataset without using any costly handcrafted features.",
        "The authors propose a new CNN for classification that uses class embeddings and a new rank loss function.",
        "The authors effectively deal with artificial classes by omitting their embeddings in CR-CNN.",
        "The authors demonstrate that using only the text between target nominals is almost as effective as using WPEs.",
        "The authors extract the most representative contexts of each relation type from the CR-CNN model."
    ],
    "272": [
        "The proposed model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The model learns multiple embeddings per word type efficiently.",
        "The method jointly performs word sense discrimination and embedding learning.",
        "The number of senses per word type is non-parametrically estimated.",
        "The model learns multiple senses and achieves new state-of-the-art results in the word similarity in context task.",
        "The global vectors, sense vectors, and cluster centers of the model are available for learning.",
        "In future work, the multiple embeddings per word type will be used in downstream NLP tasks."
    ],
    "273": [
        "The embeddings of GloVe, ELMo, and BERT encode gender, religion, and nationality biases.",
        "A projection-based method can attenuate biases in the static GloVe embeddings.",
        "The approach to attenuating biases for the static GloVe embeddings can also be applied to contextualized embeddings (ELMo, BERT) by debiasing the first non-contextual layer alone.",
        "The method works for both gender and nationality directions.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The performance of the ensemble of RNNs is still less than the performance of RNN + PRA Classifier-b, indicating that multiple local optima issues affect the performance.",
        "The RNN model may not capture some of the important local structure, and using bigram features may help overcome this drawback.",
        "In future work, the author plans to explore compositional models with a longer memory and include vector representations for entities to address the issue of polysemy in verb phrases."
    ],
    "274": [
        "The beam problem in neural machine translation (NMT) can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our proposed solution to the brevity problem is a method to learn the parameters of corrections to the model, which is helpful and easy to implement.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized models for NMT.",
        "Solving the brevity problem leads to significant BLEU gains, but there remains to be gained by solving label bias in general.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset, and it is possible to train more general globally-normalized models in a similarly inexpensive way.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "By restructuring our AMRs, we are able to convert a sophisticated SBMT engine into a baseline semantic parser with little additional effort.",
        "By further restructuring our data to appropriately model the behavior we want to capture, we are able to rapidly achieve state-of-the-art results.",
        "We are able to increase quality even more by incorporating novel language models and external semantic resources."
    ],
    "275": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The Novel Visual Concept learning from Sentences (NVCS) task can be performed with a small number of images containing novel concepts, and our method performs comparably with the model retrained from scratch on all of the data if the number of novel concept images is large, and performs better when there are only a few training images of novel concepts available."
    ],
    "278": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our framework can achieve good performance and outperform state-of-the-art unsupervised systems.",
        "We propose a new MDS paradigm called reader-aware multidocument summarization (RA-MDS).",
        "We propose a sparse-coding-based method jointly considering news reports and reader comments.",
        "We propose a compression-based unified optimization framework which explores a finer syntactic unit, namely, noun/verb phrase, to generate compressive summaries, and meanwhile it conducts entity rewriting aiming at better linguistic quality."
    ],
    "279": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The proposed method outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Semantic KGs are better resources than Wikipedia anchor links for relatedness measurement.",
        "The DNN is more effective to measure entity relatedness than NGD and VSP.",
        "The DSRM based on DNN significantly outperforms NGD and VSP for both relatedness measurement and entity disambiguation."
    ],
    "280": [
        "Our findings show that a pattern-based method can be very successful at identifying pairs of scalemates, as long as the corpus is big enough.",
        "One of our contributions is the use of a wide range of similarity measures as well as an antonymy filter and a polarity filter to clean up the results.",
        "We have also proposed a new evaluation method, combining the MPQA subjectivity lexicon with the WKB arousal norms.",
        "The combination of these two data sets makes the evaluation of scale ordering methods more reliable.",
        "This alleviates, but does not eliminate the need for a true gold standard, which could finally enable us to move towards the automatic identification of adjectival scales."
    ],
    "283": [
        "a gated RNN conditioned directly on CNN activations (an MRNN) achieves better BLEU performance than an ME LM or LSTM conditioned on a set of discrete activations.",
        "the ME LM + DMSM method significantly outperforms the MRNN in terms of human quality judgments.",
        "the lack of novelty in the captions produced by the MRNN\" is partially responsible for the difference in performance between the MRNN and the ME LM + DMSM.",
        "a k-nearest neighbor retrieval algorithm introduced in this paper performs similarly to the MRNN in terms of both automatic metrics and human judgements.",
        "the MRNN system alongside the DMSM to provide additional scores in MERT reranking of the n-best produced by the imageconditioned ME LM advances the state of the art results on the COCO dataset by 1.6 BLEU points."
    ],
    "286": [
        "The proposed model, FCM, can easily handle arbitrary types of input and handle global information for composition, while remaining easy to implement.",
        "FCM alone attains near state-of-the-art performances on several relation extraction tasks.",
        "In combination with traditional feature-based loglinear models, FCM obtains state-of-the-art results.",
        "The model has the potential to contribute useful components to various tasks, such as dependency parsing, SRL, and paraphrasing.",
        "The proposed model can be applied to the TAC-KBP tasks by replacing the training objective to a multi-instance multilabel one."
    ],
    "287": [
        "The proposed fast shift-reduce RST discourse segmenter and parser achieves near state-of-the-art accuracy.",
        "The parser processes Penn Treebank documents in less than a second, which is about an order of magnitude faster than recent results reported by Feng and Hirst (2014).",
        "The proposed parser is able to process documents in less than a second, significantly improving the efficiency of discourse segmentation and parsing."
    ],
    "292": [
        "The proposed recursive convolutional neural network (RCNN) architecture can capture both syntactic and compositional-semantic representations of phrases and words.",
        "RCNN is a general architecture that can deal with k-ary parsing tree, making it suitable for many NLP tasks to minimize the effort in feature engineering with an external dependency parser.",
        "The use of RCNN for re-ranking of the dependency parser can be regarded as semantic modeling of text sequences and handle input sequences of varying length into a fixed-length vector.",
        "The parameters in RCNN can be learned jointly with other NLP tasks, such as text classification.",
        "The integrated parser, which combines RCNN with a decoding algorithm, has the potential to achieve better performance without the limitation of base parser.",
        "The model has the ability to handle other NLP tasks, and further investigation is needed for future research."
    ],
    "293": [
        "We have presented a set of improvements to our English Switchboard system that lowered the error rate substantially compared to our previous best result [13].",
        "In decreasing order of importance, these are: rescoring with strong language models trained on diverse data sources; joint training of an RNN and a CNN with 32000 outputs on 2000 hours of audio and maxout networks with annealed dropout.",
        "We expect additional accuracy gains by training the maxout nets and larger CNNs with a 512/512 filter configuration on all the data.",
        "Extrapolating from historical trends, we believe that human accuracy on this task can be reached within the next decade.",
        "We think that the way to get there will most likely involve an increase of several orders of magnitude in training data and the use of more sophisticated neural network architectures that tightly integrate multiple knowledge sources (acoustics, language, pragmatics, etc.)."
    ],
    "294": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We introduce a dataset of 3D scenes annotated with natural language descriptions, which we believe will be of great interest to the research community.",
        "Using this corpus of scenes and descriptions, we present an approach that learns from data how to ground textual descriptions to objects.",
        "Our grounding approach impacts generated scene quality, as demonstrated by human judgments of generated scenes.",
        "We demonstrate that rich lexical grounding can be learned directly from an unaligned corpus of 3D scenes and natural language descriptions.",
        "Our model can successfully ground lexical terms to concrete referents, improving scene generation over baselines adapted from prior work."
    ],
    "296": [
        "The proposed approach of combining probabilistic logic with distributional semantics can effectively represent natural language semantics, which has many important applications.",
        "The system maps natural-language sentences to logical formulas and uses them to build probabilistic logic inference problems, demonstrating state-of-the-art performance on the SICK RTE task.",
        "The approach of using distributional semantics to capture graded aspects of natural language has been shown to be effective in deep semantic understanding tasks.",
        "The use of precompiled resources and on-the-fly distributional resources in the system allows for efficient inference using Markov Logic.",
        "The proposed method outperforms state-of-the-art baseline parsers by achieving high F1 scores on standard WSJ and CTB evaluations."
    ],
    "297": [
        "The proposed fine-tuning framework can boost unsupervised word representation learning algorithms.",
        "The automatic label generation approach and inverse error weighted minibatch SGD optimization algorithm can provide additional supervised fine-tuning phases for all unsupervised word representation learning algorithms.",
        "Many experiments involving 10 datasets and 6 well-trained embeddings have demonstrated the effectiveness of the proposed framework in improving the quality of word representation."
    ],
    "299": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Performance with these models comes close to the best previous alignment-based results.",
        "When we go further, and inform a bidirectional neural network models with alignment information, we are able to make significant advances over previous methods."
    ],
    "301": [
        "The proposed CNN model outperforms state-of-the-art methods on public image QA datasets.",
        "The model relies on convolutional architectures to generate image representations and learn interactions between the image and question.",
        "The model uses consecutive words to form the question representation.",
        "The model demonstrates superiority over state-of-the-art methods in answer prediction."
    ],
    "304": [
        "A relatively simple feedforward neural network can still provide significant improvement to Statistical Machine Translation (SMT).",
        "The proposed multi-pronged approach that addresses modeling, architectural, and learning aspects of pre-existing neural network-based SMT features can improve the performance of machine translation systems.",
        "The new set of neural network-based SMT features can capture important translation phenomena and extend feedforward neural network with tensor layers.",
        "Multi-task learning can be applied to integrate the SMT features more tightly, leading to improved performance in machine translation.",
        "The proposed approaches successfully produce an improvement over state-of-the-art machine translation systems for Arabic-to-English and Chinese-to-English, and for both BOLT web forum and NIST conditions."
    ],
    "307": [
        "The proposed tensor factorization method, Tatec, achieves better performance than the best of either constituent by combining 2and 3-way interaction terms.",
        "Different data patterns can be properly encoded using different embedding spaces and a two-phase training process.",
        "The model is versatile and can be applied to various tasks and quality measures, as proven by experiments on four benchmarks.",
        "Soft regularization appears slightly more efficient than hard regularization, but with one extra hyperparameter.",
        "Tatec can be seen as a generalization of many existing works in the field."
    ],
    "309": [
        "The proposed approach achieves state-of-the-art performance on simple word similarity matching tasks.",
        "Incorporating multisense embeddings into NLP applications can introduce performance boosts.",
        "Simply increasing the dimensionality of baseline skip-gram embeddings can achieve the same performance wins as using multi-sense embeddings.",
        "The results suggest that testing embedding models in real NLP applications is important for evaluating their effectiveness.",
        "The use of multisense embeddings can improve performance on certain NLP tasks (part-of-speech tagging and semantic relation identification).",
        "Other multi-sense embedding systems may find stronger effects of multi-sense models than the proposed approach."
    ],
    "310": [
        "Compositional training leads to state-of-the-art performance on both path query answering and knowledge base completion.",
        "Regularization by augmenting the dataset with paths is a key idea in the paper.",
        "Representing sets as low-dimensional vectors in a context-sensitive way is a useful approach.",
        "Performing function composition using vectors is a promising technique for vector space models."
    ],
    "313": [
        "The proposed approach of using visual classifiers and LSTM for automatic movie description outperforms existing methods.",
        "The approach relies on three main ingredients to handle weak sentence annotations: distinguishing semantic groups of labels, training them discriminatively, and selecting only a small number of reliable classifiers.",
        "Exploring different LSTM architectures and learning configurations can benefit sentence generation.",
        "The highest performance is obtained on the MPII-MD dataset as shown by all automatic evaluation measures and extensive human evaluation.",
        "The challenges in the movie description task include presence of frequent words, sentence length and simplicity, and presence of \"visual\" verbs.",
        "Textual and visual difficulties of sentences/clips strongly correlate with the performance of all methods.",
        "There is a high bias in the data towards humans as subjects and verbs similar to \"look\".",
        "Future work should focus on dealing with less frequent words and handling less visual descriptions, which may require considering external text corpora, modalities other than video, such as audio and dialog, and looking across multiple sentences."
    ],
    "314": [
        "The proposed method for converting word vectors into sparse and optionally binary word vectors outperforms the original vectors on a suite of semantics and syntactic evaluation benchmarks.",
        "The sparse word vectors are more interpretable than the dense vectors according to a word intrusion detection test.",
        "The word2vec tool (Mikolov et al., 2013) is fast and widely-used.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets using Dynamic Memory Induction Networks (DMIN).",
        "The use of external working memory with dynamic routing in the DMIN model allows for better adaptation and generalization to support sets and unseen classes.",
        "The model achieves state-of-the-art results on the miniRCV1 and ODIC datasets, demonstrating the effectiveness of the proposed method."
    ],
    "316": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The proposed novel constituent hierarchy predictor based on recurrent neural networks captures global sentential information, outperforming the state-of-the-art baseline parser."
    ],
    "317": [
        "The proposed method, WordRank, learns word representations through a ranking-based approach that is robust to noise and has better performance when training data is sparse.",
        "WordRank achieves attention mechanism and robustness to noise naturally through its ranking loss function \u03c1(\u2022), which is not present in other ranking-based approaches.",
        "The proposed method is publicly available for general usage, and its multi-node distributed implementation can significantly boost the performance of Wor-dRank in cases where training data are sparse and noisy."
    ],
    "319": [
        "The current approach to training recurrent neural networks (RNNs) for sequence prediction tasks is prone to the accumulation of errors due to the way we use them in real-life scenarios.",
        "Our proposed curriculum learning approach can improve the performance of RNNs on sequence prediction tasks while not incurring longer training times.",
        "The curriculum learning approach involves gradually changing the training objective from an easy task to a realistic one, where the previous token is provided by the model itself.",
        "Experimental results on several sequence prediction tasks have shown performance improvements using our proposed approach.",
        "Future work includes back-propagating the errors through the sampling decisions and exploring better sampling strategies, including conditioning on some confidence measure from the model itself."
    ],
    "320": [
        "Our approach achieves new state-of-the-art performance on FewRel 2.0 dataset.",
        "The proposed Cosine Annealing Strategy combines two methods to improve domain adaptation.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The use of pseudo-labeled target-domain data and labeled source-domain data improves the domain adaption performance.",
        "Our approach shows an improvement in end-to-end recall for AMR parsing with only a small drop in precision.",
        "A clear direction of future work is improving the coverage of the defined actions, such as using a richer lemmatizer to shift the burden of lemmatizing unknown words into the AMR lemma semantics.",
        "Our decomposition provides a useful framework to guide future work in NER++ and AMR in general."
    ],
    "323": [
        "The proposed end-to-end, sequence-to-sequence approach achieves a new state-of-the-art on single-sentence execution.",
        "The model demonstrates competitive results on the more challenging multi-sentence domain despite using very small training datasets and no specialized linguistic knowledge or resources.",
        "The primary model components, such as the bidirectional LSTM-RNN and multilevel aligner, contribute to the performance of the model.",
        "The model achieves competitive results with minimal linguistic knowledge or resources.",
        "The small training datasets used in the evaluation do not hinder the performance of the model."
    ],
    "324": [
        "The proposed generative dependency parsing model outperforms previous models in terms of speed and accuracy.",
        "The model is able to accurately estimate probabilities conditioned on long context sequences.",
        "The model is scalable to large training and test sets.",
        "Decoding speed is efficient, despite defining a full probability distribution over sentences and parses.",
        "The generative model gives strong performance as a language model.",
        "The model can be successfully applied to natural language generation tasks such as machine translation in future work."
    ],
    "328": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our model can generate meaningful word representations and learn complex substructures.",
        "The ability of our model to learn complex substructures makes it possible to aggregate multiple words within a low-dimensional vector space.",
        "Inference of new phrase representations is also easily feasible when relying on counts.",
        "Both alternatives can give different but meaningful information about phrases."
    ],
    "332": [
        "The DMN model is a potentially general architecture for a variety of NLP applications, including classification, question answering, and sequence modeling.",
        "A single architecture is a first step towards a single joint model for multiple NLP problems.",
        "The DMN is trained end-to-end with one, albeit complex, objective function.",
        "Future work will explore additional tasks, larger multi-task models, and multimodal inputs and questions."
    ],
    "333": [
        "The proposed convolutional neural network (CNN) model is effective in learning robust relation representations from shortest dependency paths for relation extraction.",
        "The simple negative sampling method proposed by the authors helps make correct assignments for subjects and objects within a relationship.",
        "The model significantly outperforms state-of-the-art systems, indicating the effectiveness of the proposed approach.",
        "The treatment of dependency paths can well capture the syntactic features for relation extraction."
    ],
    "334": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "In this paper, we propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We have performed extensive experiments on three public datasets of machine reading comprehension: CoQA, QuAC, and TriviaQA. Our approach outperforms benchmark models across different datasets.",
        "Using side by side funniness assessments from AMT, we found that the methods that consistently select funnier captions are negative sentiment, human-centeredness, and lexical centrality.",
        "Captions that relate to people were consistently deemed funnier. The first two methods (negative sentiment and human-centeredness) are consistent with the findings in Mihalcea and Pullman [9] .",
        "We are making our corpus public for research and for a shared task on funniness detection. The corpus includes our 50 selected cartoons, more than 5,000 captions per cartoon, manual annotations of the entities in the cartoons, automatically extracted topics from each contest, and the funniness scores."
    ],
    "336": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach outperforms benchmark models across different datasets.",
        "The availability of a dataset of this size opens up several interesting possibilities for research into dialogue systems based on rich neural-network architectures.",
        "We obtain significantly better results with the LSTM architecture.",
        "There are several interesting directions for future work."
    ],
    "341": [
        "The proposed question similarity model can extract effective features from word alignment between two questions.",
        "The bootstrap-based feature extraction method can extract a small set of effective lexical features.",
        "The learning-to-rank algorithm can be used to train the model for both the FAQ-based QA task and the answer sentence selection task.",
        "The model works well for both tasks, as demonstrated by extensive experiments.",
        "The proposed method can significantly improve the efficiency of the model while maintaining comparable performance."
    ],
    "344": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The ConMask model has some limitations and room for improvement, such as the nature of the relationship-dependent content masking that can lead to incorrect predictions.",
        "The ConMask model can capture the correct relationships even when the name of the entity does not appear in the description.",
        "The ConMask model has the ability to incorporate prior knowledge into a statistical framework, which is an advantage over other approaches.",
        "The proposed approach (RPN) has competitive performance and is more stable than many major statistical approaches.",
        "The performance of RPN can be influenced by how reliable the SLU's confidence scores are.",
        "Future work will investigate the influence of SLUs on the performance of RPN, and rich features for RPN.",
        "Future work will also address applying RPN to other domains, such as the bus timetables domain in DSTC-1, and theoretic analysis of RPN."
    ],
    "346": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset."
    ],
    "358": [
        "The use of character-based representations is useful for NLP tasks, particularly for transition-based dependency parsing.",
        "These representations can capture morphological information crucial for analyzing syntax, and they provide strong improvements for agglutinative languages such as Basque, Hungarian, Korean, and Turkish.",
        "The use of character-based representations can help overcome the out-of-vocabulary problem, enabling the parser to substantially improve performance when OOV rates are high.",
        "These representations can be used in conjunction with a pretraining regime or distributional word embeddings to realize further improvements."
    ],
    "359": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improves the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The RNN-based approach can deal with long-distance patterns and is particular suitable for learning relations within a long context.",
        "The proposed modifications, including a max-pooling feature aggregation, a position indicator approach to specify target nominals, and a bi-directional architecture to learn both the forward and backward contexts, improved the basic model.",
        "The RNN model exhibits clear advantages for sentences with long-distance relations."
    ],
    "368": [
        "Training increasingly accurate image classifiers does not lead to better captions.",
        "Many of the apparently highly-specific generated captions output by models like NIC are likely due to language models capturing coarse-grained information and generating corresponding plausible natural language sequences.",
        "The success of models that generate language based on discretized image representations demonstrates that algorithms are capable of state-of-the-art performance without consideration of rich, real-valued vector features.",
        "It's likely that these types of models are less prone to overfitting."
    ],
    "369": [
        "Our approach achieves new state-of-the-art performance on FewRel 2.0 dataset.",
        "The proposed Cosine Annealing Strategy combines two methods to improve domain adaptation.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The use of pseudo labels improves the domain adaption performance.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results suggest that the community is ready for more difficult CQA datasets."
    ],
    "373": [
        "GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone.",
        "For the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Our primary contribution is approximation-aware training for structured BP.",
        "While our experiments only consider dependency parsing, our approach is applicable for any constraint factor which amounts to running the inside-outside algorithm on a hypergraph.",
        "Our training methods could be applied to such tasks as well."
    ],
    "375": [
        "Regularization methods (except reembedding words) basically help generalization.",
        "Penalizing 2-norm of embeddings unexpectedly helps optimization as well.",
        "Regularization performance depends largely on the dataset's size."
    ],
    "377": [
        "\"ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "\"The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "\"Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "\"Continuous online learning using backpropagation leads to a more informative representation of the language model, achieving lower perplexity with a more optimal use of parameters.",
        "\"The idea of continuous training and adaptation is natural and also established in biological learning processes, yet it is not widely used due to computational complexity.",
        "\"By including this active learning component in the neural network model, the system is able to achieve higher accuracy, while also decreasing the parameters needed to store the model and decreasing the computation required."
    ],
    "379": [
        "The proposed attentional mechanisms for neural machine translation achieve large gains in BLEU scores.",
        "The global attention mechanism always looks at all source positions, while the local attention mechanism only attends to a subset of source positions at a time.",
        "The ensemble model outperforms existing best systems for English-German translation by more than 1.0 BLEU.",
        "Attention-based NMT models are superior to non-attentional ones in many cases, such as translating names and handling long sentences.",
        "The analysis shows that different alignment functions are best for different attentional models."
    ],
    "380": [
        "We proposed a feature mapping operator for convolutional neural networks by modeling n-gram interactions based on tensor product and evaluating all non-consecutive n-gram vectors.",
        "The associated parameters are maintained as a low-rank tensor, which leads to efficient feature extraction via dynamic programming.",
        "The model achieves top performance on standard sentiment classification and document categorization tasks."
    ],
    "381": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification, which builds on external working memory with dynamic routing, leveraging the latter to track previous learning experience and the former to adapt and generalize better to support sets and hence to unseen classes.",
        "The ensemble methods have the added advantage of increasing vocabulary coverage.",
        "We will release the meta-embeddings."
    ],
    "382": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The resulting approach is significantly simpler than the dominating HMM-DNN one, with fewer training stages, much less auxiliary data and less domain expertise involved.",
        "Our system shows decent, although not yet state-of-the-art performance.",
        "We present two methods to improve the computational complexity of the investigated model.",
        "Unlike CTC networks, our model has an intrinsic language modeling capability.",
        "Investigations in this direction are likely to be a part of our future work."
    ],
    "383": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The use of pseudo labels improves the domain adaptation performance.",
        "The approach is limited by the insufficiency of high-quality annotated data.",
        "The proposed method can be improved by developing annotated datasets from multiple sources and combining it with distant supervision and reinforcement learning.",
        "NLI is an ideal testing ground for theories of semantic representation, and training for NLI tasks can provide rich domain-general semantic representations.",
        "Existing NLI resources are limited, and the new corpus presented in this paper provides a valuable opportunity for evaluating and improving models.",
        "Simple lexicalized models and neural network models perform well on the new corpus, and the representations learned by a neural network model can be used to improve performance on a standard challenge dataset."
    ],
    "384": [
        "Our approach outperforms benchmark models across different datasets.",
        "We have performed extensive experiments on three public datasets of machine reading comprehension: CoQA, QuAC, and TriviaQA.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "Our data analysis shows that persuasive sentences are generally euphonic.",
        "Phonetic features consistently help in the detection of persuasiveness.",
        "When combined with lexical features, they help improving classification performance on three of the four datasets that we considered.",
        "The key role played by phonetic features is further underlined by the cross-dataset experiments, in which we observed that phonetic features alone generally outperform the lexical features.",
        "Our results should encourage researchers dealing with different aspects of persuasiveness to consider the inclusion of phonetic attributes in their models.",
        "As future work, we will investigate the impact of other phonetic devices such as assonance, consonance and rhythm on persuasiveness.",
        "It would also be interesting to focus on the connection between sound symbolism and persuasiveness, and investigate how the context or domain of persuasive statements interacts with the sounds in those statements."
    ],
    "386": [
        "Our model outperforms baseline models that utilize word/morpheme embeddings in the input layer, despite having fewer parameters.",
        "The model is able to encode rich semantic and orthographic features from characters only.",
        "Using CharCNN and highway layers for representation learning remains an avenue for future work.",
        "Sequential processing of words as inputs is ubiquitous in natural language processing, and the architecture introduced in this paper is viable for other tasks such as neural machine translation."
    ],
    "389": [
        "The short-time fan-chirp transform (STFChT) is a new transform domain for speech enhancement that demonstrates advantages over traditional transform domains.",
        "The STFChT allows for extension of analysis window duration, which concentrates more direct-path signal into time-frequency bins and enables superior enhancement results in terms of objective metrics like PESQ and SRMR.",
        "Despite better objective enhancement scores, long-window (128 ms) STFT processing yielded the lowest WERs, indicating that the STFChT may be useful for speech enhancement.",
        "The utility of the STFChT warrants further investigation, and interesting future directions include moving beyond linear models of instantaneous frequency and combining the STFChT with other coherence-extending transforms and deep neural network (DNN) enhancement and recognition methods."
    ],
    "390": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR), with performance not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "We have presented a neural attention-based model for abstractive summarization, based on recent developments in neural machine translation.",
        "As a next step we would like to further improve the grammaticality of the summaries in a data-driven way, as well as scale this system to generate paragraph-level summaries.",
        "Both pose additional challenges in terms of efficient alignment and consistency in generation."
    ],
    "391": [
        "Our approach outperforms benchmark models across different datasets.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our model employs a bidirectional LSTM-RNN model with a novel coarse-to-fine aligner that jointly learns content selection and surface realization.",
        "We achieve state-of-the-art selection and generation results on the benchmark WEATHERGOV dataset.",
        "Our model generalizes to a different, data-starved domain (ROBOCUP), where it achieves results competitive with or better than the state-of-the-art.",
        "We also consider different ways of using pretrained word embeddings to bootstrap the quality of our learned embeddings."
    ],
    "393": [
        "a simple reweighting approach offers robust advantages in lexicon-based sentiment analysis",
        "a recursive neural network can substantially outperform a bag-of-words classifier",
        "documentlevel structure in sentiment analysis is now easy to include with off-the-shelf RST discourse parsers",
        "future work will focus on combining models of discourse structure with richer models at the sentence level"
    ],
    "399": [
        "We inferred semantic classes from a large syntactic classification of German AO-selecting verbs based on findings from formal semantics about correspondences between verb syntax and meaning.",
        "Our thorough evaluation and analysis yields detailed insights into the semantic characteristics of the inferred classes.",
        "We hope that this allows an informed use of the resulting resource in various semantic NLP tasks."
    ],
    "402": [
        "The results show that when our method is applied without restrictions, it leads to minor improvements.",
        "When applied only to rare words, the splitter produces statistically significant improvements in both BLEU and METEOR over the best frequency-based compound splitter.",
        "The output of the analogy-based compound splitter is more beneficial to the machine translation system than the baseline splitter.",
        "Our novel compound splitter is particularly adept at splitting highly ambiguous compounds.",
        "Our method outperforms a commonly used compound splitter on a gold standard task.",
        "The analogy-based compound splitter compares favorably to the commonly used shallow frequency-based method in a machine translation task."
    ],
    "405": [
        "We propose TransA, a translation-based knowledge graph embedding method with an adaptive and flexible metric.",
        "Our adaptive metric approach could effectively model various and complex entities/relations in knowledge base.",
        "Experiments are conducted with two benchmark tasks and the results show TransA achieves consistent and significant improvements over the current state-of-the-art baselines.",
        "Our approach applies elliptical equipotential hypersurfaces to characterize the embedding topologies and weights several specific feature dimensions for a relation to avoid much noise."
    ],
    "413": [
        "The proposed model, SentiCap, uses a specialized word-level supervision scheme to effectively make use of a small amount of training data with sentiments.",
        "The model is designed to generate sentimental yet descriptive captions using a crowd-sourced caption re-writing task.",
        "The proposed model demonstrates effectiveness in generating emotional captions for over 90% of the images, with the majority of the generated captions rated as having the appropriate sentiment by crowd workers.",
        "Future work can include unifying the model for positive and negative sentiment, developing models for linguistic styles beyond the word level, and designing generative models for a richer set of emotions such as pride, shame, anger."
    ],
    "415": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Using a feedforward neural network to map initial embeddings to the task-specific embedding space can significantly improve dependency parsing accuracy across several domains, as well as improvements on a downstream task.",
        "This method is simple, effective, and applicable to many other settings, both inside and outside NLP."
    ],
    "419": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We propose Bridge Correlational Neural Networks which can learn common representations for multiple views even when parallel data is available only between these views and a pivot view.",
        "Our method performs better than the existing state of the art approaches on the cross language classification task and gives very promising results on the cross modal access task.",
        "We release a new multilingual image caption benchmark (MIC benchmark) which will help in further research in this field."
    ],
    "423": [
        "We found that the majority of the gains were related to improvements in the accuracy of transfer of correct grammatical structure to the target sentence.",
        "The most prominent gains were related to errors regarding reordering of phrases, insertion/deletion of copulas, coordinate structures, and verb agreement.",
        "Accuracy gains scaled approximately log-linearly with the size of the n-best list.",
        "In most cases, accuracy gains were not saturated even after examining 1000 unique hypotheses."
    ],
    "433": [
        "using less than 1% (1000 sentences) of the training data\" can achieve competitive performance compared to previous systems.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Using both text and audio sources as inputs can complement each other and help the models achieve better scores in the question detection task.",
        "Attention mechanism (c 2 ) helps the models that receive long audio sequences as inputs.",
        "Regularization methods can help the models to generalize better, but we need to be more careful when using these regularization methods with multimodal inputs."
    ],
    "439": [
        "The USFD system achieved state-of-the-art Twitter NER performance at the 14th Conference of the European Chapter of the Association for Computational Linguistics.",
        "The system uses unsupervised feature generation through Freebase gazetteers to achieve high performance.",
        "The system weights input data according to its origin date to account for drift and improve performance.",
        "The system's performance is due to the use of weighted input data and Freebase gazetteers.",
        "The system's approach to handling drift in the data is effective in improving performance."
    ],
    "440": [
        "The proposed reasoning module can generate textual descriptions from images using an intermediate semantic representation called Scene Description Graph (SDG).",
        "The SDG integrates direct visual knowledge with background commonsense knowledge, and has a structure similar to semantic representations of sentences.",
        "The SDG allows for reasoning and question/answering about the scene, and has great potential for various applications.",
        "The proposed method can automatically create sentences describing the scene from noisy labels using the prediction system.",
        "The sentences generated by the method perform well in terms of relevance and thoroughness, as shown by AMT evaluations on popular datasets.",
        "The output SDGs can detect events and entities with comparable accuracy as a state-of-the-art system, as shown by a Gold-Standard based evaluation.",
        "The Image Retrieval experiment shows that the Image-Sentence alignment quality is comparable with state-of-the-art results."
    ],
    "442": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The conditionally generated reviews achieve significantly lower mean perplexity than those achieved with standard RNN language models.",
        "The GCN can accurately identify authors.",
        "The model does a good job of predicting items ratings and categories, nearly matching the performance of purely discriminative models.",
        "The classification accuracy of the generative model provides a straightforward way to determine what the model knows.",
        "The GCN learns nonlinear dynamics of negation, and appears to respond intelligently to a large vocabulary despite lacking any a priori notion of words.",
        "The capability of the generative model to perform classification is intriguing, but more work needs to be done to tune this approach.",
        "Inference becomes slow as the number of classes becomes large, and running each review through the network roughly 1k times to obtain a likelihood of the review separately conditioned upon each of the roughly 1k authors may not be scalable."
    ],
    "449": [
        "The embeddings of GloVe, ELMo, and BERT encode gender, religion, and nationality biases.",
        "A projection-based method can be used to attenuate these biases.",
        "The method works for static GloVe embeddings and can effectively attenuate bias in contextualized embeddings without loss of entailment accuracy.",
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our model using partial conditioning with a blocked transducer naturally alleviates the problem of \"losing attention\" suffered by sequence-to-sequence models.",
        "Increasing the block size so that it is as large as the input utterance makes the model similar to vanilla end-to-end models.",
        "The model can produce results comparable to the state of the art from sequence-to-sequence models."
    ],
    "450": [
        "We introduce a new setting for learning an unbounded number of facts in images, which could be referred to as a model for gaining visual knowledge.",
        "The facts could be of different types like objects, attributes, actions, and interactions.",
        "We consider Uniformity, Generalization, Scalability, Bi-directionality, and Structure when studying this task.",
        "Our proposed method outperforms several baselines from multi-view learning literature, mainly due to the advantage of relating facts by structure."
    ],
    "453": [
        "The proposed Spatial Memory Network (SMem) for VQA improves the results over previously published models on challenging datasets like DAQUAR and VQA.",
        "The SMem model learns inference rules based on spatial attention, as demonstrated by visualizing the attention weights.",
        "The model can be used to visualize the inference steps learned by the deep network, providing insight into its processing.",
        "Future work may include exploring the inference ability of the SMem model and comparing it with other VQA attention models.",
        "The proposed Spatial Memory Network (SMem) for VQA improves the results over previously published models on challenging datasets like DAQUAR and VQA. [Paragraph 3, Page 2]",
        "The SMem model learns inference rules based on spatial attention, as demonstrated by visualizing the attention weights. [Paragraph 4, Page 2]",
        "The model can be used to visualize the inference steps learned by the deep network, providing insight into its processing. [Paragraph 5, Page 2]",
        "Future work may include exploring the inference ability of the SMem model and comparing it with other VQA attention models. [Paragraph 6, Page 3]"
    ],
    "454": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "Our model's ability to integrate new vocabulary into generated image and video descriptions by effectively using existing vision datasets and unpaired text data.",
        "DCC improves upon current deep caption models by providing rich descriptions which are not limited by the availability of paired image-sentence corpora."
    ],
    "458": [
        "We propose a novel approach to combine CNNs, RNNs, and their hybrid models for integrating the log-linear model into NNs.",
        "Our experimental results demonstrate that simple majority voting between CNNs, RNNs, and their corresponding hybrid models is the best combination method.",
        "We achieve state-of-the-art performance for both relation extraction and relation classification.",
        "In the future, we plan to further evaluate the proposed methods on other tasks such as event extraction and slot filling in the KBP evaluation."
    ],
    "459": [
        "The proposed image-text embedding method achieves state-of-the-art retrieval results on Flickr30K and MSCOCO datasets, and demonstrates significant improvements over CCA in phrase localization on the Flickr30K Entities dataset.",
        "The architecture of the proposed system is simple and flexible, and can be applied to various kinds of visual and textual features.",
        "The objective function consisting of bi-directional ranking terms and structure-preserving terms inspired by metric learning is well-justified and effective in training the two-branch network with multiple layers.",
        "The method accurately locates the hat and jacket in the last example, and gives much tighter boxes for the horse and clown in the second example.",
        "The proposed method significantly improves the efficiency of image-text retrieval by considering the structure-preserving terms inspired by metric learning."
    ],
    "461": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "We hope to investigate end-to-end approaches, or develop alternative approaches that generate titles more similar to how humans write titles.",
        "Our model can effectively impute missing words and generate coherent and diverse sentences through purely continuous sampling.",
        "Our latent space learned by our model is able to provide interpretable homotopies that smoothly interpolate between sentences."
    ],
    "463": [
        "Our approach significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "By jointly embedding English and Spanish corpora along with a KB, we can train an accurate Spanish relation extraction model using no direct annotation for relations in the Spanish data.",
        "This approach has the added benefit of providing significant accuracy improvements for the English model, outperforming the top system on the 2013 TAC KBC slot filling task, without using the hand-coded rules or additional annotations of alternative systems.",
        "By using deep sentence encoders, we can perform prediction for arbitrary input text and for entities unseen in training.",
        "Sentence encoders also provide opportunities to improve cross-lingual transfer learning by sharing word embeddings across languages.",
        "In future work, we will apply this model to many more languages and domains besides newswire text.",
        "We would also like to avoid the entity detection problem by using a deep architecture to both identify entity mentions and identify relations between them."
    ],
    "464": [
        "Providing training examples with dummy source context can be successful to some extent, but using back-translation of monolingual target data into the source language and treating this synthetic data as additional training data can achieve substantial gains in all tasks and new SOTA results.",
        "Using small amounts of indomain monolingual data, back-translated into the source language, can be effectively used for domain adaptation.",
        "The effectiveness of using monolingual data for training is due to domain adaptation effects, a reduction of overfitting, and improved fluency.",
        "Our approach can be easily applied to other NMT systems, and the effectiveness of our approach not only varies with the quality of the MT system used for back-translation but also depends on the amount (and similarity to the test set) of available parallel and monolingual data, and the extent of overfitting of the baseline model.",
        "Future work will explore the effectiveness of our approach in more settings."
    ],
    "469": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The use of a representation extractor with the Clustering Promotion Mechanism improves the performance of the few-shot classifier.",
        "The proposed Cosine Annealing Strategy combines two methods to improve the domain adaption performance.",
        "The approach achieves state-of-the-art performance on FewRel 2.0 dataset, but there is room for improvement in reducing the noise of pseudo labels to further improve the domain adaption performance."
    ],
    "475": [
        "Our neural network model achieves state-of-the-art results in named entity recognition with little feature engineering.",
        "Our model improves over previous best reported results on two major datasets for NER, suggesting that the model is capable of learning complex relationships from large amounts of data.",
        "Preliminary evaluation of our partial matching lexicon algorithm suggests that performance could be further improved through more flexible application of existing lexicons.",
        "Evaluation of existing word embeddings suggests that the domain of training data is as important as the training algorithm.",
        "More effective construction and application of lexicons and word embeddings are areas that require more research.",
        "Our model has the potential to be extended to perform similar tasks such as extended tagset NER and entity linking."
    ],
    "476": [
        "The proposed method of using hashtags to cluster documents and \"synthesize\" reference summaries is effective in collecting large-scale news multi-document summaries with reference to social media reactions.",
        "The use of ROUGE metrics and an ILP solution to measure the coverage ratio of the collected reference summaries is effective in discovering the upper bound of the coverage ratio.",
        "The manual evaluation of the informativeness and readability of the collected reference summaries verifies their effectiveness.",
        "Training a SVR summarizer on DUC generic multi-document summarization benchmarks with the collected data as extra training resource improves the performance of the summarizer significantly on all test sets.",
        "The dataset collected in this work can be used in many other scenarios, such as learning sentence compression or updating summarization by tracking the same hashtag published on different dates."
    ],
    "479": [
        "We propose a novel task of analyzing small pieces of text containing expressions of desires to identify if the desires were fulfilled in the given text.",
        "We adopt three approaches based on different assumptions to solve this problem.",
        "Our first approach uses a textual entailment model to analyze small fragments of texts independently, and our second approach assumes that it is not sufficient to analyze different pieces of text independently; instead, the complete text should be analyzed as a whole to identify desire fulfillment.",
        "Our third approach is based on the hypothesis that identifying desire fulfillment requires an understanding of the narrative structure and models the same using latent variables.",
        "Our experiments establish the need to incorporate the narrative structure of the storyline offered by the text to better understand desire fulfillment."
    ],
    "481": [
        "Our framework for automatically inferring interpersonal cooperation and conflict in narrative summaries can potentially apply to other domains of texts with social narratives, such as news stories.",
        "Our clustering framework provides a natural approach for domain adaptation.",
        "In the future, our framework could be extended to handle nuanced relation categorizations and asymmetric relationships.",
        "Conceptually, a natural extension would be to use predictions about character relations to infer subtle character attributes such as agenda, intentions, and goals."
    ],
    "485": [
        "We extend TD-LSTM by adding a target connection component to capture the semantic relatedness between target and context words.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Incorporating target information could boost the performance of a long short-term memory model.",
        "The target-dependent LSTM model obtains state-of-the-art classification accuracy."
    ],
    "487": [
        "The proposed locally adaptive translation method, called TransA, effectively adapts the representation of entities and relations in a knowledge graph.",
        "Choosing an appropriate margin in the margin-based loss function is necessary for effective knowledge embedding.",
        "The proposed method achieves experimental results that validate its effectiveness.",
        "The optimal margin for the entity and relation aspects can be integrated into the commonly used loss function for knowledge embedding."
    ],
    "490": [
        "We have presented a framework for minimum risk training in end-to-end neural machine translation.",
        "The basic idea is to minimize the expected loss in terms of evaluation metrics on the training data.",
        "Experiments show that MRT leads to significant improvements over maximum likelihood estimation for neural machine translation, especially for distantly-related languages such as Chinese and English.",
        "In the future, we plan to test our approach on more language pairs and more end-to-end neural MT systems.",
        "It is also interesting to extend minimum risk training to minimum risk annealing following Smith and Eisner (2006) .",
        "Our approach is transparent to loss functions and architectures, we believe that it will also benefit more end-to-end neural architectures for other NLP tasks."
    ],
    "491": [
        "Our results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Creating high-performing recognizers for two very different languages, English and Mandarin, required essentially no expert knowledge of the languages.",
        "We believe our results confirm and exemplify the value of end-to-end Deep Learning methods for speech recognition in several settings.",
        "In those cases where our system is not already comparable to humans, the difference has fallen rapidly, largely because of application-agnostic Deep Learning techniques.",
        "We believe these techniques will continue to scale, and thus conclude that the vision of a single speech system that outperforms humans in most scenarios is imminently achievable."
    ],
    "492": [
        "The MovieQA data set, which contains multiple sources of information (video clips, subtitles, scripts, plots, and DVS), is unique in evaluating automatic story comprehension.",
        "The authors provided several intelligent baselines and extended existing QA techniques to analyze the difficulty of their task.",
        "The authors have an online evaluation server for their benchmark at <http://movieqa.cs.toronto.edu>.",
        "The dataset contains video clips, subtitles, scripts, plots, and DVS as sources of information.",
        "The task of automatic story comprehension is challenging, as it requires analyzing multiple sources of information.",
        "Existing QA techniques can be extended to analyze the difficulty of the task."
    ],
    "496": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We plan to further validate the effectiveness of our approach on more language pairs in the future."
    ],
    "499": [
        "The beam problem can largely be explained by the brevity problem.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "We have constructed lexicons for 11 different languages using our graph-based semi-supervised method.",
        "The lexicons thus constructed help improve performance in morphological tagging and dependency parsing, when used as features."
    ],
    "501": [
        "The proposed method using gated convolutions can effectively map questions to their semantic representations, as demonstrated by the averaged values across all questions in the dev and test set.",
        "The model employing pre-training within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus achieves success in question retrieval in community QA forums.",
        "The use of gated convolutions enables the model to glean key pieces of information from lengthy, detail-riddled user questions.",
        "The pre-training within an encoder-decoder framework is integral to the model's success in question retrieval."
    ],
    "503": [
        "Our model obtains state-of-the-art results and performs at par or better than existing inflection generation models on seven different datasets.",
        "Our model is able to learn long-range dependencies within character sequences for inflection generation, which makes it specially suitable for morphologically rich languages.",
        "Our approach is able to perform at par or better than existing inflection generation models on seven different datasets.",
        "Our model is able to learn long-range dependencies within character sequences for inflection generation, which makes it well-suited for morphologically rich languages."
    ],
    "504": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improves the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Our case study demonstrates that the asynchronous B/F LM can generate sentences that contain the given word and are comparable to sequential LM in quality."
    ],
    "506": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed TBCNN-pair model for natural language inference captures sentence-level semantics using tree-based convolutional neural networks (TBCNN) and combines two sentences' information through several heuristics, leading to high performance with low complexity order."
    ],
    "508": [
        "The proposed multi-turn dialogue generation model, ReCoSa, significantly outperforms existing HRED models and their attention variants.",
        "The self-attention mechanism is effective in capturing long distant dependency relations, improving the quality of multiturn dialogue generation.",
        "The relevant contexts detected by the proposed model are coherent with human judgments.",
        "Proper detection methods, such as self-attention, can improve the quality of multiturn dialogue generation.",
        "Introducing topical information or considering detailed content information in the relevant contexts can further improve the quality of generated responses."
    ],
    "509": [
        "The use of both word sequence and dependency tree structures is effective for end-to-end relation extraction.",
        "Training with shared parameters improves relation extraction accuracy, especially when combined with entity pretraining, scheduled sampling, and label embeddings.",
        "The shortest path, which has been widely used in relation classification, is also appropriate for representing tree structures in neural LSTM models."
    ],
    "512": [
        "The proposed multi-way, multilingual attention-based neural machine translation model outperforms previous works on multilingual neural translation, such as (Luong et al., 2015a) and (Dong et al., 2015).",
        "The proposed model is effective in translating low-resource language pairs, especially when translating to English.",
        "The improvement in translation quality is due to the better parameter estimation of the English decoder, which is attributed to the higher availability of English in most parallel corpora.",
        "The proposed multilingual neural translation model can be further improved by trying other techniques, such as ensembling and large vocabulary tricks, together with the proposed model.",
        "The proposed model can be used to translate between a language pair not included in a set of training corpus, which is an interesting future work."
    ],
    "513": [
        "The attentional model of translation does not capture many well-known properties of traditional word-based translation models.",
        "The authors have proposed several ways of imposing structural biases on the model to improve its performance.",
        "The proposed methods have shown improvements in perplexity and re-ranking evaluations on several challenging language pairs in a low-resource setting.",
        "The authors intend to investigate the model performance on larger datasets in future work.",
        "The authors plan to incorporate further linguistic information such as morphological representations into the model."
    ],
    "514": [
        "RMN outperforms LSTMs in terms of perplexity on three large datasets.",
        "Our RMN learns important co-occurrences regardless of their distance.",
        "Our RMN implicitly captures certain dependency types that are important for word prediction, despite being trained without any syntactic information.",
        "RMNs obtain excellent performance at modeling sentence coherence, setting a new state of the art on the challenging sentence completion task."
    ],
    "517": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.",
        "Our encoder-labeler LSTM approach in joint training should be worth investigating.",
        "Previous papers reported that jointly training the models for slot filling and intent classification boosted the accuracy of both tasks."
    ],
    "518": [
        "The dynamic neural module network (DNNM) learns to assemble neural networks on the fly from an inventory of neural models, and simultaneously learns weights for these modules so that they can be composed into novel structures.",
        "The DNNM achieves state-of-the-art results on two tasks, which is attributed to two factors: continuous representations improve the expressiveness and learnability of semantic parsers, and semantic structure prediction improves generalization in deep networks.",
        "The DNNM can extend compositional question-answering machinery to complex, continuous world representations like images.",
        "The use of dynamic neural networks improves the speed and sample efficiency of the model, even with very little training data.",
        "The observations made in this work are not limited to the question-answering domain, and can be applied similarly to tasks like instruction following, game playing, and language generation."
    ],
    "524": [
        "The proposed approach of training a system on speech transcripts is effective in generating new speeches.",
        "The use of n-grams and J&K POS tag filter as language and topic models for this task is effective.",
        "The system produces good results, with good grammatical correctness and sentence transitions.",
        "There are no comparable systems that allow for a direct comparison.",
        "The approach can be modified to summarize texts about the same topic from different sources."
    ],
    "526": [
        "\"Our proposed deep recurrent neural networks (DRNNs) improve the performance of relation classification.",
        "\"High-level layers in DRNNs are more capable of integrating information relevant to target relations.",
        "\"Our data augmentation strategy leveraging the directionality of relations improves the performance of our model.",
        "\"The performance of our model generally improves with increasing depth, reaching the highest F1-measure of 86.1% with a depth of 4."
    ],
    "530": [],
    "531": [
        "We present a simple non-parametric model for clustering short documents (such as tweets) into storylines, which are conceptually coherent and temporally focused.",
        "Future work may consider learning more flexible temporal distance functions, which could potentially represent temporal periodicity or parametric models of content popularity."
    ],
    "532": [
        "Our approach can alleviate the serious over-translation and under-translation problems suffered by traditional attention-based NMT.",
        "By encouraging NMT to pay less attention to translated words and more attention to untranslated words, our approach improves translation quality and alignment quality over NMT without coverage.",
        "We propose two variants of coverage models: linguistic coverage that leverages more linguistic information and NN-based coverage that resorts to the flexibility of neural network approximation .",
        "Experimental results show that both variants achieve significant improvements in terms of translation quality and alignment quality over NMT without coverage."
    ],
    "533": [
        "We developed a novel theory for signed normalized cuts as well as an algorithm for finding the discrete solution.",
        "Our algorithm greatly outperforms simple normalized cuts, even with Huang's word embeddings, which are designed to capture semantic relations.",
        "By accounting for antonym relationships, our algorithm can find superior synonym clusters that do not require new word embeddings.",
        "Our clustering method can be applied to a broad range of NLP tasks, such as prediction of social group clustering, identification of personal versus non-personal verbs, and analysis of clusters which capture positive, negative, and objective emotional content.",
        "Our signed clustering could be extended to evolutionary signed clustering."
    ],
    "536": [
        "We described a grammar method to generate paraphrases for questions.",
        "Our method is rather generic and can be applied to any question answering system.",
        "Using paraphrases for a question answering system is a useful way to improve its performance.",
        "We showed that using paraphrases for a question answering system is a useful way to improve its performance."
    ],
    "537": [
        "Our approach outperforms benchmark models across different datasets.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Our chunking policy network for machine reading comprehension enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "The discrimination between UL and GN is given by a skillful mixture of all the prototypical features together, where none has a clear predominance over the others.",
        "Readability (READ) and affect (AFF) play a major role in the discrimination between UL and FT."
    ],
    "542": [
        "We presented MALOPA, a single parser trained on a multilingual set of treebanks.",
        "Our parser, equipped with language embeddings and fine-grained POS embeddings, on average outperforms monolingually-trained parsers for target languages with a treebank.",
        "This pattern of results is quite encouraging.",
        "Although languages may share underlying syntactic properties, individual parsing models must behave quite differently, and our model allows this while sharing parameters across languages.",
        "The value of this sharing is more pronounced in scenarios where the target language's training treebank is small or non-existent, where our parser outperforms previous cross-lingual multisource model transfer methods."
    ],
    "543": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy."
    ],
    "546": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL exceeded the human baseline for FigureQA, but results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Charts in the wild may cause the dynamic encoding method used by PReFIL to fail, and future CQA datasets should include human-generated question-answer pairs.",
        "Human-generated questions are not captured well by templates, and document-level CQA is necessary for understanding charts in documents."
    ],
    "547": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The region embedding of one-hot LSTM rivaled or outperformed that of the state-of-the-art one-hot CNN, proving its effectiveness.",
        "The models with either one of these two types of region embedding strongly outperformed other methods including previous LSTM.",
        "Combining the two types of region embedding trained on unlabeled data, suggesting that their strengths are complementary.",
        "Substantial improvements over the previous best results on benchmark datasets.",
        "On this task, embeddings of text regions, which can convey higher-level concepts, are more useful than embeddings of single words in isolation.",
        "Useful region embeddings can be learned by working with one-hot vectors directly, either on labeled data or unlabeled data.",
        "A promising future direction might be to seek, under this framework, new region embedding methods with complementary benefits."
    ],
    "550": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Advances in deep learning algorithms, software and hardware mean that many architectures and objectives for learning distributed sentence representations from unlabelled data are now available to NLP researchers.",
        "We proposed two new objectives, FastSent and Sequential Denoising Autoencoders, which perform particularly well on specific tasks (MSRP and SICK sentence relatedness respectively).",
        "The optimal approach depends critically on whether representations will be applied in supervised or unsupervised settings - in the latter case, fast, shallow BOW models can still achieve the best performance.",
        "While we have focused on models using naturally-occurring training data, in future work we will also consider supervised architectures (including convolutional, recursive and character-level models), potentially training them on multiple supervised tasks as an alternative way to induce the 'general knowledge' needed to give language technology the elusive human touch."
    ],
    "553": [
        "The proposed CNN-based neural network system for open-domain machine comprehension tasks demonstrates promising results.",
        "The system's performance is better when solving the task in a document projection way rather than a textual entailment way.",
        "The architecture of modeling dynamic document representation by attention scheme from sentence level to snippet level shows potential in this task.",
        "In the future, more fine-grained representation learning approaches are expected to model complex answer types and question types.",
        "The proposed CNN-based neural network system for open-domain machine comprehension tasks demonstrates promising results.",
        "The system's performance is better when solving the task in a document projection way rather than a textual entailment way.",
        "The architecture of modeling dynamic document representation by attention scheme from sentence level to snippet level shows potential in this task.",
        "In the future, more fine-grained representation learning approaches are expected to model complex answer types and question types."
    ],
    "558": [
        "The attentional encoder-decoder model achieves promising results in abstractive summarization, outperforming state-of-the-art results significantly on two different datasets.",
        "Each of the proposed novel models addresses a specific problem in abstractive summarization, yielding further improvement in performance.",
        "A new dataset for multisentence summarization is proposed, and benchmark numbers are established on it.",
        "The authors plan to focus their efforts on this dataset and build more robust models for summaries consisting of multiple sentences in the future."
    ],
    "561": [
        "Our model, MatchPyramid, can automatically capture important matching patterns such as unigram, n-gram, and n-term at different levels.",
        "Our model outperforms baselines, including some recently proposed deep matching algorithms.",
        "In this paper, we view text matching as image recognition, and propose a new deep architecture, namely MatchPyramid. Our model can automatically capture important matching patterns such as unigram, n-gram, and n-term at different levels.",
        "Experimental results show that our model can outperform baselines, including some recently proposed deep matching algorithms."
    ],
    "563": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our method is more effective than other competitors."
    ],
    "566": [
        "Our model represents each word with its context vector to bridge the lexical gap problem.",
        "We designed several methods to decompose the word vector into a similar component and a dissimilar component.",
        "To extract features at multiple levels of granularity, we employed a two-channel CNN model and equipped it with multiple types of ngram filters.",
        "Our model is quite effective on both the answer sentence selection task and the paraphrase identification task."
    ],
    "570": [
        "Our method, DENSIFIER, is trained to focus embeddings used for an application to an ultradense subspace that contains the information relevant for the application.",
        "The ultradense subspace is 100 times smaller than the original space, which means that unnecessary noisy information is removed from the embeddings and robust learning without overfitting is better supported.",
        "The subspace can be learned with just 80-300 training examples, achieving state-of-the-art results on lexicon creation.",
        "Our method allows for the possibility of factoring all information present in an embedding into a dozen or so orthogonal subspaces, making them more compact for any given application, more meaningful and more interpretable.",
        "The nine large DENSIFIER lexicons shown in Table 1 are publicly available."
    ],
    "571": [
        "The proposed generative model is effective both as a parser and as a language model.",
        "Relaxing conventional independence assumptions (e.g., context-freeness) and inferring continuous representations of symbols alongside non-linear models of their syntactic relationships are key factors in the effectiveness of the generative model.",
        "The discriminative model performs worse than the generative model, possibly due to larger, unstructured conditioning contexts being harder to learn from, and providing opportunities for overfitting.",
        "The fully discriminative model of Vinyals et al. (2015) was able to obtain results similar to those of the generative model using much larger training sets obtained through semisupervision.",
        "Generative models should be considered as a more statistically efficient method for learning neural networks from small data.",
        "The approach requires no feature design or transformations to treebank data, and the generative model outperforms every previously published parser built on a single supervised generative model in English, and a bit behind the best-reported generative model in Chinese.",
        "RNNGs outperform the best single-sentence language models."
    ],
    "576": [
        "We have presented a novel counter-fitting method for injecting linguistic constraints into word vector space representations.",
        "The method efficiently postprocesses word vectors to improve their usefulness for tasks which involve making semantic similarity judgments.",
        "Its focus on separating vector representations of antonymous word pairs lead to substantial improvements on genuine similarity estimation tasks.",
        "We have also shown that counter-fitting can tailor word vectors for downstream tasks by using it to inject domain ontologies into word vectors used to construct semantic dictionaries for dialogue systems."
    ],
    "578": [
        "The proposed joint entity-driven model can automatically induce event schemas.",
        "The model uses word embedding and PMI to measure the inner connection of entities.",
        "The model uses normalized cut for more accurate clustering.",
        "The model uses sentence constraint to extract templates and slots simultaneously.",
        "The experiment has proved the effectiveness of the proposed model."
    ],
    "579": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Our proposed neural network architecture for sequence labeling is a truly end-to-end model relying on no task-specific resources, feature engineering or data pre-processing.",
        "We achieved state-of-the-art performance on two linguistic sequence labeling tasks, comparing with previously state-of-the-art systems.",
        "There are several potential directions for future work, such as exploring multi-task learning approaches to combine more useful and correlated information, or applying the model to data from other domains."
    ],
    "580": [
        "Our proposed neural architectures for sequence labeling achieve the best NER results ever reported in standard evaluation settings, even compared with models that use external resources such as gazetteers.",
        "Our models model output label dependencies using a simple CRF architecture or by explicitly constructing and labeling chunks of the input.",
        "Word representations are crucial for success, and we use both pre-trained word representations and \"character-based\" representations that capture morphological and orthographic information.",
        "To prevent the learner from depending too heavily on one representation class, dropout is used."
    ],
    "583": [
        "Our model achieves a new state-of-the-art accuracy on all evaluated datasets.",
        "The ensemble of our models is near to the maximal accuracy achievable on certain datasets (CNN and Daily Mail).",
        "A significant proportion of questions in certain datasets (CNN and Daily Mail) are ambiguous or too difficult to answer, even for humans.",
        "Our model is simpler than previously published models."
    ],
    "587": [
        "The proposed novel constituent hierarchy predictor outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation. (Claim 1)",
        "The joint model of part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages. (Claim 2)",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\"). (Claim 3)",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving the accuracy of both tagging and parsing. (Claim 4)",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality. (Claim 5)",
        "The proposed MTNN model using convolutional neural networks with dynamic pooling can synthesize different corpus-specific discourse classification tasks and optimize each other without bringing useless noise. (Claim 6)"
    ],
    "589": [
        "Domain adaptation methods significantly improve the tagger performance on two historical English treebanks, with relative error reductions of 30% in the temporal adaptation setting.",
        "FEMA outperforms other domain adaptation approaches, showing the importance of adapting the entire feature vector, rather than simply using word embeddings.",
        "Normalization and domain adaptation combine to yield even better performance, with a total of 5% raw accuracy improvement over a baseline classifier in the most difficult setting.",
        "Error analysis reveals that tagset mismatch is the most common source of errors for in-vocabulary words.",
        "Our work encourages further research on domain adaptation for historical texts and provides useful baselines in these efforts."
    ],
    "591": [
        "The sample used in the study is biased, leading to a high accuracy for the trivial baseline of predicting no participants to be over BMI 28.7 (claim based on Table 4 and Figure 3).",
        "The overall accuracy of the random forest backing the quiz was 78.7%, but the accuracy on participants who reported a BMI over 28.7 was only 16.0% (claim based on the difference between the two groups).",
        "Participants who are overweight may be more reluctant to mention food-and health-related topics on social media, leading to lower-quality training data for this group (conjecture based on the distribution of BMI for participants classified as under 28.7 and those classified as over 28.7)."
    ],
    "592": [
        "Adding sequential information improves the quality of predictions for sequential short-text classification.",
        "The performance of the model depends on the type of sequential information used.",
        "Our model achieves state-of-the-art results on three different datasets for dialog act prediction.",
        "Adding sequential information improves the quality of predictions for sequential short-text classification.",
        "The performance of the model depends on the type of sequential information used.",
        "Our model achieves state-of-the-art results on three different datasets for dialog act prediction."
    ],
    "594": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Multichannel initialization and variable-size filters enhance system performance on sentiment classification and subjectivity classification tasks.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Multichannel initialization and variable-size filters enhance system performance on sentiment classification and subjectivity classification tasks."
    ],
    "595": [
        "Our proposed generative, unsupervised ranking model for entity coreference resolution significantly improves the accuracy on different evaluation metrics over the baseline systems.",
        "Introducing resolution mode variables to distinguish mentions resolved by different categories of information improves the performance of the system.",
        "Differentiating more resolution modes could be a future direction for improvement.",
        "Adding more precise or even event-based features could improve the model's performance."
    ],
    "596": [
        "Vec2Topic achieves speed and robustness in extracting topics.",
        "Vec2Topic performs well on diverse datasets.",
        "Vec2Topic can be implemented with additional considerations in mind."
    ],
    "599": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "DocNADE had competitive results while its deep test set perplexity.",
        "The similarly good performances were observed when we used these models as feature extractors to represent documents for the task of information retrieval.",
        "Combining contextual information by leveraging the DocNADE neural network architecture can significantly improve the performance of a neural probabilistic N-gram language model."
    ],
    "600": [
        "The proposed fully-supervised parser outperforms the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "The use of self-attention mechanism in the proposed ReCoSa model effectively captures long-distance dependency relations, leading to significant improvements in multi-turn dialogue generation compared to existing HRED models and attention variants.",
        "The relevant contexts detected by the proposed model are coherent with human judgments, indicating the effectiveness of using proper detection methods for improving the quality of multiturn dialogue generation.",
        "The proposed ReCoSa model significantly outperforms existing HRED models and attention variants, demonstrating the advantages of global models in multi-turn dialogue generation.",
        "The label bias problem is a common issue in practice, and the proposed model addresses this problem by using proper detection methods to improve the quality of generated responses."
    ],
    "601": [
        "The proposed tree-based encoder is a natural extension of the sequential encoder model, allowing for better performance in NMT tasks.",
        "The attention mechanism in the tree-based encoder allows for aligning input phrases with output words, leading to improved performance.",
        "The proposed model achieves the best RIBES score and outperforms the sequential attentional NMT model.",
        "The use of a parsed tree in the encoder improves the performance of the NMT model.",
        "The extension of the sequential encoder model to a tree-based encoder leads to better performance in NMT tasks."
    ],
    "607": [
        "The addition of dependency structure features does not improve the performance of a system (Toh and Wang, 2014).",
        "The proposed joint model, RNCRF, achieves state-of-the-art performance for explicit aspect and opinion term extraction on a benchmark dataset.",
        "The use of DT-RNNs allows for the learning of high-level features that are not restricted to certain observed relations and POS tags.",
        "RNCRF outperforms traditional rule-based methods in terms of flexibility, as aspect terms and opinion terms are not limited to certain observed relations and POS tags.",
        "The proposed model saves much effort in composing features compared to feature engineering methods with CRFs.",
        "The use of non-linear transformations allows for the extraction of higher-level features."
    ],
    "608": [
        "Our model outperforms multiple benchmarks, demonstrating the importance of combining different types of predictors.",
        "We propose a generative model for code generation that combines a character level softmax to generate language-specific tokens and multiple pointer networks to copy keywords from the input.",
        "Our model is applied on both existing datasets and also on a newly created one with implementations of TCG game cards.",
        "We introduce Latent Prediction Network, a neural network architecture that allows efficient marginalization over multiple predictors.",
        "Our experiments show that our model out-performs multiple benchmarks, which demonstrate the importance of combining different types of predictors."
    ],
    "610": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our approach is consistently better than or comparable to a strong feature-rich baseline system, and is superior to an IR-based system when there is a reasonable amount of training data.",
        "Our model is language independent and can be applied to other languages with minimal modifications.",
        "Incorporating available metadata and preprocessing data with morphological normalization and out-of-vocabulary mappings could enrich the existing system.",
        "Reinforcing our model by carrying out word-by-word and history-aware attention mechanisms instead of attending only when reading the last word could improve the performance."
    ],
    "618": [
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The proposed Cosine Annealing Strategy combines two methods to improve domain adaption performance.",
        "Our approach improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our work is an important step towards explaining deep visual models."
    ],
    "621": [
        "The novel Parallel-Hierarchical model for machine comprehension achieves state-of-the-art results on the small but complex MCTest, outperforming several feature-engineered and neural approaches.",
        "Good comprehension of language is supported by hierarchical levels of understanding.",
        "Exogenous attention (the trainable word weights) may be broadly helpful for NLP.",
        "The training wheels approach, that is, initializing neural networks to perform sensible heuristics, appears helpful for small datasets.",
        "Reasoning over language is challenging, but easily simulated in some cases."
    ],
    "622": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The performance of a conditional language model initialized with an aggregate action feature vector obtained through maxpooling is substantially better than widely-known baselines.",
        "The system improves substantially over baseline systems on ROUGE while still maintaining good linguistic quality.",
        "Providing lexical information to parsing: The claim states that providing lexical information to parsing is more beneficial than providing syntactic information to tagging. This suggests that the joint model should be designed to prioritize the interaction between tagging and parsing, especially when it comes to leveraging lexical information.",
        "Substantial improvement over baselines: The claim also states that the system improves substantially over baseline systems on ROUGE, indicating that the proposed approach is more effective than previous methods. This suggests that the integration of tagging and parsing in the joint model is a key factor in achieving better performance.",
        "Maintenance of good linguistic quality: The claim also mentions that the system maintains good linguistic quality, which suggests that the approach is effective in preserving the accuracy of the output while improving the overall performance. This is an important consideration, as many NLP tasks prioritize accuracy and fluency over other factors like speed or simplicity."
    ],
    "623": [
        "We have proposed a method for learning multi-sense embeddings that performs sense estimation and context prediction jointly.",
        "The bilingual signal improves the sense predictor, even though the crosslingual information is not available at test time.",
        "We are able to obtain word representations that are of better quality than the monolingually-trained multi-sense representations, and that outperform the Skip-Gram embeddings on intrinsic tasks.",
        "Varying the dimensionality, vocabulary size, amount of data, and size of the second-language context affects the model performance.",
        "Bilingual information is useful even when using the entire sentence as context, suggesting that sentence-only alignment might be sufficient in certain situations."
    ],
    "625": [
        "We have introduced the new task of visual verb sense disambiguation.",
        "We developed the new VerSe dataset for this task, based on the existing COCO and TUHOI datasets.",
        "Our proposed unsupervised visual sense disambiguation model based on the Lesk algorithm demonstrated that both textual and visual information associated with an image can contribute to sense disambiguation.",
        "In an in-depth analysis of various image representations, we showed that object labels and visual features extracted using state-of-the-art convolutional neural networks result in good disambiguation performance, while automatically generated image descriptions are less useful."
    ],
    "628": [
        "Learning features directly from waveforms can outperform spectrograms, especially when applied at multiple scales.",
        "The use of waveform-based features can improve the performance of speech recognition systems.",
        "There is room for further research to answer questions such as how to choose the best scale and how to combine waveform and spectrogram information.",
        "The use of waveform-based features may lead to widespread adoption of such techniques in speech recognition systems."
    ],
    "631": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Despite operating at the character level, the network is occasionally able to perform rearrangements of words to form common phrases and insert and delete words where appropriate.",
        "The network can also sometimes mangle rare words and fail to split common words missing a separating space, suggesting that while common patterns are captured, the network lacks semantic understanding.",
        "Increasing the language model weight \u03bb tends to improves recall at the expense of precision.",
        "Using edit classification to filter spurious edits increases precision, often with smaller drops in recall.",
        "The attention mechanism helps to prevent the decoded output from diverging from the input sentence.",
        "The use of a character-based model with an attention mechanism allows for orthographic errors to be captured and avoids the OOV problem suffered by word-based neural machine translation methods."
    ],
    "634": [
        "LRFR attains state-of-the-art performance on several NLP tasks, including relation extraction, PP-attachment, and preposition disambiguation.",
        "LRFR exploits the inner structure of complex lexical features and applies a low-rank tensor to efficiently score features with this representation.",
        "The implementation of LRFR is available for general use.",
        "LRFR improves upon previous state-of-the-art models in relation extraction, PP-attachment, and preposition disambiguation tasks."
    ],
    "635": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach to joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets using Dynamic Memory Induction Networks (DMIN) for few-shot text classification.",
        "Our approach to learning to generate pragmatic descriptions about general referents, even without training data collected in a pragmatic context, produces correct behavior in human listeners significantly more often than existing baselines.",
        "Using reasoning to obtain novel contextual behavior from neural decoding models might be more broadly applied."
    ],
    "636": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "PReFIL outperforms prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL's performance on DVQA is nuanced due to OCR model variations.",
        "All OCR versions exceed the human baseline for structure questions, and only PRe-FIL using oracle OCR exceeds humans across all question types.",
        "The beam problem can be explained by the brevity problem, which results from the locally-normalized structure of the model."
    ],
    "638": [
        "The proposed multi-field weighted indexing approach for question answering improves the performance of the model.",
        "Adding more semantic fields and applying averaged perceptron learning to statistically designate weights for the fields leads to significant improvement.",
        "Integrating additional layers of fields (e.g., Freebase, WordNet) can further improve the performance of the model.",
        "Improving NLP tools can enable even deeper understanding of the context for more complex question answering."
    ],
    "640": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Developing annotated datasets from multiple sources based on existing datasets and the causality tagging scheme can improve the performance of SCITE.",
        "Combining SCITE with distant supervision and reinforcement learning can achieve better performance without needing a high-quality annotated corpus for causality extraction.",
        "The use of very deep convolutional networks in speech recognition in the hybrid NN-HMM framework can obtain better performance than other published models.",
        "The combination of a state-of-the-art RNN acoustic model and better language models with SCITE can achieve significantly better performance on Hub5 than any other published model.",
        "Time-padding and time-pooling are important factors in the architecture of SCITE.",
        "A CNN architecture that does pool but does not pad in time can combine the best of both (pooling and efficient convolutional evaluation)."
    ],
    "643": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our proposed recurrent chunking mechanisms outperform benchmark models across different datasets.",
        "We propose a novel topic model called Sentence Level Recurrent Topic Model (SLRTM) which models the sequential dependency of words and topic coherence within a sentence using Recurrent Neural Networks, and shows superior performance in both predictive document modeling and document classification.",
        "Integrating SLRTM into RNN-based STC systems can make the dialogue more topic sensitive.",
        "We plan to conduct large scale SLRTM training on bigger corpus with more topics using specially designed scalable algorithms and computational platforms."
    ],
    "645": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our transfer NMT system can come close to the performance of a very strong SBMT system, even exceeding its performance on Hausa-English.",
        "We consistently and significantly improve state-of-the-art SBMT systems on low-resource languages when the transfer NMT system is used for re-scoring.",
        "There is still room for improvement in selecting parent languages that are more similar to child languages, provided data for such parents can be found."
    ],
    "649": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The general low magnitude of the BLEU scores presented in Table 1, especially in comparison to the scores typically reported in Machine Translation results, indicates the difficulty of the task.",
        "In open-domain text, a sentence is typically not straightforwardly predictable from preceding text; if it were, it would likely not be stated.",
        "The difference between t 1 t 2 and e 1 e 2 [0] is fairly marginal, raising the question of how much explicit syntactic analysis is required for the task of event inference.",
        "Models operating on raw text perform roughly comparably to identical models operating on predicate-argument event structures when predicting the latter, and that text models provide superior predictions of raw text.",
        "Encoder/decoder models mediated by automatically extracted events may not be learning appreciably more structure than systems trained on raw tokens alone."
    ],
    "656": [
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Our method jointly learns from symbolic triples and textual descriptions, allowing for the discovery of semantic relevance and precise semantic expression.",
        "The proposed Cosine Annealing Strategy combines Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "Our approach uses a representation extractor to encode unlabeled target-domain data into features, which are then passed to a k-means cluster miner to generate pseudo labels.",
        "The use of pseudo-labeled target-domain data and labeled source-domain data improves the domain adaption performance.",
        "Our method achieves substantial improvements against state-of-the-art baselines."
    ],
    "658": [
        "We systematically study the problem of representing a segment in neural semi-CRF model.",
        "We propose a concatenation alternative for representing segment by composing input units which is equally accurate but runs faster than SRNN.",
        "We also propose an effective way of incorporating segment embeddings as segment-level representation and it significantly improves the performance.",
        "Experiments on named entity recognition and Chinese word segmentation show that the neural semi-CRF benefits from rich segment representation and achieves state-of-the-art performance."
    ],
    "661": [
        "The proposed approach outperforms state-of-the-art methods in various experimental settings.",
        "The novel zero pronoun-specific neural network (ZPSNN) for Chinese zero pronoun resolution exploits a learning algorithm that is capable of modeling contextual information to represent zero pronouns at the semantic level.",
        "A two-level candidate antecedent encoder is employed to explicitly capture the global and local information of candidate antecedents with respect to the whole candidate set.",
        "The proposed approach achieves better performance in various experimental settings.",
        "The state-of-the-art methods are outperformed by the proposed approach."
    ],
    "662": [
        "We have demonstrated that FM provide an approach to KB completion that can incorporate embeddings for bigrams naturally.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We have shown that bigram models can improve prediction performances substantially over more straightforward unigram models.",
        "A surprising but important result is that bigrams other than entity pairs are particularly appealing.",
        "The bigger question behind our work is about compositionality vs. non-compositionality in a broader class of knowledge bases involving higher order information such as time, origin or context in the tuples.",
        "Deciding which modes should be merged into a high order embedding without having to rely on heavy cross-validation is an open question."
    ],
    "663": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias.",
        "The solution to the brevity problem is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to improvement in NMT systems.",
        "The approach of combining multiple, semi-formally expressed facts to answer questions is viable and promising for natural language question answering.",
        "The use of a semi-structured knowledge base for QA significantly outperforms both the previous best attempt at structured reasoning for this task and an IR engine provided with the same knowledge."
    ],
    "664": [
        "The proposed attention-based sequence-to-sequence learning approach for Chinese Song iambics generation is simple in model structure, flexible in learning variable-length sentences, and powerful in learning complex regulations.",
        "Compared to several popular poetry generation methods, the new approach is more effective in generating Song iambics.",
        "With a large-scale hybrid training, the attention model can generate Song iambics pretty well, achieving a high score of 4.08.",
        "Although the current result is not yet at the level of human artists, it is highly encouraging and suggests that machines may be able to generate human-level iambics with more data and continuous refinement of the model.",
        "The attention model has the potential to be applied to other forms of literary genres in Chinese, such as Tang poetry, Han Fu, Yuan qu, and even novels."
    ],
    "668": [
        "Not all alignment intensities contribute equally to task performance, as evidenced by the preference of TE tasks for weaker alignments and AS tasks for stronger alignments.",
        "The effectiveness of a GRU system can be improved by incorporating a flexible attention pooling mechanism that satisfies the different requirements of various tasks.",
        "Experimental results support the argument that different task intensities require different attention pools, and that our proposed attention pooling mechanism is effective in improving task performance."
    ],
    "672": [
        "We have observed a good complementarity between recurrent nets and convolutional nets and their combination led to significant accuracy gains.",
        "In this paper, we have presented an improved unfolded RNN (with maxout instead of sigmoid activations) and a stronger CNN obtained by adding more convolutional layers with smaller kernels and ReLu nonlinearities.",
        "These improved models still have good complementarity and their frame-level score combination in conjunction with a multi-layer LSTM leads to a 0.4%-0.7% decrease in WER over the LSTM.",
        "Multilayer LSTMs were the strongest performing model followed closely by the RNN and VGG nets.",
        "We also believe that LSTMs have more potential for direct sequence-to-sequence modeling and we are actively exploring this area of research.",
        "On the language modeling side, we have increased our vocabulary from 30K to 85K words and updated our component LMs.",
        "At the moment, we are less than 3% away from achieving human performance on the Switchboard data (estimated to be around 4%).",
        "Unfortunately, it looks like future improvements on this task will be considerably harder to get and will probably require a breakthrough in direct sequence-to-sequence modeling and a significant increase in training data."
    ],
    "673": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results suggest that the community is ready for more difficult CQA datasets.",
        "Charts in the wild may contain variations that are not captured by current datasets, and human-generated questions should be included in future CQA datasets.",
        "Document-level CQA requires document question answering, page segmentation, and more.",
        "The proposed PReFIL system has the potential to improve retrieval of information from charts, which has numerous applications."
    ],
    "674": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model of part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves the accuracy of tagging and parsing.",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The structural models are on-par with or surpass state-of-theart published systems."
    ],
    "675": [
        "The proposed TACNTN model can leverage topic information in message-response matching for retrieval-based chatbots, leading to improved performance compared to state-of-the-art models.",
        "Experimental results show that our model significantly outperforms state-of-the-art matching models on public and human annotated data sets.",
        "The use of topic information in message-response matching can improve the performance of retrieval-based chatbots.",
        "Our model achieves state-of-the-art results on public and human annotated data sets, indicating the effectiveness of our approach.",
        "The proposed TACNTN model provides a new and effective way of leveraging topic information in message-response matching for retrieval-based chatbots."
    ],
    "676": [
        "The Hierarchical Composition Recurrent Network (HCRN) model, consisting of a 3-level hierarchy of compositional models, improves the quality of word representation, especially for rare and OOV words.",
        "The inclusion of the compositional character model significantly improves the performance of dialogue act classification by embedding intersentence dependency into sentence representation.",
        "The HCRN is trained in a hierarchy-wise language learning fashion, alleviating optimization difficulties with end-to-end training.",
        "The proposed HCRN using the hierarchy-wise learning algorithm achieves state-of-the-art performance with a test classification error rate of 22.7% on the dialogue act classification task on the SWBD-DAMSL database.",
        "Future work involves learning hierarchy information of given sequential data without explicitly given hierarchy information.",
        "Another direction involves applying the HCRN to other tasks which might benefit from OOV-free sentence representation in large contexts such as document summarization."
    ],
    "680": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our method is simple to implement in automatic differentiation frameworks and can lead to more readily interpretable unsupervised representations.",
        "Our approach yields coherent topics and learns linear relationships between words that allow it solve word analogies in the specialized vocabulary of this corpus."
    ],
    "684": [
        "The use of word similarity tasks for evaluating word vectors can lead to incorrect inferences.",
        "Existing solutions for evaluating word vectors, such as word similarity tasks, may not be effective.",
        "A better solution for intrinsic evaluation of word vectors is needed.",
        "Task-specific evaluation is a viable alternative for evaluating word vector models.",
        "Different word vector models can capture different types of information that may be more or less useful for a particular task."
    ],
    "685": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; more general globally-normalized models can be trained in a similarly inexpensive way.",
        "The updated version of GLEU that does not require tuning (GLEU + ) should be used instead of the originally presented GLEU due to the issues identified in Section 1."
    ],
    "687": [
        "Our model learns a special coverage embedding vector for each source word to start with, and keeps updating those coverage embeddings with neural networks as the translation goes.",
        "Experiments on the large-scale Chinese-to-English task show significant improvements over the strong LVNMT system.",
        "Our approach achieves significant improvements in translation performance.",
        "The proposed coverage embedding models for attention-based NMT are simple yet effective.",
        "The use of coverage embeddings with neural networks improves translation performance."
    ],
    "688": [
        "We propose a new approach to address the large vocabulary issue in neural machine translation.",
        "The proposed approach uses a sentence-level target vocabulary, which is much smaller than the full target vocabulary.",
        "The small size of the target vocabulary reduces the computing time of the softmax function in each predict step.",
        "The large vocabulary of the full target vocabulary enables us to model rich language phenomena.",
        "We decrease the size of the output vocabulary under 3k for each sentence, and we speed up and improve the large-vocabulary NMT system."
    ],
    "690": [
        "Our proposed method is effective in extracting causality.",
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "GloVe, ELMo, and BERT embeddings all encode gender, religion, and nationality biases.",
        "A projection-based method can effectively attenuate biases in contextualized embeddings without loss of entailment accuracy.",
        "Incorporating semantics such as word embeddings, frame semantics, and coreference resolution into our system can greatly improve model performance.",
        "Our model is poor at high-level text summarization, event detection, and inference, and we will investigate how to solve these problems using semantics in the future."
    ],
    "693": [
        "The novel multilingual language model architecture obtains substantial gains in perplexity.",
        "The model improves downstream text and speech applications.",
        "The approach is general and can be applied in problems that integrate divergent modalities, e.g., topic modeling, and multilingual tagging and parsing."
    ],
    "695": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Better curricula yield stronger models.",
        "The proposed novel technique for finding an optimal curriculum is general, and can be used with other datasets and models."
    ],
    "696": [
        "Our approach outperforms state-of-the-art approaches.",
        "Our method incorporates taxonomy hierarchy from large scale knowledge bases.",
        "Experiments on both concept categorization and semantic relatedness show that our approach outperforms state-of-the-art approaches.",
        "We aim at applying our method to more applications such as hierarchical document classification."
    ],
    "697": [
        "The proposed method, SICTF, achieves better accuracy and speed compared to state-of-the-art baselines for relation schema induction.",
        "The use of non-negative coupled tensor matrix factorization in SICTF allows for the incorporation of various types of side information during factorization.",
        "The proposed method is scalable and can be applied to real-world datasets.",
        "The model achieves significant speedup (about 11.8x) compared to state-of-the-art baselines.",
        "The induced categories can be labeled and analyzed further in future work.",
        "The model can be applied to more domains.",
        "The code and datasets used in the paper will be made publicly available upon publication of the paper."
    ],
    "701": [
        "Our approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "We have demonstrated a viable approach to Syntactically Guided Neural Machine Translation (SGNMT) that exploits the rich, structured search space generated by Hiero and the long-context translation scores of NMT.",
        "SGNMT does not suffer from the severe limitation in vocabulary size of basic NMT and avoids any difficulty of extending distributed word representations to new vocabulary items not seen in training data.",
        "Our proposed method uses a self-attentive BiLSTM-CRF-based solution for causality extraction, which is effective in extracting causality in natural language text.",
        "The performance of SCITE, our proposed method for causality extraction, is still limited by the insufficiency of high-quality annotated data."
    ],
    "704": [
        "The proposed memory-efficient CA method based on randomized SVD can significantly reduce computation time while maintaining performance.",
        "The proposed CA method can outperform existing methods in the word-vector representation task.",
        "The tail-cut kernel, an extension of the skip-gram approach within KCA, can also outperform existing word-vector representation methods.",
        "The use of randomized SVD in the CA method can improve the efficiency and effectiveness of the algorithm.",
        "The proposed method can be applied to the word-vector representation task with promising results."
    ],
    "712": [
        "We propose an active reward learning model using Gaussian process classification and an unsupervised neural network-based dialogue embedding to enable truly on-line policy learning in spoken dialogue systems.",
        "The system enables stable policy optimization by robustly modelling the inherent noise in real user feedback and uses active learning to minimize the number of feedback requests to the user.",
        "We found that the proposed model achieved efficient policy learning and better performance compared to other state-of-the-art methods in the Cambridge restaurant domain.",
        "A key advantage of this Bayesian model is that its uncertainty estimate allows active learning and noise handling in a natural way.",
        "The unsupervised dialogue embedding function required no labeled data to train while providing a compact and useful input to the reward predictor.",
        "Overall, the techniques developed in this paper enable for the first time a viable approach to on-line learning in deployed real-world dialogue systems which does not need a large corpus of manually annotated data or the construction of a user simulator."
    ],
    "714": [
        "The proposed novel vector representation enhances the prediction of word similarity, as demonstrated by improved performance using traditional distributional semantics models and word embeddings.",
        "The incorporation of lexical contrast information into a skip-gram model successfully predicts degrees of similarity and identifies antonyms.",
        "The use of weighted features to distinguish antonyms from synonyms significantly improves the quality of the representation.",
        "The proposed method improves the state-of-the-art performance on word similarity prediction tasks."
    ],
    "721": [
        "Our approach (stacking with auxiliary features) can be generalized across multiple diverse tasks and domains, achieving promising results.",
        "The use of auxiliary features enables information fusion and discrimination among component systems, leading to better performance.",
        "Our approach outperforms other baseline ensembling methods and best component systems on all three tasks (NLP, computer vision, and object detection).",
        "The gain in performance from our approach comes from output decisions that are difficult to make without context, but using auxiliary features enables fusion of additional relevant information, allowing the stacker to make the right decision.",
        "SWAF (stacking with auxiliary features) performs better when the component systems differ widely on their outputs and have low confidences."
    ],
    "723": [
        "The proposed approach performs comparable to state-of-the-art feature based SVM system, and substantively better than LSTM architectures.",
        "Empirical results on two datasets verify that the proposed approach performs better than LSTM architectures.",
        "Using multiple computational layers in memory network could obtain improved performance.",
        "Leveraging both content and location information could learn better context weight and text representation.",
        "Incorporating sentence structure like parsing results into the deep memory network could improve performance."
    ],
    "725": [
        "We introduce MultiATIS++, a multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy."
    ],
    "727": [
        "The proposed hierarchical co-attention model outperforms other state-of-the-art models for visual question answering.",
        "Co-attention allows the model to attend to different regions of the image and different fragments of the question, improving its performance.",
        "Modeling the question hierarchically at three levels captures information from different granularities and improves the final performance.",
        "Ablation studies demonstrate the roles of co-attention and question hierarchy in the final performance.",
        "The model can be potentially applied to other tasks involving vision and language."
    ],
    "730": [
        "Our model is trained on a significantly larger conversational dataset than previously published efforts.",
        "We train two scalable neural network models using bags of ngram embeddings and user embeddings.",
        "Integrating what has been said in the conversation so far improves the task of selecting the next response.",
        "The length of the conversation history affects performance, and we study this effect.",
        "Personalizing the selection process by learning an identity feature for each user yields further improvement.",
        "Our multi-loss model shows improvements over the baseline single-loss model using any subset of the features."
    ],
    "732": [
        "A Brownian agent model defines a strong theoretical framework mixing stochastic and deterministic aspects of self-organization over time.",
        "Time, order and stochasticity are main parameters of real-world natural systems in which complexity is due to large number of components and can not be explained by traditional function modeling.",
        "This theoretical framework is also implicitly the core of a pragmatic implementation as a large-scale multiagent system ensuring observation of dynamics and convergence of a specific required relational model.",
        "The set of rules are defined through a set of interactions primitives and a matrix of relations between populations.",
        "We show that choosing a target population, its neighbourhood is not random and consistant to reality either with a gene neighbourhood and a semantic neighbourhood, with a same way of self-organization (i.e. contextual aggregation).",
        "It should lead us to think in part that the way of agregation between terms could be inspired from the way of genes organization.",
        "In this way, semantic nature of language is not fully deterministic but contains also a stochastic part not guided by intrinsic organization of biology.",
        "Such principle should not be so surprising since any language speaker is, initially, a biological entity."
    ],
    "735": [
        "The proposed approach could achieve better performance compared with other state-of-the-art NN-based methods.",
        "The attention of the answer aspect for each word in the question is used, which is more precise and flexible.",
        "Leveraging the global KB information could take full advantage of the complete KB and alleviate the OOV problem.",
        "The extensive experiments demonstrate that the proposed approach achieves better performance compared with other state-of-the-art NN-based methods."
    ],
    "736": [
        "Our approach significantly improves over baseline systems under various experiment settings.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Our framework can flexibly incorporate richer treebanks and more related tasks.",
        "We study two scenarios, respectively using multilingual universal source treebanks and monolingual heterogeneous source treebanks, and design effective parameter sharing strategies for each scenario."
    ],
    "740": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism.",
        "The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We have presented a coreference system that captures entity-level information with distributed representations of coreference cluster pairs.",
        "The learned, dense, high-dimensional feature vectors provide our cluster-ranking coreference model with a strong ability to distinguish beneficial cluster merges from harmful ones.",
        "Our system is trained with a learning-to-search algorithm that allows it to learn how local decisions will affect the final coreference score.",
        "We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task and report a substantial improvement over the current state-of-the-art."
    ],
    "743": [
        "The proposed reinforcement learning framework for neural response generation outperforms earlier neural SEQ2SEQ models in capturing the compositional models of the meaning of a dialogue turn and generating semantically appropriate responses.",
        "The proposed reinforcement learning framework for neural response generation is able to generate utterances that optimize future reward, successfully capturing global properties of a good conversation.",
        "The proposed reinforcement learning framework for neural response generation is able to generate semantically appropriate responses that foster a more sustained conversation."
    ],
    "744": [
        "Our proposed approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Our approach has the potential to benefit a wide range of generation tasks in NLP.",
        "Our latent variable neural models offer a new way to learn latent discourse-level features of a text, which may bring benefits to any discourse-aware NLP task.",
        "The traditional evaluation metric (ordering pairs of sentences in small domains) is completely solvable by our discriminative models.",
        "We suggest the community move to the harder task of open-domain full-paragraph sentence ordering."
    ],
    "745": [
        "The proposed method significantly outperforms state-of-the-art systems for zero pronoun resolution.",
        "The approach utilizes an attention-based neural network model to resolve zero pronouns.",
        "A two-step training approach is employed, including pre-training and adaptation steps.",
        "The proposed method can be easily applied to other tasks as well.",
        "Experimental results on OntoNotes 5.0 corpus are encouraging, showing improved performance compared to state-of-the-art systems.",
        "Handling unknown words processing is a critical part in comprehending context, and further research is needed to explore more effective ways to handle this issue.",
        "Developing other neural network architectures may be appropriate for zero pronoun resolution task."
    ],
    "746": [
        "Our model utilizes character-level input, especially when encountering rare words.",
        "The gate can be efficiently trained to find a good balance between word-level and character-level inputs.",
        "Empirically, our model performs well when encountering rare words.",
        "The experimental results suggest that the gate can be efficiently trained."
    ],
    "747": [
        "Our approach outperforms benchmark models across different datasets.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Increasing the depth up to 29 convolutional layers steadily improves performance.",
        "Our models are much deeper than previously published convolutional neural networks and they outperform those approaches on all data sets.",
        "The 'benefit of depths' was shown for convolutional neural networks in NLP for the first time.",
        "Texts are compositional for many languages, and characters combine to form n-grams, stems, words, phrase, sentences etc.",
        "Applying similar ideas to other sequence processing tasks, such as neural machine translation, is left for future research."
    ],
    "749": [
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "Introducing MCB pooling leads to improved phrase localization accuracy, indicating better interaction between query phrase representations and visual rep-resentations of proposal bounding boxes."
    ],
    "751": [
        "The proposed approach, CFO, achieves state-of-the-art accuracy on a large question dataset.",
        "The method employs a conditional factoid factorization to infer the target subject and relation.",
        "The type-vector scheme requires no training and reduces the candidate space without loss of recall rate.",
        "The focused pruning significantly improves the overall accuracy.",
        "The proposed method can handle complex questions in future work."
    ],
    "752": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We propose two self-supervised tasks to tackle the training data bottleneck.",
        "Experimental results show that our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Enhancing the RNN decoder in a neural machine translator with external memory can significantly improve its performance."
    ],
    "755": [
        "The EpiReader framework achieves state-of-the-art results on two large, complex datasets (CNN and CBT) outperforming all previous approaches.",
        "The novel EpiReader framework for machine comprehension is evaluated on two large, complex datasets.",
        "The model achieves state-of-the-art results on these corpora.",
        "In future work, the authors plan to augment their framework with a more powerful model for natural language inference.",
        "The authors plan to explore the effect of pretraining a specific model specifically on an inference task.",
        "The authors plan to simplify the model by reusing the Extractor's biGRU encodings in the Reasoner."
    ],
    "756": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our results demonstrate that when comparing the expectation-maximization with coarse-to-fine techniques to our spectral algorithm with latent state optimization, spectral learning performs better on six of the datasets.",
        "Our results are comparable to other state-of-the-art results for these languages.",
        "Using a diverse set of models to parse these datasets further improves the results."
    ],
    "758": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our analysis confirms our hypotheses that the proposed models implicitly capture essential information in the latent dialog states.",
        "One limitation of the proposed approach is poor scalability due to the large number of samples needed for convergence.",
        "Future studies will include developing full-fledged task-oriented dialog systems and exploring methods to improve the sample efficiency.",
        "Investigating techniques that allow easy integration of domain knowledge so that the system can be more easily debugged and corrected is another important direction."
    ],
    "759": [
        "Our model performs better on words with richer morphology.",
        "It learns morphosyntactic features to help solve the syntactic analogy task.",
        "Our model is language-agnostic and easy to port across different languages.",
        "Our model can segment words into morphemes and embed the word into a representation space.",
        "Our model is competitive at the task of morpheme boundary recovery compared to a dedicated morphological analyzer.",
        "Our model can learn morphological features, such as word affixation corresponding to linear shifts.",
        "Character-level models are competitive at representing rare morphologically rich words.",
        "The character-level models can predict good quality representations for unseen words, with the morphologically aware character-level model doing slightly better."
    ],
    "760": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The proposed model was capable of modifying its behavior via a delayed reward signal and achieved better success rate in a mismatched environment.",
        "The experiments demonstrated the efficiency of the proposed model with only a few hundred supervised dialogue examples."
    ],
    "761": [
        "We employed an attention-based method to generate an informative and concise summary.",
        "To cope with the large number of input text, we deploy an importance-based sampling mechanism for model training.",
        "Experiments showed that our system obtained state-of-the-art results using both automatic evaluation and human evaluation.",
        "Our system obtained state-of-the-art results using both automatic evaluation and human evaluation.",
        "We presented a neural approach to generate abstractive summaries for opinionated text."
    ],
    "762": [
        "The CNN/Daily Mail reading comprehension task is a valuable dataset for training effective statistical models.",
        "The dataset is still quite noisy due to its method of data creation and coreference errors.",
        "Current neural networks have almost reached a performance ceiling on this dataset.",
        "The required reasoning and inference level of this dataset is still quite simple.",
        "There is a need to consider how to utilize these datasets (and the models trained upon them) to help solve more complex RC reasoning tasks with less annotated data."
    ],
    "763": [
        "Using synthetic parallel training data obtained by back-translating in-domain monolingual target-side data leads to large improvements in translation quality.",
        "Pervasive dropout on all layers was used for English\u2194Romanian, and gave substantial improvements.",
        "Training a right-to-left model with reversed target side was helpful for English\u2194German and English\u2192Czech.",
        "Reranking the system output with these reversed models is helpful."
    ],
    "764": [
        "The inclusion of linguistic input features in neural machine translation can be beneficial, as our empirical evidence suggests.",
        "Our generalization of the encoder in the popular attentional encoder-decoder architecture allows for the inclusion of an arbitrary number of input features.",
        "The addition of various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels, and morphological features, improves English\u2194German, and English\u2192Romanian neural MT systems.",
        "Our experiments show that the linguistic features yield improvements over our baseline, resulting in improvements on new-stest2016 of 1.5 BLEU for German\u2192English, 0.6 BLEU for English\u2192German, and 1.0 BLEU for English\u2192Romanian.",
        "The usefulness of linguistic (or other) input features is an open question, and future developments may shed more light on their benefit or lack thereof.",
        "The machine learning capability of neural architectures is likely to increase, which may decrease the benefit provided by the features we tested.",
        "There is potential to explore the inclusion of novel features for neural MT, which might prove to be even more helpful than the ones we investigated."
    ],
    "765": [
        "The authors have introduced a new variant of the seq2seq model that addresses exposure bias and label bias, and allows for training with both sequence-level cost functions and hard constraints.",
        "The proposed approach can be scaled to larger datasets in future work.",
        "The locally-normalized structure of the model is a source of the beam problem.",
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "The authors have shown that their models can effectively preserve salient source relations in summaries.",
        "The structural models are on-par with or surpass state-of-the-art published systems, as evidenced by results on benchmark datasets."
    ],
    "775": [
        "feature-based tagging models adequately enriched with external morphosyntactic lexicons perform, on average, as well as bi-LSTMs enriched with word embeddings.",
        "the best accuracy levels are reached by feature-based models, and in particular by our improved version of the MEMM-based system MElt, on datasets with high lexical variability (e.g. for morphologically rich languages).",
        "neural-based results perform better on datasets with lower lexical variability (e.g. for English).",
        "integrating lexical information from an external lexicon and from word vector representations into tagging models can lead to improvements over the state of the art.",
        "feature-based models such as a CRF with an LSTM-based layer can perform well, following recent proposals such as the one proposed by Lample et al. (2016) for named entity recognition."
    ],
    "776": [
        "The proposed neural belief tracking (NBT) framework offers advantages such as coupling Spoken Language Understanding and Dialogue State Tracking, without relying on hand-crafted semantic lexicons to achieve state-of-the-art performance.",
        "The NBT models match the performance of models which make use of such lexicons and vastly outperform them when these are not available.",
        "The performance of NBT models improves with the semantic quality of the underlying word vectors.",
        "The authors are the first to move past intrinsic evaluation and show that semantic specialisation boosts performance in downstream tasks.",
        "The NBT framework has the potential to be applied to multi-domain dialogue systems, as well as in languages other than English that require handling of complex morphological variation."
    ],
    "779": [
        "The proposed novel modular neural framework can automatically generate concise yet sufficient text fragments to justify predictions made by neural networks.",
        "The encoder-generator framework, trained in an end-to-end manner, gives rise to quality rationales in the absence of any explicit rationale annotations.",
        "The approach could be modified or extended in various ways to other applications or types of data.",
        "The encoder and generator can be realized in numerous ways without changing the broader algorithm.",
        "When rationales can be expected to conform to repeated stereotypical patterns in the text, a simpler encoder consistent with this bias can work better.",
        "Rationales are flexible explanations that may vary substantially from instance to another.",
        "The training method employs a REINFORCE-style algorithm where the gradient with respect to the parameters is estimated by sampling possible rationales.",
        "Additional constraints on the generator output can be helpful in alleviating problems of exploring potentially a large space of possible rationales in terms of their interaction with the encoder."
    ],
    "781": [
        "The beam problem in NMT can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our model is efficient in sequence generation, and the best results can be obtained with a beam size of 3.",
        "Deep models are more advantageous for learning long sequences, and the deep topology is resistant to the over-fitting problem.",
        "Deeper models may provide further improvements, but the current depth of 16 is not very deep compared to models in computer vision."
    ],
    "783": [
        "The proposed translation model incorporates a hard attentional mechanism.",
        "The use of morpheme- and character-level word representations for the source language leads to more robust encodings of words in morphological rich languages, resulting in better translations than simple word embeddings.",
        "Word-embeddings are superior for frequent words, while the convolutional method is better for capturing the lemma, which is critical for translating out-of-vocabulary words and would be key in many other semantic applications.",
        "The convolutional method better captures the lemma, which is of critical importance for translating out-of-vocabulary words.",
        "The recurrent methods over character sequences have shown that the convolutional method better captures the lemma."
    ],
    "784": [
        "The proposed method can successfully induce morphological taggers for resource-scarce languages using tags projected across bitext.",
        "The method relies on access to a morphological tagger for a source-language and a moderate amount of bitext.",
        "The method obtains strong performance on a range of language pairs.",
        "Using the predictions from the tagger as features can improve downstream tasks such as dependency parsing.",
        "The results provide a strong baseline for future work in weakly-supervised morphological tagging."
    ],
    "789": [
        "Our approach achieves significant improvements in both alignment and translation evaluations.",
        "In the future, we plan to apply our approach to real-world non-parallel corpora to further verify its effectiveness.",
        "It is also interesting to extend the phrase translation model to more sophisticated models such as IBM models 2-5 (Brown et al., 1993) and HMM (Vogel and Ney, 1996)."
    ],
    "790": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Siamese CBOW provides a robust way of generating high-quality sentence representations.",
        "Word and sentence embeddings are ubiquitous and many different ways of using them in supervised tasks have been proposed.",
        "It would be interesting to see how Siamese CBOW embeddings would affect results in supervised tasks.",
        "The model can be applied to larger pieces of texts, such as documents, for document clustering or filtering tasks."
    ],
    "791": [
        "Our approach produces an effective natural language understanding model and word sense embeddings of high quality.",
        "Comparing with previous work training word sense embeddings on a corpus, our approach is less time-consuming and better for rare word senses.",
        "Experimental results show our word sense embeddings are of high quality."
    ],
    "796": [],
    "800": [
        "The proposed Semantic Language Models (SemLMs) have a good level of abstraction and are of high quality, as proven by perplexity and a narrative cloze test.",
        "The SemLMs are successful in performing co-reference resolution and shallow discourse parsing, exhibiting improvements over state-of-the-art systems.",
        "The SemLMs can be applied to other semantic related NLP tasks, such as machine translation and question answering, in future work.",
        "The use of neural embeddings for semantic frames in the SemLMs leads to improved performance in the two tasks mentioned above."
    ],
    "802": [
        "The proposed socially-informed timeline generation system can produce more informative timelines than state-of-the-art systems.",
        "The system uses an alternating optimization algorithm to maximize connectivity and importance of summaries, leading to more informative timelines.",
        "Human evaluations showed that the comment summaries produced by the system were rated as very insightful.",
        "The system can generate timelines that are more informative than those produced by existing systems."
    ],
    "803": [
        "Our proposed approach outperforms state-of-the-art methods in both automatic evaluation and human evaluation.",
        "Our framework is capable of including statistically learned sentence relevance and encouraging the summary to cover diverse topics.",
        "We also study different metrics on text similarity estimation and their effect on summarization."
    ],
    "804": [
        "The proposed agreement and disagreement detection model based on isotonic CRFs can output labels at the sentence-or segment-level.",
        "The sentiment lexicon for online discussions is constructed by encoding domain knowledge for the isotonic CRF learner.",
        "The sentiment-tagging model outperforms state-of-the-art approaches on both Wikipedia Talk pages and online debates."
    ],
    "805": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We benefited from careful pre-processing, cleaning, and normalization, which yielded an improvement of up to 3 BLEU points over a strong baseline.",
        "We further added a number of extra advanced features, which yielded 2.5 more BLEU points of absolute improvement on top of that due to pre-processing."
    ],
    "806": [
        "The proposed attention-based neural model for Chinese poetry generation is simple in structure and strong in theme preservation.",
        "The new approach is flexible to produce innovation and easy to be extended to other genres.",
        "The model can generate traditional Chinese quatrains pretty well and weakly pass the Feigenbaum Test.",
        "Employing more generative models, such as variational generative deep models, can achieve more natural innovation in the future work.",
        "The approach can be extended to other genres of traditional Chinese poetry, such as Yuan songs."
    ],
    "807": [
        "Full-time supervision is the key in BRNN for QA.",
        "Pooling-based supervision is not as effective as full-time supervision in BRNN for QA.",
        "GRU is better than LSTM for BRNN based QA.",
        "Our FTS-BRNN method can outperform other state-of-the-art baselines in real applications.",
        "We will apply our method to other QA tasks, especially those with long answers in future work."
    ],
    "808": [
        "The LAMBADA dataset is difficult for language models to predict, even for cutting-edge neural network approaches.",
        "The ability to store information in a longer-term memory will be a crucial component of successful models.",
        "The ability to perform some kind of reasoning about what's stored in memory, in order to retrieve the right information from it.",
        "Leveraging human performance on word prediction is a very promising strategy to construct benchmarks for computational models that are supposed to capture various aspects of human text understanding.",
        "The influence of broad context as explored by LAMBADA is only one example of this idea."
    ],
    "815": [
        "Our proposed models significantly outperform strong baselines on both tasks.",
        "A VQA agent that utilizes our detector and refuses to answer certain questions significantly outperforms a baseline (that answers all questions) in human studies.",
        "There are several directions for future work, such as identifying the premise entailed in a question, or determining what external knowledge is needed to answer non-visual questions.",
        "Our system can be further augmented to communicate to users what the assumed premise of the question is that is not satisfied by the image.",
        "The LSTM has 40 hidden units using a 4-layer MLP with 40 and 20 hidden units respectively."
    ],
    "817": [
        "QVEC-CCA is a general approach that can evaluate word vector content with respect to desired linguistic properties.",
        "The proposed evaluation can approximate a range of downstream tasks, but may not be sufficient to evaluate finer-grained features.",
        "In future work, we propose to exploit existing semantic, syntactic, morphological, and typological resources to construct better linguistic matrices for evaluating vectors used in additional NLP tasks.",
        "The current linguistic dimension matrices used in QVEC-CCA are coarse and may not capture all the desired features of word vectors.",
        "Using more fine-grained resources, such as universal dependencies treebank and WALS, can improve the evaluation of word vectors."
    ],
    "820": [
        "The results of the configurations with query expansion As presented above, in most of the cases, the expanded approach fails to improve results.",
        "We may explain these results by the quality of the corpus.",
        "In fact, the documents collection describes the reviews of users for books. Therefore, it's still difficult to extract the context of terms and select the similar terms in the similar context.",
        "The personalized Word Embedding fails to improve the results comparing to any configurations.",
        "We first can explain the results by the same quality problem than for the documents collection.",
        "In fact, the user is represented by the documents that appear in his catalog.",
        "Therefore, the amount of learning data may not reach the limit under which no convergence is possible for word2vec."
    ],
    "821": [
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our framework uses a novel combination of CorEx and the information bottleneck to extract more precise, interpretable topics through a lightweight interactive process.",
        "We plan to perform further empirical evaluations to extend the algorithm to handle complex latent structures present in health care data."
    ],
    "822": [
        "The beam problem in neural machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "We have explored simple and effective ways to alleviate or eliminate the beam problem.",
        "The solution to the beam problem requires globally-normalized training on only a small dataset.",
        "Our solution to the brevity problem leads to significant BLEU gains.",
        "Solving label bias in general may lead to more gain in terms of BLEU scores.",
        "Our model, P 3 , jointly reasons about question and environment interpretations using background knowledge to produce answers.",
        "P 3 uses a domain theory -a probabilistic program -to define the information to be extracted from the environment and background knowledge.",
        "The semantic parser and execution model in P 3 are jointly trained in a loglinear model, which learns to both parse questions and interpret environments.",
        "Importantly, the model includes global features of the logical form and executions, which help the model avoid implausible interpretations.",
        "P 3 outperforms several competitive baselines on a challenging new data set of 5000 science diagram questions."
    ],
    "828": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The strength of the NN-grams model comes primarily from the n-gram counts but both n-gram counts and word embeddings are important for long-form content such as dictation.",
        "The NN-grams model is a promising neural network LM that is scalable to large training texts.",
        "By avoiding the output softmax layer, it has a substantially lower overhead at training and run time compared to current neural network approaches such as LSTMs.",
        "This model will spur newer hybrid architectures which will increase the adoption of neural network approaches to language modeling."
    ],
    "830": [
        "The proposed task of \"sequencing\" in a set of image-caption pairs is motivated to learn temporal common sense.",
        "The authors propose multiple neural network models based on individual and pairwise element-based predictions, utilizing both image and text features, to achieve strong performance on the task.",
        "The best system achieves an average distance error of 0.8 (out of 5) positions in predicting the ordering of sentences, demonstrating strong performance.",
        "The authors analyze their predictions and provide qualitative examples that demonstrate temporal common sense."
    ],
    "832": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Incorporating contextual information into the model using smoothed dictionary features performs competitively with BoW features.",
        "Constructing features in an interactive loop may be more efficient in terms of labeling effort and more interpretable.",
        "Exploring the use of a dynamic threshold, rather than a fixed one, may improve the performance of the model."
    ],
    "833": [
        "The proposed framework for query-focused multi-document summarization based on sentence compression shows substantial improvement over pure extraction-based methods and state-of-the-art systems, as measured by ROUGE scores.",
        "The tree-based compression method can easily incorporate measures of query relevance, content importance, redundancy, and language quality into the compression process.",
        "The best system yields better results for human evaluation based on Pyramid and achieves comparable linguistic quality scores.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "The use of conditional language models with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features performs substantially better than widely-known baselines."
    ],
    "836": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments.",
        "We demonstrate our model's captioning capabilities on a held-out set of MSCOCO objects as well as several hundred ImageNet objects.",
        "Our model is able to describe many more novel objects compared to the state-of-the-art methods."
    ],
    "838": [
        "There is only limited benefit to adding more cores when parallel learning a single model in shared memory.",
        "The original Word2Vec implementation is most efficient when trained using 8 cores and becomes less efficient when more cores are added.",
        "We propose a straightforward caching strategy that caches the weight vectors that are used most frequently, and updates their change to main memory after a short delay reducing concurrent access of shared memory.",
        "Our proposed caching strategy shows up to 4x improvement in efficiency compared to existing Word2Vec implementations."
    ],
    "839": [
        "We propose a token-level summarization framework based on topic models.",
        "Modeling topic structure at the utterance-level is better at identifying relevant words and phrases than document-level models.",
        "The role of context is also studied and shown to be able to identify additional summaryworthy words."
    ],
    "843": [
        "The use of multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our novel end-to-end model for joint slot label alignment and recognition requires no external label projection and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The simple projection baseline using fast-align is outperformed by our model on most languages.",
        "The use of a multi-label classifier trained to predict visual entities from action features can provide a more expressive signal for captioning compared to the raw features themselves.",
        "The conscious choice to omit causal regulation reactions from this task has led to extremely low recall in deterministic models created so far, resulting in difficulty in determining their precision.",
        "The analysis of the Reichenbach model reveals one source of this low coverage, specifically the lack of temporal descriptions in the database.",
        "Temporal description is rare enough in this domain not to be represented in our randomly sampled database.",
        "The use of a sieve-based system of the models tested here improves performance over the top-performing individual component.",
        "The initial results are promising and demonstrate that a sieve-based system can improve performance in this domain, despite the dearth of temporal expressions and other regular linguistic cues."
    ],
    "849": [
        "The model surpasses the performance of previous joint models on the CoNLL 2008 and 2009 English tasks without using expert-crafted, expensive features of the full syntactic parse.",
        "The model is incremental and greedy, parsing sentences in a single pass without requiring the entire sentence to be available.",
        "The model does not use expert-crafted, expensive features of the full syntactic parse, making it more practical for real-world applications."
    ],
    "850": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improves the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "We propose a probabilistic Bayesian model which leverages distributed representations for images and words, and demonstrate superior performance of our model, and the effectiveness of exploiting visual contents for taxonomy induction.",
        "We further conduct qualitative studies and distinguish the relative importance of visual and textual features in constructing various parts of a taxonomy."
    ],
    "851": [
        "The proposed framework based on CNN for extracting relations among clinical entities in clinical texts has shown better performance compared to the SVM-based baseline model, using only a small fraction of features.",
        "The CNN model is able to learn global features that capture contextual features well, improving the performance of the model.",
        "The use of a small fraction of features in the proposed model leads to better performance, indicating that the model is efficient and effective.",
        "The proposed model is able to improve the performance of the baseline model by capturing contextual features using global learning."
    ],
    "853": [
        "Our model computes alignment scores as dot products between representations of windows around source and target words.",
        "We improve over Fast Align, a popular loglinear reparameterization of IBM Model 2 (Dyer et al., 2013) by up to 6 AER on Romanian-English, 7 AER on English-Czech data and 1.7 AER on English-French alignment.",
        "The aggregation operation acts as a filter over alignment scores and allows us to determine which source words explain a given target word.",
        "Our alignments are better or on par compared to Fast Align in terms of BLEU.",
        "We apply an aggregation operation borrowed from the computer vision literature to make unsupervised training possible."
    ],
    "857": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "HUME can be reliably and efficiently annotated in multiple languages.",
        "Annotation quality is robust to sentence length.",
        "Comparison to direct assessments further support HUME's validity.",
        "HUME allows for a finer-grained analysis of translation quality.",
        "A future automated version of HUME will be useful in informing the development of a more semantically aware approach to MT."
    ],
    "858": [
        "Improving NER in a resource-deprived language by using additional annotated corpora from another language.",
        "Sharing decoder, filters for extracting character-level features, and bilingual word embeddings between two languages can improve performance.",
        "Joint training with annotated corpora from both languages improves the performance of NER in a resource-deprived language.",
        "Using shared word embeddings leads to larger gains than using monolingual word embeddings, but poorer quality bilingual embeddings hinder overall performance.",
        "Training bilingual word embeddings on a larger corpus may help correct the situation.",
        "Currently, word embeddings are trained independently of the NER task and fine-tuned during training; jointly embedding words and predicting tags in multiple languages could be beneficial.",
        "Using only two languages at a time; jointly training with multiple languages could give better results."
    ],
    "860": [
        "The proposed fully relational system using Relational Dependency Networks (RDNs) outperforms the state-of-the-art Relation Factory system on the Knowledge Base Population task.",
        "RDNs can effectively learn the relation extraction task and incorporate various concepts in a relational framework, including word2vec, human advice, joint learning, and weak supervision.",
        "Weak supervision and word2vec do not significantly improve performance, but advice is extremely useful and validates long-standing results in the Artificial Intelligence community for the relation extraction task.",
        "Possible future directions include considering a larger number of relations, deeper features, and comparisons with more systems.",
        "Further work on developing word2vec features and utilizing more weak supervision examples may reveal further insights into how to effectively utilize such features in RDNs."
    ],
    "864": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Future work might investigate end-to-end approaches, or develop alternative approaches that generate titles more similar to how humans write titles.",
        "We also demonstrate that pooling evidence across multiple paths improves both training speed and accuracy.",
        "Finally, we also address the problem of reasoning about infrequently occurring relations and show significant performance gains via multitasking."
    ],
    "865": [
        "We presented a novel guided alignment training for a NMT model that utilizes IBM model 4 Viterbi alignments to guide the attention mechanism.",
        "Experimentally, our approach brought consistent improvements of translation quality on e-commerce and spoken language translation tasks.",
        "Our proposed novel way of utilizing topic meta-information improved BLEU and TER scores.",
        "We also showed improvements when using domain adaptation by continuing training of an out-of-domain NMT system on in-domain parallel data.",
        "In the future, we would like to investigate how to effectively make use of the abundant monolingual data with human-labeled product category information that we have available for the envisioned e-commerce application."
    ],
    "869": [
        "Our approach enables training of tensors under conditions where individual training would result in low-quality or null tensors.",
        "Collaborative training often outperforms individual training even with all of the available training data.",
        "Using distributed vectors achieves state-of-the-art results on some datasets.",
        "Low-rank approximations can be used to improve the quality of tensors.",
        "Individual training would result in low-quality or null tensors."
    ],
    "873": [
        "The proposed character-based compositional architectures can improve semantic similarity tasks.",
        "The simplest architecture converges fastest to high performance in NLP tasks.",
        "Practitioners should begin with simple architectures rather than moving immediately to RNNs and CNNs.",
        "The use of negative examples taken from a mini-batch during optimization can improve the performance of the model.",
        "The proposed model can achieve better performance with careful empirical comparison."
    ],
    "874": [
        "The performance of SCITE is still limited by the insufficiency of high-quality annotated data.",
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "State-of-the-art models do not efficiently map word embeddings to model-theoretic vectors in these datasets."
    ],
    "877": [
        "The proposed memory augmented neural networks have achieved state-of-the-art results on five representative NLP tasks.",
        "NSE is capable of building an efficient architecture for single, shared, and multiple memory accesses for a specific NLP task.",
        "For the NLI task, NSE accesses premise encoded memory when processing hypothesis.",
        "For the QA task, NSE accesses answer encoded memory when reading question for QA.",
        "In machine translation, NSE shares a single encoded memory between encoder and decoder.",
        "The initial state of the NSE memory stores information about each word in the input sequence.",
        "Different variations of word representations such as character-based models can be used for memory initialization.",
        "We plan to extend NSE so that it learns to select and access a relevant subset from a memory set.",
        "One could also explore unsupervised variations of NSE, such as training them to produce encoding memory and representation vector of entire sentences or documents using either new or existing models."
    ],
    "878": [
        "'ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.'",
        "'Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.'",
        "'We explore the use of a projection-based method for attenuating biases.'",
        "'The proposed AoA Reader aims to compute the attentions not only for the document but also the query side, which will benefit from the mutual information.'",
        "'Our model could give consistent and significant improvements over various state-of-the-art systems by a large margin.'",
        "'We believe that our model is general and may apply to other tasks as well.'",
        "'We are going to fully investigate the usage of this architecture in other tasks.'",
        "'We are interested to see if the machine really \"comprehend\" our language by utilizing neural networks approaches, but not only serve as a \"document-level\" language model.'",
        "'In this context, we are planning to investigate the problems that need comprehensive reasoning over several sentences.'"
    ],
    "881": [
        "Our approach, which incorporates character n-grams into the skipgram model, outperforms baselines that do not take into account subword information.",
        "Our model trains fast and does not require any preprocessing or supervision.",
        "We will open source the implementation of our model to facilitate comparison of future work on learning subword representations.",
        "Our method is related to an idea introduced by Sch\u00fctze (1993).",
        "Our model does not rely on morphological analysis, and yet outperforms methods relying on morphological analysis."
    ],
    "882": [
        "not much understanding is there in the current literature\" - This claim suggests that there is limited knowledge or understanding about the architecture for bisequence classification tasks, indicating a need for further research in this area.",
        "winning architecture recipes for different kinds of datasets\" - This claim highlights the importance of identifying effective architectures for various types of datasets, which could inform future research and improve performance in these areas.",
        "established deep learning based baselines for argument mining tasks with zero feature engineering\" - This claim indicates that the authors have established a baseline for argument mining tasks using deep learning techniques, which could serve as a starting point for future research.",
        "adding attention on top of winning simple architectures fare in terms of benchmark performance\" - This claim suggests that adding attention to existing architectures may improve performance, but further research is needed to confirm this and determine the best approach."
    ],
    "883": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Integrating DLM-based features into the parser improves parsing performance.",
        "Our approach can improve part-of-speech tagging for Chinese."
    ],
    "886": [
        "We target on domain-adaptive and robust conversation generation by end-to-end learning without any feature-engineering.",
        "The Context-Attn model also outperforms traditional seq2seq models on perplexity tests.",
        "Training on the QA dataset helps conversation generation from two aspects: 1) Gain context-awareness from the question-label learning; 2) Gain additional robustness from the question-answer learning.",
        "Our proposed model is demonstrated to generate interesting and context-sensitive responses from variations of source sentences.",
        "Our future works will be further improving the robustness and consistency."
    ],
    "888": [
        "The proposed system is able to interpret and produce natural language phrases with dynamic and static spatial relations, making it useful for human-robot interaction in aspects of the environment.",
        "The system can be used in question-answer scenarios or command-driven human-robot interfaces, and can understand the need to move to a certain location based on descriptions of regions, source, and goal.",
        "The system is robust to perturbations in visual processing and language transmission, and integrates various cues from vision, reasoning, semantics, and syntax to cope with robustness issues.",
        "The system can be tested for command language with human subjects, and has the potential to improve the quality of generated responses by using proper detection methods such as self-attention.",
        "The proposed ReCoSa model outperforms existing HRED models and attention variants, and can be further improved by introducing topical information or considering detailed content information in the relevant contexts."
    ],
    "891": [
        "Our results show that modeling phrase-level co-occurrence and phrase compositionality helps improve word and phrase similarity tasks.",
        "If the phrases contain syntactic information, it would also help improve syntactic tasks.",
        "As our compositionality function is very general, it would be interesting to see different variations and choices of composition function in different applications with neural networks.",
        "Our compositionality function is very general, which suggests that it could be useful in a variety of applications."
    ],
    "896": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our proposed model can easily capture phrase-level alignment.",
        "Experiments on Stanford Natural Language Inference Corpus demonstrate the efficacy of our proposed model and its superiority to competitor models.",
        "We have made an elaborate experiment design and case analysis to evaluate the effectiveness of our syntax-based matching model and explain why attention over trees is a good idea.",
        "In future, we wish to use our SAT-LSTMs matching model to learn the representation of phrasal or syntactic paraphrases from massive paraphrase dataset."
    ],
    "900": [
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our approach utilizes \"supervised\" alignments, and puts the alignment cost to the NMT objective function, directly optimizing the attention model in a supervised way.",
        "Experiments show significant improvements in both translation and alignment tasks over a very strong LVNMT system."
    ],
    "904": [
        "Our proposed approach significantly outperforms the state-of-the-art method [3] and a relevant method originally utilized for image annotation [20].",
        "Our model is generic and works for any unseen pair of topic and image.",
        "Our approach outperforms benchmark models across different datasets.",
        "We have performed extensive experiments on three public datasets of machine reading comprehension: CoQA, QuAC, and TriviaQA.",
        "The model could have knowledge beyond the current segment when selecting answers.",
        "We add a recurrent mechanism to allow the information to flow across segments.",
        "Our chunking policy network for machine reading comprehension enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning."
    ],
    "909": [
        "We have presented a reordering approach for phrase-based SMT, guided by sparse dependency swap features.",
        "We have contributed a new approach for learning and performing reordering in phrase-based MT by the incorporation of dependency-based features.",
        "Our experiments show that utilizing source dependency parse for reordering sentences helps to significantly improve translation quality over a phrase-based baseline system with state-of-the-art reordering orientation models."
    ],
    "910": [
        "The proposed method achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic Memory Induction Networks (DMIN) can be a learning mechanism more general than what has been used here for few-shot learning.",
        "The model utilizes external working memory with dynamic routing to track previous learning experience and adapt to unseen classes.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to promote clustering and align similar class distribution across domains.",
        "The approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The proposed Cosine Annealing Strategy combines the Clustering Promotion Mechanism and Adversarial Distribution Alignment to generate pseudo labels.",
        "The use of pseudo-labeled target-domain data and labeled source-domain data improves the domain adaption performance.",
        "The Brown clustering algorithm has applications in creating class-based language models and providing features for Natural Language Processing tasks, but its quality needs to be improved.",
        "The author has revealed some weaknesses in the Brown clustering algorithm and proposed two heuristics to address them.",
        "The author's proposals, ALLSAME and RESORT, consistently achieve better values of average mutual information, but high values of AMI are not strong indicators for high performing language models."
    ],
    "911": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptive search for efficient BERT models using Neural Architecture Search can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Using an external lexicon and heterogeneous labeled data can improve performance on Chinese WS for Weibo Text.",
        "The coupled approach of combining the outputs of different base models is consistently more effective than the guide-feature based approach in exploiting multiple heterogeneous data.",
        "The joint WS&POS model produces better WS results than a pure WS model, indicating that POS tags are helpful for determining word boundaries.",
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size."
    ],
    "914": [
        "The similarity scale of the SimLex-999 dataset is different from other word similarity benchmarks, with antonymous pairs assigned relatively low similarities.",
        "Tweaking the similarity scale of the system by dividing similarity scores between antonyms can result in significant performance improvement on the SimLex-999 dataset.",
        "The performance on the SimLex-999 dataset increased when the similarity of a word pair was divided by five whenever the two words belonged to synsets linked by the antonymy relation.",
        "The increase in performance on the SimLex-999 dataset was significant, with Pearson correlation scores improving from 54.2 to 61.1 and Spearman correlation scores improving from 51.7 to 59.0."
    ],
    "916": [
        "The word embedding approach for measuring similarity between a query and a document is efficient and effective, even with the Q \u2192 D flow only.",
        "The proposed approach cannot distinguish documents when they include all query words, but surprisingly, the word embedding approach shows remarkable performance on the TREC Genomics datasets.",
        "The semantic measure allows for further improving BM25 ranking performance, and is an important feature for IR that is closely related to user clicks.",
        "Our approach is more straightforward and can be easily added as a feature in the current PubMed relevance search framework, without adding much overhead to the system.",
        "The semantic measure improves ranking performance without requiring slow training or deep learning solutions."
    ],
    "918": [
        "The proposed bi-directional attention model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The bi-directional attention model incorporating the agreement objective during training is able to implicitly capture the high-order parsing history without suffering from the issue of high computational complexity for graph-based dependency parsing.",
        "The proposed memory-network-based dependency parser achieves state-of-the-art performance in 6 languages in terms of UAS, demonstrating the effectiveness of the proposed mechanism of bi-directional attention with agreement and its use in dependency parsing.",
        "The proposed BiAtt-DP model outperforms all graph-based parsers for English and achieves state-of-the-art performance in 14 languages.",
        "The squared Hellinger distance can be upper bounded by 2 H 2 (p, g) + H 2 (q, g), where p and q are two k-simplexes and g is a simplex.",
        "The KL-divergence can be bounded by 2 D(g||p) + D(g||q), where g is a simplex and p and q are two k-simplexes."
    ],
    "923": [
        "Experimental evaluation on Visual Madlibs dataset shows effectiveness of averaging over object proposals representation.",
        "Achieves state-of-the-art performance on multi-choice \"filling the blank\" task.",
        "Different parameters affecting proposal obtainment have a significant impact on model performance.",
        "More proposals lead to better overall performance, and highly overlapping proposals even benefit the model.",
        "Proposed representation can be considered a valid alternative to 'soft' attention representations like those in recent work for visual question answering using memory networks.",
        "CNN+LSTM approach that chooses a prompt completion candidate by comparing directly in the embedding space is effective for image question answering tasks."
    ],
    "924": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Our approach projects the input space into a low-dimensional representation, and then converts it back into an instance in the output space.",
        "Our technique is promising for generating text from images."
    ],
    "926": [
        "The proposed model, C2V2L, outperforms previous work on the TweetLID task.",
        "Smoothed character n-gram language models can be effective as classifiers for language ID for short texts.",
        "The ngram model achieved better performance than eleven out of twelve submissions in the TweetLID shared task.",
        "The ngram model provides the best reported performance on the Twitter70 dataset, despite having limited training data for some languages.",
        "The proposed model is adaptable to analyzing code-switching and has shown good performance without any change to the architecture."
    ],
    "929": [
        "We have demonstrated the complexity of the WIKIREADING task and its suitability as a benchmark to guide future development of DNN models for natural language understanding.",
        "End-to-end sequence-to-sequence models are the most promising.",
        "Our character-level model improved substantially after language model pretraining, suggesting that further training optimizations may yield continued gains.",
        "Document length poses a problem for RNN-based models, which might be addressed with convolutional neural networks that are easier to parallelize.",
        "These models are not intrinsically limited to English, as they rely on little or no pre-processing with traditional NLP systems, which means that they should generalize effectively to other languages."
    ],
    "931": [
        "The effect of pre-processing by lemmatization on the interpretability of topic models in a morphologically rich language has been measured, and empirical justification for this practice has been found.",
        "Our approach to measuring the effects of lemmatization is distinct from prior studies, and further work is required to determine what factors contribute to our different conclusions.",
        "The practice of stemming during post-processing is recommended, as it can improve the interpretability of topic models.",
        "The effectiveness of lemmatization in improving the interpretability of topic models should be measured rather than assumed.",
        "Prior work has suggested stemming during post-processing as needed, and our study echoes this suggestion."
    ],
    "932": [
        "By including named entities to the set of opinion words, the obtained topic-viewpoint associations become more consistently correct compared to the opinion words used in previous studies.",
        "Using the learned relationships between the modalities, groups of topics-aspect are formed, creating human interpretable representations of viewpoints.",
        "We introduce a principled method to evaluate which viewpoint a topic or aspect is associated with, by leveraging the magnitudes and signs of the feature weights of a linear SVM.",
        "The learned groups of topics-aspect are contextually coherent, and consistently correctly associated with the viewpoints."
    ],
    "937": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "the experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "the proposed ReCoSa model can improve the quality of multiturn dialogue generation by using proper detection methods, such as self-attention.",
        "introducing topical information to make the detected relevant contexts more accurate",
        "considering the detailed content information in the relevant contexts to further improve the quality of generated response."
    ],
    "941": [
        "The beam problem in neural machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The proposed ReCoSa model utilizes the self-attention mechanism to effectively capture the long distant dependency relations.",
        "The use of proper detection methods, such as self-attention, can improve the quality of multiturn dialogue generation.",
        "Introducing topical information or considering detailed content information can further improve the quality of generated responses."
    ],
    "943": [
        "'We propose using context gates in NMT to dynamically control the contributions from the source and target contexts in the generation of a target sentence, to enhance the adequacy of NMT.'",
        "'Experimental results show that NMT with context gates achieves consistent and significant improvements in translation quality over different NMT models.'",
        "'Context gates are in principle applicable to all sequence-to-sequence learning tasks in which information from the source sequence is transformed to the target sequence (corresponding to adequacy) and the target sequence is generated (corresponding to fluency).'",
        "'In the future, we will investigate the effectiveness of context gates to other tasks, such as dialogue and summarization.'",
        "'It is also necessary to validate the effectiveness of our approach on more language pairs and other NMT architectures (e.g., using LSTM as well as GRU, or multiple layers).'"
    ],
    "945": [
        "We presented a large-scale multilingual corpus of disambiguated glosses.",
        "Our method outperforms previous approaches and a standard state-of-the-art disambiguation system in terms of coverage, precision, and recall.",
        "We obtained a fully disambiguated corpus of textual definitions coming from multiple sources and multiple languages, which constitutes the largest available corpus of its kind.",
        "Our refined version of the corpus has great potential in high-precision low-coverage applications, where having a disambiguation error as low as possible is the first requirement.",
        "Our system outperforms previous approaches and a standard state-of-the-art disambiguation system in terms of coverage, precision, and recall."
    ],
    "947": [
        "Using Bi-LSTM-RNN model based on low-cost sequence features can effectively classify relations.",
        "The middle context plays the most important role in relation classification.",
        "Dependency parsing is not necessary for relation classification.",
        "Reducing noisy information in contexts is worth studying."
    ],
    "955": [
        "The equal weight of all references might not be a good idea for gauging the success of research and tracking follow-up work or recommending research papers.",
        "The annotated dataset has tremendous potential to be utilized for other research.",
        "GraLap can be used for any semi-supervised learning problem.",
        "Each application mentioned here needs separate attention.",
        "In future, the authors plan to look into more linguistic evidences to improve their model."
    ],
    "956": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our proposed recurrent chunking mechanisms outperform benchmark models across different datasets.",
        "We have developed a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "The approach of combining reinforcement learning with recurrent mechanisms allows the model to have knowledge beyond the current segment when selecting answers.",
        "The use of a new corpus annotated with citation function and centrality reveals salient behaviors of writers, readers, and the field as a whole.",
        "Authors are sensitive to discourse structure and venue when citing.",
        "Readers follow temporal threads of ideas in their browsing of citation networks rather than methodological threads.",
        "The way in which an author relates their work to the literature is predictive of the number of citations it receives.",
        "The NLP field as a whole has seen increased consensus in what constitutes valid work, with a reduced need for positioning and excessive comparison."
    ],
    "962": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Joint training of intent detection and language modeling achieves advantageous performance in intent detection and language modeling with slight degradation on slot filling compared to independent training models.",
        "Our joint model produces 11.8% relative reduction on LM perplexity, and 22.3% relative reduction on intent detection error when using true text as input.",
        "The joint model also shows consistent performance gain over the independent training models in the more challenging and realistic setup using noisy speech input."
    ],
    "963": [
        "A mismatched crowd is reluctant to use a Reject Option.",
        "In future, we may have to provide improved interaction to un-bias the crowd.",
        "Even among fine-granular (phonetically proximate) choices, a mismatched crowd can indeed identify the correct option.",
        "Some language pairs or demographies may be better matches.",
        "Voting errors are not correlated as shown by the gain in accuracy through even majority voting based strategies."
    ],
    "965": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Lexical entries extracted from different but similar robotic tasks can be shared across tasks.",
        "Machine teaching by paraphrasing can be integrated with active learning techniques.",
        "Robots can perform reasoning over the confusing phrases and actively ask their human partners for paraphrasing."
    ],
    "968": [
        "The proposed end-to-end automatic speech recognition system achieves competitive results on the LibriSpeech corpus with MFCC features, with a WER of 7.2%.",
        "The system also achieves promising results with power spectrum and raw speech, with WERs of 9.4% and 10.1%, respectively.",
        "The AutoSegCriterion used in the system is faster than CTC and as accurate.",
        "The approach breaks free from HMM/GMM pre-training and force-alignment, and is less computationally intensive than RNN-based approaches.",
        "The system processes a LibriSpeech sentence in less than 60ms on average, and the decoder runs at 8.6x on a single thread."
    ],
    "969": [
        "We presented a unified computational approach for studying constrained language.",
        "Many of the features were theoretically motivated.",
        "Translations and nonnative productions are two distinct language varieties, but they share similarities.",
        "The language modeling experiments reveal salient ties between the native language of non-native speakers and the source language of translationese.",
        "Our findings are intriguing: native speakers and translators use their native language, yet translation seems to gravitate towards non-native language use.",
        "The main contribution of this work is empirical, establishing the connection between these types of language production.",
        "We believe that these common tendencies are not incidental, but more research is needed to establish a theoretical explanation for the empirical findings.",
        "We intend to replicate the experiments with more languages and more domains, investigate additional varieties of constrained language, and employ more complex lexical, syntactic, and discourse features."
    ],
    "972": [
        "Our approach outperforms benchmark models across different datasets.\" (related to the effectiveness of the proposed method)",
        "We first decompose the learning problem into three tractable subproblems: learning within-event structures, learning event-event relations, and learning for entity extraction.\" (related to the methodology of the approach)",
        "Experimental results demonstrate that our approach outperforms the state-of-the-art event extractors by a large margin and substantially improves a strong entity extraction baseline.\" (related to the results of the proposed method)",
        "We plan to integrate entity and event coreference as additional components into the joint inference framework.\" (related to future work)",
        "We are also interested in investigating the integration of more sophisticated event-event relation models of causality and temporal ordering.\" (related to potential future research directions)"
    ],
    "975": [
        "attention mechanism in NMT is worse than conventional word alignment models in its alignment accuracy.",
        "the proposed approach achieves better alignment results leading to significant gains relative to standard attention-based NMT.",
        "the atten-tion mechanism from the point view of reordering.",
        "supervised attention for NMT with guidance from external conventional alignment models, inspired by the supervised reordering models in conventional SMT."
    ],
    "976": [
        "The proposed model achieves significantly higher transliteration quality compared to traditional statistical models.",
        "The model consists of two components: encoding a source name sequence and decoding to a target name sequence.",
        "Different parts of the proposed model are jointly trained using stochastic gradient descent to minimize the log-likelihood.",
        "The model is able to achieve state-of-the-art results with hyperparameter optimization, multi-task sequence to sequence learning, and multiway transliteration.",
        "The proposed model is based on successful studies in sequence to sequence learning.",
        "The model is not optimized for achieving state-of-the-art results in this paper, leaving room for future work."
    ],
    "977": [
        "Our method obtains performance on par with the exact model, while achieving a speed-up going from 2\u00d7 to 10\u00d7 compared to the exact model.",
        "Our approach is general enough to be applied to other parallel computing architectures and other losses, as well as to other domains where the distributions of the class are unbalanced.",
        "Our method consistently maintains a low perplexity while enjoying a speed-up going from 2\u00d7 to 10\u00d7 compared to the exact model.",
        "This is achieved by explicitly taking into account the computation time of matrix-multiplication on parallel systems and combining it with a few important observations, namely keeping a shortlist of frequent words in the root node (Schwenk, 2007) and reducing the capacity of rare words (Chen et al., 2015)."
    ],
    "979": [
        "The proposed hybrid feature extraction algorithm based on MFCCs successfully implements frequency modulation, loudness induction, and time-varying induction with a simple 2D psychoacoustic filter.",
        "The adaptive scheme of the proposed algorithm better reflects the frequency-dependent property of masking effects in the human auditory system.",
        "The speech spectrum is divided into multiple bands, and different psychoacoustic filters are designed to better fit the specific frequency band.",
        "The proposed method does not need any additional training process, making the computational burden very low.",
        "The proposed algorithm can be easily combined with other algorithms.",
        "The double transform analysis technique enables quantitative analysis of the performance of time-frequency domain filters for different noise types.",
        "Extensive comparison is made against state-of-the-art ASR algorithms based on the AURORA2 database, and statistically significant improvements are achieved as manifested in the experimental results."
    ],
    "980": [
        "We proposed an NMT architecture which produces a factored representation of the target language words.",
        "We showed that we are able to train Factored NMT systems with similar performance to word based systems but with the advantage of modeling an almost 6 times bigger word vocabulary with only a slight increase of the computational cost.",
        "The use of additional linguistic resources allows us to generate new word forms that would not be included in the standard NMT system shortlist.",
        "By reducing the target language vocabulary, we simulated an out-of-domain setup, and we showed that our factored NMT method performs better than the basic NMT system in this case.",
        "The proposed Factored NMT method could even show better performance if applied on highly inflected languages like German, Arabic, Czech, Russian or Hindi on the target side."
    ],
    "982": [
        "We present the first approach for applying distant supervision to cross-sentence relation extraction.",
        "Our DIS-CREX system doubled the yield of unique interactions, while maintaining the same accuracy.",
        "Using distant supervision, DISCREX improved the coverage of the Gene Drug Knowledge Database (GDKD) by two orders of magnitude, without requiring annotated examples.",
        "Future work includes: further exploration of features; improved integration with coreference and discourse parsing; combining distant supervision with active learning and crowd sourcing; evaluate the impact of extractions to precision medicine; applications to other domains."
    ],
    "984": [
        "We have presented a visual attention-based model for OCR of presentational markup.",
        "Our new dataset IM2LATEX-100K provides a test-bed for this task.",
        "We propose a coarse-to-fine attention layer to reduce the attention complexity.",
        "The coarse-to-fine attention mechanism is general and directly applicable to other domains, including document summarization.",
        "Our contributions provide a new view on the task of structured text OCR, and show data-driven models can be effective without any knowledge of the language."
    ],
    "986": [
        "The proposed model and algorithm can mitigate the effects of confounding factors in the learning process.",
        "The representation of utterances forms clear clusters, with each cluster belonging to one person.",
        "Despite individual differences, the model learns to assign more weights to certain dimensions after noise is introduced.",
        "The inter-cluster distance ratio is higher for representations clustered by category of sentiment compared to those clustered by individual's identity, indicating a clearer clustering structure.",
        "SAL almost maintains the clustering structure of identity but greatly improves the clustering structure of category of sentiment.",
        "The increased inter-cluster distance ratio for representations clustered by category of sentiment and individual's identity indicates the effectiveness of SAL."
    ],
    "992": [
        "modern approaches to natural language processing offer a promising avenue for the extraction of politically-relevant events.",
        "these methods can work across both ontologies and languages, offering a level of flexibility unseen in the realm of CAMEO-coded political event data.",
        "the implementation of these methods will allow for the exploration of languages and ontologies beyond the limits of CAMEO.",
        "these methods are promising, but there is still much work left to develop a fully operational event extraction pipeline.",
        "there is still the issue of handling the many nuances of event coding, such as the various combinations of actors and binary relations.",
        "the methods presented on this paper do not touch upon the extraction of actor information, which is another area for which modern NLP approaches are highly applicable and will likely improve on the state-of-the-art."
    ],
    "993": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our model can achieve state-of-the-art performance on recognizing implicit discourse relations.",
        "[Paragraph 1] Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "[Paragraph 2] Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "[Paragraph 4] ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "[Paragraph 5] Our model can achieve state-of-the-art performance on recognizing implicit discourse relations."
    ],
    "997": [
        "We have applied character-level CNNs to large-scale authorship attribution.",
        "We have extensively evaluated combinations of different CNN input channels and introduced a novel model that combines character and word channels to leverage both stylistic and topical information.",
        "We have compared CNNs against state-of-the-art methods for a large range of author numbers, shedding new light on traditional approaches.",
        "We have presented state-of-the-art results for four out of five datasets in different domains.",
        "We have introduced two new Twitter and reddit datasets that we make available for further research."
    ],
    "1005": [
        "We proposed the STKRL model, a novel model for knowledge representation learning, which jointly considers triple facts and sequential text information.",
        "Our models achieve significant improvements compared to other baselines.",
        "We will explore the following further work: (1) We assume that for entity representations, there are naturally three types of representations including word, sentence and knowledge representations. (2) The STKRL model can extract definition sentences of entities as by-products."
    ],
    "1009": [
        "The proposed TSP solver with a few real-valued features can be useful for AMR-to-text generation.",
        "The method based on a set of graph-to-string rules is significantly better than a PBMT-based baseline.",
        "The rule induction algorithm is effective in finding better solutions than beam search.",
        "The TSP solver finds better solutions than beam search."
    ],
    "1013": [
        "The pointer sentinel mixture model achieves state-of-the-art results in language modeling over the Penn Treebank dataset, using few additional parameters and little additional computational complexity at prediction time.",
        "There is a need to move from the Penn Treebank dataset to a new language modeling dataset that can handle long range dependencies, providing WikiText-2 and WikiText-103 as potential options.",
        "The new dataset (WikiText-2 and WikiText-103) has the potential to improve handling of rare words and the usage of long term dependencies in language modeling."
    ],
    "1016": [
        "Our approach outperforms benchmark models across different datasets.\" (related to the effectiveness of the proposed method for machine reading comprehension)",
        "We have performed extensive experiments on three public datasets of machine reading comprehension: CoQA, QuAC, and TriviaQA.\" (related to the experimental evaluation of the proposed method)",
        "By modeling the compositions of vector representations of author, entity, and mention, our approach is able to exploit the social network as a source of contextual information.\" (related to the use of vector compositions for exploiting social networks)",
        "To avoid predicting overlapping entity mentions, we employ a structured prediction algorithm, and train the system with loss-augmented decoding.\" (related to the methodology of the proposed approach)",
        "Exploiting these networks is a possible direction for future work.\" (related to potential future research directions)",
        "We would also like to investigate other metadata attributes that are relevant to the task, such as spatial and temporal signals.\" (related to the possibility of incorporating additional metadata attributes for improving the task)"
    ],
    "1017": [
        "We presented a framework for creating customized embeddings tailored to the information need of causal questions.",
        "We trained three popular models (embedding, alignment, and CNN) using causal tuples extracted with minimal supervision by bootstrapping cause-effect pairs from free text, and evaluated their performance both directly (i.e., the degree to which they capture causality), and indirectly (i.e., their real-world utility on a high-level question answering task).",
        "Our analysis suggests that the models that perform best in the real-world QA task are those that have consistent performance across the precision-recall curve in the direct evaluation.",
        "In QA, where the vocabulary is much larger, precision must be balanced with high recall, and this is best achieved by our causal embedding model.",
        "Additionally, we showed that vanilla and causal embedding models address different information needs of questions, and can be combined to improve performance.",
        "Extending this work beyond causality, we hypothesize that additional embedding spaces customized to the different information needs of questions would allow for robust performance over a larger variety of questions, and that these customized embedding models should be evaluated both directly and indirectly to accurately characterize their performance."
    ],
    "1018": [
        "The proposed GNMT system achieves high accuracy and speed for machine translation tasks, approaching or surpassing all currently published results on a public benchmark.",
        "Wordpiece modeling effectively handles open vocabularies and morphologically rich languages for translation quality and inference speed.",
        "A combination of model and data parallelism can be used to efficiently train state-of-the-art sequence-to-sequence NMT models in roughly a week.",
        "Model quantization drastically accelerates translation inference, allowing the use of large models in a deployed production environment.",
        "Length-normalization, coverage penalties, and other details are essential to making NMT systems work well on real data.",
        "The GNMT system delivers a 60% reduction in translation errors compared to the previous phrase-based production system on several popular language pairs."
    ],
    "1019": [
        "Our proposed segment-to-segment neural transduction model tackles the limitations of vanilla encoder-decoders by introducing a latent segmentation that determines correspondences between tokens of the input and output sequences.",
        "By employing a unidirectional LSTM as encoder, our model is capable of doing online generation.",
        "Experiments on two representative natural language processing tasks showed that our model significantly outperforms encoder-decoder baselines while requiring much smaller hidden layers.",
        "Our model learns to generate and align jointly during training, using dynamic programming to marginalize out the hidden alignment.",
        "For future work, we would like to incorporate attention-based models to our framework to enable such models to process data online."
    ],
    "1025": [
        "The proposed knowledge-based weak training approach for PLDA is effective in speaker verification, especially when human-labelled data are limited.",
        "The assumption that speakers in different sessions are different can be used to produce easy-to-obtain weak labels, which can be used to train PLDA.",
        "The weak training approach outperforms the unsupervised adaptation method when the 'different session, different speaker' assumption is held.",
        "Future work will investigate the possibility of utilizing both knowledge-based weak labels and model-based weak labels, as well as active learning to select the most valuable data for human labeling."
    ],
    "1027": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our reward-rescaling approach also increases the model's accuracy, resulting in significant gains over the current state-of-the-art."
    ],
    "1029": [
        "Our experimental results are quite satisfying and raise a few interesting issues.",
        "It suggests that predicting equation parses using a pipeline of structured predictors performs better than jointly trained alternatives.",
        "As discussed, it also points out the limitation of the current NLP tools in supporting these tasks.",
        "Our current formulation has one key limitation; we only deal with expressions that are described within a sentence.",
        "Our future work will focus on lifting this restriction, in order to allow relations expressed across multiple sentences and multiple relations expressed in the same sentence."
    ],
    "1030": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Our model significantly outperforms existing HRED models and its attention variants.\" (Paragraph 1)",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.\" (Paragraph 2)",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).\" (Paragraph 3)"
    ],
    "1035": [
        "Our approach uses recurrent neural networks (RNNs) to induce multilingual text analysis tools.",
        "We have studied Simple and Bidirectional RNN architectures on multilingual POS and SST tagging.",
        "Our approach provides truly multilingual taggers, with one tagger for N languages.",
        "Our method uses a language-independent word representation based only on word co-occurrences in a parallel corpus.",
        "Our approach can be easily adapted to a new target language when a small amount of supervised data is available.",
        "We have shown the effectiveness of our method in a weakly supervised context.",
        "Short-term perspectives include applying multi-task learning to build systems that simultaneously perform syntactic and semantic analysis.",
        "Adding out-of-language data to improve our RNN taggers is also possible with our common (multilingual) vector representation."
    ],
    "1045": [
        "'We show the benefit of features based on word embedding for sarcasm detection.'",
        "'Our features use the similarity score values returned by word embeddings, and are of two categories: similarity-based and weighted similarity-based.'",
        "'Experiments with four past works in sarcasm detection show that our features improve performance by at most 5%.'",
        "'Word2Vec and dependency weight-based features outperform LSA and GloVe.'",
        "'Our word embedding-based features may work better if the similarity scores are computed for a subset of words in the sentence, or using weighting based on syntactic distance instead of linear distance as in the case of our weighted similarity-based features.'"
    ],
    "1047": [
        "The performance of Word2vec embeddings decreases in small corpora.",
        "LSA outperforms Word2vec when trained with medium-sized datasets.",
        "LSA can accurately capture semantic word relations in cases of low-frequency target words and low number of dreams.",
        "The application of word embeddings to the analysis of dreams content can provide new insights into this old research area of psychology.",
        "LSA can be used to explore words associations in dreams reports."
    ],
    "1050": [
        "The authors propose a novel approach to improving word embeddings by reducing noise in state-of-the-art models.",
        "The proposed models use a deep feed-forward neural network filter to reduce noise in word embeddings.",
        "The first model generates complete word denoising embeddings, while the second model yields overcomplete word denoising embeddings.",
        "The word denoising embeddings outperform the originally state-of-the-art word embeddings on several benchmark tasks."
    ],
    "1059": [
        "The attention mechanism in the dynamic window approach for CCG supertagging is effective on both MLPs and RNNs.",
        "Using dropout on the dynamic window improves the generalization performance of the model.",
        "The logistic gates used in the model filter the context window surrounding the center word, which shows effectiveness in the task.",
        "The model does not need an encoder to memorize the input and another decoder to generate the output, as the attention mechanism can be seen as an attention-based system.",
        "The difference between the two attention mechanisms lies in the type of memories used, with the encoder-decoder architecture using a weighted average of the output of the encoder.",
        "The model can be easily applied to other sequence tagging tasks such as POS tagging and named entity recognition (NER)."
    ],
    "1060": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Incorporating attention into the Child-Sum Tree-LSTM and Tree-GRU can be effective for modelling sentence pairs and outperform all non-attentional counterparts.",
        "Our attentive models are effective for modelling sentence pairs and can outperform all non-attentional counterparts.",
        "In the future, we will evaluate our models on the other sentence pair modelling tasks (such as RTE) and extend them to the seq2seq learning framework."
    ],
    "1061": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The beam problem can be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "More research is needed to determine how much improvement remains to be gained by solving label bias in general.",
        "Our model can be extended to networks with more diverse contents such as images."
    ],
    "1067": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The novel end-to-end model for joint slot label alignment and recognition does not require external label projection.",
        "Multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "The model improves particularly on non-conventional spelling and fragmented noun phrases typical for Twitter.",
        "Keystroke data captures mostly structural shallow syntactic information, but not significant morphosyntactic information.",
        "Further experiments are warranted to explore the use of keystroke logging data in more ways."
    ],
    "1070": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Using parse trees for identifying the context of each location can improve the baselines.",
        "Data augmentation can be used to make the models and especially LSTM more robust to variations in the data.",
        "Our new dataset provides a valuable resource for researchers to explore and develop new models.",
        "The task of targeted aspect-based sentiment analysis is an important and challenging task that has not been well explored in previous work."
    ],
    "1071": [
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone.",
        "For the wellcharacterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Our system is able to generate domain-relevant questions in wide scope, while human effort is significantly reduced.",
        "Questions generated by our system are significantly better than those from Serban et al. (2016) on 500 random-selected triples from Freebase.",
        "Our current system only generates questions without answers, leaving automatic answer mining as our future work."
    ],
    "1073": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The beam problem can largely be explained by the brevity problem.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "More general globally-normalized models can be trained in a similarly inexpensive way."
    ],
    "1074": [
        "The proposed method for session segmentation in open-domain dialogue systems is effective.",
        "The use of embedding learning and similarity measuring in the approach improves the performance of a retrieval-based dialogue system.",
        "The novel notion of virtual sentences used in training embeddings is effective in session segmentation.",
        "The approach based on embedding-enhanced TextTiling shows improved performance compared to traditional retrieval-based dialogue systems.",
        "The use of heuristics for similarity measure is beneficial in session segmentation."
    ],
    "1076": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We propose a simple yet effective INTERACTIVE ATTENTION approach, which models the interaction between the decoder and the representation of source sentence during translation by using reading and writing operations.",
        "Our empirical study on Chinese-English translation shows that INTERACTIVE ATTENTION can significantly improve the performance of NMT."
    ],
    "1080": [
        "Neural MT is not without limitations; on language pairs that have abundant amount of monolingual and bilingual train data, phrase-based MT still perform better than Neural MT.",
        "Neural MT is limited on the vocabulary size and deficient utilization of monolingual data.",
        "Attention mechanism is still not at the satisfactory status and it needs to be more accurate for better controlling the translation output and for better user interactions.",
        "We have begun to making even more experiments with injection of various linguistic knowledges.",
        "We will also apply our engineering know-hows to conquer the practical issues of NMT one by one."
    ],
    "1081": [
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We contrast the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "The latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "WANs were built for a large set of texts in the corpus of the analyzed authors and were compared via a measure of relative entropy.",
        "The profile networks were then compared to one another to determine the general similarity between author styles.",
        "An attribution accuracy of 92.6% was achieved when attributing amongst all authors.",
        "The classification power was then further evaluated with respect to plays written by multiple authors, both through the attribution of an entire play as well as its individual act and scene components.",
        "We overall find WANs to be simple yet effective tools in distinguishing between playwrights from the Early Modern era by considering relational structures between function words not previously considered in authorship attribution studies from this time period."
    ],
    "1083": [
        "The authors have used a contemporary bidirectional LSTM-CRF model for clinical concept extraction.",
        "The bidirectional LSTM-CRF model has shown promising results in recognizing clinical concepts with end-to-end recognition initialized with general purpose off-the-shelf word embeddings.",
        "The authors have used the bidirectional LSTM-CRF model for concept extraction from clinical records, and their approach is the first to adopt this model for this task.",
        "The experimental results of the bidirectional LSTM-CRF model are close to the state of the art.",
        "Initializing the training of the bidirectional LSTM-CRF model with unsupervised word embeddings such as Word2Vec and GloVe could further improve its performance.",
        "Using domain-specific resources such as Monitoring in Intensive Care (MIMIC) II corpus for training the unsupervised word embeddings could be a potential way to improve the performance of the bidirectional LSTM-CRF model."
    ],
    "1085": [
        "The proposed CRP-based clustering algorithm is threshold-free and takes less than two minutes to cluster a large dataset of 100 languages.",
        "The algorithm yields close or better results than LexStat for clustering putative language relations.",
        "The algorithm can be useful for comparative linguists to analyze putative language relations at a quick pace.",
        "The main limitation of the algorithm is its inability to retrieve clusters for meanings with high phonological divergence, such as \"what\", \"who\", and \"we\" in Indo-European.",
        "The algorithm groups all reflexes into a single cluster without any error when the reflexes show similar word forms, as seen in the case of Mayan (meanings: \"water\" and \"die\").",
        "In future work, the authors plan to use the CRP algorithm for clustering meanings across different language families available in the ASJP database and then supply the cognate clusters to a Bayesian phylogenetic inference software such as MrBayes for inferring Bayesian trees for the languages of the world."
    ],
    "1089": [
        "The NAIST-CMU system for the Japanese-English task at WAT achieved the most accurate results on this language pair.",
        "Incorporating discrete translation lexicons was found to be useful in achieving accurate results.",
        "Minimum risk training was found to be useful in achieving accurate results."
    ],
    "1096": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "the addition of more reordering rules improves translation quality.",
        "plans are there going for a comparative study of improved reordering system and hierarchical models."
    ],
    "1098": [
        "Our results suggest that performance variation across models is partly a function of document preprocessing effectiveness.",
        "Supervised keyphrase extraction models are more robust to noisy input.",
        "Future works should use a common preprocessing to evaluate the interest of keyphrase extraction approaches.",
        "We recommend using four levels of preprocessing for evaluating the interest of keyphrase extraction approaches."
    ],
    "1103": [
        "The beam problem in Semitic languages can be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "The discovered rules for root-and-pattern morphology in Semitic languages are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of the pattern discovery method and root extraction method (JZR) are validated with performance not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains, and more research is needed to determine how much improvement remains to be gained by solving label bias in general.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset, and it is possible to train more general globally-normalized models in a similarly inexpensive way."
    ],
    "1106": [
        "Adding features to an ANN-based model for patient note de-identification can increase recall, particularly when leveraging information from associated EHRs such as patient names and doctor names.",
        "Constructing patient note de-identification systems using structured information from EHRs, such as patient and doctor names, can be effective in a typical real-life setting.",
        "The use of EHR-derived features, such as patient and doctor names, can improve the performance of patient note de-identification systems.",
        "There is potential for improving patient note de-identification systems by incorporating more structured information from EHRs beyond just patient and doctor names.",
        "The use of structured information from EHRs can be effective in increasing the recall of patient note de-identification systems."
    ],
    "1110": [
        "The Manager-Programmer-Computer framework for neural program induction achieves new state-of-the-art results on a challenging semantic parsing dataset with weak supervision.",
        "The Neural Symbolic Machine integrates a neural sequence-to-sequence \"programmer\" with key-variable memory and a symbolic Lisp interpreter with code assistance.",
        "The use of REIN-FORCE and pseudo-gold programs found by an iterative ML training process improves the performance of the Neural Symbolic Machine.",
        "The Neural Symbolic Machine does not require any feature engineering or domain-specific knowledge.",
        "The Manager-Programmer-Computer framework is able to support abstract, scalable and precise operations through a friendly neural computer interface."
    ],
    "1118": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The proposed method has been evaluated in several popular NER and mention detection tasks, including the CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks.",
        "Our methods have yielded pretty strong performance in all of these examined tasks.",
        "This FOFE-based local detection approach can be easily extended to tackle many other NLP tasks, such as chunking, POS tagging, entity linking, semantic parsing."
    ],
    "1120": [
        "The proposed model significantly outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by the proposed model are coherent with human judgments.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The proposed model can improve the quality of multiturn dialogue generation by using proper detection methods, such as self-attention.",
        "The proposed model can be further improved by introducing topical information or considering the detailed content information in the relevant contexts.",
        "The joint model for part-of-speech tagging and dependency parsing improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages."
    ],
    "1121": [
        "The proposed model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The existing HRED based models violate an important characteristic of multi-turn dialogue generation, which is the response being related to only a few contexts.",
        "The self-attention mechanism can effectively capture long distant dependency relations.",
        "The relevant contexts detected by the proposed model are significantly coherent with humans' judgements.",
        "Existing systems do not perform well on the new SQA dataset, and there is potential for question and table rewriting strategies to handle coreferences.",
        "Incorporating large external knowledge sources into semantic parsers can resolve semantic matching errors between question text and cells or column headers in the table.",
        "The proposed model has the potential to improve the quality of multiturn dialogue generation by using proper detection methods, such as self-attention.",
        "The SQA dataset provides a new and interesting research direction for sequential question answering, which is a more natural interface for information access."
    ],
    "1124": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The beam problem can be alleviated or eliminated by solving the brevity problem.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Our hard attention model for morphological inflection generation performs better than previous neural and non-neural approaches on various datasets, while staying competitive with dedicated models even with very few training examples.",
        "Our model enables linear time decoding while staying resolution preserving, i.e. not requiring to compress the input sequence to a single fixed-sized vector.",
        "Future work may include applying our model to other nearly-monotonic alignand-transduce tasks like abstractive summarization, transliteration or machine translation."
    ],
    "1127": [
        "The training strategies for the JMT model are not obvious, and it is important to maximize the accuracy of dependency parsing on development data.",
        "The sizes of the training data are different across different tasks, and dependency parsing requires more training epochs than the semantic tasks.",
        "The same strategy for decreasing the learning rate is shared across all tasks, but developing a method for achieving the best scores for all tasks at the same time is important future work.",
        "Our joint many-task model successfully handles multiple NLP tasks with growing depth in a single end-to-end model, and our single model achieves state-of-the-art or competitive results on five NLP tasks."
    ],
    "1131": [
        "Our coarse-to-fine framework for QA over long documents quickly focuses on the relevant portions of a document.",
        "We plan to deepen the use of structural clues and answer questions over multiple documents using paragraph structure, titles, sections, and more in future work.",
        "Developing systems that can efficiently answer the information needs of users over large quantities of text requires using paragraph structure, titles, sections, and more."
    ],
    "1132": [
        "The proposed encoder-decoder-reconstructor framework for NMT improves translation performance by introducing an auxiliary score to measure the adequacy of translation candidates.",
        "The proposed approach improves parameter training for producing better translation candidates.",
        "The proposed approach consistently improves translation performance when the decoding space increases, while conventional NMT fails to do so.",
        "There is still a significant gap between de facto translation and oracle of k-best translation candidates, especially when the decoding space increases.",
        "Rich features can better measure the quality of translation candidates.",
        "Validating the effectiveness of the approach on more language pairs and other NMT architectures is necessary."
    ],
    "1134": [
        "Our method, TopicCoRank, builds two graphs: one with the document topics and one with controlled keyphrases (training keyphrases).",
        "We designed a strategy to unify the two graphs and rank by importance topics and controlled keyphrases using a co-ranking vote.",
        "Our approach benefits from both controlled keyphrases and document topics, improving both keyphrase extraction and keyphrase assignment baselines.",
        "TopicCoRank can be used to annotate keyphrases in scientific domains in a close way of professional indexers."
    ],
    "1137": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "DSCNN consistently outperforms traditional CNNs, and achieves state-of-the-art results on several sentiment analysis, question type classification and subjectivity classification datasets."
    ],
    "1142": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our approach to the ordering problem deviates from most prior work that use handcrafted features.",
        "Entity distribution patterns can provide useful features about named entities that are treated as out-of-vocabulary words.",
        "The ordering problem can be further studied at higher-level discourse units such as paragraphs, sections and chapters."
    ],
    "1143": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism.",
        "Two methods are combined by the proposed Cosine Annealing Strategy.",
        "The representation extractor is used to encode unlabeled target-domain data into features.",
        "We utilize pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR."
    ],
    "1152": [
        "CLDDC for SWLs can be improved by bridging with LWLs.",
        "The existing ranking results can be generalized to other languages.",
        "CLDDC for SWLs is not acceptable when Wikipedia for a language is not large enough.",
        "Dataless classification based on LWLs can be effective for SWLs."
    ],
    "1158": [
        "The beam problem in neural machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Incorporating character-level information into the model improves performance on every benchmark, indicating that capturing features regarding characters and morphemes is useful in a general-purpose tagging system.",
        "The attention-based model for combining character representations outperformed the concatenation method used in previous work in all evaluations.",
        "The proposed method requires fewer parameters but allows for more control over how much character-level information is used for each word, leading to improved performance on a range of different tasks."
    ],
    "1159": [
        "The proposed solution, Dynamic Memory Induction Networks (DMIN), achieves new state-of-the-art results on few-shot text classification tasks.",
        "The model can adapt and generalize better to support sets and unseen classes by leveraging external working memory with dynamic routing.",
        "The use of dynamic memory as a learning mechanism is more general than what has been used in previous few-shot learning studies.",
        "The model can effectively preserve salient source relations in summaries when combined with an abstractive summarization system.",
        "The structural models are on-par with or surpass state-of-the-art published systems on benchmark datasets.",
        "It is possible to train multilingual NMT models that can be used to translate between a number of different languages using a single model where all parameters are shared, which also improves the translation quality of low-resource languages in the mix.",
        "Zero-shot translation without explicit bridging is possible, which is the first time to our knowledge that a form of true transfer learning has been shown to work for machine translation.",
        "Adding small amounts of parallel data can improve zero-shot translation quality.",
        "The model learns a form of interlingua representation between all involved language pairs.",
        "The simple architecture makes it possible to mix languages on the source or target side to yield interesting translation examples.",
        "The approach has been shown to work reliably in a Google-scale production setting and enables quick scaling to a large number of languages."
    ],
    "1161": [
        "KEHNN can leverages prior knowledge in semantic matching.",
        "Our model can significantly outperform state-of-the-art matching models on two matching tasks.",
        "Claim 1: KEHNN can leverage prior knowledge in semantic matching. (Paragraph 3)",
        "Claim 2: Our model can significantly outperform state-of-the-art matching models on two matching tasks. (Paragraph 3)"
    ],
    "1165": [
        "The proposed set of elementary property prediction tasks can understand different tweet representations in an application-independent, fine-grained fashion.",
        "The open nature of social media provides opportunities to understand the basic characteristics of posts and draw novel insights about representation models.",
        "Among supervised models, CNN, LSTM, and BLSTM encapsulate most of the syntactic and social properties with great accuracy, while BOW, DSSM, STV, and T2V do so among unsupervised models.",
        "Tweet length affects task prediction accuracies, but all models behave similarly under variation in tweet length.",
        "LDA is insensitive to input word order, while CNN, LSTM, and BLSTM are extremely sensitive to word order."
    ],
    "1166": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Our approach achieves significant improvements in joint training for pivot-based neural machine translation.",
        "We plan to study better connection terms for our joint training."
    ],
    "1168": [
        "The proposed topic estimation criterion based on word-word co-occurrence/correlation matrix exhibits better robustness to model mismatch compared to the anchor-word assumption.",
        "The identifiability conditions of the proposed approach are proven to be milder than those of the anchor-word assumption.",
        "The proposed approach features topic identifiability guarantee under much milder conditions.",
        "The simple procedure for dealing with the formulated criterion only involves one eigen-decomposition and a few small linear programs.",
        "The proposed approach is effective in estimating topics on real text corpus data."
    ],
    "1172": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The performance of PReFIL is improved due to better OCR methods.",
        "The current state-of-the-art for CQA is not challenging enough, and more difficult datasets are needed.",
        "Document-level CQA is a key area for future research, as it requires the ability to answer questions about charts in documents and to handle human-generated queries.",
        "The ConMask model has limitations, such as the potential for error due to similar names and the need for further improvement."
    ],
    "1174": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "our approach achieves new state-of-the-art on FewRel 2.0 dataset",
        "curriculum learning regimens based on shorter-first approach help LSTM construct a partial representation of the sequence in a more intuitive way",
        "curriculum learning helps smaller models improve performance, contributes more in a low resource setup",
        "a model trained with Baby Step curriculum significantly improves for sentences with conjunctions, suggesting that curriculum learning helps LSTM learn longer sequences and functional role of the conjunctions."
    ],
    "1176": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our approach is different from traditional image captioning and requires a tailored model to suit these differences.",
        "Experimentally, we have demonstrated the advantages of our approach over traditional image captioning methods and shown how region-level knowledge can be effectively transferred to paragraph captioning.",
        "We have also demonstrated the benefits of our model in interpretability, generating descriptive paragraphs using only a subset of image regions.",
        "We anticipate further opportunities for knowledge transfer at the intersection of vision and language, and project that visual and lingual compositionality will continue to lie at the heart of effective paragraph generation."
    ],
    "1177": [
        "Existing LSTM models for constituent tree structures are limited by not considering direct lexical input in the computing of cell values for non-leaf constituents.",
        "A headlexicalization method can be used to address this issue.",
        "Learning the heads of constituent automatically using a neural model improves the unidirectional Tree LSTM model.",
        "Bidirectional Tree LSTM gives superior labeling results compared with both unidirectional Tree LSTMs and bidirectional sequential LSTMs.",
        "The proposed lexicalized tree LSTM is applicable to arbitrary binary branching trees in form of CFG, and is formalism-independent.",
        "Lexical information on the root further allows a top-down extension to the model, resulting in a bi-directional constituent Tree LSTM."
    ],
    "1178": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our model can correct 61.5% of the errors and could provide a correction in top three suggestions for 25.9% of the uncorrected errors.",
        "By suggesting three candidates for each error, our model can correct 71.5% error cases in a theoretical correction upperbound of 78%."
    ],
    "1180": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The Con-Mask model was able to capture the correct relationship because the words \"The Time Machine\" appeared in the description of David Duncan as one of his major works.",
        "The reason for this error is because the indicator word for The Vampire Diaries was \"consulting producer\", which was not highly correlated to the relationship name \"notable work\" from the model's perspective.",
        "Some entities with names that are similar to the given relationships, such as the Language-entity in the results of the language-Family-relationship and the Writer-entity in the results of the writer-relationship, are ranked with a very high score.",
        "It may be easy to apply a filter to modify the list of predicted target entities so that entities that are same as the relationship will be rearranged."
    ],
    "1181": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Existing readers benefit greatly from external reference resolution-a specification of which phrases in the given passage refer to candidate answers.",
        "Aggregation readers seem to demonstrate a stronger learning ability in that they essentially learn to mimic explicit reference readers by identifying reference annotation and using it appropriately.",
        "The current state of the art in reading comprehension is such that systems still benefit from externally provided linguistic features including externally annotated reference resolution."
    ],
    "1182": [
        "The proposed framework, Semantic Compositional Network (SCN), effectively composes the individual semantic meaning of tags for visual captioning.",
        "The SCN extends each weight matrix of the conventional LSTM to be a three-way matrix product, with one of these matrices dependent on the inferred tags.",
        "The SCN can be viewed as an ensemble of tag-dependent LSTM bases, with the contribution of each LSTM basis unit proportional to the likelihood that the tag is present in the image.",
        "Experiments conducted on three visual captioning datasets validate the superiority of the proposed approach."
    ],
    "1183": [
        "The proposed bidirectional LSTM-CRF model for clinical concept extraction shows promising results, outperforming recent approaches and ranking closely to the best submission from the original challenge.",
        "Using general-purpose, off-the-shelf word embeddings can provide end-to-end recognition without requiring time-consuming feature construction.",
        "Exploring the use of unsupervised word embeddings trained from domain-specific resources such as the MIMIC-III corpora could further improve the performance of the model."
    ],
    "1185": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We propose a united representation for knowledge graph, utilizing both structure and text description information of the entities.",
        "Experiments show that our proposed jointly representation learning with gating mechanism is effective, which benefits to modeling the meaning of an entity.",
        "In the future, we will consider the following research directions to improve our model: 1. Currently, our score function is based on TransE since the main focus of this work is how to integrate both structural and textual information. We believe our models can be further improved with the recently proposed knowledge graph embedding models.",
        "We will try to design dynamical gating strategy, which is estimated according to the context information.",
        "Intuitively, images of relations and entities may further improve the representation."
    ],
    "1187": [
        "Our proposed method does not rely on parallel corpora.",
        "Our approach is competitive with state-of-the-art methods in cross-lingual sentiment classification.",
        "However, our approach ignores polysemy in the one-to-many mappings.",
        "In the future, we will explore the method for learning sense-specific word embedding."
    ],
    "1188": [
        "The proposed method can learn effective dependency parsers in a short period of time using human-sourced partial annotations.",
        "The ConvexMST method combines constraints from language universals and partial annotations, providing greater robustness.",
        "Actual annotations produced for English and Spanish demonstrate the effectiveness of the method.",
        "Overreliance on creeping supervision may lead to an inaccurate picture of the cross-lingual and low-resource applicability of various models.",
        "Viable models can be produced without relying on having annotations a priori, but rather learning representations on the fly that need not conform to any one set of standards."
    ],
    "1192": [
        "AutoMOS tends to avoid very-high or very-low predictions, likely reflecting the distribution of the training data.",
        "It also learns patterns in the data around certain common \"types\" of utterances which usually achieve high (\"OK, setting your alarm\") or low MOS (reading dictionary definitions).",
        "It's possible that different distributions of texts per synthesizer could yield easily predictable differences in synthesizer-MOS.",
        "A future improvement would be predicting MOS for the raw text and evaluating the advantage of an utterance relative to this baseline.",
        "We have begun tuning a TTS engine using AutoMOS.",
        "Subsequent human evaluations will provide concrete results on the model and evaluation criteria we've selected.",
        "It may be possible to leverage AutoMOS to do stratified sampling of utterances to send to human raters.",
        "This would allow raters to focus energy more evenly across the quality spectrum.",
        "To probe what's been learned, we have explored artificial truncation, as in Figure 2 (right).",
        "Methods like layerwise relevance propagation [2] or activation difference propagation [15] have shown promise with image models, and could be interesting to apply to a unit selection cost function."
    ],
    "1194": [
        "The authors' work on dense prediction in computer vision and framewise sequence labeling has led to the development of a tool (time-dilated convolutions) that allows for efficient processing and batch normalization on full utterances.",
        "The use of time-dilated convolutions in CNN acoustic models has resulted in a 10% relative improvement in WER (from 9.4% to 8.5%) on the Hub5'00 dataset.",
        "A big language model was used to achieve a single-pass performance of 7.7% WER, which is the best single model performance reported so far."
    ],
    "1195": [
        "We bring MWEs, sarcasms and metaphors under a common umbrella of compositionality, followed by a simple unified framework to study it; this is our central contribution.",
        "The method proposed to detect word/phrase compositionality based on local context is simple and affords a clear geometric view.",
        "We do not depend on external resources and perform very well across multiple languages and in a large variety of settings (metaphors, sarcastic and idiomatic usages).",
        "The method naturally scales to handle complications such as unseen phrases and polysemy, achieving comparable or superior results to the state-of-art (which are supervised methods based on elaborate feature engineering and using, at times, plentiful external linguistic resources) on standard datasets.",
        "A careful understanding of the geometry of our context representations (subspace of the principle components of the word vectors) and compositionality scoring method, along with a study of the connections to neural network methods of sentence representation (eg., LSTM (Greff et al., 2015) ) are interesting future avenues of research."
    ],
    "1198": [
        "The proposed approach significantly outperforms the C2S model as it adds skip-connections between the context representations and the words in the sequences, allowing the information from the contexts to be able to directly affect the generation of words.",
        "More than 50% of the fake reviews generated by our approach are misclassified by human judges, and more than 90% of the reviews are misclassified by existing fake review detection algorithm.",
        "In the future, we plan to integrate more context information, e.g., the user, the detailed descriptions of the products, the product prices, into our approaches and also evaluate our approaches in other scenarios, e.g., generating the titles of scientific papers based on the author, venue, and time information.",
        "It may be also beneficial to improve our model through the attention mechanical (Bahdanau, Cho, and Bengio 2014) , i.e., attending to different types of contexts when generating words in different positions."
    ],
    "1199": [
        "The proposed policy gradient method is robust and efficient.",
        "The standard COCO metrics do not correlate well with human judgment.",
        "A new metric, SPIDEr, is proposed to optimize qualitatively superior results.",
        "The proposed algorithm produces state-of-the-art results according to the leaderboard.",
        "The new metric, SPIDEr, and the proposed algorithm produce qualitatively superior results as judged by human raters."
    ],
    "1201": [
        "The Temporal Attention-Gated Model (TAGM) is a new model for classifying noisy and unsegmented sequences that can detect salient parts of the sequence while ignoring irrelevant and noisy ones.",
        "The resulting hidden representation suffers less from the effect of noise, leading to better performance.",
        "The learned attention scores provide a physically meaningful interpretation of relevance of each time step observation for the final decision.",
        "Our approach could be extended to help with document or video summarization in future work."
    ],
    "1202": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "The gated update function that controls the influence of the word being defined on the model at each time step improves accuracy.",
        "A character-level convolutional layer further improves performance.",
        "A well-trained set of word embeddings is crucial to the models.",
        "Some failure modes of the generated definitions may provide insight into shortcomings of the word embeddings.",
        "Word definition models can be utilized to improve word embeddings or standard language models."
    ],
    "1203": [
        "Our model outperforms both existing BOW approaches and the popular doc2vec neural document embedding technique on all three tasks.",
        "Accounting for word and phrase compositionality is crucial for identifying important text patterns.",
        "The findings have impact beyond the immediate context of automatic prediction tasks and suggest promising directions for clinical machine learning research to reduce patient mortality.",
        "[Our model outperforms both existing BOW approaches and the popular doc2vec neural document embedding technique on all three tasks.]",
        "[Accounting for word and phrase compositionality is crucial for identifying important text patterns.]",
        "[The findings have impact beyond the immediate context of automatic prediction tasks and suggest promising directions for clinical machine learning research to reduce patient mortality.]"
    ],
    "1204": [
        "The proposed novel constituent hierarchy predictor based on recurrent neural networks can capture global sentential information.",
        "The resulting constituent hierarchies can be fed to a baseline shift-reduce parser as lookahead features, addressing the limitation of shift-reduce parsers in not leveraging right-hand side syntax for local decisions.",
        "The fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation."
    ],
    "1208": [
        "The authors address the strong language priors for the task of Visual Question Answering and elevate the role of image understanding required to be successful on this task.",
        "The authors develop a novel data-collection interface to balance the popular VQA dataset by collecting complementary images.",
        "The balanced dataset results in a dataset that is not only more balanced than the original VQA dataset but also twice the size.",
        "The 'tails' of the answer distribution are heavier in this balanced dataset, which reduces the strong language priors that may be exploited by models.",
        "The complete balanced dataset is publicly available at http://visualqa.org/ as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0).",
        "Testing near-state-of-art VQA models on the balanced dataset results in a significant drop in performance, confirming the hypothesis that these models had exploited language biases.",
        "The framework around complementary images enables the development of a novel explainable model that produces a list of similar images that it considers counter-examples."
    ],
    "1214": [
        "The proposed approach of using generative adversarial networks and invariance training for noise robust speech recognition improves word error rate when a small number of noise types are seen during training.",
        "In contrast to image recognition tasks, the domain adaptation network in speech recognition suffers from underfitting, leading to unreliable and noisy gradients.",
        "The gradient of the L3 term in Eq. 1 is unreliable and noisy in the domain adaptation network for speech recognition.",
        "Future research includes enhancements to the domain adaptation network while exploring alternative network architectures and invariance-promoting loss functions."
    ],
    "1219": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We have explored simple and effective ways to alleviate or eliminate the beam problem.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The proposed model is based on the assumption that entities can express all the information of text.",
        "In future research, we will further explore its ability by considering more components in text."
    ],
    "1224": [
        "Our model identifies the answer span by matching each time-step of the passage with the question from multiple perspectives.",
        "Ablation studies show that all aspects of matching inside the MPCM model are crucial.",
        "Experimental result on the test set of SQuAD shows that our model achieves a competitive result on the leaderboard."
    ],
    "1229": [
        "The choice of distance metric for clustering is crucial and can affect the results.",
        "Different evaluation methods can provide different views of the same clustering algorithm, and each view can be affected by different critical aspects.",
        "An overall view of all measures is necessary to suggest reliable conclusions.",
        "The effectiveness of a clustering algorithm can depend on the specific distance metric used.",
        "The dendrogram view of the clustering algorithm can be misleading and not reflective of the actual performance of the algorithm.",
        "The Silhouette coefficient can be affected by the presence of 0-valued clusters, while the dendrogram is affected by the choice of atomic points to aggregate."
    ],
    "1230": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Under a model like (TA) 3, the embeddings for dog en and perro es should be similar, leading to embeddings for words in multiple languages that keeps translations close to each other.",
        "Models like T 1 A 1 and T 1 A 1 D 1 do not have this property, but nevertheless give rise to embeddings for words in multiple languages with properties that we analyze and quantify in Section 4."
    ],
    "1231": [
        "The proposed attentive explanation model is capable of providing natural language justifications of decisions and pointing to the evidence.",
        "Using reference explanations to train the model helps achieve high-quality explanations.",
        "The model can point to the evidence and provide natural sentence justifications, similar to those given by humans.",
        "[Paragraph 3] The proposed attentive explanation model is capable of providing natural language justifications of decisions and pointing to the evidence.",
        "[Paragraph 4] Using reference explanations to train the model helps achieve high-quality explanations.",
        "[Paragraph 5] The model can point to the evidence and provide natural sentence justifications, similar to those given by humans."
    ],
    "1234": [
        "We implemented context enhancement on the arc-eager transition base parser with stack LSTMs, the dynamic oracle and dropout supporting.",
        "The results show that the parser is competitive with previous state-of-the-art models.",
        "By considering the future reward taken an action, the global scorer re-scores the actions to improve the parsing accuracy further.",
        "The UAS of the parser increases as much as 1.20% for English and 1.66% for Chinese, and LAS increases as much as 1.32% for English and 1.63% for Chinese.",
        "We get state-of-the-art LASs, achieving 87.58% for Chinese and 93.37% for English.",
        "The complexity is slightly higher than the first-order parsing, but the parser can be accelerated with the more efficient implementation.",
        "We ignore the label of the base parser in the global scorer, which is beneficial to the accuracy.",
        "Future work will focus on different types of global scorers considering dependency labels, which will be more efficient and precise than the model in here."
    ],
    "1238": [
        "'Background knowledge is useful for machine reading.'",
        "'There are still some open questions regarding the comprehensiveness of our coverage of current knowledge and the types of knowledge that we have captured.'",
        "'Other natural language understanding tasks could benefit from background knowledge.'",
        "'Context interacts with background knowledge in complex ways.'"
    ],
    "1239": [
        "The proposed two-phase approach towards identification of argument structure in natural language text can achieve high performance, with a contribution of 11.4% coming from the novel use of word vectors trained on an external corpus.",
        "The first phase of the approach involves building models for classifying text-hypothesis pairs into argument relations, and the second phase uses the classifier confidence scores to construct the argument structure.",
        "The proposed approach can be extended to arguments containing Attack relations as well, and the experiments predicted an average of 38% edges correctly for Debatepedia dataset with 7 arguments having size 13 or less.",
        "The use of ablation study in the approach helps to identify the crucial features contributing to the performance of the argument relation classifier."
    ],
    "1240": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Our proposed model can safely reduce the window size of attention dynamically according to the context.",
        "In qualitative analysis, we found that Flexible Attention only needs to put attention on a large window occasionally, especially when long-range reordering is required.",
        "By cutting down unnecessary computation, NMT models can translate extremely long sequence efficiently or incorporate more expensive score functions."
    ],
    "1243": [
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The ratio of 'time spent' to 'score gained' seems impressive.",
        "This specialization approach can be seen as an optimization process (like in classical Phrase-Based approach), which aims to tune the model.",
        "In this paper, we propose a study of the 'specialization' approach.",
        "This domain adaptation approach shows good improvements with few in-domain data in a very short time.",
        "To gain 2 BLEU points, we used 5K lines of in-domain data, which takes 1 minute to be performed.",
        "This approach reaches the same results as a full retraining, when 10 documents are available."
    ],
    "1246": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our CNNs with external attention improved state of the art by more than 3.5 F 1 points on a Wikipedia benchmark.",
        "Our architectures are applicable to sentiment classification as well."
    ],
    "1247": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "SCDV outperforms state-of-the-art models in multi-class and multi-label classification tasks.",
        "SCDV introduces sparsity in document vectors to handle high dimensionality.",
        "SCDV embeddings provide a robust estimation of the query and document language models, thus improving the MAP of language model based retrieval systems.",
        "SCDV is simple, efficient and creates a more accurate semantic representation of documents."
    ],
    "1248": [
        "Our proposed method achieves significant gains in translation quality on the in-domain test set, with up to 9.9 points increase in BLEU and 12.2 points increase in TER.",
        "However, the continue model tends to overfit the small amount of in-domain training data and even degrades translation quality on the in-domain test sets if trained beyond one or two epochs.",
        "Ensembling the baseline model with the continue model ensures that the performance does not drop significantly on the out-of-domain test set while still getting significant improvements of up to 7.2 points in BLEU and 10 points in TER on the in-domain test set.",
        "Our method achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The continue model can be trained beyond one or two epochs without significant degradation of translation quality on the in-domain test sets."
    ],
    "1250": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Our best model sets a new state of the art in corpus-level entity typing.",
        "Using type probabilities is more robust than binary predictions of types, and joint training gives the best results."
    ],
    "1253": [
        "The proposed multimodal machine comprehension task (DMC) combines the challenges of understanding visual scenes and complex language constructs simultaneously, and can benefit research in both machine visual understanding and language comprehension.",
        "The Vec2seq+FFNN architecture is a generic multi-task model that can be trained end-to-end to display both the ability to choose the most likely text associated with an image and the ability to generate a complex description of that image.",
        "The empirical results validate the underlying hypothesis of the work, showing that improvements in comprehension and generation happen in tandem.",
        "Incorporating specialized vision systems, such as object detection, scene recognition, pose detection, etc., can lead to a direct and measurable impact on a computer system's ability to perform image understanding, and can express that understanding in an end-to-end complex task."
    ],
    "1254": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.",
        "We have demonstrated qualitatively and quantitatively that LRP constitutes a useful tool for fine-grained analysis at the document level or as a dataset-wide introspection across documents.",
        "The resulting document vector is the basis of a new measure of model explanatory power, which could beyond find applications in various visualization and search tasks.",
        "Our work is a first step toward applying the LRP decomposition to the NLP domain, and we expect this technique to be suitable for various types of applications.",
        "LRP could contribute to the design of more accurate and efficient classifiers, not only by inspecting and leveraging the input space relevances, but also through the analysis of intermediate relevance values at classifier \"hidden\" layers."
    ],
    "1255": [
        "We introduce a convolutional neural network for language modeling with a novel gating mechanism.",
        "Our approach builds a hierarchical representation of the input words that makes it easier to capture long-range dependencies, similar in spirit to the tree-structured analysis of linguistic grammar formalisms.",
        "The same property eases learning since features are passed through a fixed number of layers and non-linearities, unlike for recurrent networks where the number of processing steps differs depending on the position of the word in the input.",
        "Our gated convolutional network achieves a new state of the art on WikiText-103.",
        "On the Google Billion Word benchmark, we show competitive results can be achieved with significantly fewer resources."
    ],
    "1258": [
        "Our proposed approach for multi-modal representation learning uses adversarial backpropagation and does not require image-text pair information for multi-modal embedding.",
        "Our method can learn semantic relations between image and text features without using image-text pair information.",
        "Our work can be easily extended to other multi-modal representation learning tasks, such as sound-image, sound-text, or video-text.",
        "Our approach has the potential to bridge the gap between cross-lingual transfer and supervised methods in NLU.",
        "We release our MultiATIS++ corpus to facilitate future research on cross-lingual NLU."
    ],
    "1265": [
        "Experimental results on 17 treebanks across 14 languages show that our parser significantly improves the accuracy of both dependency structures (UAS) and edge labels (LAS), over several previously state-of-the-art systems.",
        "Our proposed neural probabilistic model for non-projective dependency parsing using the BLSTM-CNNs architecture for representation learning achieves better performance than previous state-of-the-art systems.",
        "The use of the BLSTM-CNNs architecture for representation learning improves the accuracy of both dependency structures and edge labels.",
        "Our parser is able to accurately parse both dependency structures and edge labels in multiple languages."
    ],
    "1270": [
        "Not all alignment intensities are created equal, and different tasks may prefer different levels of alignment strength.",
        "The TE task prefers weaker alignments, while the AS task prefers stronger alignments.",
        "Flexible attentive pooling in GRU systems can be used to satisfy the different requirements of different tasks.",
        "Experimental results show the soundness of our argument and the effectiveness of our attention pooling-based GRU systems.",
        "Future work includes investigating phrase representation learning in context and automatically conducting attentive pooling regardless of task categories."
    ],
    "1273": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "We hope to further develop OpenNMT to maintain strong MT results at the research frontier, providing a stable and framework for production use."
    ],
    "1275": [
        "We have shown that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; we hope to see it included in stronger baseline NMT systems.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our approach outperforms benchmark models across different datasets.",
        "We have proposed a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We have added a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers."
    ],
    "1277": [
        "The new discourse parser obtains state-of-the-art performance for English.",
        "The harmonized discourse treebanks enable the presentation of results for five other languages, including the first cross-lingual discourse parsing results in the literature.",
        "The approach of joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality."
    ],
    "1278": [
        "The proposed model AntSynNET significantly outperforms previous baselines in terms of recall.",
        "The distance feature outperforms a previously suggested direction feature.",
        "The embeddings used in the proposed models outperform the state-of-the-art GloVe embeddings.",
        "The two proposed models rely only on corpus data, making them easily applicable to other languages and relations.",
        "The use of a recurrent neural network with long short-term memory units improves the performance of the model.",
        "The combination of lexical, syntactic, and path distance information improves the performance of the model."
    ],
    "1279": [
        "Our algorithm outperforms several serial implementations and a GPU implementation using cuSPARSE.",
        "A system with newer and faster cores might achieve higher speedups than a GPU on smaller datasets.",
        "Building a multi-core system that beats a GPU setup can be more expensive.",
        "Newer GPU models offer previous generation CPU's the opportunity to obtain speedups for a lower price.",
        "Speeding up computation on a GPU would allow users to speed up applications cheaper without investing on a newer multicore system.",
        "Our implementation has been open-sourced and is available online.",
        "In the future, we plan to expand this software into a toolkit that includes other algorithms needed to run a full machine translation system."
    ],
    "1285": [
        "The segmentation results of Models I and II are comparable to the baseline on text chunking data and ATIS data, but worse than the baseline on LARGE data. (based on the comparison of the segment-F1 scores)",
        "The use of IOB labels is not suitable for building segmentation models independently. (based on the observation that Model III outperforms the other two models)",
        "Model III consistently performs better than the baseline and achieves state-of-the-art performance on both text chunking and slot filling tasks. (based on the comparison of the final F1 scores)",
        "The segmentation step improves labeling on slot filling, but not on the text chunking task. (based on the observation that Model I and II do not give consistent improvements on the final F1 score)",
        "The proposed neural sequence chunking models achieve state-of-the-art performance on both text chunking and slot filling. (based on the comparison of the final F1 scores)",
        "The segmentation is a major challenge in the LARGE dataset, but not in the ATIS dataset. (based on the comparison of the segment-F1 scores)",
        "The insight that segmentation is a major challenge in certain datasets encourages more research efforts on similar tasks. (based on the observation that the segmentation score is almost 100 percent in one dataset and not in the other)"
    ],
    "1286": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Developing annotated datasets from multiple sources based on existing datasets and our causality tagging scheme can improve the performance of SCITE.",
        "Combining our method with distant supervision and reinforcement learning can achieve better performance without having to build a high-quality annotated corpus for causality extraction."
    ],
    "1288": [
        "We show that these attributes can be predicted using text features based on Yahoo! Answers discussions about neighbourhoods with a slightly higher correlation coefficient than predictions made using Twitter data.\" - This claim suggests that the authors have found a positive correlation between predicting demographic attributes of neighborhoods using text features from Yahoo! Answers discussions, compared to predictions made using Twitter data.",
        "Our approach outperforms benchmark models across different datasets.\" - This claim indicates that the authors' proposed method for predicting demographic attributes of neighborhoods using text features from QA platforms has achieved better performance than existing benchmark models on different datasets.",
        "We investigate predicting values for real-world entities such as demographic attributes of neighbourhoods using discussions from QA platforms.\" - This claim states that the authors are exploring the possibility of using text features from QA platforms to predict real-world entities like demographic attributes of neighborhoods."
    ],
    "1291": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The combined feature set improves the overall accuracy over the traditional feature set based SA by a margin of 3.6% and 9.3% respectively for Datasets 1 and 2.",
        "It is significantly effective for text with complex constructs, leading to an improvement of 6.1% on our held-out data.",
        "Our general approach may be useful in other problems like emotion analysis, text summarization and question answering, where textual clues alone do not prove to be sufficient."
    ],
    "1295": [
        "'Our approach achieves new state-of-the-art on FewRel 2.0 dataset.'",
        "'The experimental results show that our model significantly outperforms existing HRED models and its attention variants.'",
        "'The relevant contexts detected by our model are significantly coherent with humans' judgements.'",
        "'Our model provides a general and easy-to-implement way to control neural generation models to meet their specific needs, while improving results on a variety of generation tasks.'"
    ],
    "1296": [
        "The proposed hierarchical recurrent attention network (HRAN) for multi-turn response generation in chatbots outperforms state-of-the-art models.",
        "Empirical studies on large scale conversation data show that HRAN can significantly outperform state-of-the-art models.",
        "The proposed HRAN model is capable of generating more accurate and informative responses compared to existing methods.",
        "The use of a hierarchical recurrent attention network (HRAN) allows for more flexible and effective response generation, leading to better performance."
    ],
    "1300": [
        "Our proposed algorithm for training CRF in a semi-supervised manner using unaligned data for SLU significantly improves the performance compared to supervised and self-trained CRF.",
        "By saving many informative labels in the alignment phase, the base model is trained using fewer features.",
        "The parameters of the CRF model are estimated using much less labeled data by regularizing the model using a nearest-neighbor graph.",
        "Our algorithm can be used to train CRF models with fewer labels, which can be beneficial for tasks where labeled data is scarce."
    ],
    "1303": [
        "We showed that synchronous node replacement grammar is useful for AMR-to-text generation.",
        "Our method performs better than previous systems, empirically proving the advantages of our graph-to-string rules.",
        "We developed a system that learns a synchronous NRG in the training time and applies a graph transducer to collapse input AMR graphs and generate output strings according to the learned grammar at test time.",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks."
    ],
    "1308": [
        "The proposed method achieves competitive performance compared to previous systems using less than 1% of the training data.",
        "The method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed approach improves translation for two translation directions: German\u2192English and Romanian\u2192English.",
        "The SNMT models predict the target words together with their CCG supertags accurately.",
        "The use of target syntax in the SNMT system produces more grammatical translations than the baseline NMT system.",
        "The proposed method improves the accuracy of CCG tagging.",
        "The SNMT system with target syntax predicts the correct CCG supertag for \"what\" and correctly re-orders the preposition \"about\".",
        "The SNMT system correctly attaches \"Prentiss\" as an object and \"his wife\" as a modifier to the verb \"called (bezeichnete)\" in the subordinate clause."
    ],
    "1312": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We propose the creation of a new dataset with human-generated question-answer pairs.",
        "Document-level CQA requires information in the rest of the document to answer questions about the chart.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications."
    ],
    "1313": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "One can improve parsing accuracy by training models over multiple KBs.",
        "Training character-level models that learn to map language phrases to KB constants across datasets can further reduce the burden of data gathering.",
        "Pre-training language side models that improve the encoder from data that is independent of the KB can be applied on datasets where only denotations are provided rather than logical forms."
    ],
    "1317": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The approach of joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Knowledge Adaptation, an extension of the Knowledge Distillation idea to the domain adaptation scenario, is able to perform adaptation without re-training.",
        "A student model that takes into account the predictions of multiple teachers and their domain similari-ties is able to outperform the state-of-the-art for multi-source unsupervised domain adaptation on a standard sentiment analysis benchmark.",
        "A simple measure can be used to gauge the trustworthiness of a single teacher and achieve state-of-the-art results on 8 out of 12 domain pairs for single-source unsupervised domain adaptation."
    ],
    "1318": [
        "The proposed iterated dilated convolutional neural networks (DCON) achieve impressive speed improvements for sequence labeling tasks, particularly when processing entire documents at a time.",
        "The DCON model efficiently aggregates broad context without losing resolution, making it suitable for NLP tasks with richer structured output.",
        "The future work of extending the DCON model to NLP tasks with richer structured output, such as parsing, is expected to be promising."
    ],
    "1319": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We show state-of-the-art results on WikiQA and SemEval-2016 (Task 3A) as well as an entailment task, SICK, outperforming previous results by 8%, 1%, and 2%, respectively.",
        "Question answering with sentence-level supervision can greatly benefit from standard transfer learning of a question answering model trained on a large, span-level supervision.",
        "Such transfer learning can be applicable in other NLP tasks such as textual entailment."
    ],
    "1321": [
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Experimental results on the commonly used English Switchboard test set show that our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR), with performance not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "We propose two self-supervised tasks to tackle the training data bottleneck.",
        "Our method is able to extract Semitic roots, which are the basic units of these languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages."
    ],
    "1328": [
        "The quality of the corpus is more important than its quantity for word embedding.",
        "The proposed approaches for embedding Wikipedia Concepts have a performance as good as or even higher than state-of-the-art approaches for Concept Analogy and Concept Similarity tasks.",
        "Wikipedia Concepts Embedding is not ambiguous, and there is a different vector for concepts with similar surface form but different mentions.",
        "The feature of having a different vector for concepts with similar surface form but different mentions is important for many NLP tasks such as Named Entity Recognition, Text Similarity, and Document Clustering or Classification.",
        "In the future, the authors plan to use multiple resources such as Infoboxes, Multilingual Version of a Wikipedia Page, Categories, and syntactical features of a page to improve the quality of Wikipedia Concepts Embedding."
    ],
    "1330": [
        "The proposed Deep Neural Networks for cQA can effectively exploit the characteristics of the task for carrying out interesting MTL.",
        "The network designed and trained in an MTL setting shows better results and a higher convergence rate than individual models that are trained independently.",
        "The results are competitive with those of the models participating at the SemEval 2016 competition for cQA.",
        "All the other challenge systems use domain specific features, which are both very important but also rather costly to engineer.",
        "In the future, it would be beneficial to use more effective features and combine them with other machine learning methods."
    ],
    "1331": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Our model achieves the state-of-the-art performance on all tasks."
    ],
    "1336": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Training PMI and PHMM in an online fashion speeds up convergence and yields comparable or better results than the batch variant and the state-of-the-art LexStat system.",
        "Online PMI system shows the best performance across different language families.",
        "PMI systems can be trained faster in an online fashion and yield better accuracies than the current state-of-the-art systems."
    ],
    "1340": [
        "Meaningful vector algebraic operations can be learned from training examples.",
        "GP is a natural solution for this type of problem and even straightforward implementations can deal with the problem.",
        "GP programs can learn the same rule designed by humans and, therefore, can reach the same results.",
        "The high transferability of the programs across groups of questions may be supported by the general underlying commonality between analogies that these group of questions represent.",
        "Transferability opens an additional opportunity for efficiency gain. Programs can be learned using small vector spaces, and then validated or refined on more computationally costly large vector spaces."
    ],
    "1343": [
        "The proposed neural model for mapping between structured and unstructured data improves upon base models by jointly learning to generate text and reconstruct facts.",
        "The task of creating Wikipedia biographic summary sentences from Wikidata slot-value pairs is challenging, and evaluation in this domain is difficult.",
        "The proposed sequence-to-sequence autoencoding RNN outperforms template baselines in human preference evaluations, with the best model preferred 40% of the time to the gold standard Wikipedia reference.",
        "The authors analyze statistical measures, human preference judgements, and manual annotation to help characterize the task and understand system performance."
    ],
    "1344": [
        "Using an RNN encoder for feature extraction, both CTC and SCRF can be trained end-to-end.",
        "The multitask learning approach improved the recognition accuracies of both CTC and SCRF acoustic models.",
        "CTC can be used to pretrain the RNN encoder, speeding up the training of the joint model.",
        "In the future, we will study the multitask learning approach for larger-scale speech recognition tasks, where the CTC pretraining approach may be more helpful to overcome the problem of high computational cost."
    ],
    "1349": [
        "The proposed VQA solution integrates region pre-selection and a novel attention method to capture generic class regions and richer fused feature representation, which contributes to better VQA performance.",
        "The model achieves comparative or higher empirical results than state-of-the-art models despite being simple.",
        "Possible future works include adopting finer-grained grids that capture more precise regions, employing stacked attention layers for multi-step reasoning, and applying the general pre-selection method to other learning problems.",
        "The novel attention method can capture generic class regions and richer fused feature representation, which improves VQA performance.",
        "The region pre-selection procedure is independent of the attention method and both contribute to better VQA performance."
    ],
    "1350": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The success of the proposed system confirms the importance of data processing in training a successful open-domain dialogue system.",
        "We anticipate that strategies resembling the one we propose can be used more generally for controlling properties of dialogue generation other than specificity, by training several models on different subsets of a single dataset that differ in the desired property, and choosing among these to produce outputs that tailor the quality of interest to the situation at hand."
    ],
    "1355": [
        "The proposed model significantly outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by the proposed model are coherent with human judgments.",
        "The proposed model can improve the quality of multiturn dialogue generation by using proper detection methods, such as self-attention.",
        "The use of self-attention mechanism can effectively capture long distant dependency relations.",
        "The proposed model can reduce the need for expert engineering in the machine learning loop.",
        "The featlets-based approach can generate a large amount of novel templates and features.",
        "The proposed model achieves performance on par with models that had considerable expert intervention."
    ],
    "1359": [
        "Surface features are important for anaphoricity detection.",
        "Incorporating surface properties and generalized representations have different effects for different types of mentions.",
        "There should be consideration for different models or at least different features for processing different types of mentions.",
        "Word embeddings are useful for capturing semantic relatedness.",
        "A coreference resolver that uses word embeddings has an advantage in better resolution of common nouns and pronouns.",
        "The use of surface features in current state-of-the-art coreference resolvers is limited.",
        "Incorporating more generalizable surface properties, especially for proper names, can lead to easy victories."
    ],
    "1361": [
        "Current Deep Learning approaches are viable for all components of a high-quality text-to-speech engine.",
        "Inference performance can be further improved through careful optimization, model quantization on GPU, and int8 quantification on CPU.",
        "Our system is trainable without any human involvement, dramatically simplifying the process of creating TTS systems.",
        "Inference performance can be improved by removing the separation between stages and merging the segmentation, duration prediction, and fundamental frequency prediction models directly into the audio synthesis model.",
        "Improving the duration and frequency models via larger training datasets or generative modeling techniques may have an impact on voice naturalness.",
        "Our work opens many new possible directions for exploration, such as removing the separation between stages and merging the segmentation, duration prediction, and fundamental frequency prediction models directly into the audio synthesis model."
    ],
    "1363": [
        "The proposed chunking policy network for machine reading comprehension enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning, leading to better performance compared to benchmark models.",
        "The addition of a recurrent mechanism allows the model to have knowledge beyond the current segment when selecting answers, further improving performance.",
        "The proposed ReCoSa model utilizes self-attention to effectively capture long distant dependency relations, outperforming existing HRED models and their attention variants.",
        "The relevant contexts detected by the proposed model are significantly coherent with humans' judgments, indicating that the model can accurately detect contexts that are relevant for improving the quality of multiturn dialogue generation.",
        "The use of a sentence importance detector can significantly outperform the baseline, and the ability to identify unimportant content allows for better performance in single-document summarization."
    ],
    "1370": [
        "Our proposed approaches outperform the state-of-the-art BiDAF model.",
        "The proposed embeddings play a significant part in correctly identifying answers for the machine comprehension task.",
        "Our model can perform especially well on exact match metrics, which requires syntactic information to accurately locate the boundaries of the answers.",
        "Similar approaches can be used to encode other tree structures such as knowledge graphs and ontology relations.",
        "An attention mechanism that is designed for to utilize syntactic information should be studied.",
        "Incorporating SEST with deeper neural networks such as VD-CNN (Conneau et al., 2017) could improve learning capacity for syntactic embedding.",
        "Tree structured information such as knowledge graphs and ontology structure should be studied and improved question answering tasks using similar techniques to the ones proposed in the paper."
    ],
    "1379": [
        "Generative models are better than their discriminative counterparts in small-data regime.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Our collection of results showed that generative models are more suitable in continual and zero-shot settings, and they were able to obtain comparable performance to generative models trained on the full datasets in the standard setting.",
        "Formal characterization of the generalization behavior of complex neural networks is difficult, with findings from convex problems failing to account for empirical facts about generalization.",
        "The results of Ng & Jordan (2001) can be extended empirically to nonlinear models."
    ],
    "1380": [
        "The authors have achieved a new record word error rate on the English Switchboard task with their improvements to acoustic and language modeling.",
        "The improvements were due to a combination of bidirectional LSTM modeling, feature fusion, and the replacement of VGG nets with residual nets.",
        "The authors do not believe that human parity has been reached on this task, citing reasons such as a large overlap between training and test speakers, and a higher WER compared to expert transcribers.",
        "The future of research in conversational speech recognition looks bright for at least a few more years."
    ],
    "1382": [
        "Our method is computationally efficient and achieves better performance than all other unsupervised competitors, except for SkipThought, on average.",
        "SkipThought vectors show poor performance on sentence similarity tasks, while our model is state-of-the-art for these evaluations on average.",
        "Our model is generalizable, fast to train, simple to understand, and easily interpretable.",
        "Simple and well-grounded representation models like ours are more relevant than models using deep architectures.",
        "Our model can be augmented to exploit data with ordered sentences.",
        "Our model has the ability to use pre-trained embeddings for downstream transfer learning tasks."
    ],
    "1385": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "FCA can improve both query pre-processing and query post-processing of modern IR systems.",
        "The community needs to endeavor (by theoretical advances and system engineering) to deploy a comprehensive FCA-based tool for information retrieval and integrate it with existing search and indexing, taking into account both the intrinsic complexity issues and the problem of good features generation.",
        "Concept lattices and its applications in social sciences including Social Network Analysis deserve a special treatment.",
        "The grounding steps have been done by Vincent Duquenne [85], Linton Freeman [86], and their collaborators.",
        "Another large and interesting domain is Software Engineering [244, 245]."
    ],
    "1386": [
        "We propose a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "We have performed extensive experiments on three public datasets of machine reading comprehension: CoQA, QuAC, and TriviaQA.",
        "The most popular intentions of using emojis are expressing sentiment, strengthening expression, and adjusting tone.",
        "Subtle differences exist between emoji types, such as negative emojis being more intended to be used than positive emojis to express sentiment, and neutral emojis being the most proper to express irony.",
        "Recipients feel more positive emotions when they receive positive emojis, but not when they receive negative emojis.",
        "The usage of emojis does not affect the sentiment of plain positive verb messages, nor does it affect the sentiment of plain negative messages.",
        "The duplicate usage of emojis does not express more intense sentiment than single emoji in most cases."
    ],
    "1390": [
        "The proposed method for estimating degrees of compositionality in phrases is based on the premise that substitution by synonyms is meaning-preserving.",
        "Unlike previous approaches that represent phrases as vectors or language models, the method represents them as ranked lists.",
        "The ranked list representation is a novel contribution.",
        "The elements of these lists are contextual terms extracted from some appropriate corpus and ranked according to their TF-IDF (any other term informativeness score can be used).",
        "Moving to ranked list representations allows for the approximation of semantic similarity between two phrases using a range of well-known and more refined distance and correlation metrics, designed specifically for lists.",
        "The proposed method outperforms state-of-art baselines for this task."
    ],
    "1392": [
        "The proposal that nouns with similar feature marking (here, number) may be more confusable due to feature overwriting in some proportion of trials, which in turn leads to occasional increase in difficulty in accessing the correct noun when a dependency is to be completed between the subject and the verb, is supported by the data.",
        "The feature overwriting account for the ungrammatical sentences (2a, 2b) appears to be superior to both the retrieval interference and feature percolation accounts.",
        "The modelling presented here allows us to quantitatively compare the relative fit of these proposals for these otherwise indistinguishable accounts.",
        "The predictions of these models can be evaluated for grammatical sentences such as those considered in Franck, Colonna, and Rizzi (2015) ; Villata and Franck (2016) ."
    ],
    "1398": [
        "Our simple jointly-trained BiRNN model, trained only on lexical features, outperforms several complex models.",
        "The utility of models jointly trained on two correlated tasks, punctuation and capitalization, to learn better representations for each of them.",
        "Future work will involve the joint training of a variety of other correlated NLP tasks like POS tagging and NER."
    ],
    "1403": [
        "We introduced a context-based classification method to appropriately sample a list of 57 English target words.",
        "Our metrics predict the likeliness of borrowing better for the young population (age group below 30).",
        "If we compute the metrics from the tweets of those users who seldom code-mix, we obtain the best signals of borrowing.",
        "Remarkably, the rankings based on our metrics are more than two times more correlated to the ground-truth ranking compared to that of the baseline.",
        "Our findings could be incorporated into standard tasks of multilingual IR and NLP and test if this leads to systematic performance enhancements.",
        "We would like to obtain more theoretical insights into the better performance of these metrics by possibly having a dynamical model of word borrowing."
    ],
    "1404": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Feature selection affects significantly to the performance of SVM-Rank, and a set of features consisting of (LSI, Manhattan, Jaccard) gives promising results for information retrieval task.",
        "The CNN model is sensitive to initial values of parameters and exerts higher accuracy when adding auxiliary features.",
        "There are still many characteristics of legal texts that need to be investigated in order to utilize them for building a legal QA system, such as references between articles or structured relations in legal sentences.",
        "More evaluation of SVM-Rank and other L2R methods is needed to observe how they perform on this legal data using the same feature set."
    ],
    "1406": [
        "The proposed approach can distinguish unnatural language from natural language in technical documents for NLP tools to work effectively.",
        "The method extracts five sets of line-based textual features and achieves above 82% F1-score for the four categories of unnatural language.",
        "Removing unnatural language improves document clustering in many settings by up to 15% and 11% at best, while not significantly hurting the original clustering in any setting."
    ],
    "1407": [
        "Our method outperforms previous methods by using less than 1% of the training data.",
        "Our approach significantly reduces the error by 21% on English Switchboard.",
        "Our proposed method takes a relatively simple LSTM-based architecture, using shortest dependency paths as input, and re-deploys it in a set of subtasks needed for extraction of temporal relations from text.",
        "We introduce two techniques that leverage confidence scores produced by different system components to substantially improve the results of TLINK classification.",
        "Our proposed method outperforms state-of-the-art methods by a large margin in a QA-based evaluation.",
        "Our approach obtains state-of-the-art results in an intrinsic evaluation on a very different TimeBank-Dense dataset, proving generalizability of the proposed model."
    ],
    "1409": [
        "Our proposed method outperforms other methods in both general and fine-grained image classification tasks, with substantial improvements in some cases.",
        "Combining supervised and unsupervised learning techniques can lead to learning more useful representations from multimodal data.",
        "Our method consistently outperforms other methods in both inductive and transductive settings.",
        "The advantages of combining supervised and unsupervised learning techniques shed light on the potential benefits of this approach.",
        "Our work makes a step towards learning more useful representations from multimodal data."
    ],
    "1410": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptive search method incorporating two kinds of losses finds task-suitable structures of compressed BERT automatically and efficiently.",
        "The proposed AdaBERT model is effective and efficient for various downstream tasks.",
        "The extensive experiments demonstrate the superiority of AdaBERT over other state-of-the-art models in terms of efficiency and performance.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The constructed Japanese word similarity dataset contains various parts of speech and includes rare words in addition to common words, which can facilitate research in Japanese distributed representations."
    ],
    "1426": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "The end-to-end trained model outperforms an ASR-MT cascade even though it never explicitly searches over transcriptions in the source language during decoding.",
        "Jointly training decoder networks for multiple languages regularizes the encoder and improves overall speech translation performance.",
        "An interesting extension would be to construct a multilingual speech translation system following [34] in which a single decoder is shared across multiple languages, passing a discrete input token into the network to select the desired output language."
    ],
    "1431": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4)."
    ],
    "1432": [
        "The type of word embeddings and the training corpus affect the AZ performance.",
        "The simple model, AV GW V EC, performs better than others, indicating that averaging the word vectors in a sentence can capture the semantic property of statements.",
        "Training specific argumentation word embeddings can improve the performance.",
        "Feature dimension does not dominate the results.",
        "The size of the feature pool does not matter too much on the results, nor does the vocabulary size.",
        "The domain of the training corpus affects the classification performance.",
        "Word embeddings trained on a relevant corpus can capture the semantic features of statements.",
        "Integrating word-specific embedding strategy helps improve the sentence classification for a specific category."
    ],
    "1438": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "To understand charts in documents, information in the rest of the document may be necessary to answer questions about the chart.",
        "Our method integrates search, distant supervision, and multitask learning to provide an effective complete system.",
        "Evaluating the individual components as well as the full system across multiple benchmarks showed the efficacy of our approach.",
        "Future work should aim to improve over our DrQA system."
    ],
    "1439": [
        "Our experiments showed that information from a high-resource language can be leveraged for paradigm completion in a related low-resource language.",
        "Our analysis showed that the degree to which the source language data helps for a certain target language depends on their relatedness.",
        "Our method led to significant improvements in settings with limited training data -up to 58% absolute improvement in accuracy -and, thus, enables the use of state-of-the-art models for paradigm completion in low-resource languages.",
        "We presented a cross-lingual transfer learning method for paradigm completion, based on an RNN encoder-decoder model."
    ],
    "1440": [
        "We introduced the Multimodal Dialogs (MMD) dataset, consisting of over 150K multimodal conversation sessions between shoppers and sales agents.",
        "We proposed 5 new sub-tasks along with their evaluation methodologies.",
        "Our multimodal neural baselines using the encode-attend-decode paradigm demonstrate their performance on both text response generation and best image response selection.",
        "Their performance demonstrates the feasibility of the involved sub-tasks and highlights the challenges present.",
        "We suggest new research directions for addressing the challenges in multimodal conversation."
    ],
    "1442": [
        "The proposed method uses only synthetic parallel data to construct NMT systems, which has significance in low-resource NMT environments.",
        "The novel pseudo parallel corpus called PSEUDO mix shows enhanced results for bidirectional translation and substantial improvement when fine-tuned with ground truth parallel data.",
        "The proposed method can be extended to other learning areas where parallel samples are employed without any adjustment.",
        "Future work will explore robust data sampling methods to maximize the quality of the mixed synthetic parallel data."
    ],
    "1444": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "SA-LSTM is able to model dependency information directly in an architecture engineering way.",
        "SA-LSTM gains more improvement from the SA-LSTM architecture than from the input of extra dependency parsing information.",
        "The proposed Syntax Aware LSTM model for Chinese semantic role labeling achieves state-of-the-art performance, with an F 1 score of 79.64%, significantly outperforming previous state-of-the-art models (p < 0.05)."
    ],
    "1446": [
        "The use of a neural lattice-to-sequence model can improve the translation of uncertain inputs from an error-prone up-stream component.",
        "Consideration of lattice scores, especially in the attention mechanism, is crucial for obtaining improvements in the translation task.",
        "Consensus networks may be investigated for potential gains in terms of speed or quality as compared to lattice inputs.",
        "Explicitly dealing with rare or unknown words in the lattice may lead to improvements in the translation task.",
        "Facilitating GPU training via autobatching may improve the performance of the model."
    ],
    "1448": [
        "Our differentiable approach to enforcing monotonic alignments can produce models which, following the decoding process of section 2.2, provide efficient online decoding at test time without sacrificing substantial performance on a wide variety of tasks.",
        "We believe our framework presents a promising environment for future work on online and linear-time sequence-to-sequence models.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, it improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages."
    ],
    "1449": [
        "The proposed method outperforms previous methods on a Japanese sentiment classification task.",
        "The use of lexical items from a polar dictionary as supervised labels for each phrase or word improves the accuracy of sentiment analysis.",
        "The attention mechanism and polar dictionary used in the Tree-LSTM-based RvNN improve the performance of sentiment analysis.",
        "Fine-grained phrase-level annotation has a positive effect on Japanese sentiment analysis.",
        "The proposed method uses distant supervision to train the model, which improves the accuracy of sentiment analysis.",
        "The use of a Transformer as the attention mechanism is a potential extension of the current approach."
    ],
    "1452": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets."
    ],
    "1454": [
        "Results on Switchboard and CallHome show consistent improvements over baseline attention-based models.",
        "We have compared several types of auxiliary tasks, obtaining the best performance with a combination of a phoneme decoder and frame-level state loss.",
        "The addition of auxiliary tasks can help in either optimization or generalization.",
        "Future work includes studying a broader range of auxiliary tasks and model configurations.",
        "It would be interesting to study even deeper models and word-level output, which would allow for more options of intermediate tasks and placements of the auxiliary losses.",
        "Viewing the approach more broadly, it may be fruitful to also consider higher-level task supervision, incorporating syntactic or semantic labels, and to view the ASR output as an intermediate output in a more general hierarchy of tasks."
    ],
    "1459": [
        "Our ensemble-based keyphrase classification system achieves close-to-the-best results in the ScienceIE Subtask (B) while using only a fraction of the available training data.",
        "With the full training data, our approach ranks 1st.",
        "Incorporating additional task-neutral information beyond words and word order would benefit the system performance.",
        "Using part-of-speech tags, syntactic relations, and simple named entity recognition would very likely boost the performance of our systems."
    ],
    "1460": [
        "Our two-step method for sentence simplification outperforms various baselines with better readability, flexibility, and simplicity achieved.",
        "We plan to take more factors (e.g., sentence length or grammar rules) into account and formulate them as constraints into our proposed model in the future.",
        "Our method achieves better readability, flexibility, and simplicity compared to various baselines.",
        "The results show that our methods outperform various baselines on parallel datasets of Wikipedia and Simple Wikipedia."
    ],
    "1461": [
        "Our approach based on tensor factorization outperforms existing matrix-based techniques on downstream semantic tasks when trained on the same data.",
        "We introduced a novel joint symmetric tensor factorization problem, Joint Symmetric Rank-R CP Decomposition, which allows for utilizing both second and third order co-occurrence information in word embedding training.",
        "Our new technique, JCP-S, outperforms existing matrix-based techniques on a number of downstream semantic tasks when trained on the same data."
    ],
    "1463": [
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension that enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Our fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our approach directly uses the default ranker in Lucene for similar sentences, which can be improved in future work."
    ],
    "1465": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "GloVe, ELMo, and BERT embeddings all encode gender, religion, and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "The system performing best for evaluation scenario 1 uses an SVM with a well-engineered lexical feature set.",
        "Identifying keyphrases is the most challenging subtask, since the dataset contains many long and infrequent keyphrases, and systems relying on remembering them do not perform well."
    ],
    "1467": [
        "Character-based representations are better than word-based ones for learning morphology, especially in rare and unseen words.",
        "Lower layers of the neural network are better at capturing morphology, while deeper networks improve translation performance.",
        "Translating into morphologically-poorer languages leads to better source-side representations.",
        "There are only little differences between encoder and decoder representation quality.",
        "The attention mechanism does not seem to significantly affect the quality of the decoder representations, while it is important for the encoder representations.",
        "Jointly learning translation and morphology can possibly lead to better representations and improved translation.",
        "Extending the analysis to other word representations, deeper networks, and more semantically-oriented tasks such as semantic role labeling or semantic parsing, could be a promising area for future work."
    ],
    "1468": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The resulting fully-supervised parser outperforms the state-of-theart baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "With a term bank and some vectors, we can use lexical cohesion to guide us to the correct answer.",
        "Multivex is different from much recent work in that it uses sparse, high-dimensional vectors instead of dense, low-dimensional embeddings.",
        "The intuition is that word meanings are distributional and general, but facts are intersections of word meanings; facts tend to be rare and specific.",
        "Replacing sparse vectors with dense embeddings reduces the test scores.",
        "As QA systems mature, the emphasis in research will shift from word meanings to sentence meanings."
    ],
    "1472": [
        "The beam problem can largely be explained by the brevity problem.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "We proposed new discourse segmenters with good performance for many languages and domains, at the document level, within a fully predicted setting and using only language independent tools."
    ],
    "1473": [
        "We have shown that the framework of NMT with multiple encoders/decoders can be used to learn joint fixed-size sentence representations which exhibit interesting linguistic characteristics.",
        "We have explored several training paradigms which correspond to partial paths in the whole architecture.",
        "We have proposed a new evaluation protocol of multilingual similarity search which easily scales to many languages and large corpora.",
        "We were able to obtain an average cross-lingual similarity error rate of 1.2% for all 21 language pairs between six languages5 which differ significantly with respect to morphology, inflection, word order, etc.",
        "We have also studied the evolution of the similarity error rate when scaling up to 1.4 million sentences, drawn from an out-of-domain corpus."
    ],
    "1476": [
        "The proposed recurrent chunking mechanisms enable a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "The approach outperforms benchmark models across different datasets.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "The optimal \u03b2 value may be suboptimal on the other set due to the statistical difference between the training set and the development set.",
        "The relaxations might not be close to the true evaluation metrics enough, and annealing should be used to confirm/reject this.",
        "L \u03b2,B 3 and L \u03b2,LEA have similar performance in non-extreme cases."
    ],
    "1477": [
        "The proposed crowdsourcing scheme allows us to obtain proper importance annotations for propositions.",
        "As workers are not required to read all documents, the annotation is much more efficient and scalable as with traditional methods.",
        "We presented low-context importance annotation, a novel crowdsourcing scheme that we used to create a new benchmark corpus for concept-map-based MDS.",
        "The corpus has largescale document clusters of heterogeneous web documents, posing a challenging summarization task.",
        "Our efforts facilitate future research on this variant of summarization."
    ],
    "1479": [
        "The proposed NMT model can be used on any language pair without language-specific knowledge for technical terms selection.",
        "The proposed NMT model outperforms the baseline NMT system in all of the language pairs (Japanese-to-Chinese/Chineseto-Japanese and Japanese-to-English/English-to-Japanese).",
        "The use of a branching entropy to select phrases containing out-of-vocabulary words can improve the translation performance.",
        "Incorporating in-vocabulary non-compositional phrases into the NMT model is expected to achieve better translation performance.",
        "Using a phrase-based SMT instead of NMT for translating certain types of phrases (e.g., in-vocabulary non-compositional phrases) can improve the translation performance."
    ],
    "1482": [
        "The proposed modularized framework for unsupervised sense representation learning supports flexible design of modular tasks and joint optimization among modules.",
        "The proposed model is the first work that implements purely sense-level representation learning with linear-time sense selection, achieving state-of-the-art performance on benchmark contextual word similarity and synonym selection tasks.",
        "The proposed model uses a novel approach to implement sense selection in linear time, which has not been previously possible.",
        "The authors plan to investigate reinforcement learning methods to incorporate multi-sense word representations for downstream NLP tasks in the future."
    ],
    "1485": [
        "Our approach outperforms benchmark models across different datasets.\" (related to the effectiveness of the proposed chunking policy network and recurrent mechanism for machine reading comprehension)",
        "There is a significant gap between the performance of state-of-the-art machine comprehension models and that of the human\" (related to the difficulty of existing models and the potential for improvement with more advanced models)",
        "We hope this dataset will stimulate the development of more advanced machine comprehension models.\" (related to the potential of the proposed dataset, RACE, to drive advancements in machine comprehension technology)",
        "Our approach enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.\" (related to the flexibility and adaptability of the proposed chunking policy network and recurrent mechanism)",
        "We have performed extensive experiments on three public datasets of machine reading comprehension: CoQA, QuAC, and TriviaQA.\" (related to the scope and scale of the experimental evaluation)"
    ],
    "1496": [
        "The existing NLI datasets like SNLI have limited headroom and coverage of the full diversity of meanings expressed in English.",
        "The new dataset, MultiNLI, improves upon SNLI in its empirical coverage and difficulty, and serves as a benchmark for cross-genre domain adaptation.",
        "The MultiNLI corpus has a much higher percentage of sentences tagged with one or more elements from a tag set of thirteen difficult linguistic phenomena, leading to dramatically lower baseline model performance compared to SNLI.",
        "The MultiNLI corpus has a lot of headroom remaining for future work, suggesting that it will be an effective source task for pre-training and transfer learning in the context of sentence-to-vector models.",
        "Models trained on SNLI and MultiNLI substantially outperform all prior models on a suite of established transfer learning benchmarks."
    ],
    "1499": [
        "Our results show that TUPLEINF is a new state-of-the-art structured solver for elementary-level science that does not rely on curated knowledge and generalizes to higher grades.",
        "TUPLEINF is a new QA system that can reason over a large, potentially noisy tuple KB to answer complex questions.",
        "Our method does not rely on curated knowledge and generalizes to higher grades.",
        "Incorporating context and distributional measures may help reduce errors due to lossy IE and misalignments."
    ],
    "1500": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "Our approach captures the sharedprivate separation of different tasks.",
        "Our model demonstrates effectiveness in applying 16 different text classification tasks.",
        "Extensive qualitative analysis derives insights and indirectly explains the quantitative improvements in overall performance."
    ],
    "1501": [
        "Previous work has used crowdsourcing to generate paraphrases, but this is the first systematic study of factors influencing the process.",
        "The most substantial variations in paraphrase quality are caused by priming effects, with simpler examples leading to lower diversity and more frequent semantic equivalence.",
        "Prompting workers with paraphrases collected from other workers (rather than re-using the original prompt) increases diversity.",
        "Our findings provide clear guidance for future paraphrase generation, supporting the creation of larger, more diverse future datasets."
    ],
    "1502": [
        "The choice of context window is important for learning word embedding models.",
        "Cross-sentential contexts have a positive role in word embedding learning.",
        "Right-side contexts are more important than left-side contexts for similarity tasks in English corpora.",
        "The performance of similarity tasks using right-side contexts is comparable to that of symmetric windows.",
        "The study will be extended to the CBOW algorithm, other weighting schemes, and non-English corpora in the future."
    ],
    "1506": [
        "Scalability, including vertical scalability and horizontal scalability, is one of the most critical challenges semantic parsing is facing today.",
        "With a sequence-to-sequence paraphrase model, we showed that cross-domain training of semantic parsing can be quite effective under a domain adaptation setting.",
        "We also studied how to properly standardize pre-trained word embeddings in neural networks, especially for domain adaptation.",
        "Many conventional domain adaptation and representation learning ideas can find application in cross-domain semantic parsing.",
        "Incorporating paraphrase corpora into the paraphrase model can further facilitate domain adaptation.",
        "Requiring a full mapping from logical form to canonical utterance could be costly for large domains.",
        "It is of practical interest to study the case where only a lexicon for mapping schema items to natural language is available."
    ],
    "1508": [
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a novel KB relation detection model, HR-BiLSTM, that performs hierarchical matching between questions and KB relations.",
        "Our model outperforms the previous methods on KB relation detection tasks.",
        "We will investigate the integration of our HR-BiLSTM into end-to-end systems.",
        "We will also investigate new emerging datasets like GraphQuestions and ComplexQuestions to handle more characteristics of general QA."
    ],
    "1509": [
        "programmatic supervision can achieve competitive performance to state-of-the-art systems trained on hand-labeled data.",
        "SWELLSHARK accepts much weaker forms of supervision, allowing NER taggers to be built in less time and in a more intuitive fashion for domain experts.",
        "Our approach intrinsically scales to automatically construct large training sets, allowing SWELLSHARK to train high performance taggers using state-of-the-art recent deep learning models."
    ],
    "1511": [
        "The extensive use of lexical features biases coreference resolvers towards seen mentions, hindering the development of more robust and generalizable coreference resolvers.",
        "Coreference resolution is an important step for text understanding, but it is not an end task; therefore, generalizability should be brought into attention when developing coreference resolvers.",
        "There is a significant overlap between the training and validation sets in the CoNLL dataset, which can lead to reliable evaluations.",
        "The LEA metric (Moosavi and Strube, 2016) is an attempt to make coreference evaluations more reliable, but it is not enough to have reliable evaluation metrics; the validation set on which the evaluations are performed also needs to be reliable.",
        "Incorporating word embeddings can be an efficient way for capturing semantic relatedness, but it may not be effective for describing the context and less for describing the mentions themselves.",
        "Pruning rare lexical features and incorporating more generalizable features could help prevent overfitting.",
        "Out-of-domain evaluations should be incorporated in the current coreference evaluation scheme to ensure more meaningful improvements."
    ],
    "1513": [
        "We showed two orthogonal ways to apply deep multitask learning to graph-based parsing.",
        "The first shares parameters when encoding tokens in the input with recurrent neural networks, and the second introduces interactions between output structures across formalisms.",
        "Without using syntactic parsing, these approaches outperform even state-of-the-art semantic dependency parsing systems that use syntax.",
        "Because our techniques apply to labeled directed graphs in general, they can easily be extended to incorporate more formalisms, semantic or otherwise.",
        "In future work we hope to explore cross-task scoring and inference for tasks where parallel annotations are not available."
    ],
    "1515": [
        "The CopyRNN model not only performs well on predicting present keyphrases but also has the ability to generate topically relevant keyphrases that are absent in the text.",
        "The model can potentially be applied to improve information retrieval performance by generating high-quality index terms and assisting user browsing by summarizing long documents into short, readable phrases.",
        "The model has the ability to capture universal language patterns and extract key information from unfamiliar texts.",
        "The model has a greater potential to be generalized to other domains and types, like books, online reviews, etc., if it is trained on a larger data corpus.",
        "The model can be applied to locate the core information on other data resources, such as summarizing content from images and videos."
    ],
    "1516": [
        "The proposed translation criterion outperforms conventional machine translation approaches in recovering the content of message vectors and facilitating collaboration between humans and learned agents.",
        "The framework for interpreting message vectors can be more generally applied to any encoder-decoder model, allowing for the explanation of what features of the input are being transmitted.",
        "Learning a purely categorical model of the translation process supported by an unstructured inventory of translation candidates has been focused on, and future work could explore the compositional structure of messages and attempt to synthesize novel natural language or neuralese messages from scratch.",
        "The current work has focused on learning a purely categorical model of the translation process, but the framework can be more broadly applied to the compositional structure of messages and attempting to synthesize novel natural language or neuralese messages from scratch.",
        "The denotational perspective from formal semantics provides a framework for precisely framing the demands of interpretable machine learning, and particularly for ensuring that human users without prior exposure to a learned model are able to interoperate with it, predict its behavior, and diagnose its errors."
    ],
    "1517": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The resulting fully-supervised parser outperforms the state-of-theart baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Mechanisms for credit assignment can be useful when added to the models that aim to ameliorate exposure bias.",
        "Continuous relaxations of the argmax operation can be used as effective approximations to hard decoding during training."
    ],
    "1519": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We contrast the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Our model becomes simpler, faster and more accurate.",
        "Beam size has little influence on the performance. Actually, greedy search achieves very high accuracy."
    ],
    "1520": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The resulting fully-supervised parser outperforms the state-of-theart baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Investigating the utility of applying and extending the method to other applications such as Information Extraction applications.",
        "Investigating how to handle other forms of global structure including tendencies that are.",
        "Developing more theory to more precisely understand some of the nuances of using global structure when it's applicable and making connections with other areas of machine learning such as semi-supervised learning, active learning, etc.",
        "Investigating how to have a machine learn that global structure exists and learn what form of global structure exists."
    ],
    "1521": [
        "The proposed method achieves better performance in parsing sentences to linguistically expressive semantic representations without requiring a high-quality annotated corpus for causality extraction.",
        "The method is limited by the insufficiency of high-quality annotated data, but the authors plan to address this issue by developing annotated datasets from multiple sources and combining their method with distant supervision and reinforcement learning.",
        "The proposed parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "The authors believe that there are many future avenues to explore to further increase the accuracy of such parsers, including different training objectives, more structured architectures, and semisupervised learning."
    ],
    "1522": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Human-generated questions are not captured by current CQA datasets.",
        "Document-level CQA is necessary for understanding charts in documents.",
        "PReFIL has the potential to improve retrieval of information from charts, which has numerous applications."
    ],
    "1525": [
        "the best in-domain system is not necessarily the best system in environments where the target domain is unknown",
        "mixing source domains and training on two rather noisy datasets (OC and VG) gave the best results in the cross-domain setup",
        "the essence of a claim is not much more than a few lexical clues",
        "future work should address the problem of vague conceptualization of claims as central components of arguments",
        "a more consistent notion of claims, which also holds across domains, would potentially not just benefit cross-domain claim identification, but also higher-level applications relying on argumentation mining"
    ],
    "1527": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Including word-level acoustic-prosodic features over using only transcriptions obtained strong results.",
        "The acoustic-prosodic features provide the largest gains when sentences are disfluent or long.",
        "In cases where prosodic features hurt performance, we observe a statistically significant negative effect caused by imperfect human transcriptions.",
        "We thus plan to investigate aligning the Treebank and MS-State versions of Switchboard for future work.",
        "Our study builds on a parser that uses reverse order text processing, since it provides a stronger text-only baseline."
    ],
    "1529": [
        "The proposed Ruminating Reader model surpasses the original BIDAF model's performance on SQuAD by a large margin and ties with the best published system.",
        "The model successfully fuses the information from two passes of reading using gating and uses the result to identify appropriate answers to Wikipedia questions.",
        "An ablation experiment shows that each component of the complex model contributes substantially to its performance.",
        "The authors aim to find ways to simplify the model without impacting performance, explore the possibility of deeper models, and expand their study to machine comprehension tasks more broadly in future work."
    ],
    "1531": [
        "ASNs provide a modular encoder-decoder architecture that can readily accommodate a variety of tasks with structured output spaces.",
        "They are particularly applicable in the presence of recursive decompositions, where they can provide a simple decoding process that closely parallels the inherent structure of the outputs.",
        "Our results demonstrate their promise for tree prediction tasks, and we believe their application to more general output structures is an interesting avenue for future work."
    ],
    "1533": [
        "The brevity problem in NMT can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The joint model of part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "The interaction between tagging and parsing in the joint model is effective in improving the accuracy of both tagging and parsing.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\")."
    ],
    "1534": [
        "Our approach to multilingual taxonomy induction is simpler, principled, and easy to replicate, unlike previous approaches that are complex and heuristic-heavy.",
        "Taxonomies induced using our approach outperform the state of the art on both edge-level and path-level metrics across multiple languages.",
        "Our approach provides a parameter for controlling the trade-off between precision and branching factor of the induced taxonomies.",
        "The taxonomies induced by our approach are significantly more accurate than the state of the art and provide higher coverage across 280 languages."
    ],
    "1536": [
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Our approach can align equivalent API sequences from a large-scale commented code corpus through multi-modal sequence-to-sequence learning.",
        "The proposed approach significantly increases the accuracy and scale of API mappings compared to state-of-the-art approaches.",
        "Our work demonstrates the effectiveness of deep learning in API migration and is one step towards automatic code migration."
    ],
    "1542": [
        "Our results confirm previous findings that character-level models are effective for many languages, but these models do not match the predictive accuracy of models with explicit knowledge of morphology.",
        "Moreover, our qualitative analysis suggests that they learn orthographic similarity of affixes, and lose the meaning of root morphemes.",
        "Across languages with different typologies, our experiments show that the subword unit models are most effective on agglutinative languages.",
        "However, these results do not generalize to all languages, since factors such as morphology and orthography affect the utility of these representations.",
        "We plan to explore these effects in future work."
    ],
    "1545": [
        "GAtt produces new source representations that vary across different decoding steps according to the partial translation so as to improve the discrimination of context vectors for translation.",
        "Our model is able to significantly reduce repeated redundant translations (over-translations).",
        "Our model is easily adaptable to other sequence-to-sequence tasks.",
        "Except for the GRU unit, we will explore more different end-to-end neural architectures.",
        "The gating layer plays a very important role in our model.",
        "We are interested in adapting our GAtt model as a tree-structured unit to compose different nodes in a dependency tree."
    ],
    "1546": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Our approach uses an attention-based neural sequence-to-sequence model, with data augmentation from the target database and paraphrasing, to parse utterances to SQL.",
        "The semantic parsing model is comparable in performance to previous systems that either map from utterances to logical forms, or generate SQL, on two benchmark datasets, GEO880 and ATIS.",
        "We find that the semantic parsing model is comparable in performance to previous systems that either map from utterances to logical forms, or generate SQL, on two benchmark datasets, GEO880 and ATIS.",
        "A key advantage of our approach is that it is not language-specific, and can easily be ported to other commonly used query languages, such as SPARQL or ElasticSearch.",
        "We release a new dataset of utterances and SQL queries for an academic domain."
    ],
    "1551": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "The novel end-to-end model for joint slot label alignment and recognition requires no external label projection and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Rich pretraining leads to 15.4% relative error reduction, and our model gives results highly competitive to the best systems on six different benchmarks."
    ],
    "1552": [
        "The use of a novel feature set for sentence-level supporting argument detection can improve the performance of the task.",
        "Leveraging argument type information can further improve the performance of supporting argument detection.",
        "A rich feature set can be used to characterize arguments of different types for a user-specified claim.",
        "The task of sentence-level supporting argument detection can be improved by leveraging a novel dataset and feature set.",
        "The use of a fully-supervised parser can improve the performance of a shift-reduce parser by maintaining the same model size and speed."
    ],
    "1554": [
        "'Our method significantly outperforms current state-of-the-art models in two popular datasets for NER and Chunking.'",
        "'Adding a backward LM in addition to traditional forward LMs consistently improves performance.'",
        "'The proposed method is robust even when the LM is trained on unlabeled data from a different domain, or when the baseline model is trained on a large number of labeled examples.'"
    ],
    "1558": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning.",
        "The approach of debiasing the first (non-contextual) layer alone can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Modifying and regularizing LSTMs improves their performance for learning paraphrastic sentence embeddings in both transfer and supervised settings.",
        "The GATED RECURRENT AVER-AGING NETWORK (GRAN) improves upon both AVG and LSTMs for paraphrastic sentence embedding tasks.",
        "Analyzing the L 1 norm of GRAN's gate can help understand the compositional phenomena it is learning.",
        "Future work will explore additional data sources, including aligning different translations of novels, aligning new articles of the same topic, or using machine translation systems to translate bilingual text into paraphrastic sentence pairs.",
        "The new techniques combined with the promise of new data sources offer a great deal of potential for improved universal paraphrastic sentence embeddings."
    ],
    "1559": [
        "\"We proposed a means of tagging a low-resource language without the need for bilingual parallel corpora.",
        "\"Our approach leads to consistent and substantial improvements over benchmark methods.",
        "\"Traditional uncertainty sampling strategies do not work well on low-resource settings, and introduce new methods based around labelling word types.",
        "\"Incorporating distant supervision in the form of model transfer with cross-lingual word embeddings can be effective with limited supervision.",
        "\"Deep neural network models can be effective with limited supervision by incorporating distant supervision."
    ],
    "1567": [
        "The scarcity of benchmark datasets for chat detection tasks has limited the development of task-oriented SDS.",
        "Using external resources such as tweets and Web search queries can improve the accuracy of chat detection.",
        "Off-the-shelf supervised methods augmented with external resources perform accurately, outperforming baseline approaches.",
        "The study aims to remove the long-standing boundary between task-oriented and non-task-oriented SDS.",
        "The dataset and feature values derived from tweets and Web search queries will be released for future research."
    ],
    "1568": [
        "The proposed word-level sampling method significantly outperforms state-of-the-art pivot-based methods and multilingual methods in terms of translation quality and decoding efficiency.",
        "Our approach obtains a significant improvement over the pivot-based baseline using small source-pivot data and zero-resource translation.",
        "The proposed teacher-student framework is transparent to architectures and can be extended to other cross-lingual NLP applications.",
        "The use of sentence-level and word-level teaching in the student model can improve the learning process and achieve better translation quality and decoding efficiency.",
        "The approach can be applied to diverse language pairs, such as zero-resource Uyghur-English translation using Chinese as a pivot, and other cross-lingual NLP applications."
    ],
    "1570": [
        "The proposed models can improve the translation accuracy of NMT systems.",
        "The simplest model (Mixed RNN) achieves the best performance, resulting in significant improvements over the state-of-the-art baseline NMT system.",
        "There is still much room for NMT translation to be consistent with source syntax.",
        "Designing novel syn-tactic features for NMT can improve its translation accuracy.",
        "Employing the source syntax to constrain and guide the attention models can further improve the performance of NMT systems."
    ],
    "1572": [
        "Incorporating cluster membership features in the feature extraction pipeline improves the performance of Named-Entity recognition, sentiment classification, and quantification tasks.",
        "The performance improvements are consistent across four tasks, highlighting the usefulness of incorporating cluster membership features.",
        "Skip-gram embeddings of low dimensionality (D = 40) and high number of clusters (e.g., K \u2208 {500, 1000, 2000}) are consistently competitive in the tasks.",
        "Using out-of-domain data to construct word embeddings is a good practice, as the results obtained with these vectors are also competitive.",
        "Combining out-of-domain embeddings with in-domain ones remains to be further studied."
    ],
    "1573": [
        "'Our proposed model outperforms the standard attention-based neural machine translation baseline.'",
        "'The target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged.'",
        "'Future work includes abandoning labeled chunk data and adopting reinforcement learning to explore the boundaries of phrase automatically.'"
    ],
    "1575": [
        "Our argumentation corpus is the first Chinese corpus, and it was built using crowdsourcing techniques.",
        "The argumentation model used in our corpus extends classic models and is suitable for product reviews in general.",
        "We novelly use clustering techniques to aggregate annotations, which can resolve annotation conflicts and provide confidence scores.",
        "The annotation quality of our corpus is comparable to that of widely used argumentation corpora in other languages.",
        "Our corpus is publicly available for further research."
    ],
    "1576": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our joint model can effectively use the argumentative-or-not information to improve the boundary detection performance.",
        "Applying deep learning techniques to other sub-tasks of argumentation mining is a natural next step.",
        "A deep-learning-based end-to-end argumentation mining tool is also worthy of further investigation."
    ],
    "1577": [
        "The proposed Deep Speaker embedding scheme achieves significant improvement in text-independent speaker recognition compared to traditional DNN-based i-vector approaches.",
        "The use of a triplet loss layer based on cosine similarities for metric learning improves the performance of the speaker recognition system.",
        "Softmax pre-training can be used to achieve better performance in the speaker recognition system.",
        "The Deep Speaker algorithm can take full advantage of transfer learning to solve speaker recognition problems on small data sets.",
        "The proposed system can reduce the error cases, improve model size, and reduce CPU requirements in future work.",
        "The use of a Mandarin dataset UIDs and an English dataset MTurk for evaluating the performance of the speaker recognition system.",
        "The significant improvement in the performance of the speaker recognition system, with a decrease in EER of roughly 50% in the Mandarin dataset and a decrease in error of 60%, and a decrease in equal error rate of 30% in the English dataset."
    ],
    "1579": [
        "We have constructed a large-scale radiology image database (ChestX-ray8) with realistic clinical and methodological challenges, similar to \"ImageNet\" in natural images.",
        "Our database includes tens of thousands of patients and is extensively quantitatively performance benchmarked on eight common thoracic pathology classifications and weakly-supervised localization.",
        "The main goal of ChestX-ray8 is to initiate future efforts by promoting public datasets in this important domain, enabling the creation of clinically meaningful applications such as disease pattern mining, disease correlation analysis, and automated radiological report generation.",
        "Building truly large-scale, fully-automated high-precision medical diagnosis systems remains a strenuous task, but ChestX-ray8 can enable data-hungry deep neural network paradigms to create clinically meaningful applications.",
        "In the future, we plan to extend ChestX-ray8 to cover more disease classes and integrate it with other clinical information, such as follow-up studies across time and patient history."
    ],
    "1580": [
        "We propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "We use the observation that biased representations lead to biased inferences to construct a systematic probe for measuring biases in word representations using the task of natural language inference.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We explore the use of a projection-based method for attenuating biases.",
        "By exploring various architectures, we showed that a BiLSTM network with max pooling makes the best current universal sentence encoding methods, outperforming existing approaches like SkipThought vectors.",
        "We believe that this work only scratches the surface of possible combinations of models and tasks for learning generic sentence embeddings.",
        "Larger datasets that rely on natural language understanding for sentences could bring sentence embedding quality to the next level."
    ],
    "1583": [
        "Our approach uses only the information of a user reviewing an item at an explicit timepoint for this task, making it generalizable across all communities and domains.",
        "Unlike prior works, our model does not rely on syntactic and domain-specific features, but instead uses only the information of a user reviewing an item at an explicit timepoint.",
        "Our approach provides interpretable explanations as to why a review is helpful, in terms of salient words from latent word clusters that are used by experts to describe important facets of the item in the review.",
        "Our experiments on real-world datasets from Amazon like books, movies, music, food, and electronics demonstrate the effectiveness of our approach over state-of-the-art baselines."
    ],
    "1585": [
        "The proposed constituent hierarchy predictor based on recurrent neural networks can capture global sentential information and improve the performance of shift-reduce parsers.",
        "The fully-supervised parser outperforms the state-of-the-art baseline parser in terms of F1 score on standard WSJ and CTB evaluations.",
        "The structural models can effectively preserve salient source relations in summaries and are on par with or surpass state-of-the-art published systems.",
        "The feature groups, including explicit labels and ratings available in the NewsTrust community, have a moderate correlation with various factors that indicate information credibility.",
        "The stylistic features of language, such as assertives, hedges, implicatives, factives, discourse, and affective play a significant role in the credibility detection of news, in conjunction with other language features like topics and increase in expertise.",
        "The CCRF model brings all these features together to build a strong signal for news credibility in a news community."
    ],
    "1589": [
        "The model achieves state-of-the-art results on the SQuAD dataset, outperforming several strong competing systems.",
        "The reattention mechanism introduced in the Reinforced Mnemonic Reader alleviates the problems of attention redundancy and deficiency in multi-round alignment architectures.",
        "The dynamic-critical reinforcement learning approach presented addresses the convergence suppression problem existed in traditional reinforcement learning methods.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The proposed model can be applied to other NLP tasks such as natural language inference, and the compatibility of the proposed methods with other tasks is a future work."
    ],
    "1590": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results suggest that the community is ready for more difficult CQA datasets.",
        "Charts in the wild may contain variations that are not captured by current datasets.",
        "Human-generated questions should be included in future CQA datasets.",
        "Document-level CQA is necessary to understand charts in documents.",
        "The proposed system has the potential to improve retrieval of information from charts, which has numerous applications."
    ],
    "1596": [],
    "1599": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "Our model supports both exact chart-based decoding and a novel top-down inference procedure.",
        "Many of the key insights from recent neural transition-based approaches to parsing can be easily ported to the chart parsing setting, resulting in a pair of extremely simple models that nonetheless achieve excellent performance."
    ],
    "1602": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "the experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "the use of self-attention mechanism can effectively capture long distant dependency relations.",
        "the proposed ReCoSa model outperforms existing HRED models and its attention variants.",
        "the relevant contexts detected by our model are significantly coherent with humans' judgements."
    ],
    "1604": [
        "The proposed method outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by the proposed model are significantly coherent with human judgments.",
        "The proposed method can improve the quality of multiturn dialogue generation by using proper detection methods, such as self-attention.",
        "The method is fast and flexible, requiring a small amount of computational resources to learn/build a model.",
        "The proposed method shows performance competitive to prediction-based methods in both intrinsic and extrinsic evaluation setups."
    ],
    "1606": [
        "Our approach achieves state-of-the-art results on the Multi30K dataset without using images for translation.",
        "Training on separate parallel text and described image datasets does not hurt performance, encouraging future research on multitasking with diverse sources of data.",
        "We still find improvements from image prediction when we improve our text-only baseline with the out-of-domain parallel text.",
        "Future work includes adapting our decomposition to other NLP tasks that may benefit from out-of-domain resources, such as semantic role labelling, dependency parsing, and question-answering.",
        "Exploring methods for inputting the (predicted) image into the translation model.",
        "Multitasking different translation languages into a single shared encoder.",
        "Multitasking in both the encoder and decoder(s)."
    ],
    "1607": [
        "Multiple sources of bias in end-to-end speech systems\" can lead to large neural network structures that make deployment impractical.",
        "Proposed methods can address these issues and build a model that performs significantly better on the target dev set while still being good for streaming inference.",
        "The addition of cross entropy alignment training and the GramCTC loss allows models to fit the training and validation data better with respect to the WER of a greedy max decoding.",
        "Using an LC-BRGU layer in place of lookahead convolutions conveys benefits across the board, as does using a PCEN layer at the front end.",
        "Generalization to unseen data is improved by the addition of farfield augmentation."
    ],
    "1609": [
        "The proposed model can outperform state-of-the-art methods for both phrase-level content selection and discourse relation prediction tasks.",
        "The model's output features can be used to predict consistency-of-understanding in meetings with superior performance compared to the state-of-the-art model.",
        "The model can be successfully applied to other prediction tasks in spoken meetings.",
        "[Paragraph 2] The proposed model can outperform state-of-the-art methods for both phrase-level content selection and discourse relation prediction tasks.",
        "[Paragraph 3] The model's output features can be used to predict consistency-of-understanding in meetings with superior performance compared to the state-of-the-art model.",
        "[Paragraph 4] The model can be successfully applied to other prediction tasks in spoken meetings."
    ],
    "1610": [
        "The proposed model outperforms previous systems by achieving competitive performance with less than 1% of the training data.",
        "The approach significantly reduces the error by 21% on English Switchboard compared to previous methods.",
        "The model learns latent persuasive strengths of topics, linguistic style of arguments, and the interactions between the two.",
        "The model outperforms comparisons using audience responses or linguistic features alone.",
        "Winners use stronger arguments and strategically shift topics to stronger ground.",
        "Strong and weak arguments differ in their language usage in ways relevant to various behavioral theories of persuasion."
    ],
    "1613": [
        "We believe social media is a promising platform for both studying SUD-related human behaviors as well as engaging the public for substance abuse prevention and screening.",
        "Employing unsupervised features learning to take advantage of a large amount of unsupervised social media user data.",
        "Employing multi-view feature learning to combine heterogeneous user information such as 'likes' and 'status updates' to learn a comprehensive user representation.",
        "Building SUD prediction models based on learned user features.",
        "Incorporating unsupervised heterogeneous user data for SUD prediction has demonstrated benefits.",
        "Our investigation has not only produced models with the state-of-the-art prediction performance (e.g., for all three types of SUD, our models achieved over 80% prediction accuracy based on AUC)."
    ],
    "1614": [],
    "1616": [
        "The proposed method achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic Memory Induction Networks (DMIN) can be a learning mechanism more general than what has been used here for few-shot learning.",
        "The brevity problem in neural machine translation can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "More stringent regulations in journalism, especially for entertainment news and society news, are needed based on the results of data analysis."
    ],
    "1617": [
        "Unexpectedly, for some languages the bi-directional LSTM is outperformed by unsupervised strategies like the addition of the word embeddings obtained from a Skip-Gram model.",
        "This demonstrates i) that no algorithm is the best across the board; and ii) that some simple models are to be preferred even for downstream tasks, which partially contrasts with the conclusions of Hill et al. (2016).",
        "Representation algorithms sensitive to word order have similar trends, but they do not always achieve performance superior to algorithms based on the sentence order.",
        "Some properties of languages (i.e. their type of negation) appear to have an impact on the scores: in particular, the asymmetry of negative and affirmative clauses and the doubling of negative markers."
    ],
    "1619": [
        "The proposed method of neural stacking for feature transfer can improve the Singlish dependency parsing performance by a significant margin.",
        "The enhanced parser for Singlish dependency parsing can be built by leveraging on knowledge transferred from a larger English treebank of Universal Dependencies.",
        "The released annotated Singlish dependency treebank, trained model, and source code for the parser are available for free public access.",
        "There is potential for expanding the investigation to other regional languages such as Malay and Indonesian."
    ],
    "1623": [
        "The proposed approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The proposed approach can be used for a wide array of problems, including open-domain question answering.",
        "The approach can incorporate both the source-and target-side information from a small subset of training translation pairs to improve the translation quality.",
        "The proposed model, called SEG-NMT, learns to incorporate both the source-and target-side information from retrieved pairs to improve the translation quality.",
        "The approach can be used for embedding an input of any modality into a fixed vector space and using approximate search."
    ],
    "1624": [
        "high-quality embeddings and topics can be obtained using the method\" - This claim suggests that the proposed method for training interpretable corpus-specific word embeddiments is effective in producing high-quality results.",
        "big data is not always best\" - This claim highlights the idea that domain-specific data, even when it is small, can be very valuable for certain tasks, and that big data may not always be the best approach.",
        "domain-specific data can be very valuable\" - This claim emphasizes the importance of using domain-specific data in computational social science applications.",
        "I plan to use this approach for substantive social science applications\" - This claim indicates that the proposed method will be used for practical applications in social science research.",
        "to address algorithmic bias and fairness issues\" - This claim suggests that the author is aware of potential issues with algorithmic bias and fairness, and plans to address these issues in their work."
    ],
    "1625": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "RANs are also considerably more transparent than other RNNs; their limited use of non-linearities enables the state vector to be expressed as a component-wise weighted sum of previous input vectors.",
        "This also allows the individual impact of the sequence inputs on the state to be recovered explicitly, directly pointing to which factors are most influencing each part of the current state.",
        "We demonstrate that RAN-like components exist within LSTMs and GRUs.",
        "Furthermore, we provide empirical evidence that the use of recurrent non-linearities within those architectures is perhaps unnecessary for language modeling.",
        "While we demonstrate the robustness of RANs on several language modeling benchmarks, it is an open question whether these findings will generalize across a wider variety of tasks."
    ],
    "1626": [
        "The proposed approach achieves significantly improved results compared to the widely used cosine measure, while yielding competitive results on human evaluation datasets.",
        "The use of word embeddings instead of high-dimensional vector representations for tagging data improves the semantic content and reduces computational complexity when learning a metric.",
        "The approach can exploit semantic relatedness information from HIDs to more realistically assess semantic relatedness, regardless of the underlying embedding dataset.",
        "The algorithm can transfer knowledge from one HID to another, sometimes with a very large increase of correlation with human intuition.",
        "The approach yields negative results when training a metric on false information, supporting the robustness of the results.",
        "The exploration of other graph embedding algorithms for tagging data and crowdsourcing strategies to gather data suitable for metric learning can further improve the results.",
        "Adapting metric learning algorithms to specific properties of social tagging systems can lead to better fit to human intuition."
    ],
    "1629": [
        "We proposed a method based on factor analysis to infer latent personality traits from the everyday language of users on social media.",
        "A person is 'an organized, dynamic, agentic system functioning in the social world', with characteristics, behaviors, feelings, and thoughts that are both consistent and variable across time and situations' [41].",
        "Individual differences have often been based on questionnaire items, which may not capture the richness of everyday human behavior.",
        "Our method infers latent factors from linguistic behavior in social media - a medium that allows access to large sample sizes; unprompted access to user's thoughts, emotions, and language; and data-driven approaches.",
        "We derived five factors from the language, and demonstrated that these factors are generalizable with good predictive power, and stable across \"]"
    ],
    "1630": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "the proposed encoder-decoder model with local monotonic attention significantly improved the performance on three different tasks and able to reduce the computational complexity more than one that used standard global attention architecture.",
        "disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality",
        "tagging ambiguity is mostly local while dependency parsing heavily depends on POS tags to predict syntactic structures",
        "enabling 'tag \u2192 parse' only also improves the tagging accuracy itself"
    ],
    "1631": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We introduced sluice networks, a meta-architecture for multitask architecture search.",
        "In our experiments across four tasks and seven different domains, the meta-architecture consistently improved over strong single-task learning, architecture learning, and multi-task learning baselines.",
        "We also showed how our meta-architecture can learn previously proposed architectures for multi-task learning and domain adaptation."
    ],
    "1632": [
        "Our approach outperforms previous multimodal approaches which are sometimes more complex and optimize word embeddings as opposed to sentence embeddings.",
        "Our proposed models can create very competitive embeddings, even compared to more sophisticated models trained on orders of magnitude more text data, especially when the vocabulary is related to visual concepts.",
        "State-of-the-art encoder models, like LSTMs, performed significantly worse than much simpler encoders like bag-of-word models.",
        "Using a multimodal approach in order to improve general text embeddings is under-explored and we hope that our results motivate further developments.",
        "The fact that the best models are very simple suggests that there is a large headroom in that direction."
    ],
    "1633": [
        "The TPRN model uses Tensor Product Representations in recurrent networks to encode input words, and learns how to deploy a set of symbols into a set of structural roles.",
        "The learned symbols and roles are interpretable as encoding abstract grammatical concepts without ever being exposed to data labeled with anything like grammatical structure.",
        "The TPRN model illustrates how learning to perform a natural language question-answering task can lead a deep learning system to create representations that are interpretable as encoding abstract grammatical concepts.",
        "The learned symbols and roles correlate with abstract notions of grammatical roles from linguistic theory.",
        "The words assigned to a given symbol tend to be semantically related, and the words assigned to a given role correlate with abstract notions of grammatical roles."
    ],
    "1635": [
        "We conducted a deep investigation of cross-language plagiarism detection methods on a challenging dataset.",
        "Our results have shown a common behavior of methods across different language pairs.",
        "We revealed strong correlations across languages but also across text units considered.",
        "This means that when a method is more effective than another on a sufficiently large dataset, it is generally more effective in any other case.",
        "This also means that if a method is efficient on a particular language pair, it will be similarly efficient on another language pair as long as enough lexical resources are available for these languages.",
        "We also investigated the behavior of the methods through the different types of texts on a particular language pair: English-French.",
        "We revealed strong correlations across types of texts.",
        "This means that a method could be optimized on a particular corpus and applied efficiently on another corpus.",
        "Finally, we have shown that methods behave differently in clustering match and mismatched units, even if they seem similar in performance."
    ],
    "1638": [
        "The proposed approach improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The approach allows lexicality and syntax to interact with each other in the joint search process, improving the accuracy of both tagging and parsing.",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The approach uses a five-action transition system, and three classifiers to resolve structural, tagging, and labeling conflicts.",
        "The theoretical view of kernel neural networks can be helpful for future model exploration.",
        "The proposed class of deep recurrent neural architectures can be used to derive new families of neural architectures for sequences and graphs."
    ],
    "1641": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Speech recognition is prone to show errors and its performance is significant for the success of the overall performance of the application it is designed for.",
        "Exploring the use of semantic information from knowledge graphs to improve recognition accuracy is an extremely promising research direction."
    ],
    "1642": [
        "We have presented a neural framework for generalized topic models to enable flexible incorporation of metadata with a variety of options.",
        "Our model demonstrates the tradeoff between perplexity, coherence, and sparsity, and outperforms SLDA in predicting document labels.",
        "Furthermore, the flexibility of our model enables intriguing exploration of a text corpus on US immigration.",
        "We believe that our model and code will facilitate rapid exploration of document collections with metadata."
    ],
    "1644": [],
    "1647": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "Experiments show that the expanded knowledge is useful in improving the performance of predictions, especially for products without training data."
    ],
    "1651": [
        "The named entity recognition system achieves an F 1 score of 92.05% on the standard dataset, outperforming the first-ranked system by a large margin (claim 1).",
        "Using automatic syntactic features for the Bi-LSTM model surpasses the combination of Bi-LSTM-CNN-CRF models, despite requiring less time for computation (claim 2).",
        "The system achieves high performance on the standard dataset, with an F 1 score of 92.05% (claim 3).",
        "The proposed method outperforms the related NER shared task with a large margin (claim 4).",
        "The use of automatic syntactic features for the Bi-LSTM model is effective and surpasses the combination of Bi-LSTM-CNN-CRF models (claim 5)."
    ],
    "1653": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our system consistently improves the parsing accuracy, especially for agglutinative languages.",
        "Our system outperforms the previous best greedy parser in an external comparison on the SPMRL data sets.",
        "The CNN model alleviates the OOV problem and is better suited than the LSTM in dependency parsing."
    ],
    "1655": [
        "The effect of dependency information in a PASA is improved accuracy.",
        "The cost of using dependency information in an analyzer is high, disproportionate to the improvement in accuracy.",
        "Indirect features can compensate for the absence of dependency features and work well enough.",
        "Preparing dependency information is reasonable to use in realistic situations.",
        "The authors plan to design a framework of rapid data preparation and adopt the system to a variety of domains in the future."
    ],
    "1656": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to improvements in BLEU scores.",
        "The brevity problem can be seen as an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Our framework for learning representations invariant to a specified factor or trait results in better generalization and improvements on three tasks from different domains empirically.",
        "An invariant representation is learned, resulting in better generalization and improvements on the three tasks."
    ],
    "1657": [
        "We addressed the problem of learning when to attend to source sentence.",
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The experimental results show that our model achieved a significant improvement of 0.8 BLEU score.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We introduced a new component named attention sentinel, based which we built an adaptive attention model.",
        "The results were achieved on a union set of NIST 2003 NIST , 2004 NIST , and 2005 data.",
        "The top 15 most frequent words from testing data that are assigned a sentinel gate value \u2265 0.9."
    ],
    "1658": [
        "We proposed a hierarchical phrase-based RNN as our captioning model, which allowed natural integration with human feedback.",
        "We showed that by exploiting descriptive feedback our model learns to perform better than when given independently written captions.",
        "Our model learns to perform better than when given independently written captions."
    ],
    "1660": [
        "The proposed neural topic models using Gaussian Softmax, Gaussian Stick-Breaking, and Recurrent Stick-Breaking constructions can exhibit similar sparse topic distributions as found with traditional Dirichlet-Multinomial models.",
        "By exploiting the ability of recurrent neural networks to model sequences of unbounded length, the proposed method allows the number of topics to dynamically increase, without the need for truncation.",
        "The evaluation results show that the proposed neural models achieve state-of-the-art performance on a range of standard document corpora."
    ],
    "1661": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Our semantically rich cross-lingual vectors facilitate language transfer in DST, providing an effective method for bootstrapping belief tracking models for new languages."
    ],
    "1663": [
        "The results with logMel, MFCC, and eGeMAPS features are similar, but notably lower with prosodic features.",
        "A reason for that could be the smaller number of features in the latter.",
        "The similar results suggest that for a CNN the particular choice of features is not as important as the model architecture and the amount and kind of training data.",
        "We found strong differences between improvised and scripted speech, obtaining better results on the former.",
        "Experiments with decreasing signal length showed that the performance decreases slightly, but remains at a relatively high level even for short signals down to two seconds.",
        "Future work includes testing the presented ACNN on a different database."
    ],
    "1665": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our method obtains promising results and outperforms traditional slot filling due to the knowledge sharing of atomic concepts.",
        "The atomic concepts are constructed manually in this paper, and future work may explore more flexible concept definition for adaptive LU.",
        "A competitive method based on slot name embedding can be extracted from the literal description of the slot name automatically, laying foundation for finding a more flexible concept definition method for adaptive LU."
    ],
    "1667": [
        "Our proposed model outperforms its QA-only counterpart by a significant margin on the SQuAD dataset.",
        "The joint model demonstrates the effectiveness of joint training between QA and question generation, and thus offers a novel perspective and a promising direction for advancing the study of QA.",
        "Although evaluation scores are still lower than the state-of-the-art results achieved by dedicated QA models, the proposed model nonetheless demonstrates the effectiveness of joint training between QA and question generation.",
        "The proposed model learns to dynamically switch between copying words from the document and generating words from a vocabulary.",
        "Question answering can benefit from synergistic interaction between the two tasks through parameter sharing and joint training under this multitask setting."
    ],
    "1668": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Generative models for visual dialog are typically trained with an MLE objective, which can lead to safe and generic responses.",
        "Discriminative (or retrieval) models have been shown to significantly outperform their generative counterparts.",
        "Transferring knowledge from a powerful discriminative visual dialog model to a generative model can improve its performance.",
        "Employing a novel visual dialog encoder that reasons about image-attention informed by the history of the dialog can enable the discriminator to learn meaningful structure in dialog responses.",
        "A Gumbel-Softmax (GS) approximation to the discrete distribution, coupled with a ST gradient estimator for end-to-end differentiability, can be used to leverage the power of discriminative models in generative ones."
    ],
    "1669": [
        "The proposed multichannel dependency-based convolutional neural network outperforms current deep learning models and single feature-based or kernel-based models on the sentence-based PPI task.",
        "The model is substantially more generalizable across different datasets, utilizing the dependency structure of sentences as a separated channel to capture global information more effectively.",
        "Assembling different resources into the model, similar to what has been done with rich-featured methods, can potentially improve performance further.",
        "The method can be extended to PPIs beyond the sentence boundary and tested on other biomedical relations such as chemical-disease relations.",
        "The use of dependency structure in the model enables it to capture global information more effectively, leading to better performance."
    ],
    "1670": [
        "We have explored a full-fledged pipeline using AMR for summarization for the first time.",
        "Our method for extracting summary graph outperformed previous methods.",
        "We provide a strong baseline for text summarization using AMR for possible future works.",
        "ROGUE can't be used for evaluating the abstractive summaries generated by our AMR pipeline."
    ],
    "1671": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets using Dynamic Memory Induction Networks (DMIN) for few-shot text classification.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning, and it is worth investigating this type of models in other learning problems.",
        "It remains unclear whether unsupervised versions of deep learning models are capable of capturing even simple linguistic phenomena.",
        "Further investigation is needed to determine whether more complex models might be able to generate text with higher productivity and to compare AEs against simpler models such as a basic LSTM language model."
    ],
    "1672": [
        "We have shown how back-translating can be an effective paraphrase-generation technique.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We have also shown that filtering can improve the generated paraphrase corpus.",
        "In doing so, we also identified characteristics that distinguish NMT output from human-written sentences.",
        "Future work will aim to improve this generation process, for instance by devising more sophisticated filtering techniques."
    ],
    "1677": [
        "The recurrent ladder network (RLN) can perform as well as similarly parametrised BLSTM models while using only 50% of the labelled data, demonstrating its ability to effectively regularise itself using unsupervised training data.",
        "Current state-of-the-art methods performed better overall, but this does not come as a surprise given that these models use up to 160 times more parameters.",
        "The proposed recurrent decoder proved to be better at denoising than the feed-forward decoder.",
        "Recurrent noise injection does not perform as expected and may need the help of normalisation (e.g. batch normalisation) to work efficiently.",
        "In the future, the authors plan to take advantage of the semi-supervised learning abilities of the RLN in conjunction with more complex recurrent models such as bidirectional and attention-based RNNs to utilise unlabelled data even more effectively.",
        "The learning framework may scale with more complex temporal dynamics in more challenging tasks such as end-to-end speech recognition or question answering."
    ],
    "1681": [
        "We propose a feature-based approach and a neural network based approach for content-based table retrieval.",
        "Our approach verifies the effectiveness of our feature-based approach and presents future challenges for content-based table retrieval.",
        "We release a new dataset consisting of web queries and web tables to conduct comprehensive experiments on two datasets.",
        "Our approach outperforms existing methods and presents opportunities for future research.",
        "We conduct extensive experiments to verify the effectiveness of our approach and identify potential areas for improvement."
    ],
    "1682": [
        "Our work aims at improving semantic relevance of generated summaries and source texts for Chinese social media text summarization.",
        "Our model is able to transform the text and the summary into a dense vector, and encourage high similarity of their representation.",
        "Experiments show that our model outperforms baseline systems, and the generated summary has higher semantic relevance.",
        "Improving semantic relevance of generated summaries and source texts for Chinese social media text summarization.",
        "Transforming the text and the summary into a dense vector and encouraging high similarity of their representation.",
        "Outperforming baseline systems and generating higher semantic relevance."
    ],
    "1683": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The Con-Mask model ranked the correct screenwriter David Duncan as the 2nd candidate, but the name 'David Duncan' does not actually appear in the film's description.",
        "The reason for this error is because the indicator word for The Vampire Diaries was 'consulting producer', which was not highly correlated to the relationship name 'notable work' from the model's perspective.",
        "The ConMask model was able to capture the correct relationship because the words 'The Time Machine' appeared in the description of David Duncan as one of his major works."
    ],
    "1684": [
        "Sampled word-level EMBR training provides a simple and effective way to optimize expected word error rate during training, which improves empirical performance on two speech recognition tasks with disparate architectures.",
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks."
    ],
    "1685": [
        "The use of token embeddings can improve the performance of syntactic analysis of Twitter.",
        "Token embeddings can encode sense and POS information, grouping together tokens of different types with similar in-context meanings.",
        "Using token embeddings in simple predictors consistently improves performance, even rivaling the performance of strong structured prediction baselines.",
        "The use of unlabeled data for learning representations of words can be effective and efficient.",
        "The proposed method can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks."
    ],
    "1688": [
        "Our non-monotonic parser obtains better performance than the monotonic version, regardless of the loss expression used.",
        "One of the loss expressions developed proved very promising by providing the best average accuracy, despite being the farthest approximation from the actual loss.",
        "The proposed lower bound makes the non-monotonic oracle the fastest one among all dynamic oracles developed for the non-projective Covington algorithm.",
        "Our approach is a feasible alternative when it is not practical to compute the actual loss.",
        "Our oracle could also be used in neural network implementations of greedy transition-based parsing, providing an interesting avenue for future work.",
        "Gains from both techniques (non-monotonicity and transition-based parsing) should be complementary, although we might see a \"diminishing returns\" effect."
    ],
    "1689": [
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Our work provides new approaches for contextualizing citations which is a sub-task for enriching citation texts and thus can benefit various bibliometric enhanced NLP applications such as information extraction, information retrieval, article recommendation, and article summarization.",
        "Our work provides a comprehensive new framework for summarizing scientific papers that helps generating better scientific summaries."
    ],
    "1691": [
        "The neural embeddings are learned from a very limited set of data; one interesting future direction is to study the limits of the approach as the amount of training data is varied, or to extend it to use no labeled data at all.",
        "It is encouraging that this approach works despite the lack of non-word examples in the training data, and an interesting avenue for future work is to attempt to further improve performance by explicitly training on both word and non-word segments.",
        "Additional future directions include training a QbE system end-to-end and extending our model to operate at the level of multi-word phrases."
    ],
    "1692": [
        "The proposed approach, TextGAN, delivers superior performance compared to related approaches.",
        "The learned latent representation space can \"smoothly\" encode plausible sentences.",
        "The proposed methods can produce realistic sentences.",
        "Conditional GAN models can be applied to disentangle the latent representations for different writing styles.",
        "Leveraging an additional refining stage can improve the quality of generated sentences."
    ],
    "1694": [
        "The proposed approach outperforms benchmark models across different datasets.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Experimental results show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "The proposed model learns an improved autoencoder and exhibits a smooth latent space.",
        "The model is sensitive to hyperparameters, and only simple structures such as binarized digits and short sentences were tested.",
        "Training deep latent variable models that can robustly model complex discrete structures (e.g., documents) remains an important open issue in the field."
    ],
    "1695": [
        "We observed a 1.0-4.4% absolute accuracy improvement on our internal test set with 10k-200k training data.",
        "On ATIS, we observed a > 6% accuracy gain.",
        "The results demonstrate the capabilities of sequence-to-sequence modeling to capture a canonicalized representation between tasks, particularly when the architecture uses shared parameters across all its components.",
        "By utilizing an auxiliary task like syntactic parsing, we can improve the performance on the target semantic parsing task.",
        "In future work, we want to use this architecture to build models in an incremental manner where the number of sub-tasks K continually grows.",
        "We also want to explore auxiliary tasks across multiple languages so we can train multilingual semantic parsers simultaneously, and use transfer learning to combat labeled data sparsity."
    ],
    "1697": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our proposed method exhibits significantly better results compared to an entity tagging baseline.",
        "The proposed question generator uses a sequence-to-sequence model and pointer-softmax mechanism to dynamically switch between copying a word from the document and generating a word from a vocabulary.",
        "The generated questions exhibit both syntactic fluency and semantic relevance to the conditioning documents and answers, and appear useful for assessing reading comprehension in educational settings.",
        "Fine-tuning the two-stage framework end-to-end is an interesting direction for future work.",
        "Exploring abstractive key-phrase extraction is another promising direction for future research."
    ],
    "1704": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed Graph-NN model is effective for the KBC task, especially when entities are unobserved at training time.",
        "The model showed state-of-the-art performance on WordNet11 in the standard KBC setting.",
        "4. Claim: The use of a Graph-NN tailored to KBC with OOKB entities can improve the performance of the model."
    ],
    "1705": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "LDA-based methods can improve on the keyword heuristic when it comes to detection of concept expression.",
        "The Online LDA/Top 30 Rank method in particular, can yield important improvements over the keyword heuristic that is currently used as a concept detection heuristic in many contexts.",
        "Detecting concepts through topics can serve as a general-purpose method for at least some forms of concept expression that are not captured using naive keyword approaches.",
        "Further research should try to integrate other methods of detecting concept presence in textual data that focus on other means of expressing concepts in texts and discourse."
    ],
    "1708": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Minimum risk training proves to improve over standard maximum likelihood estimation substantially.",
        "Semi-supervised training is capable of exploiting monolingual corpora to improve low-resource translation.",
        "The toolkit also features a visualization tool for analyzing the translation process of THUMT.",
        "The toolkit is freely available at http://thumt.thunlp.org."
    ],
    "1712": [
        "The proposed approach outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "ConMask is able to capture the correct relationship between entities, even when the name of the entity does not appear in the description.",
        "The approach demonstrates new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The use of a structured memory representation allows the model to reason about entities and relations for question answering.",
        "Future work will involve investigating the performance of these models on more real-world datasets and interpreting what the models learn.",
        "Scaling these models to answer questions about entities and relations from reading massive text corpora is a potential future direction."
    ],
    "1715": [
        "The proposed dataset of goal-oriented dialogs with user profiles can be used as a testbed for training and analyzing end-to-end conversational agents.",
        "The tasks in the dataset can be used to evaluate the strengths and weaknesses of models in a systematic way before applying them to real data.",
        "Building mechanisms that can solve synthetically generated tasks is a reasonable starting point towards developing sophisticated personalized dialog systems.",
        "The model based on Memory Networks was unable to perform compositional reasoning or personalization, indicating the need for further work on learning methods.",
        "A split memory architecture for end-to-end trained dialog agents can improve overall performance.",
        "Formulating personalization in dialog as a multi-task learning problem can lead to significant improvements and allow for leveraging shared features and relationships between various conversation styles."
    ],
    "1716": [
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our base model with three layers of Stacked Residual RNN already outperforms the current state-of-the-art for Spanish.",
        "The numerical gradient for training is noisy, and sometimes the SGD process might take several epochs to find an improvement on the development set.",
        "The English dataset because the base model trained with sequence level log-likelihood fits very well on the training set.",
        "Interestingly, the Language Model embeddings seem to have opposite effects on Spanish and English.",
        "For English, the LMs are arguably better, with much lower perplexities than the LMs for Spanish.",
        "The Spanish models also have less data to train with, and it might affect the performance."
    ],
    "1720": [
        "The encoder-decoder-reconstructor offers significant improvement in BLEU scores for English-Japanese translation tasks.",
        "Pre-training the forward translation model is necessary to train the encoder-decoder-reconstructor well.",
        "The encoder-decoder-reconstructor can alleviate the problem of repeating and missing words in the translation on English-Japanese translation tasks.",
        "The encoder-decoder-reconstructor cannot be trained well without pre-training.",
        "Training the forward translation model in a manner similar to conventional attention-based NMT is effective for pre-training."
    ],
    "1726": [
        "The proposed model significantly outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by the proposed model are coherent with human judgments.",
        "The use of self-attention mechanism can effectively capture long distant dependency relations.",
        "The proposed model does not require syntactic parsing at train or test time, making it a cheaper alternative to syntactic features.",
        "Applying the technique to other tasks that rely on pipelining syntactic parsing is a promising avenue for future work."
    ],
    "1727": [
        "We have set to investigate the effectiveness of the Bidirectional LSTM and Bidirectional LSTM-CRF for drug name recognition and clinical concept extraction.",
        "The neural network models have obtained significantly better results than the CRF.",
        "We have reported state-of-the-art results over the i2b2/VA, DrugBank and MedLine datasets using the B-LSTM-CRF model.",
        "Retraining GloVe on a domain-specific dataset such as MIMIC-III can help learn vector representations for domain-specific words and increase the classification accuracy.",
        "Adding hand-crafted features does not further improve performance since the neural networks can capture the relevant information."
    ],
    "1729": [
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?"
    ],
    "1731": [
        "We introduce a two-stage SynNet for the task of transfer learning for machine comprehension, which is both challenging and of practical importance.",
        "With our network and a simple training algorithm, we are able to generalize a MC model from one domain to another with no annotated data.",
        "We present strong results on the NewsQA test set, with a single model improving performance of a baseline BIDAF model by 5.3% and an ensemble by 7.6% F1.",
        "Through ablation studies and error analysis, we provide insights into our methodology on the SynNet and MC models that can help guide further research in this task."
    ],
    "1736": [
        "Our novel memory-based attention mechanism results in a linear computational time of O(KD(|S|+|T |)) during decoding in seq2seq models.",
        "Our technique leads to consistent inference speedups as sequences get longer.",
        "Our attention mechanism learns meaningful alignments despite being constrained to a fixed representation after encoding.",
        "We encourage future work that explores the optimal values of K for various language tasks and examines whether or not it is possible to predict K based on the task at hand.",
        "We also encourage evaluating our models on other tasks that must deal with long sequences but have compact representations, such as summarization and question-answering, and further exploration of their effect on memory and training speed."
    ],
    "1737": [
        "Our proposed model, SAM (Semantic Attribute Modulated), leverages diverse and meaningful attributes to generate interpretable texts.",
        "The attention mechanism in our model allows for flexible scoring of attributes, making it more powerful and interpretable.",
        "The use of semantic attribute representations in the hidden feature spaces as generation model inputs brings flexibility to the whole model.",
        "Our SAM model is effective and interpretable, as demonstrated by extensive experimental results.",
        "Incorporating more attributes with semantic meaning for the language model task can further improve the performance of our SAM model.",
        "The flexibility of our SAM model allows it to be used in other language generation tasks beyond lyric generation, such as speech recognition."
    ],
    "1740": [
        "The approach described in this paper was inspired by successful submissions to past editions of the PAN task on gender identification, to the Discriminating between Similar Languages (DSL), and to Arabic Dialect Identification (ADI) shared tasks.",
        "The best results for gender identification were obtained on English data, with an accuracy of 0.79, and the best results for language variety identification were obtained for Portuguese, with an accuracy of 0.97.",
        "In the official evaluation carried out on the test set, our system was ranked 11th on language variety identification and 12th on gender identification, out of 22 submissions, achieving 0.85 and 0.75 accuracy respectively.",
        "To the best of our knowledge, the PAN labs 2017 was the first shared task to include language varieties and dialects in author profiling, opening avenues for future research.",
        "There is still room for improvement in our system's performance, and we are currently investigating ways to improve it by testing a meta-classifier which achieved very good results on German dialect identification."
    ],
    "1745": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "A novel neural transition-based approach can deal well with an extreme low-resource setup.",
        "With abundant data, encoder/decoder architectures with soft attention are very strong.",
        "HACM achieves a comparable development set performance.",
        "Ensembling different system runs is important for optimal results.",
        "At the low setting, our best system combination achieves an average test set accuracy of 50.61% (an average Levenshtein distance (LD) of 1.29).",
        "At the medium setting, 82.8% (LD 0.34) of the entities are correctly predicted.",
        "At the high setting, 95.12% (LD 0.11) of the entities are correctly predicted."
    ],
    "1747": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Context-aware weights, composed of an IDF base and a context-related bias, are induced by deep neural networks, which naturally learn representations with the least entropy, even when the target is generated homogeneously.",
        "The proposed AdaBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages."
    ],
    "1748": [
        "Our method is competitive with the state of the art when it comes to unsupervised methods to learn polysemous word representations.",
        "The context is irrelevant to the word, the embedding norms will be almost 0.",
        "Our method is simple both from the modeling and the learning point of view."
    ],
    "1749": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The proposed algorithm for joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The proposed algorithm is compatible with other techniques, such as threshold-based pruning, reinforcement learning based scoring, reducing Softmax computation, and diverse decoding.",
        "The increased memory usage of the proposed algorithm does not cause a problem unless T and B are both large numbers."
    ],
    "1750": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "The locally-normalized structure of the model can be explained by the brevity problem.",
        "TFBA (tensor factorization-based method for higher-order RSI) is the first attempt at inducing higher-order (n-ary) schemata for relations from unlabeled text.",
        "The back-off based factorization idea exploited in TFBA can be useful in other sparse factorization settings."
    ],
    "1752": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "The use of self-attention mechanism in our model allows for more effective capture of long distant dependency relations.",
        "The performance of our model is better than existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are accurate and coherent with humans' judgments.",
        "Our model can improve the quality of multiturn dialogue generation by using proper detection methods, such as self-attention.",
        "The use of self-attention mechanism in our model allows for more effective capture of long distant dependency relations, leading to better performance.",
        "The results suggest that a lot of progress can be made by improving on the event extraction part of the pipeline."
    ],
    "1758": [
        "This is one of the first papers to synthesize adversarial samples from complicated text samples.",
        "Unlike images, in texts, a number of conditions must be satisfied while doing the modification steps to ensure the preservation of semantic meaning and the grammar of the text sample.",
        "In this paper, we are considering each word at a time and selecting the most appropriate modification in a greedy way.",
        "However, a much better approach is to consider each of the sentences in the text sample and modifying it which eventually confuses the classifier.",
        "The steps adopted for modifications are heuristic in nature, which can be improved and automated further to obtain better results."
    ],
    "1760": [
        "Our model achieves state-of-the-art visual reasoning on CLEVR without explicitly incorporating reasoning priors.",
        "Our model learns an underlying structure required to answer CLEVR questions by finding clusters in the CBN parameters of our model.",
        "Simply by manipulating feature maps with CBN, a RNN can effectively use language to influence a CNN to carry out diverse and multi-step reasoning tasks over an image.",
        "It is unclear whether CBN is the most effective general way to use conditioning information for visual reasoning or other tasks.",
        "Other approaches employ a similar, repetitive conditioning, so perhaps there is an underlying principle that explains the success of these approaches.",
        "Regardless, we believe that CBN is a general and powerful technique for multi-modal and conditional tasks, especially where more complex structure is involved."
    ],
    "1761": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "Using less than 1% of the training data, our method can achieve competitive performance compared to previous systems trained using the full dataset.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The generative models can achieve good results on their own, with the LSTM generative model showing particularly strong and self-contained performance.",
        "Explicitly combining scores allows the reranking setup to achieve better performance than implicit combination.",
        "The increase in performance from augmenting the candidate lists with the results of the search may not be worth the required computational cost in a practical parser."
    ],
    "1765": [
        "By jointly learning ternary and fine-grained classification tasks with a multitask learning model, one can greatly improve performance on the second task.",
        "Sentiment expression varies in different textual types, languages, and granularity levels, which opens avenues for future research using multitask approaches that combine such resources.",
        "BiLSTM networks were used here, but other types of networks or combinations of different types of networks and tasks could be investigated to improve performance.",
        "The foundations of (Caruana, 1997) were mainly relied upon, but the internal mechanisms and theoretical guarantees of multitask learning remain to be better understood."
    ],
    "1767": [
        "The proposed model for extraction of hypernymy relations based on the projection of distributional word vectors shows significant improvements over the state-of-the-art model without negative sampling.",
        "The model incorporates information about explicit negative training instances represented by relations of other types, such as synonyms and co-hyponyms, and enforces asymmetry of the projection operation.",
        "The proposed approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The use of negative sampling in the model improves the performance of the proposed approach."
    ],
    "1769": [
        "We defined reformulation cause prediction as a four-class classification problem.",
        "The proposed method, which combines all feature sets, outperforms the baseline which uses component-independent features such as session information and reformulation related information.",
        "Our approach demonstrates the effectiveness of using user logs to predict reformulation causes in IAs.",
        "The experimental results show that our method significantly outperforms the baseline.",
        "The proposed method combines all feature sets, which leads to better performance compared to the baseline.",
        "Our approach provides a more flexible and accurate way of predicting reformulation causes in IAs."
    ],
    "1770": [
        "The proposed simple unigram model outperforms previous work on scientific fraud detection.",
        "High-level linguistic features beyond syntactic treelets do not lead to improvements in fraud detection.",
        "A feature analysis shows that comparison and explanation (at the semantic and discourse level) are indicators of non-fraud.",
        "Fraudulent writing uses slightly different hedging strategies."
    ],
    "1773": [
        "Our approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The proposed Cosine Annealing Strategy combines two methods to improve domain adaptation performance.",
        "Pseudo-labeled target-domain data is used to train the few-shot classifier.",
        "Our approach can achieve competitive performance with less than 1% of the training data.",
        "Our method significantly outperforms previous systems by reducing the error by 21% on English Switchboard.",
        "The QA system obtains reasonable performance on COMPLEXQUESTIONS, but CONJUNCTION and N-ARY questions are challenging for a web-based QA system.",
        "Reproducibility code, data, annotations, and experiments are available on the CodaLab platform."
    ],
    "1775": [
        "Our proposed approach (Dynamic Memory Induction Networks) achieves new state-of-the-art results on the miniRCV1 and ODIC datasets for few-shot text classification.",
        "Our self-supervised tasks can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our structure reranking approach improves P@1 by more than 4% for the cross-genre document retrieval task.",
        "Adding more structural information such as coreference relations to our structure matching and applying a more sophisticated parameter optimization technique such as Bayesian optimization can further improve the results."
    ],
    "1779": [
        "utilizing the machine-readable emoji sense definitions",
        "explored multiple emoji embedding models to measure emoji similarity",
        "created EmoSim508 dataset, which consist of 508 emoji pairs and used it as the gold standard",
        "released the EmoSim508 dataset and our emoji embedding models with our paper",
        "the first effort that explored utilizing a machine-readable emoji sense inventory and distributional semantic models to learn emoji embeddings",
        "extend our emoji embedding models to understand the differences in emoji interpretations due to how they appear across different platforms or devices",
        "apply our emoji embedding models to other emoji analysis tasks such as emoji-based search",
        "whether emoji similarity results could be used to improve the recall in emoji-based search applications."
    ],
    "1782": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our proposed approach enabled ASR and TTS to further improve their performance by teaching each other using only unpaired data.",
        "It is necessary to further validate the effectiveness of our approach on various languages and conditions (i.e., spontaneous, noisy, and emotion)."
    ],
    "1783": [
        "The proposed end-to-end IE model does not require detailed token-level labels and is competitive with baseline models that rely on token-level labels. (Claim 1)",
        "The proposed model can be used on real-life IE tasks where intermediate token-level labels are not available or feasible to create. (Claim 2)",
        "The model is stable to hyperparameter changes and has a small difference in performance between the worst and best cases. (Claim 3)",
        "The proposed model can only output values that are present in the input, which is a limitation for outputs that are normalized before being submitted as machine-readable data. (Claim 4)",
        "The model is an important step towards applicable end-to-end IE systems, and future work may involve experimenting with adding character-level models on top of the pointer network outputs. (Claim 5)"
    ],
    "1784": [
        "The proposed algorithm addresses the issue of rare concepts in visual question answering.",
        "The main pipeline of the proposed algorithm includes a coattention module, a memory module, and an LSTM module.",
        "The algorithm successfully answers questions involving rare concepts where other VQA methods fail.",
        "The proposed algorithm performs well against state-of-the-art VQA systems on two large-scale benchmark datasets.",
        "The algorithm is demonstrated to be effective in answering questions that involve rare concepts."
    ],
    "1785": [
        "The proposed psycho-linguistically motivated constituent parsing system achieves state-of-the-art results on standard WSJ benchmark.",
        "The system outperforms bottom-up parsing on non-local ambiguity and top-down parsing on local decision.",
        "The resulting parser obtains high constituent parsing results, with an F1 score of 94.2%.",
        "The parser also achieves high dependency parsing results, with a UAS score of 96.2% and a LAS score of 95.2%."
    ],
    "1789": [
        "Our proposed neural reranking architecture improves NER performance significantly, obtaining the best results on CoNLL 2003 English task.",
        "The current method has a problem where all candidates share the same non-entity words, leading to similar and hard-to-distinguish neural representations.",
        "Our reranking system improves NER performance by exploiting neural structure to learn sentence patterns and using a mixture reranking strategy.",
        "The proposed method can enlarge the difference between candidate sequences by developing the neural tree structures based on entity position.",
        "Intuitively, entities contribute more than non-entity when modeling the sequence vector, attention model (Bahdanau et al., 2014) may help collect more information from the intermediate vector of sentences.",
        "Using semisupervised methods to construct a bigger training data can help reranker learn more sentence patterns.",
        "An auxilliary classifier of predicting the probability of the replaced words being a real entity may be an important compensation for the outside-entity sentence patterns."
    ],
    "1792": [
        "Our results outperform existing domain similarity metrics on three tasks (sentiment analysis, POS tagging and parsing), and are competitive with a state-of-the-art domain adaptation approach.",
        "We present the first study on the transferability of such measures, showing promising results to port them across models, domains, and related tasks.",
        "Our proposed method uses Bayesian Optimization to learn data selection measures for transfer learning.",
        "Existing domain similarity metrics are outperformed by our method on three tasks (sentiment analysis, POS tagging and parsing).",
        "Our method is competitive with a state-of-the-art domain adaptation approach."
    ],
    "1796": [
        "Our method could lead to significant improvements of translation quality in different feature sets and beam size.",
        "Compared to previous efforts in SMT tuning, our method directly models the order information of the complete translation list.",
        "Experiments show that our method could achieve significant improvements in different feature sets and beam sizes.",
        "Our current work focuses on the traditional SMT task, but it would be interesting to integrate our methods with modern neural machine translation systems or other structure prediction problems.",
        "It may also be interesting to explore more methods on listwise tuning framework, such as investigating different methods to enhance top order of translation list directly w.r.t a given evaluation metric."
    ],
    "1797": [
        "The proposed system is based on a general supposition about the semantic structure of puns and combines two types of algorithms: supervised learning and rule-based.",
        "The supervised learning algorithm showed better results in solving an NLP task than the rule-based approach.",
        "The system attempts to combine two very different dictionaries (Roget's Thesaurus and Wordnet) to improve the results.",
        "The reliability of Thesaurus in reproducing a universal semantic map can be doubted, but it is still an effective source of data for certain tasks.",
        "The attempts to map Thesaurus to Wordnet are weak, raising questions about the objectivity and universality of semantic maps for WSD tasks."
    ],
    "1801": [
        "ASR errors pose a major obstacle to accurate DST in SDSs.",
        "To reduce the error propagation, we suggest to exploit the rich ASR hypothesis space encoded in cnets.",
        "We develop a novel method to encode cnets via a GRU-based RNN and demonstrate that this leads to improved DST performance compared to encoding the best ASR hypothesis on the DSTC2 dataset.",
        "In future experiments, we would like to explore further ways to leverage the scores of the hypotheses."
    ],
    "1807": [
        "The proposed method for crowdsourcing the creation of multiple choice QA data has been presented, with a particular focus on science questions.",
        "A dataset of 13.7K science questions, called SciQ, has been constructed using this methodology and released for future research.",
        "The baseline evaluations have shown that this dataset is a useful research resource for investigating neural model performance in medium-sized data settings and augmenting training data for answering real science exam questions.",
        "There are multiple strands for possible future work, including a systematic exploration of multitask settings to best exploit this new dataset, and the adaptation of this idea in negative sampling.",
        "The method could be further used to improve automatic document selection, question generation, and distractor prediction to generate questions fully automatically."
    ],
    "1808": [
        "The authors introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "The proposed model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The authors contrast the performance of widely-known baselines to conditional language models for sarcasm/irony detection by looking at a particular type of context, conversation context.",
        "Modeling conversation context helps in sarcasm detection, and can determine what part of the conversation context triggered the sarcastic reply.",
        "Conditional LSTM networks and LSTM networks with sentence-level attention achieved significant improvement in sarcasm detection.",
        "Attention-based models are able to identify inherent characteristics of sarcasm, such as sarcasm markers and sarcasm factors.",
        "The authors plan to study larger contexts, such as the full thread in a discussion forum, and analyze sarcastic replies that do not contain sarcasm markers or explicit incongruence."
    ],
    "1809": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The proposed method, ReCoSa, significantly outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by the model are coherent with human judgments.",
        "The use of self-attention mechanism can effectively capture long distant dependency relations.",
        "The introduction of qualitatively different information (i.e., grounding) leads to improved performance.",
        "The system learned high-quality grounded word embeddings that outperform non-grounded ones on standard semantic similarity benchmarks.",
        "Exploring multi-task learning for sentence representations where one of the tasks involves grounding may be an avenue worth exploring in future work.",
        "Investigating the effect of average abstractness or concreteness of the evaluation datasets on performance."
    ],
    "1814": [
        "syllable-aware language models fail to outperform competitive character-aware ones.",
        "usage of syllabification can reduce the total number of parameters and increase the training speed, albeit at the expense of languagedependent preprocessing.",
        "Morphological segmentation is a noteworthy alternative to syllabification: a simple morpheme-aware model which sums morpheme embeddings looks promising."
    ],
    "1817": [
        "The content of task descriptions can be used to create a classification of micro-tasks.",
        "Proposed two additional similarity measures for micro-tasks.",
        "Evaluation shows that a classification is feasible using the proposed setup.",
        "Propose similarity measures that can be applied to find similarities between micro-tasks.",
        "The proposed similarity measures can model similarities that are different from the known categories."
    ],
    "1822": [
        "We present the first openly available word sense disambiguation system that is unsupervised, knowledge-free, and interpretable at the same time.",
        "The system performs extraction of word and super sense inventories from a text corpus.",
        "The disambiguation models are learned in an unsupervised way for all words in the corpus on the basis on the induced inventories.",
        "The user interface of the system provides efficient access to the produced WSD models via a RESTful API or via an interactive Web-based graphical user interface.",
        "The system is available online and can be directly used from external applications.",
        "Besides, in-house deployments of the system are made easy due to the use of the Docker containers.",
        "12 A prominent direction for future work is supporting more languages and establishing cross-lingual sense links."
    ],
    "1823": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Our proposed method improves over random initialization on a variety of tasks.",
        "Reconstruction with other choices of architecture is an important future direction."
    ],
    "1824": [
        "The proposed algorithm can infer OOV word embedding vectors from pre-trained data with good performance.",
        "The method is useful for low-resource languages and tasks with little labeled data available, and is task-agnostic.",
        "The use of characters as input improves performance on annotated sequence-tagging tasks for a large variety of languages across dimensions of family, orthography, and morphology.",
        "Future work may consider the use of other subword units, such as morphemes, phonemes, or even bitmap representations of ideographic characters."
    ],
    "1826": [
        "The model achieves state-of-the-art performance on the OntoNotes benchmark without using external preprocessing tools.",
        "The model learns to generate useful mention candidates from the space of all possible spans.",
        "A novel head-finding attention mechanism is introduced, which learns a task-specific preference for head words that correlates strongly with traditional head-word definitions.",
        "The model substantially pushes the state-of-the-art performance, but the improvements are potentially complementary to a large body of work on various strategies to improve coreference resolution.",
        "The model's performance is potentially improved by incorporating entity-level inference and world knowledge, which are important avenues for future work."
    ],
    "1827": [
        "Pre-training models with extractive summaries helps improve domain adaptation for abstractive neural summarization.",
        "The model is capable of selecting salient information even when trained on out-of-domain data, indicating potential for future directions in developing domain adaptation techniques for content selection.",
        "The attention weight distribution over input tokens can be analyzed to determine the model's ability to select salient information.",
        "The model can learn language generating behavior with in-domain data while acquiring content selection from out-of-domain data."
    ],
    "1828": [
        "an orderless visual input representation of concepts is not enough to produce good descriptions\" - This claim suggests that using only object categories and locations to generate visual descriptions is not sufficient, and that other information such as object extents, locations, and counts are important for producing accurate descriptions.",
        "our encoding mechanism is able to capture useful spatial information\" - This claim suggests that the proposed LSTM network-based encoding mechanism is effective at capturing useful spatial information, even when the input is provided as a sequence rather than an explicit 2D representation of objects.",
        "using our proposed OBJ2TEXT model in combination with an existing image captioning model and a robust object detector we showed improved results in the task of image captioning\" - This claim suggests that combining the proposed OBJ2TEXT model with existing image captioning models and robust object detectors leads to improved results in the task of image captioning.",
        "our OBJ2TEXT model is able to generate more accurate image descriptions\" - This claim suggests that the proposed OBJ2TEXT model is effective at generating more accurate image descriptions compared to other models.",
        "object extents, locations, and object counts all contribute to generate more accurate image descriptions\" - This claim suggests that these factors are important for generating accurate image descriptions, and that the proposed OBJ2TEXT model takes these factors into account when generating descriptions."
    ],
    "1831": [
        "We highlighted the overlap between NLI and dialect, language variety, and similar language identification.",
        "Our approach achieved competitive results of 83.55% accuracy ranking 3rd in the fusion track.",
        "There is still room for improvement in our approach.",
        "In previous shared tasks (e.g. NLI 2013 , DSL 2015 , and ADI 2016) we observed that SVM ensembles ranked higher in the results tables than our method did in the NLI 2017.",
        "We are investigating whether the combination of features or the implementation itself can be optimized for better performance."
    ],
    "1834": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; more general globally-normalized models can be trained in a similarly inexpensive way.",
        "Solving the brevity problem leads to significant BLEU gains, and there is still room for improvement by solving label bias in general.",
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Introducing topical information or considering detailed content information can further improve the quality of generated responses.",
        "Our approach brings state-of-the-art results for language-only, visual-only, and acoustic-only multimodal sentiment analysis on CMU-MOSI."
    ],
    "1835": [
        "The proposed dataset is useful for predicting the success of distributed representations in the relation classification task.",
        "Gated Additive Composition (GAC) is effective in composing distributed representations of relational patterns.",
        "The presented dataset can be used not only for distributed representations of relational patterns but also for other NLP tasks (e.g., paraphrasing).",
        "Analyzing the internal mechanism of LSTM, GRU, and GAC, we plan to explore an alternative architecture of neural networks that is optimal for relational patterns."
    ],
    "1836": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model of part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "HyperVec, a novel neural model, strengthens hypernymy similarity and captures the distributional hierarchy of hypernymy.",
        "The capability to generalize hypernymy and learn the relation instead of memorizing prototypical hypernyms."
    ],
    "1842": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "There is a lack of stringent annotation policies in OIE, which makes comparative analysis and design of OIE systems difficult.",
        "Current OIE systems have not reached an effective design yet for extracting higher-order n-ary tuples.",
        "Only system PP leverages well-researched ideas from normal forms in data base theory in its design.",
        "Next-generation OIE systems should offer some convenient 'knobs' for tuning it towards common downstream tasks, such as populating a knowledge base or extracting typed relations against a schema."
    ],
    "1853": [
        "The Question Dependent Recurrent Entity Network (QDREN) is used for reasoning and reading comprehension tasks, and it uses a particular RNN cell to store relevant information about the given question.",
        "The QDREN model improved the State-of-The-Art in the bAbI 1k task and achieved promising results in the Reading comprehension task on the CNN & Daily news dataset.",
        "The QDREN model has not enough expressive power to make a selective activation among different memory blocks, as seen in Figure 2(a) where all gates are open for all memories.",
        "Despite this limitation, the QDREN model still outperforms other models.",
        "There is room for improvement in the behavior of the proposed cell, and passing all the bAbI tasks could be the key to further improve the model's performance."
    ],
    "1858": [
        "We presented a new dataset for Russian verbal associations.",
        "Social factors such as gender and specialization provide a significant amount of information on the type of association.",
        "Our proposed gendersensitive associative model demonstrates the significance of incorporating social factors into traditional NLP models.",
        "Incorporating social factors into NLP models can improve the accuracy of verbal associations.",
        "Traditional NLP models do not fully capture the complexity of verbal associations, and incorporating social factors can help to address this limitation."
    ],
    "1859": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Dynamic Memory Induction Networks (DMIN) achieve new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our approach can be applied to link the implicit entity mentions of a given type in the absence of specific keywords.",
        "Contextual knowledge plays a similar role in the disambiguation step of the explicit entity linking task.",
        "The requirement to evolve the EMN is observed with the first experiment.",
        "Two events can change the EMN over time: 1) A new entity becomes popular and people start to tweet about it, or the popularity of an existing entity fades away.",
        "In future, we will implement the operators that will keep EMN up-to-date by continuously collecting the tweets and injecting derived knowledge from them, as well as from DBpedia."
    ],
    "1860": [
        "Our new dataset for highlight prediction is based on visual cues and textual audience chat reactions in multiple languages.",
        "Our methods use multimodal features to improve highlight prediction accuracy.",
        "We hope our new dataset will encourage further multilingual, multimodal research.",
        "Our approach can be applied to a variety of languages, including English and non-English languages.",
        "Our method outperforms previous baselines in highlight prediction tasks."
    ],
    "1861": [
        "We presented an algorithm for satisfying constraints in neural networks that avoids combinatorial search.",
        "Our algorithm employs the network's efficient unconstrained procedure as a black box to coax weights towards well-formed outputs.",
        "We evaluated the algorithm on three tasks including SOTA SRL, seq2seq parsing and found that GBI can successfully convert failure sets while also boosting the task performance.",
        "Accuracy in each of the three tasks was improved by respecting constraints.",
        "Additionally, for SRL, we employed GBI on a model trained with similar constraint enforcing loss as GBI's (Mehta*, Lee*, and Carbonell 2018) , and observe that the additional test-time optimization of GBI still significantly improves the model output whereas A * does not.",
        "We believe this is because GBI searches in the proximity of the provided model weights.",
        "Theoretical analysis of this hypothesis is left as a future work."
    ],
    "1862": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We believe that studying temporal shifts of such projections can lead to interesting findings far beyond the usual example of 'king is to queen as man is to woman'.",
        "Our experiments show that the models do preserve these 'directions' and that the learned projections not only hold for the word pairs known to the initial model, but can also be used to predict relations for the new words.",
        "In terms of future work, we plan to trace how quickly incremental updates to the model 'dilute' the projections, rendering them useless with time.",
        "We observed this performance drop in our experiments, and it would be interesting to know more about the regularities governing this deterioration."
    ],
    "1864": [
        "The task of detecting and explaining causes from text for a time series is novel.",
        "Causal features can be detected from online text.",
        "A large cause-effect graph can be constructed using FrameNet semantics.",
        "Our model generates causality with richer lexical variation by training on paths from the graph.",
        "The model can produce a chain of cause and effect pairs as an explanation, which shows some appropriateness.",
        "Incorporating aspects such as time, location, and other event properties remains a point for future work.",
        "Collecting a sequence of causal chains verified by domain experts is a potential direction for solid evaluation of generating explanations."
    ],
    "1865": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our method improves the performance of distantly-supervised relation extraction.",
        "Deeper convolutional models help distill signals from noisy inputs.",
        "With shortcut connections and identify mapping, the performances are significantly improved.",
        "These results aligned with a recent study (Conneau et al., 2017) , suggesting that deeper CNNs do have positive effects on noisy NLP problems."
    ],
    "1867": [
        "The proposed learning setup and infrastructure can be used for a novel shared task on bandit learning for machine translation.",
        "The task involves domain adaptation and online learning, with challenges such as non-literal post-editions and mismatched evaluation metrics.",
        "Despite these challenges, the authors found promising results for both linear and non-linear online learners that could outperform their static SMT and NMT baselines, respectively.",
        "The desideratum for a future installment of this shared task is the option to perform offline learning from bandit feedback, which would allow for more lightweight infrastructure and open the task to (mini)batch learning techniques.",
        "The authors found that both linear and non-linear online learners outperformed their static SMT and NMT baselines, respectively."
    ],
    "1868": [
        "Our experiments demonstrate that on a large, professionally annotated dataset, a sequence-to-sequence character-based model of diffs can lead to considerable effectiveness gains over a state-of-the-art SMT system with task-specific features, ceteris paribus.",
        "Furthermore, in the crowdsourced environment of the CoNLL data, in which there are comparatively few professionally annotated sentences in training, modeling diffs enables a means of tuning that improves the effectiveness of sequence-to-sequence models for the task.",
        "We leave to future work the intersection of a LM for the CoNLL environment and more generally, whether these patterns hold in the presence of additional monolingual data."
    ],
    "1869": [
        "Our proposed attention-based neural network framework can jointly model the charge prediction task and the relevant article extraction task, with the weighted relevant articles serving as legal basis to support the charge prediction.",
        "The experimental results on judgement documents of criminal cases in China show the effectiveness of our model on both charge prediction and relevant article extraction.",
        "The comparison of different variants of our model indicates the importance of law articles in making judicial decisions in the civil law system.",
        "Our model has reasonable generalization ability on fact descriptions written by non-legal professionals, even when trained on judgement documents.",
        "Our model still cannot explicitly handle multidefendant cases, and there is a clear gap between our model and the upper bound improvement that relevant articles can achieve.",
        "The experimental results on news data show that our model has reasonable generalization ability on fact descriptions written by non-legal professionals."
    ],
    "1871": [
        "The proposed method achieves new state-of-the-art results on the miniRCV1 and ODIC datasets for few-shot text classification.",
        "The model leverages external working memory with dynamic routing to adapt and generalize better to support sets and unseen classes.",
        "The newly collected dataset of real-world deceptive reviews is rich in products and reviews, and has two orders of magnitude more deceptive reviews than previously used artificial datasets.",
        "The developed method identifies generalized features for deception detection that can be used to improve both recall and precision of online deception detection using additional deceptive review data from assorted domains.",
        "The use of social network analysis for real-world deceptive review collection facilitates the development of a large dataset for deception detection."
    ],
    "1872": [
        "We presented a weakly supervised bootstrapping approach that learns both regular event pairs and a contextual temporal relation classifier.",
        "Evaluation shows that the learned regular event pairs are of high quality and rich in commonsense knowledge and domain knowledge.",
        "The weakly supervised trained temporal relation classifier achieves comparable performance with state-of-the-art supervised classifiers."
    ],
    "1873": [
        "Our results suggest that this is a viable approach to harvesting parallel data from web crawls.",
        "We achieve the best performance with a combination of classical cosine measure, 'local' cosine measure, and URL matching.",
        "The existence of duplicate and near-duplicate documents in the data raises the question whether it is reasonable to measure performance in terms of URL matches, or whether evaluation should be based on the distance between retrieved and expected documents."
    ],
    "1875": [
        "The proposed framework for multimodal sentiment analysis and emotion recognition outperforms state-of-the-art models by a significant margin.",
        "Speaker-independent models have poor performance in multimodal sentiment analysis.",
        "Cross-dataset performance of the models is a major challenge in multimodal sentiment analysis.",
        "The proposed framework will focus on extracting semantics from visual features and improving the fusion of cross-modal features.",
        "Contextual dependency learning will be included in the model to overcome limitations mentioned in Section 5.",
        "The authors have made their framework available as a demo on http://148.204.64.164/2."
    ],
    "1878": [
        "Our approach can achieve open vocabulary translation while maintaining a linguistic notion at the sub-word level.",
        "Unlike previous approaches, our method is completely unsupervised and can estimate a fixed size dictionary of sub-word units considering their individual morphological properties.",
        "We have shown that our method obtains significantly better performance due to bringing a linguistic notion into the segmentation process.",
        "Our method is able to achieve open vocabulary translation while maintaining a linguistic notion at the sub-word level, which has not been possible with previous approaches.",
        "Our approach does not require any supervision or pre-defined dictionaries, and yet it can still achieve better performance than previous methods."
    ],
    "1880": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to significant BLEU gains, but there may be additional improvement to be gained by solving label bias in general.",
        "Our approach for transferring knowledge from an encoder pretrained on machine translation to a variety of downstream NLP tasks shows promising results.",
        "The use of CoVe from our best, pretrained MT-LSTM performed better than baselines that used random word vector initialization, baselines that used pretrained word vectors from a GloVe model, and baselines that used word vectors from a GloVe model together with character n-gram embeddings.",
        "The goal of building unified NLP models that rely on increasingly more general reusable weights is a step towards the goal of building unified NLP models that rely on increasingly more general reusable weights."
    ],
    "1884": [
        "We have created the first annotated corpus of pedagogical roles for the study of pedagogical value.",
        "Our use of sentence embeddings and clustering techniques to develop a baseline for pedagogical role classification has shown substantial inter-annotator agreement.",
        "Certain sentences in a document are strong indicators of the pedagogical roles of the document.",
        "We believe it is important to make our corpus and annotations public, as feedback from other researchers will help improve the quality and scope of our corpus as we expand it.",
        "We plan to expand the set of roles as needed and apply our techniques to other domains to work towards a general approach to estimating pedagogical value."
    ],
    "1889": [
        "There are only few search errors in the state-of-the-art NMT systems.",
        "Even when better hypotheses are added in the nbest list, the models do not select a different hypothesis.",
        "The search algorithms seem to be sufficient.",
        "A relatively small n-best list of 50 entries already contains notably better translation hypotheses.",
        "Improving rescoring models are promising for performance boost.",
        "It is often sufficient to use a model in rescoring only.",
        "The development of models which are challenging to use directly during the decoding, such as bi-directional decoders, is motivated."
    ],
    "1893": [
        "The proposed neural language model, EN-TITYNLM, outperforms strong baselines and prior work on three tasks: language modeling, coreference resolution, and entity prediction.",
        "The dynamic representations generated by the model are useful for helping generate specific entity mentions and the following text.",
        "The model provides vector representations for entities and updates them dynamically in context, allowing it to capture the relationships between entities and improve the accuracy of entity mention detection.",
        "The model's ability to update the entity representations dynamically based on the context is a key innovation that sets it apart from prior work.",
        "The model's performance on coreference resolution and entity prediction tasks is improved due to its ability to capture the relationships between entities in the text."
    ],
    "1894": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\")."
    ],
    "1897": [
        "GloVe, ELMo, and BERT embeddings all encode gender, religion, and nationality biases.",
        "A projection-based method can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "L2 regularization in the form of activation regularization (AR) and temporal activation regularization (TAR) can be effective in reducing bias."
    ],
    "1898": [
        "Our proposed CRFAE-Dep-Parser model achieves competitive performance in unsupervised dependency parsing compared to state-of-the-art systems.",
        "The learning and inference processes of our model are tractable.",
        "We tested our method on eight languages and show that it is effective across multiple languages.",
        "Our model outperforms other KGC models on metrics such as Mean Rank and MRR."
    ],
    "1905": [
        "The proposed model architecture inspired by attention-based sequence-to-sequence models can improve performance for both German and English tasks.",
        "The novel search heuristic and a modest beam size provide a good trade-off between speed and quality, outperforming prior work on the PTB task.",
        "Using a model combination with the proposed heuristic and a modest beam size achieves better performance than prior work on the PTB task.",
        "The proposed approach can improve the performance of the wordordering task.",
        "The use of attention-based sequence-to-sequence models can help improve the performance of the wordordering task."
    ],
    "1906": [
        "The proposed model integrating phrase memory into the encoder-decoder architecture can significantly improve translation performance.",
        "The SMT model generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory.",
        "The proposed model uses the balancer to make probability estimations for the phrases in the phrase memory.",
        "The NMT decoder selects a phrase from the phrase memory or a word from the vocabulary of the highest probability to generate."
    ],
    "1910": [
        "Our model is the top single result in the EMNLP RepEval 2017 Multi-NLI Shared Task, and it also surpasses the state-of-the-art encoders for the SNLI dataset.",
        "Shortcut-stacked sentence encoders can be effective for natural language inference tasks.",
        "Residual connections can achieve similar accuracies with fewer number of parameters compared to shortcut connections.",
        "Our model has the advantage of reducing the model size and following the SNLI leaderboard settings (e.g., 300D and 600D embeddings).",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data."
    ],
    "1911": [
        "Our proposed active learning algorithm can learn active learning strategies from data.",
        "We formalize active learning under a Markov decision framework, whereby active learning corresponds to a sequence of binary annotation decisions applied to a stream of data.",
        "Our learned active learning policies can be transferred between languages, providing consistent and sizeable improvements over baseline methods, including traditional uncertainty sampling.",
        "Our approach even works well in a very difficult cold-start setting, where no evaluation data is available, and there is no ability to react to annotations."
    ],
    "1914": [
        "Our proposed hierarchically-attentive RNN based models for end-to-end visual storytelling can effectively summarize and generate relevant stories from full input photo albums.",
        "The proposed method outperforms strong sequence-to-sequence baselines on selection, generation, and retrieval tasks.",
        "Automatic and human evaluations show that our method is effective in jointly summarizing and generating relevant stories from full input photo albums.",
        "Our method can generate relevant stories from full input photo albums effectively, as shown by the automatic and human evaluations.",
        "The proposed method outperforms strong sequence-to-sequence baselines on selection, generation, and retrieval tasks, as shown by the evaluations."
    ],
    "1917": [
        "The proposed hybrid search improves BLEU scores significantly compared to a strong NMT baseline.",
        "The hybrid search outperforms phrase-based SMT by a large margin.",
        "The NMT beam search was modified to insert phrasal translations based on the current and accumulated attention weights of the NMT decoder RNN.",
        "The NMT model score was used in a log-linear model with standard phrase-based scores as well as an n-gram language model.",
        "The algorithm includes separate beams for NMT word hypotheses and hypotheses with an incomplete phrasal translation, as well as introduce parameters which control the source sentence coverage.",
        "The hybrid search improves BLEU scores significantly compared to a strong NMT baseline that already outperforms phrase-based SMT by a large margin."
    ],
    "1918": [
        "Our approach can operate on existing word embeddings and perform comparably to the state of the art.",
        "Our method can either induce or reuse a word sense inventory.",
        "Our approach does not require learning sense embeddings directly from the corpus like existing approaches.",
        "Our implementation with several pre-trained models is available online."
    ],
    "1920": [
        "'Our approach achieves new state-of-the-art on FewRel 2.0 dataset.'",
        "'We first train a representation extractor with the Clustering Promotion Mechanism.'",
        "'The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.'",
        "'CWBLSTM outperforms task specific as well as task independent baselines in all three tasks.'",
        "'Pre-trained word embeddings and character based word embedding play complementary roles and along with incorporation of tag dependency are important ingredients for improving the performance of NER tasks in biomedical and clinical domains.'"
    ],
    "1924": [
        "Extraction beyond the sentence boundary produced far more knowledge\" - This claim suggests that extracting information beyond the sentence level can lead to a greater amount of knowledge being extracted.",
        "Encoding rich linguistic knowledge provided consistent gain\" - This claim indicates that incorporating rich linguistic knowledge into the extraction process leads to consistent improvements in performance.",
        "Machine reading can already be useful in precision medicine\" - This claim suggests that machine reading technology can already be useful in the field of precision medicine, despite there being room for improvement in both recall and precision.",
        "Automatically extracted facts can serve as candidates for manual curation\" - This claim indicates that automatically extracted facts can be used as candidates for manual curation, which can save time and effort compared to manually curating all the facts.",
        "Our current models are already quite capable\" - This claim suggests that the current models being used are already quite capable, but there is still room for improvement in terms of both recall and precision."
    ],
    "1927": [
        "We introduced the notion of dialog complexity to understand and compare a collection of dialogs that are routinely used in services industry.",
        "A dialog complexity measure can conceivably help improving service operation.",
        "We discuss its usage for tailoring service handling for varying customer interactions, and demonstrate its usage for improving service evaluation by taking into consideration the difficulty of dialogs that agents handle.",
        "One can explore deeper dialog content (e.g., ngrams) and structure information, or develop machine learning based approach providing that complexity annotation is available, to create more sophisticated metrics and evaluate whether they can effectively predict the complexity of service dialog handling.",
        "One can also explore using the complexity metric to manage many aspects of service center operation, such as determining the most cost-effective way of handling requests, or even optimizing a contact center dynamically."
    ],
    "1928": [
        "The proposed corpus for emotion detection task outperformed the base CNN.",
        "Annotating emotions from text is usually subjective, and therefore, future work is needed to improve the quality of the current corpus by assigning more annotators.",
        "Implementing different combinations of attention mechanisms and expanding the size of the corpus by annotating more seasons of Friends TV show can further evaluate the performances of the proposed models.",
        "The proposed corpus for emotion detection task outperformed the base CNN. (Paragraph 3)",
        "Annotating emotions from text is usually subjective, and therefore, future work is needed to improve the quality of the current corpus by assigning more annotators. (Paragraph 4)",
        "Implementing different combinations of attention mechanisms and expanding the size of the corpus by annotating more seasons of Friends TV show can further evaluate the performances of the proposed models. (Paragraph 5)"
    ],
    "1929": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Manual annotation is much less\" than other methods for uency modeling.",
        "Collecting uency annotation is more e cient.",
        "Uency labels are discrete, allowing us to easily obtain consistent and reliable uency annotation by majority voting on labels from distinct annotators.",
        "Uency prediction as binary classification is less challenging than caption generation, so less amount of training samples is needed.",
        "Uency-guided learning allows us to perform cross-lingual image captioning with a ordable annotation e orts."
    ],
    "1930": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The X-means approach is a better choice for clustering.",
        "The simple approach achieves higher results than the MI one.",
        "The manual evaluation showed that the simple approach achieves high results because of the size of the clusters.",
        "However, as discussed the simply approach achieved high results because of the size of the cluster and led to poor results when the size of the cluster grew.",
        "We plan to enrich the Chart Summary with additional details such as enabling the users to see example debates for each pair of clusters."
    ],
    "1931": [
        "The brevity problem in machine translation can largely be explained by the locally-normalized structure of the model.",
        "Our proposed solution to the brevity problem involves globally normalizing the training data, leading to significant BLEU gains.",
        "The brevity problem is an example of label bias, and our solution is a limited form of globally-normalized models for NMT.",
        "Solving the brevity problem leads to improvements in BLEU scores, but there may still be room for improvement by solving label bias in general.",
        "Our method for learning the parameters of the corrections to the model is helpful and easy, and we hope it will be included in future NMT systems.",
        "The brevity problem can be seen as an example of the more general problem of label bias and the more general solution of globally-normalized models for NMT.",
        "Our proposed method for generating multi-turn dialogue uses self-attention to capture long-distance dependencies, leading to significant improvements in performance compared to existing HRED models and attention variants.",
        "The relevant contexts detected by our model are significantly coherent with human judgments, indicating that the model is able to effectively capture the long-distance dependencies.",
        "Our proposed model, ReCoSa, utilizes self-attention to improve the quality of multi-turn dialogue generation, and we plan to further investigate its capabilities in future work."
    ],
    "1932": [
        "By linking instance segmentations provided by COCO to questions and answers in VQA, we can transfer human supervision between individual tasks of semantic segmentation and VQA, enabling us to study at least two problems with better leverage than before.",
        "We obtain state-of-the-art results on the VQA real multiple-choice task by simply augmenting multilayer perceptrons with some attention features.",
        "Our new approach based on mask aggregation for a novel question-focused semantic segmentation task achieves better performance than a baseline method and an upper-bound method.",
        "The existing and seemingly distinct annotations about MSCOCO images are inherently connected, revealing different levels and perspectives of human understandings about the same visual scenes.",
        "Explicitly linking instance segmentations up can significantly benefit not only individual tasks but also the overarching goal of unified vision-language understanding."
    ],
    "1933": [
        "The proposed framework for text modeling using purely convolutional and deconvolutional operations can avoid issues associated with exposure bias and teacher forcing training.",
        "The approach enables the model to fully encapsulate a paragraph into a latent representation vector, which can be decompressed to reconstruct the original input sequence.",
        "The proposed approach achieved excellent long paragraph reconstruction quality and outperforms existing algorithms on spelling correction, and semi-supervised sequence classification and summarization, with largely reduced computational cost."
    ],
    "1935": [
        "The current state-of-the-art methods for emotional speech recognition are vulnerable to overfitting due to the imbalanced distribution of categories and the harsh conditions of realistic challenges.",
        "LSTM modeling temporal dynamics of emotional speech is inefficient, especially when the number of parameters is large.",
        "Compared to other tasks, emotional speech corpora are limited and have a smaller number of samples.",
        "1D-CNN-LSTM with a large number of parameters is vulnerable to overfitting.",
        "The variance of the performance shows the importance of large-scale cross-validation and statistical tests.",
        "Optimizing deep architectures with great variance in the number of samples and class distribution from aggregated corpora is arduous.",
        "Our evaluation is able to present the realistic performance in the wild."
    ],
    "1936": [
        "The performance of AI agents can be evaluated more effectively through interactive downstream tasks (cooperative games) performed by human-AI teams, rather than in isolation.",
        "The current practice of measuring AI progress using supervised learning and questioner bots may not accurately reflect the performance of AI agents in real-world scenarios.",
        "There is a disconnect between the benchmarking of AI in isolation versus in the context of human-AI interaction, as evidenced by the fact that AL-ICE RL, which has been found to be more accurate in AI literature, performs less accurately when evaluated via a human-ALICE team.",
        "Evaluating QBOT via QBOT-human teams may provide valuable insights into the performance of AI agents in real-world scenarios.",
        "Designing live interactive settings on AMT presents unique computation and infrastructure challenges, such as the need for a publicly available code and infrastructure."
    ],
    "1939": [
        "Our results show that this approach leads to significantly better results on both RumourEval and PHEME datasets compared to current state-of-the-art systems.",
        "The omission of the AF features proposed in this work leads to significantly lower performance.",
        "Adding AF to the feature set causes our approach to outperform the best performing system on the RumourEval dataset.",
        "These results show the importance of task-or problem-oriented feature engineering.",
        "The proposed features are content based and work on text level.",
        "In our future work, we plan to investigate features that are able to capture communication behaviors between users.",
        "We also plan to apply stance information as a feature in rumor veracity classification."
    ],
    "1943": [
        "'We have trained a deep neural network with fusion of lexical and word vector based features to achieve state-of-the-art results on 3-way (dis)agreement classification on the largest (dis)agreement classification dataset available till date (ABCD corpus).'",
        "'We've shown that by using this model, we no longer need to rely on hand-crafted features to exploit the meta-thread structure.'",
        "'We've also shown the benefit of transfer learning from pre-trained models on large corpora to achieve competitive results on small domain datasets.'",
        "'The research for (dis)agreement classification has not moved at the pace comparable to some other NLP tasks primarily because of unavailability of large standard dataset.'",
        "'We plan to enhance the ABCD dataset by performing semi-supervised tagging of labels by training models on hand-annotated datasets.'",
        "'With the availability of a large standard dataset for (dis)agreement classification, we can try various recent advanced state-of-the-art architectures for modelling sentences pairs [41] , [49] to further improve the performance.'"
    ],
    "1946": [
        "We have described a new psycholinguistic corpus of English, consisting of edited naturalistic text designed to contain many rare or hard-to-process constructions while still sounding fluent.",
        "We believe this corpus will provide an important part of a suite of test sets for psycholinguistic models, exposing their behavior in uncommon constructions in a way that fully naturalistic corpora cannot.",
        "We also hope that the corpus as described here forms the basis for further data collection and annotation.",
        "Adverbial relative clause: An relative clause with an extracted adverbial, e.g. the valley where you would find the city of Bradford.",
        "Free relative clause",
        "NP/S ambiguity: A local ambiguity where it is unclear whether a clause is an NP or the subject of a sentence. For example, I know Bob is a doctor.",
        "Main Verb/Reduced Relative ambiguity (easy/hard): A local ambiguity between a main verb and a reduced relative clause. For example, The horse raced past the barn fell.",
        "PP attachment ambiguity",
        "Nonlocal SV: The appearance of any material between a verb and the head of its subject.",
        "Nonlocal Verb/DO: The appearance of any material between a verb and its direct object."
    ],
    "1947": [
        "The combination of 16 features, grouped into five classes of surface, lexical, syntactic, cohesion, and coherence features, results in high accuracy for pairwise text complexity assessment.",
        "Using only 4 coherence features performs statistically as well as using all features on both data sets.",
        "Removing one class of features (coherence features) from the combination of all features leads to a statistically significant decrease in accuracy, indicating a strong correlation between text coherence and text complexity.",
        "The correlation between text coherence and text complexity is weaker for other classes of features."
    ],
    "1948": [
        "The proposed method for causality extraction is effective, as demonstrated by experimental results.",
        "The performance of the proposed method is limited by the insufficiency of high-quality annotated data.",
        "The use of gaze features and part-of-speech information can achieve accuracy similar to that of linguistic-based models and state-of-the-art systems, without the need for text processing.",
        "Late gaze features are the most discriminative ones for disambiguating categories.",
        "The hybrid model achieved the highest f-measure (77.4%) on the task of evaluating semantic similarity.",
        "The approach for evaluating semantic compositionality in context had less success, with an accuracy of 55.01%."
    ],
    "1954": [
        "We introduce a semi-supervised learning algorithm that incorporates graph-based label propagation and confidence-aware data selection.",
        "The introduction of semi-supervision significantly outperforms the performance of the supervised LSTM-CRF tagging model.",
        "External resources are useful for initializing word embeddings.",
        "Both inductive and transductive semi-supervised strategies achieve state-of-the-art performance in SemEval 2017 ScienceIE task.",
        "Including in-domain data only for semi-supervised learning has slightly better performance than using cross-domain data.",
        "Reducing the amount of in-domain data hurts performance, so adding more in-domain unlabeled data may help when combined with selection schemes such as the ULM algorithms proposed here.",
        "It would be useful to assess the impact of matched unlabeled data for the physics and material science domain.",
        "Other future work includes leveraging global context, information of citation network."
    ],
    "1964": [
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "We proposed a joint sequence labeling model that combines neural features and discrete indicator features which can integrate the advantages of carefully designed feature templates over decades and automatically induced features from the future."
    ],
    "1966": [],
    "1967": [
        "The proposed model outperforms the baseline model on all datasets, and even beats a very deep neural network model (with 29 layers) in several datasets.",
        "The model shows superior performance when training instances are scarce, and when the training data is severely unbalanced.",
        "The model leverages techniques such as semi-supervised training and transfer learning quite well."
    ],
    "1972": [
        "We have presented a neural-based incremental parser that can jointly parse at both constituency and discourse levels.\" (related to the novel approach of the parser)",
        "To our best knowledge, this is the first end-to-end parser for discourse parsing task.\" (related to the uniqueness of the proposed parser)",
        "Our parser achieves the state-of-the-art performance in end-to-end parsing.\" (related to the effectiveness of the proposed parser)",
        "Unlike previous approaches, our parser needs little pre-processing effort.\" (related to the reduced pre-processing required by the proposed parser)"
    ],
    "1975": [
        "The concatenation system showed robust behavior in translating new domains.",
        "The domain-aware concatenated system performed slightly better than the concatenated system when tested on in-domain TED development sets.",
        "Testing on an unknown domain would first require finding its closest domain from the set of domains the model is trained on, and then using that tag to translate the sentence.",
        "An optimum indomain system can be built using a concatenation of out-of-domain data and fine-tuning it on in-domain data.",
        "A system built on the concatenated data resulted in a generic system that is robust to new domains.",
        "Model stacking is sensitive to the order of domains it is trained on.",
        "Data selection and weighted ensemble resulted in a less optimal solution."
    ],
    "1980": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our proposed model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The use of proper detection methods, such as self-attention, can improve the quality of multiturn dialogue generation.",
        "Introducing topical information or considering detailed content information can further improve the quality of generated responses."
    ],
    "1985": [
        "We show that we can learn lexico-functional linguistic patterns that reliably predict first-person affect.",
        "The performance of current sentiment classifiers can be enhanced by augmenting them with these patterns.",
        "By adding our AutoSlog classifier's results to existing classifiers, we were able to improve from a baseline 0.67 to 0.75 Macro F.",
        "In addition, we analyze the linguistic functions that indicate positivity and negativity for the first person experiencer, and show that they are very different.",
        "In future work, we plan to explore the integration of these observations into sentiment resources such as the +-Effect lexicon (Choi and Wiebe, 2014)."
    ],
    "1986": [
        "We have proposed and evaluated R3, a new open-domain QA framework which combines IR with a deep learning based Ranker and Reader.",
        "Our framework achieves the best performance on several QA datasets.",
        "The Ranker and Reader are trained jointly using reinforcement learning to directly optimize the expectation of extracting the groundtruth answer from the retrieved passages.",
        "The IR model retrieves the top-N passages conditioned on the question."
    ],
    "1995": [
        "The proposed deep learning architecture outperforms both a metaphor-agnostic baseline and previous corpus-driven approaches to metaphor identification.",
        "Constructing a specialized network architecture for metaphor detection, including a gating function, word embeddings mapped to a metaphor-specific space, and optimization using a hinge loss function, is beneficial.",
        "The supervised similarity network learns phrase representations with a clear boundary for metaphoricity, unlike traditional compositional methods.",
        "With a sufficiently large training set, the model can also outperform state-of-the-art metaphor identification systems based on hand-coded lexical knowledge."
    ],
    "1996": [
        "We explored several alternatives to language-dependent segmentation of Arabic and evaluated them on the tasks of machine translation and POS tagging.",
        "On the machine translation task, BPE segmentation produced the best results and even outperformed the state-of-the-art morphological segmentation in the Arabic-to-English direction.",
        "On the POS tagging task, character-based models got closest to using the state-of-the-art segmentation.",
        "Our results showed that data-driven segmentation schemes can serve as an alternative to heavily engineered language-dependent tools and achieve very competitive results.",
        "In our analysis we showed that NMT performs better when the source to target token ratio is close to one or greater."
    ],
    "2000": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Query understanding is important for question generation.",
        "Policy gradient is effective on tackling the exposure bias problem resulted by sequence learning with cross-entropy loss.",
        "Adding adversarial data may be successful in improving the performance of the model, as shown in Peng et al. (2018)."
    ],
    "2007": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The Reddit Self-reported Depression Diagnosis (RSDD) dataset contains over 9,000 users with self-reported depression diagnoses matched with over 107,000 similar control users.",
        "The approach substantially outperformed strong existing methods in terms of Recall and F1 for depression detection.",
        "The approach substantially outperformed strong previously-proposed methods for estimating the self-harm risk posed by posts on the ReachOut.com mental health support forum.",
        "The approach provides a strong approach to identifying posts indicating a risk of self-harm in social media.",
        "The approach demonstrates a means for large-scale public mental health studies surrounding the state of depression.",
        "The approach demonstrates the possibility of sensitive applications in the context of clinical care, where clinicians could be notified if the activities of their patients suggest they are at risk of self-harm.",
        "Large-scale datasets such as the one presented in this paper can provide complementary information to existing data on mental health which are generally relatively smaller collections."
    ],
    "2010": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning.",
        "The Wikipedia resource can be beneficial in the translation approach.",
        "Domain adaptation with only terminological expressions significantly improves the translation quality.",
        "Retraining a generic neural network with a limited vocabulary of the targeted domain improves the translation quality.",
        "Quality assurance of the domain-specific expressions is important for improving the translation of domain-specific vocabulary stored in semantically structured resources.",
        "Injecting multi-word terminological expressions into the NMT system can improve the translation of domain-specific vocabulary."
    ],
    "2011": [
        "We have conducted an in-depth investigation of techniques that (i) featurize discourse information, and (ii) effectively integrate discourse features into the state-of-the-art character-bigram CNN classifier for AA.",
        "Beyond confirming the overall superiority of RST features over GR features in larger and more difficult datasets, we present a discourse embedding technique that is unavailable for previously proposed discourse-enhanced models.",
        "The new technique enabled us to push the envelope of the current performance ceiling by a large margin.",
        "In using the RST features with entity-grids, we lose the valuable RST tree structure.",
        "In future work, we intend to adopt more sophisticated methods such as RecNN, as per Ji and Smith (2017) , to retain more information from the RST trees while reducing the parameter size.",
        "Further, we aim to understand how discourse embeddings contribute to AA tasks, and find alternatives to coreference chains for shorter texts."
    ],
    "2013": [
        "The effectiveness of SRU on multiple natural language tasks: \"We confirm the effectiveness of SRU on multiple natural language tasks ranging from classification to translation.",
        "High parallelization with layer-wise SRU: \"Trading capacity with layers SRU achieves high parallelization by simplifying the hidden-to-hidden dependency.",
        "Balancing representational power and performance: \"However, unlike previous work that suggests additional computation (e.g., n-gram filters) within the layer, we argue that increasing the depth of the model suffices to retain modeling capacity.",
        "Correlation between input vectors and internal state: \"In practice, multiple SRU layers are stacked to construct a deep network. The internal state c t and h t would be a weighted combination of inputs {x 1 \u2022 \u2022 \u2022 x t }, which will increase the correlation of the state vectors at different steps.",
        "Improved efficiency with increasing depth: \"As a result, we expect the actual ratio between the variance of c t and that of the input of the current layer x t lies between the two derived values, EQUATION and would finally converge to the upper bound value of 1."
    ],
    "2014": [
        "The Dryer (in prep) and Cinque (2005) model provides better predictive power than the Cysouw (2010) model.",
        "The particular distributional characteristics of adjusted frequency may be responsible for the quency and genera counts.",
        "The feature weights may be due to the distributional characteristics of adjusted frequency."
    ],
    "2015": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation."
    ],
    "2021": [
        "The proposed model, ME-CRF, extends linear-chain CRFs by including external memory, allowing it to access long-range context.",
        "The proposed method demonstrates effectiveness in two tasks: forum thread discourse analysis and named entity recognition.",
        "The inclusion of external memory enables the model to look beyond neighboring items and capture long-range context.",
        "The proposed method improves the performance of linear-chain CRFs on both tasks."
    ],
    "2025": [
        "SI-RNN jointly models who says what to whom by updating speaker embeddings in a role-sensitive way.",
        "It provides state-of-the-art addressee and response selection, which can instantly help retrieval-based dialog systems.",
        "In the future, we also consider using SI-RNN to extract sub-conversations in the unlabeled conversation corpus and provide a largescale disentangled multi-party conversation data set."
    ],
    "2027": [
        "The proposed sequence labeling framework, LM-LSTM-CRF, effectively leverages the language model to extract character-level knowledge from the self-contained order information.",
        "The incorporation of highway layers helps overcome the discordance issue in the naive cotraining approach, leading to better efficiency without loss of effectiveness.",
        "The proposed method achieved state-of-the-art performance on three benchmark datasets.",
        "In future work, the authors plan to further extract and incorporate knowledge from other \"unsupervised\" learning principles to empower more sequence labeling tasks."
    ],
    "2030": [
        "The locally-normalized structure of the model can explain the beam problem.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "GenDS outperforms traditional non-goal-driven dialogue system S2SA and generative QA models on MusicConvers and MusicQA datasets.",
        "GenDS is scalable with new KB.",
        "Transfer learning can be used to improve the performance of GenDS in other domains like sport."
    ],
    "2033": [
        "The quality of representations in a deep end-to-end ASR model is evaluable on a frame classification task.",
        "There are striking differences in the quality of feature representations from different layers of the ASR model.",
        "These differences are partly correlated with the separability of the representations in vector space.",
        "The analysis can be extended to other speech features, such as speaker and dialect ID, and to larger speech recognition datasets.",
        "Experimenting with other end-to-end systems, such as sequence-to-sequence models and acoustics-to-words systems, is a potential direction for future work.",
        "Improving the representation capacity of certain layers in the deep neural network can improve the end-to-end model."
    ],
    "2034": [
        "'Our approach can achieve state-of-the-art inference quality and outperform existing works (LSTM, etc.) on a wide range of NLP tasks with fewer parameters and higher time efficiency.'",
        "'The multi-dimensional attention performs a feature-wise selection over the input sequence for a specific task, and the directional self-attention uses the positional masks to produce the context-aware representations with temporal information encoded.'",
        "'DiSAN can achieve better performance on various benchmarks with fewer parameters and higher time efficiency than existing works.'",
        "'Future work might explore the approaches to using the proposed attention mechanisms on more sophisticated tasks, such as question answering and reading comprehension, to achieve better performance.'"
    ],
    "2037": [
        "The proposed decoder uses self-attentive residual connections to enrich the target-side contextual information in NMT.",
        "The decoder improves the BLEU score compared to the NMT baseline and two variants with memory-augmented decoders, as demonstrated through evaluations on three language pairs.",
        "The proposed model distributes weights throughout an entire sentence and learns structures resembling syntactic ones, as shown by a qualitative analysis.",
        "Future work includes enriching the present attention mechanism with the key-value-prediction technique and incorporating relative positional information to the attention function.",
        "The code for the proposed model is publicly available for further research in self-attentive residual connections for NMT and other similar tasks."
    ],
    "2040": [
        "The proposed approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The two-stage attention map in QACNN improves the performance of the model.",
        "The use of query-based CNN attention enhances the matching effect of the model.",
        "The model is efficient and can be trained using own trained embedding with TF-IDF weighting.",
        "The model can be applied to open-answer tasks like SQuaD by treating the whole corpus as an \"answer pool\"."
    ],
    "2042": [
        "Our proposed deep generative framework based on Variational Autoencoders (VAE) and sequence-to-sequence models can generate multiple paraphrases for a given sentence in a principled way.",
        "Unlike traditional VAE and unconditional sentence generation models, our model conditions the encoder and decoder sides of the VAE on the input sentence, leading to better performance.",
        "Our approach outperforms the state-of-the-art by a significant margin without any hyper-parameter tuning.",
        "The generated paraphrases are not just semantically similar to the original input sentence but also able to capture new concepts related to the original sentence.",
        "We demonstrate the remarkable performance of our approach on a recently released question paraphrase dataset."
    ],
    "2049": [
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Our method uses a Clustering Promotion Mechanism to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "We first train a representation extractor with the Clustering Promotion Mechanism, which uses Similarity Entropy Minimization to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "Our approach combines two methods by the proposed Cosine Annealing Strategy.",
        "The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.",
        "We utilize pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "Our method does not require any change to the training of linear classification models.",
        "Combining the information in the unsupervised Word Vector model with a supervised linear model improves micro and macro recall over baseline on difficult multi-class problems."
    ],
    "2050": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our approach outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We argue that despite being the main metric in research, BLEU score alone can be a poor way of tracing MT system improvement.",
        "In the future, we are going to continue running optimization related experiments, particularly around better strategies for taking advantage of multiple GPUs.",
        "We are also focusing more on the research topics of model pre-training and similar techniques.",
        "Other important research topics to us are domain adaptation and user-generated content."
    ],
    "2052": [
        "Our proposed method can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Experimental results show that our approach significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "By decoupling the extraction of aspects and sentiment labels, we obtain a flexibly applicable system.",
        "Our novel neural network based approach to tackle aspect extraction as a sequence labeling task is able to extract sentiments expressed towards a specific aspect mentioned in the text.",
        "Incorporating additional semantic knowledge from pretrained word vectors and sentiment-related features obtained from SenticNet proves to be a valuable feature for extracting aspect-based polarity labels, increasing accuracy and shortening training time considerably.",
        "For future work, we plan to modify our architecture to permit incorporation of all concepts from SenticNet, thus moving the system to concept-level sentiment analysis even further."
    ],
    "2053": [
        "Character-level information assists in the task of opinion target extraction.",
        "A more sophisticated model that includes character-level word embeddings consistently outperforms a baseline model with a substantial margin of 3.3 points F1-score.",
        "The positive influence of the character-level word embeddings is linked to the difficulty of extracting multiword expressions.",
        "The additional character information contributes to the task of extracting opinion target expressions, but it is not entirely clear how.",
        "Pretraining parts of the network to enrich the character-based word representation may improve token-based approaches or even replace the need for tokenization altogether."
    ],
    "2056": [
        "The proposed method outperforms previously proposed meta-embedding learning methods on multiple benchmark datasets.",
        "The method is able to learn meta-embeddings from a given set of pre-trained source embeddings without requiring any supervision.",
        "The proposed method has shown accuracy in NLP tasks.",
        "The authors plan to extend the proposed method to learn cross-lingual meta-embeddings by incorporating both cross-lingual as well as monolingual information."
    ],
    "2059": [
        "We have presented a latent variable model for matching natural language sentences, with deconvolutional networks as the sequence encoder.",
        "Our approach is effective at inferring robust sentence representations for determining their semantic relationship, even with limited amount of labeled data.",
        "State-of-the-art experimental results on two semi-supervised sequence matching tasks are achieved, demonstrating the advantages of our approach.",
        "This work provides a promising strategy towards training effective and fast latent-variable models for text data."
    ],
    "2065": [
        "Our model can recover from ASR errors and generate suitable responses by mitigating the impact of noise in three aspects, as shown in Table 6.",
        "Our model is able to mitigate the impact of noise in the input audio signals, resulting in more accurate transcription and better response generation.",
        "The samples in Table 6 demonstrate how our model recovers from ASR errors and generates suitable responses, showing that our approach effectively mitigates the impact of noise.",
        "Our model is able to generate suitable responses even when the input audio signals are noisy, as shown in Table 6.",
        "The examples in Table 6 illustrate how our model recovers from ASR errors and generates appropriate responses, indicating that our approach effectively mitigates the impact of noise."
    ],
    "2066": [
        "Our approach achieves strong visual reasoning using general-purpose Feature-wise Linear Modulation layers.",
        "FiLM layers are resilient to architectural modifications, test time ablations, and even restrictions on FiLM layers themselves.",
        "FiLM's success is not closely connected with normalization as previously assumed.",
        "FiLM models can generalize better, more sample efficiently, and even zero-shot to foreign or more challenging data.",
        "The results of our investigation of FiLM in the case of visual reasoning complement broader literature that demonstrates the success of FiLM-like techniques across many domains."
    ],
    "2067": [
        "We have created a large-scale, multi-lingual resource for evaluating systems to mine abbreviations and their expansions.",
        "Using this resource, we can apply a machine learning approach to incorporate key aspects of the phenomenon of abbreviation, such as synonymy, topical relatedness, and surface similarity.",
        "Our machine learning approach shows large, statistically significant improvements relative to strong rule-based baselines and a baseline that combines the rule-based systems into an ensemble.",
        "The contribution of semantic features is always large in the languages we tested, while the contribution of a learned alignment model for surface similarity produces further gains when the available training data is large."
    ],
    "2069": [
        "PReFIL surpasses prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL exceeds the human baseline for FigureQA, but results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The 'How' question is the hardest to answer, with the lowest MAP scores.",
        "Our AdaQA model improves most over the basic CNN on the 'How' type question.",
        "The adaptive convolutional filter-generation mechanism improves the model's ability to read and reason over natural language sentences.",
        "Our framework is further generalized to model question-answer sentence pairs, leveraging a two-way feature abstraction process.",
        "Our models consistently outperform the standard CNN and attention-based CNN baselines, demonstrating the effectiveness of our framework."
    ],
    "2070": [
        "EZLearn is proposed for automated data annotation, combining distant supervision and co-training.",
        "EZLearn is well suited for high-value domains with numerous classes and frequent updates.",
        "Experiments in functional genomics and scientific figure comprehension show that EZLearn is broadly applicable, robust to noise, and can learn accurate classifiers without manually labeled data.",
        "EZLearn outperforms state-of-the-art supervised systems by a wide margin."
    ],
    "2072": [
        "OONP outperforms several strong baselines by a large margin on parsing fairly complicated ontology.\" - This claim suggests that the proposed framework, OONP, achieves better performance than existing strong baselines in parsing complex documents.",
        "OONP is neural net-based, but equipped with sophisticated architecture and mechanism for document understanding.\" - This claim highlights the unique combination of neural networks and specialized architecture in OONP, which enables both interpretability and learnability.",
        "Experiments on both synthetic and real-world datasets have shown that OONP outperforms several strong baselines by a large margin on parsing fairly complicated ontology.\" - This claim supports the previous one, demonstrating that OONP performs well on diverse datasets.",
        "OONP nicely combines interpretability and learnability.\" - This claim emphasizes the advantage of OONP's design, which allows for both interpretability (understanding how the model works) and learnability (improving the model's performance).",
        "We proposed Object-oriented Neural Programming (OONP), a framework for semantically parsing in-domain documents.\" - This claim introduces the novel framework, OONP, which is designed to parse documents with semantic meaning."
    ],
    "2076": [
        "TPR computation is within the scope of the capabilities of the architecture in principle.",
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The architecture can be interpreted as containing a part that encodes a sentence and a part that selects one structural role at a time to extract from the sentence.",
        "Unlike standard LSTMs, however, the TPGN model admits a level of interpretability: we can see which roles are being unbound by the unbinding vectors generated internally within the model.",
        "We find such roles contain considerable grammatical information, enabling POS tag prediction for the words they generate and displaying clustering by POS."
    ],
    "2077": [
        "Sentence selection matters when it comes to building a corpus for ATE (automated test evaluation).",
        "The quality of ATE is affected by the choice of sentences in the corpus.",
        "Automated data labeling can be used to construct new datasets for ATE.",
        "The effectiveness of ATE is dependent on the quality of the training data."
    ],
    "2079": [
        "The proposed Read-Write Memory Network (RWMN) model improves the performance of visual question answering tasks for large-scale, multimodal movie story understanding.",
        "The approach achieved the best accuracies in multiple tasks of the MovieQA benchmark, with a significant improvement on the visual QA task.",
        "The proposed read/write networks enable the model to have highly-capable and flexible read/write operations.",
        "There are several future research directions that can be explored beyond this work, such as applying the approach to other QA tasks that require complicated story understanding, or exploring better video and text representation methods beyond ResNet and Word2Vec."
    ],
    "2083": [
        "The proposed constituent hierarchy predictor using recurrent neural networks can capture global sentential information and improve the performance of shift-reduce parsers.",
        "The fully-supervised parser based on the proposed method achieves better results than the state-of-the-art baseline parser, with an improvement of 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification can adapt and generalize better to support sets and unseen classes by leveraging external working memory with dynamic routing.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets, demonstrating the effectiveness of the proposed technique.",
        "The use of a thresholded matching score can select relevant responses and reduce the risk of poor responses.",
        "Edina demonstrated the ability to engage in a wide range of subtopics during conversations with Alexa customers using a novel technique of self-dialogues.",
        "The system's strength lies in incorporating the advantages of both data-driven and rule-based approaches while avoiding their shortcomings.",
        "The self-conversation data collection technique and unique approach to integrating rule-based, retrieval, and machine learning based methods can be useful for future open domain conversational agents."
    ],
    "2085": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The new data collection tool, data set, and associated dialogue simulation framework focus on visual language grounding and natural, incremental dialogue phenomena.",
        "The tools and data are freely available and easy to use.",
        "The n-gram user model is used to train and evaluate an optimized dialogue policy, which learns grounded word meanings from a human tutor, incrementally, over time.",
        "The dialogue policy optimization learns a complete dialogue control policy from the data, in contrast to earlier work (Yu et al., 2016c) which only optimized confidence thresholds, and where dialogue control was entirely rule-based.",
        "Ongoing work further uses the data and simulation framework to train a word-by-word incremental tutor simulation, with which to learn complete, incremental dialogue policies, i.e., policies that choose system output at the lexical level (Eshghi and Lemon, 2014).",
        "To deal with uncertainty, the system takes all the visual classifiers' confidence levels directly as features in a continuous space MDP."
    ],
    "2091": [
        "The proposed EmoNet system achieves an overall accuracy of 0.72 with 12,000 training dialogues and 100 epochs of training.",
        "The top-1 accuracy of EmoNet is higher than other Chinese text-based emotion detectors.",
        "EmoNet has the capacity to achieve a better performance with more training materials.",
        "The preprocessing step without segmentation, stemming, or lemmatization introduces difficulties but addresses the problem of loss of linguistic features.",
        "The simple resampling step used in EmoNet can replace these steps."
    ],
    "2098": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Pretraining inputs with methods like GloVe and word2vec can lead to substantial performance gains as long as the model is configured and optimized so as to take advantage of this initial structure.",
        "Random input initialization remains a common choice for some tasks, but orthogonal random initialization is superior to a Gaussian initialization.",
        "Fine-tuning highdimensional embeddings is a delicate task, as the process can alter dimensions that are essential for downstream tasks.",
        "Pretraining with methods like GloVe and word2vec can lead to better overall results than random initialization."
    ],
    "2101": [
        "The proposed Semantic Relevance Based neural network model (SRB) improves the semantic relevance between source texts and generated simplified texts for text summarization and text simplification.",
        "The SRB model introduces a similarity evaluation component to measure the relevance of source texts and generated texts, and maximizes the similarity score during training to encourage high semantic relevance.",
        "The proposed model has better performance than state-of-the-art systems on the benchmark corpus, as demonstrated by experiments conducted on three corpora (LCSTS, PWKP, and EW-SEW).",
        "The introduction of a self-gated attention encoder to memory the input text improves the model's ability to represent long source texts."
    ],
    "2108": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Attention agrees with traditional alignment to a certain extent, but differs substantially depending on the attention mechanism and the type of word being generated.",
        "Training attention with explicit alignment labels is useful for generating nouns, but not for verbs.",
        "The large portion of attention being paid to words other than alignment points is already capturing other relevant information, so training attention with alignments can force the attention model to forget useful information."
    ],
    "2109": [
        "HRDE showed higher performances in ranking answer candidates and less performance degradations when dealing with longer texts compared to conventional models.",
        "The LTC module provided additional performance improvements when combined with both RDE and HRDE models, as it added latent topic cluster information according to dataset properties.",
        "With this proposed model, we achieved state-of-the-art performances in Ubuntu datasets.",
        "We also evaluated our model in real-world question answering dataset, Samsung QA, and demonstrated the robustness of the proposed model with the best results."
    ],
    "2117": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "Exploiting implicit discourse signals can be an appealing direction for future research.",
        "Training sentence embedding models on a discourse marker prediction task can lead to high performance on established tasks for sentence embeddings.",
        "Fine-tuning larger models on this task can achieve state-of-the-art on the PDTB implicit discourse relation prediction.",
        "Collecting a dataset for this task is relatively easy compared to other supervised tasks."
    ],
    "2118": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "To improve event temporal status identification, wider contexts beyond the current sentence containing an event should be exploited."
    ],
    "2119": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "This study has argued the importance of designing a sophisticated local model before exploring global solution algorithms in Japanese PAS analysis.",
        "A sophisticated local model alone can outperform the state-of-the-art global model with 13% error reduction in F1.",
        "The local features we employed in this paper are only a part of those proposed in the literature.",
        "Selectional preference between a predicate and arguments is one of the effective information, and local models could further improve by combining these extra features."
    ],
    "2120": [
        "Strong performance: The proposed end-to-end method for tweet-level geolocation prediction achieves strong performance, outperforming comparable systems by 2-6% depending on the feature setting.",
        "Generic and portable: The model is generic and has minimal feature engineering, making it highly portable to problems in other domains/languages, such as Weibo (a Chinese social platform).",
        "Compression power: The proposed extensions to the model compress the representation learnt by the network into binary codes, demonstrating its compression power compared to state-of-the-art hashing techniques."
    ],
    "2121": [
        "Our framework has better generalizability compared to previous non-compositional and compositional phrase embedding methods.",
        "Our framework can be re-used for any length of phrases.",
        "Our method can better capture phrase semantics and achieve state-of-the-art performances on various phrase similarity tasks.",
        "Our framework has better generalizability compared to previous non-compositional and compositional phrase embedding methods.",
        "Our framework can be re-used for any length of phrases.",
        "Our method can better capture phrase semantics and achieve state-of-the-art performances on various phrase similarity tasks."
    ],
    "2125": [
        "The authors have presented a large dataset (PubMed 200k RCT) for sequential sentence classification, which is the largest such dataset they are aware of.",
        "They have evaluated the performance of several baselines to provide a reference for researchers to compare their own algorithms.",
        "The release of this dataset is expected to accelerate the development of algorithms for sequential sentence classification and increase the interest of the text mining community in RCTs."
    ],
    "2126": [
        "The proposed CASICT tibetan word segmentation system outperforms the baseline system by significantly improving the segmentation errors.",
        "The Refiner module is effective in improving the performance of the tibetan word segmentation system.",
        "The argumented label set proposed by the authors helps to deal with error propagation from the baseline system and provides a simple and efficient way to improve the system's performance.",
        "The BPE algorithm-based subword preprocessing unit is effective in handling subword sequence labeling.",
        "The LSTM unit with a gated residual connection is useful in building the deep neural network for subword sequence labeling."
    ],
    "2132": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We proposed a new CQA system that improves the state-of-the-art and surpasses human accuracy on two datasets.",
        "Our results suggest harder datasets are needed for CQA, and better OCR is also important for advancing the field.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments."
    ],
    "2137": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improves the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.\" (Paragraph: PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.)",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.\" (Paragraph: While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.)",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.\" (Paragraph: All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.)",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improves the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.\" (Paragraph: Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improves the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.)"
    ],
    "2139": [
        "\"ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "\"Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "\"Our approach uses Multi-Task Label Embedding to map labels of text classification tasks into semantic vectors.",
        "\"Our models can improve performances of most tasks with additional related information from others in all scenarios.",
        "\"We would like to explore quantifications of task correlations and generalize MTLE to address other NLP tasks, for example, sequence labeling and sequence-to-sequence learning."
    ],
    "2147": [
        "The proposed bilinear relation attention network outperforms the previous state-of-the-art on the Biocreative V CDR dataset.",
        "The model is capable of simultaneously producing predictions for all mention pairs within a document.",
        "The model has the potential to be applied to other NLP tasks such as hypernym prediction, coreference resolution, and entity resolution.",
        "The authors plan to investigate these directions in future work."
    ],
    "2153": [
        "The proposed non-projective Covington transition-based parser achieves higher accuracy than the original Covington parser and is the highest accuracy on the WSJ Penn Treebank (Stanford Dependencies) to date with greedy dependency parsing.",
        "The length of transition sequences in the proposed parser is reduced from O(n^2) to O(n).",
        "The incorporation of non-local transitions in the parser improves its performance."
    ],
    "2155": [
        "We analyzed the impact of coreference resolution on the NLP task slot filling.",
        "Coreference information improves the slot filling system performance.",
        "We outlined the most important challenges we have discovered in an analysis of coreference resolution errors.",
        "We will publish KBPchains, a resource containing the coreference chains which we have extracted automatically."
    ],
    "2161": [
        "JESC is the largest publicly available Japanese-English corpus to date.",
        "JESC covers the underrepresented domain of conversational speech.",
        "JESC is the only large-scale parallel corpus to support multi-reference BLEU evaluation.",
        "Our experimental results suggest that these data are a high quality and novel challenge for today's machine translation systems.",
        "By releasing these data to the public, we hope to increase the colloquial abilities of today's MT systems, especially for the Japanese-English language pair."
    ],
    "2162": [
        "Models that have only been trained on answer-containing paragraphs can perform very poorly in the multi-paragraph setting.",
        "The shared-norm approach was the most effective way to resolve the problem, and the no-answer and merge approaches were moderately effective.",
        "The sigmoid objective function reduces the paragraph-level performance considerably, especially on the TriviaQA datasets.",
        "We suspect that the sigmoid objective function is vulnerable to label noise.",
        "Our training method of sampling non-answer containing paragraphs while using a shared-norm objective function can be very beneficial.",
        "Combining this with our suggestions for paragraph selection, using the summed training objective, and our model design allows us to advance the state of the art on TriviaQA by a large stride."
    ],
    "2164": [
        "Our efforts on resource collection and MT design have yielded a small Swiss German / High German parallel corpus of about 60k words.",
        "A larger list of resources which await digitization and alignment.",
        "Three solutions for input normalization, to address variability of region and spelling.",
        "A baseline GSW-to-DE MT system reaching 36 BLEU points.",
        "Character-based neural MT was the most promising one.",
        "MT quality depended more strongly on the regional rather than topical similarity of test vs. training data.",
        "These findings will be helpful to design MT systems for spoken dialects without standardized spellings, such as numerous regional languages across Africa or Asia, which are natural means of communication in social media."
    ],
    "2167": [
        "The existing models cannot address the two challenges at the same time when we summarize them into a general framework.",
        "Motivated by the analysis, we propose a sequential matching framework for context-response matching.",
        "The new framework is able to capture the important information in a context and model the utterance relationships simultaneously.",
        "Both models can significantly outperform the state-of-the-art models.",
        "We conduct ablation analysis and visualize key components of the two models.",
        "We also compare the two models in terms of their efficacy, efficiency, and sensitivity to hyper-parameters."
    ],
    "2168": [
        "The proposed Tensor Product Generation Network (TPGN) outperforms widely adopted LSTM-based models on all major metrics, including METEOR, BLEU, CIDEr, and SPICE, in image/visual-captioning based CAPTCHAs.",
        "The use of Tensor Product Representations (TPRs) in the proposed model shows great promise for CAPTCHA.",
        "The proposed model has a novel architecture based on a rationale derived from the use of TPRs for encoding and processing symbolic structure through neural network computation.",
        "The unbinding subnetwork U and the sentence-encoding network S of Fig. 3 are each implemented as part of the proposed model.",
        "In the future, the authors plan to explore extending TPR to a variety of other NLP tasks and spam email detection."
    ],
    "2173": [
        "The proposed deep reinforcement learning approach for paraphrase generation significantly improves the quality of paraphrases compared to baseline methods.",
        "The generator and evaluator models are trained using sequence-to-sequence learning and supervised learning or inverse reinforcement learning, respectively.",
        "The evaluator is trained to identify paraphrases, while the generator is fine-tuned by reinforcement learning to produce more accurate paraphrases.",
        "The proposed method has the potential to be applied to other tasks such as machine translation and dialogue.",
        "The use of a well-trained evaluator improves the performance of the generator."
    ],
    "2174": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed method for discovering root-and-pattern morphology in Semitic languages is effective and validated by intrinsic and extrinsic evaluations.",
        "The method can generate appropriate responses which act as go-between in narration, and some of the generated responses are entertaining.",
        "The use of the humor model in the combined model and the use of it in the final reranking process can generate better responses.",
        "The method has some shortcomings, such as not all generated responses being fluent and readability not being good, and some responses being broken sentences.",
        "The reason for some generated responses not being fluent may be due to the data sparsity problem in the dataset."
    ],
    "2177": [
        "Our approach outperforms strong baselines on classification, structured prediction and reinforcement learning tasks.",
        "Language encourages compositional generalization: standard deep learning architectures are good at recognizing new instances of previously encountered concepts, but not always at generalizing to new ones.",
        "Language simplifies structured exploration: models with latent linguistic parameterizations can sample in this space, and thus limit exploration to a class of behaviors that are likely a priori to be goal-directed and interpretable.",
        "And generally, language can help learning. In multitask settings, it can even improve learning on tasks for which no language data is available at training or test time.",
        "While some of these advantages are also provided by techniques like program synthesis that are built on top of formal languages, natural language is at once more expressive and easier to obtain than formal supervision."
    ],
    "2183": [
        "extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "the adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "the results obtained are promising and encourage for further exploration of novel methods.",
        "the authors would like to thank AICS 2018 Organizing Committee for timely support."
    ],
    "2198": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Our extension to recurrent networks for language modeling allows the language models to adapt to the current distribution of the data dynamically.",
        "We proposed to scale this simple mechanism to large amounts of data (millions of examples) by using fast approximate nearest neighbor search."
    ],
    "2200": [
        "The proposed method for inducing sense-aware semantic classes using distributional semantics and graph clustering can be used for post-processing of noisy hypernymy databases extracted from text.",
        "The optimal parameters of the approach can be determined by comparing to existing lexical-semantic networks.",
        "The approach improves precision and recall of a hypernymy extraction method using crowdsourcing study.",
        "The induced semantic classes can be used to improve domain taxonomy induction from text.",
        "The proposed method can be useful in other tasks, such as word sense disambiguation for out-of-vocabulary words."
    ],
    "2203": [
        "Multiple-turn reasoning outperforms single-turn reasoning for all question and answer types.",
        "Enabling a flexible number of turns generally improves upon a fixed multiple-turn strategy.",
        "Our model extension to (Shen et al., 2016) achieves results competitive to the state-of-the-art on both tasks.",
        "As future work, we plan to investigate the impact of even deeper layers of reasoning and explore fast training methods to make such methods practical for large-scale datasets."
    ],
    "2207": [
        "The proposed model for semantic composition utilizes matrix multiplication.",
        "The model does not reach the state of the art on any of the three datasets under study, but outperforms all known tree-structured models.",
        "The model lays a strong foundation for future work on treestructured compositionality in artificial neural networks.",
        "The proposed model substantially outperforms all known tree-structured models."
    ],
    "2208": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We show statistically significant improvements of the translation quality on three language pairs.",
        "We intend to investigate models which incorporate specific discourse-level phenomena."
    ],
    "2209": [
        "SAMIA outperforms other deep reinforcement learning frameworks on a real-world coffee ordering dataset.",
        "The proposed SAMIA framework extends deep reinforcement learning to larger scopes and other scenarios where one party in the conversation is easy to be learned by the seq2seq model, and the other can be learned by sophisticated models.",
        "The user model helps filter response candidates to enhance the robustness of the proposed SAMIA framework.",
        "The proposed SAMIA framework has the potential to be extended to scenarios with a more complex user modeling and no clear reward in the future."
    ],
    "2210": [
        "The recent developments in the qualities and use of neural networks and the absence of any neural network-based method in the field of joint sentiment/topic modeling were the factors that encouraged the authors to try this approach for this application.",
        "Incorporating the sentiment into the document modeling, as we did in the present work, will lead to development of generative models of higher quality for document modeling.",
        "The proposed approach, which falls in the category of generative probabilistic methods, is an extension of the RS model based on the Restricted Boltzmann Machine (RBM) neural network.",
        "We also evaluated the data retrieval performance of the proposed method through comparison with the RS model. The results of the tests performed on two databases demonstrated the superior performance and precision of the proposed method in data retrieval from text documents.",
        "The proposed model was evaluated using the movie review dataset, the 20-newsgroups dataset, and the multi-domain sentiment dataset, which are the prominent databases for the performance evaluation of topic and sentiment models of text data."
    ],
    "2212": [
        "'Our approach achieves new state-of-the-art on FewRel 2.0 dataset.'",
        "'We first train a representation extractor with the Clustering Promotion Mechanism.'",
        "'Two methods are combined by the proposed Cosine Annealing Strategy.'",
        "'The representation extractor is used to encode unlabeled target-domain data into features.'",
        "'Experimental results demonstrate that our approach outperforms sequence-to-sequence approaches in low-resource domains.'",
        "'A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.'",
        "'We treated the local latent variables of paragraph vectors in a Bayesian way because we expected high uncertainty, especially for short documents.'",
        "'Our experiments confirmed this intuition, and showed that knowledge of the posterior uncertainty improves the performance of downstream supervised tasks.'",
        "'In addition to MAP and VI, we experimented with Hamiltonian Monte Carlo (HMC) inference, but our preliminary results showed worse performance; we plan to investigate further.'",
        "'More sophisticated models of document embeddings would also benefit from a Bayesian treatment of the local variables.'"
    ],
    "2218": [
        "The proposed CNN model for unsupervised document embedding achieves comparable performance to state-of-the-art models while significantly improving the efficiency in terms of computational cost.",
        "The learning algorithm based on stochastic forward prediction has few hyperparameters to tune and is straightforward to implement, making it an effective approach for training the model.",
        "The model can take full advantage of parallel execution, resulting in significant speedup compared to leading RNN models.",
        "The proposed approach achieves comparable accuracy to state-of-the-art models at a fraction of the computational cost."
    ],
    "2219": [
        "We propose several methods to solve the sentence-level relation classification problem.",
        "Existing methods do not work as well on this task.",
        "Our multi-level sentence normalization is useful.",
        "Future directions include: 1) better leveraging distant supervision to reduce human efforts, 2) incorporating knowledge graph embedding techniques, 3) applying the LOCATEDNEAR knowledge into downstream applications in computer vision and natural language processing."
    ],
    "2223": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed neural model for CWS is capable of capturing rich n-gram features automatically.",
        "Integrating the proposed model with word embeddings can achieve state-of-the-art performance on benchmark datasets without using any external labeled data.",
        "The proposed approach to integrate the proposed model with word embeddings is effective."
    ],
    "2225": [
        "The subword model best fits Chinese-to-English translation with vocabulary that is not so big.",
        "The hybrid wordcharacter approach obtains the highest performance on English-to-Chinese translation.",
        "Hybrid BPE method can acquire best result for Chinese-to-English translation task.",
        "Different granularities show that Hybrid BPE method can acquire best result for Chinese-to-English translation task.",
        "The experiments demonstrate that the subword model and hybrid wordcharacter approach are effective in Chinese-to-English and English-to-Chinese translation tasks, respectively.",
        "The advantages and disadvantages of various translation granularities have been discussed in detail."
    ],
    "2226": [
        "The proposed method outperforms existing baseline methods for estimating on-line audiences' demographic attributes based on their browsing histories.",
        "The proposed method does not require human annotations and is unsupervised.",
        "The proposed method can be applied to other demographic attributes, such as income, race, sexual orientation, and hobbies, etc."
    ],
    "2228": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our proposed POS tagging model exploiting adversarial training achieves substantial improvements on all languages tested, especially on low resource ones.",
        "Adversarial training enhances the robustness to rare/unseen words and sentence-level accuracy, alleviating the major issues of current POS taggers.",
        "Our analyses on different languages, word / neighbor statistics and word representation learning reveal the effects of AT from the perspective of NLP.",
        "The proposed AT model is applicable to general sequence labeling tasks.",
        "This work therefore provides a strong basis and motivation for utilizing AT in natural language tasks."
    ],
    "2229": [
        "The proposed method for root-and-pattern morphology discovery in Semitic languages is unsupervised.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of the discovered rules validate the pattern discovery method and root extraction method (JZR).",
        "The performance of the proposed method is not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "The proposed method presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "The intrinsic and extrinsic evaluations of the discovered rules validate the pattern discovery method and root extraction method (JZR).",
        "The proposed method is validated through empirical comparisons between two strategies for Vietnamese POS tagging from unsegmented text and between SOTA feature-and neural network-based models.",
        "The pipeline strategy produces higher scores of POS tagging from unsegmented text than the joint strategy.",
        "A traditional feature-based model (i.e. MarMoT) obtains better POS tagging accuracy than neural network-based models."
    ],
    "2230": [
        "Combining sequence-level and token-level losses is necessary to perform best",
        "Training on candidates decoded with the current model improves state-of-the-art baselines",
        "Sequence-level training improves state-of-the-art baselines for IWSLT'14 German-English translation and Gigaword abstractive sentence summarization",
        "Structured prediction losses are very competitive to recent work on reinforcement or beam optimization",
        "Classical expected risk can slightly outperform beam search optimization",
        "Future work may investigate better use of already generated candidates to speed up training"
    ],
    "2231": [
        "The proposed neural model, Dynamic Fusion Network (DFN), can adapt effectively to handling questions of different types by dynamically constructing an optimal attention strategy and number of reasoning steps for a given input sample.",
        "By training the policy of model construction with reinforcement learning, the DFN model can substantially outperform previous state-of-the-art MRC models on the challenging RACE dataset.",
        "The combination of dynamic fusion (DF) with multi-step reasoning (MR) in the DFN model leads to a statistically significant performance boost over baseline models.",
        "Future extensions of the DFN model include incorporating more comprehensive attention strategies and applying the model to other challenging MRC tasks with more complex questions that require DF and MR jointly.",
        "The Dynamic Fusion Layer can be made more flexible in future extensions, allowing for the construction of a \"composable\" structure on the fly."
    ],
    "2232": [
        "Our proposed model, SKIPFLOW LSTM, outperforms a baseline LSTM by approximately 10%.",
        "Our approach produces significantly better results compared to multi-layered and attentional LSTMs.",
        "We achieve a significant 6% improvement over feature engineering baselines.",
        "Our model incorporates the intuition of textual coherence in neural ATS systems, generating neural coherence features that support predictions."
    ],
    "2233": [
        "Our approach obtains improvements for instruction following as well as instruction generation in multiple settings.",
        "The inference procedure is capable of reasoning about sequential, interdependent actions in non-trivial world contexts.",
        "Pragmatics improves upon the performance of the base models for both tasks, in most cases substantially.",
        "Pragmatic reasoning can also improve performance for a grounded listening task with sequential, structured output spaces."
    ],
    "2239": [
        "The proposed models can significantly improve the translation quality in Chinese to English translation.",
        "The models are able to learn better word alignments than the baseline NMT.",
        "The models can alleviate the notorious problems of over and under translation in NMT.",
        "The models can learn direct mappings between source and target words.",
        "In future work, the authors intend to explore further strategies to bridge the source and target side for sequence-to-sequence and tree-based NMT.",
        "The authors also plan to apply these methods to other sequence-to-sequence tasks, including natural language conversation."
    ],
    "2248": [
        "We described the creation of PARANMT-50M, a dataset of more than 50M English sentential paraphrase pairs.",
        "Our approach only requires parallel text, and there are hundreds of millions of parallel sentence pairs available.",
        "Our procedure is immediately applicable to the wide range of languages for which we have parallel text.",
        "We release PARANMT-50M, our code, and pretrained sentence embeddings, which also exhibit strong performance as general-purpose representations for a multitude of tasks.",
        "We hope that PARANMT-50M, along with our embeddings, can impart a notion of meaning equivalence to improve NLP systems for a variety of tasks.",
        "We are actively investigating ways to apply these two new resources to downstream applications, including machine translation, question answering, and paraphrase generation for data augmentation and finding adversarial examples."
    ],
    "2252": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The QAMR labels capture the same kinds of predicate-argument structures as existing resources, but the recall on these relations is low compared to expert-annotated structures when only gathering annotations from one annotator for each target word.",
        "Improving the annotation and structure induction (e.g., by adding a second crowdsourcing stage to fill in the missing relations identified in step 3 of our structure induction algorithm) is an interesting avenue of future work."
    ],
    "2254": [
        "The system presented in the paper is an end-to-end trainable system for efficiently answering questions from large corpora of text.",
        "The system combines a text auto-encoding component and a memory-enhanced sequence-to-sequence component to translate questions into programs.",
        "The method achieves good scaling properties and robust inference on syntactic and natural language text.",
        "The system illustrates how a bottleneck in knowledge management and reasoning can be alleviated by end-to-end learning of a symbolic knowledge storage.",
        "The total expected reward for a set of valid knowledge stores G is the sum of the rewards for each knowledge store in the set, and the set of knowledge stores that contain the tuple gi is G(gi).",
        "The system uses an experience replay buffer to store the program with the highest weighted reward during training.",
        "The bAbI tasks are a set of tasks that involve answering questions from large corpora of text."
    ],
    "2258": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our proposed embeddings were able to learn the actual semantics of the radiological terms from free-text reports.",
        "We have successfully annotated the radiology reports according to the likelihood of intracranial hemorrhage with 89.08% F1 score.",
        "The techniques introduced in this paper can be used also for creating vector representation from clinical notes of different domains (e.g. oncology) given a domain-specific ontology that can be used to reduce underlying term variations in the corpus."
    ],
    "2260": [
        "Our proposed forest-based Attentional NMT model outperforms the tree-to-sequence ANMT model.",
        "Using a packed forest instead of the 1-best tree in the encoder enhances the accuracy of translation.",
        "Parsing errors are inevitable when using the phrase structure tree for computing embedding of phrases of source sentence.",
        "Our method efficiently considers exponentially many grammar trees to compensate for parsing errors.",
        "Using forest of parse tress and dynamic programming, our method can improve the accuracy of translation."
    ],
    "2261": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We achieved comparable translation scores with state-of-the-art methods, while the speed is fast.",
        "Incorporating neural models to build the soft-matrix for our method should make a positive influence."
    ],
    "2264": [
        "The proposed event models can effectively predict and generate event schemas, with significant overlap in goals but also differences in terms of script learning.",
        "The joint model of POS tagging and dependency parsing improves over previous work on joint POS tagging and dependency parsing, as it allows for the interaction between lexical information and syntactic information.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntatic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The approach of joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing."
    ],
    "2265": [
        "The proposed adversarial learning-based approach can generate more human-like dialogs, achieving state-of-the-art performance on the VisDial dataset.",
        "The combination of a sequential co-attention generative model and a discriminator with an intermediate reward can encourage the generator to produce more human-like responses.",
        "The use of an adversarial learning-based approach can improve the quality of generated visual dialog responses, as demonstrated by a Turing Test fashion study.",
        "The proposed model has the ability of multi-modal reasoning, jointly reasoning the image, dialog history, and question.",
        "The performance of the proposed model is limited by the insufficiency of high-quality annotated data for causality extraction, as mentioned in Section 4.4."
    ],
    "2267": [
        "We propose a novel method for jointly learning to compose QA pairs.",
        "Our lightweight temporal crossing (LTC) mechanism is an effective method of modeling interactions between QA pairs without incurring any parameter cost.",
        "Our CTRN model performs competitively on two CQA benchmarks and one factoid QA benchmark while being much faster than LSTM and AP-BiLSTM models.",
        "We introduce a new method for jointly learning to compose QA pairs by aligning temporal gates."
    ],
    "2270": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The BiLM is trained in an unsupervised manner using only the unlabeled data.",
        "Pretraining the NER model weights is a good initialization strategy for the optimizer as it leads to substantial improvements in the F1 scores for four benchmark datasets.",
        "A pretrained model requires less training data compared with a randomly initialized model.",
        "A pretrained model also converges faster during model fine-tuning.",
        "We observe gains in the recall score for both seen and unseen disease entities."
    ],
    "2275": [
        "The proposed method addresses three major challenges in automatically generating textual reports for medical images, including localizing abnormal regions and producing accurate descriptions.",
        "The proposed method achieves effectiveness through a multi-task learning framework that jointly predicts tags and generates descriptions, with the help of a co-attention mechanism and a hierarchical LSTM network.",
        "The method demonstrates effectiveness on two medical datasets containing radiology and pathology images, as shown by quantitative and qualitative studies.",
        "The proposed method can generate multiple heterogeneous forms of information within a unified framework.",
        "The method can produce accurate descriptions for abnormal regions and localize them effectively."
    ],
    "2279": [
        "Our approach uses sub-differentiable loss functions and is trained through back propagation, potentially allowing for seamless integration into neural models, and end-to-end training capabilities.",
        "Large scale crowd-sourced experiments show that our word embeddings are more interpretable than the embeddings generated by state-of-the-art sparse coding approaches.",
        "Our embeddings outperform popular baseline representations on a diverse set of downstream tasks.",
        "We have presented a novel mechanism to generate interpretable word embeddings using denoising k-sparse autoencoders.",
        "Our approach uses sub-differentiable loss functions and is trained through back propagation, potentially allowing for seamless integration into neural models, and end-to-end training capabilities.",
        "As a part of future work, we are investigating the effect of inducing varying amounts of sparsity at multiple hidden layers in more sophisticated networks, and studying the properties of the resultant sparse activations."
    ],
    "2281": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "Our goal is to spur discussion and lines of technical data-driven dialogue systems research, including: battling underlying bias and adversarial examples; ensuring privacy, safety, and reproducibility.",
        "Comparison of PReFIL with prior state-of-the-art methods for DVQA and FigureQA.",
        "Performance of PReFIL due to OCR model variations for DVQA and FigureQA.",
        "Use of oracle OCR for improving the performance of PReFIL for structure questions.",
        "Experimental results demonstrating the new state-of-the-art achieved by the approach on FewRel 2.0 dataset.",
        "Description of the representation extractor and its use in encoding unlabeled target-domain data into features.",
        "Goals of the authors for spurring discussion and lines of technical data-driven dialogue systems research, including battling underlying bias and adversarial examples, ensuring privacy, safety, and reproducibility."
    ],
    "2282": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism, which uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "We combine two methods using the proposed Cosine Annealing Strategy.",
        "The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.",
        "We utilize pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "Our approach uses a cachelike memory network that stores translation history in terms of bilingual hidden representations at decoding steps of previous sentences.",
        "The cache component is an external key-value memory structure with the keys being attention vectors and values being decoder states collected from translation history.",
        "We expect to shed more light on utilizing long-range contexts in future work, such as designing novel architectures and employing discourse relations instead of directly using decoder states as cache values."
    ],
    "2283": [
        "The proposed algorithm for text attribute transfer with non-parallel corpora achieves promising results.",
        "The algorithm uses an encoder-decoder architecture with attention and is augmented with a collaborative classifier and content preservation losses.",
        "There are challenges to be addressed, including achieving better results across all three metrics and extending the architecture for multi-attribute transfer.",
        "The current architecture is limited in its ability to handle more challenging text attributes, such as professional-colloquial texts that undergo significant transformation.",
        "The proposed method has the potential to improve the quality of text attribute transfer."
    ],
    "2284": [
        "Our approach significantly improves translation performances across different language pairs.",
        "With better modeling of past and future translations, our approach performs much better than the standard attention-based NMT.",
        "Our method reduces the errors of under and over translations.",
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Modeling past and future translations is crucial for encoder-decoder based NMT systems.",
        "Separating PAST and FUTURE functionalities from decoder states can maintain a dynamical yet holistic view of the source content at each decoding step."
    ],
    "2287": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The resulting fully-supervised parser outperforms the state-of-theart baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Experiments on several datasets show good results, even though the sub-vectors are randomly assigned and fixed during training.",
        "It would be useful to explore the results of using this technique in sequence-to-sequence models for natural language processing tasks such as machine translation and dialog systems."
    ],
    "2291": [
        "'Our HRL model obtains the state-of-the-art performance on both the widely used MSR-VTT dataset and the newly introduced Charades Captions dataset for fine-grained video captioning.'",
        "'In the future, we plan to explore the attention space and utilize features from multiple modalities to boost our HRL agent.'",
        "'We believe that the results of our method can be further improved by employing different types of features, i.e. C3D features [36] , optical flows, etc.'",
        "'Meanwhile, we will investigate the HRL framework in other similar sequence generation tasks like video/document summarization.'"
    ],
    "2294": [
        "The agent navigates the environment using first-person vision and intelligently explores the environment to answer questions.",
        "The neural hierarchical model decomposes navigation into a \"planner\" and a \"controller\".",
        "The agent is initialized via imitation learning and fine-tuned using reinforcement learning for the goal of answering questions.",
        "Human demonstrations are collected by connecting workers on Amazon Mechanical Turk to remotely control an embodied agent.",
        "All code, data, and infrastructure will be made publicly available.",
        "The queryptemplateq generates question strings based on the entities received as input.",
        "The model can produce 3 question strings for each entity in the input set."
    ],
    "2296": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed pipeline not only maintains the superiority of DNN-based methods, but also acquires the ability to exploit external knowledge for answering open-domain visual questions.",
        "Our method achieves competitive results on public large-scale datasets, and gains huge improvement on our generated open-domain dataset."
    ],
    "2300": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "the role of individual components cannot be deduced from the overall performance of the model.",
        "the impact of each information source on the total performance of the model",
        "grounding natural language is a difficult problem as models which combine modalities must account for the individual impact of each information source.",
        "how to learn interaction models of independent modalities that can be shown to generalize well in conjunction, and avoid the pitfall of overfitting to spurious correlations that optimize the surrogate learning objectives of each independent modality."
    ],
    "2301": [
        "The proposed multimodal imitation learning approach can generate storylines on unseen events without designing reward functions.",
        "GAN-based imitation learning is introduced to learn the latent policy given users' demonstrations.",
        "The model effectively integrates generative adversarial nets and multimodal learning via deterministic policy gradient.",
        "The different modalities learn from each other and potentially resolve confusion from each single modality.",
        "The proposed method succeeds in capturing the latent patterns across different modalities, revealing more satisfying storylines towards users' interests."
    ],
    "2303": [
        "'Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.'",
        "'Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.'",
        "'The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.'",
        "'Our SRL models improve SRL performances, leading to the new state-of-the-art.'"
    ],
    "2308": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The use of external linguistic resources allows us to generate new word forms that would not be included in the standard NMT system shortlist.",
        "The advantage of this approach is that the new generated words are controlled by the linguistic knowledge, that avoid producing incorrect words, as opposed to actual systems using BPE.",
        "We demonstrated the performance of such a system on an inflected language (French). The results are very promising for use with highly inflected languages like Arabic or Czech."
    ],
    "2312": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "DAN is superior to a wide spectrum of baselines.",
        "Applications of this model can be found in e-commerce websites and recommender systems."
    ],
    "2320": [
        "The proposed method leverages Hungarian algorithm to design a Hungarian layer that extracts aligned matched and unmatched parts exclusively from the sentence pair.",
        "The aligned unmatched parts are semantically critical, and our model is designed based on this assumption.",
        "Experimental results on benchmark datasets verify the effectiveness of the proposed method.",
        "The proposed method leverages Hungarian algorithm to design a Hungarian layer that extracts aligned matched and unmatched parts exclusively from the sentence pair.",
        "The aligned unmatched parts are semantically critical, and our model is designed based on this assumption.",
        "Experimental results on benchmark datasets verify the effectiveness of the proposed method."
    ],
    "2321": [
        "LPA-TD outperforms both the configurations of the OnlyLPA baseline on all datasets.",
        "It also performs better than all other baselines on two datasets - PC vs MAC and POLITICS vs SCIENCE.",
        "PC vs MAC is considered to be the most difficult dataset from the 20NG corpora due to significant overlap of words seen in both classes.",
        "LPA-TD outperforms all the baselines comfortably, re-iterating the merit of the new technique.",
        "It however, doesn't perform as well as the TLC and ClassifyLDA techniques in the POLITICS vs RELIGION dataset.",
        "A look at the topics learned in the 10 runs reveals mostly complex and fuzzy topics not attributable to a single class, which brings down the overall performance."
    ],
    "2322": [
        "Our approach is flexible in the sense that it can be easily adapted to other datasets or employed in various clinical settings where data availability, characteristics, format and statistical distribution of text vary.",
        "Moreover, the fact that it is only based on a subset of the information that is available upon admission time, allows our method to integrate well with the clinical setting workflow and provide timely feedback to the clinician.",
        "The efficiency is further improved by the fact that our models can be trained end-to-end, without specific need for fine-tuning hyperparameters of individual components.",
        "Unlike many existing works, such as (Bond et al., 2012) , (Grady and Berkowitz, 2011) , (Ebell, 2010), and (Achour et al., 2001) we did not use any human designed rules (based on prior medical knowledge) for clinical notes' feature learning."
    ],
    "2326": [
        "The proposed approach for predicting Facebook post reactions is effective and can enhance customer experience analytics.",
        "The dataset used in this study is available for other researchers and can be used as a baseline for further experiments.",
        "The emotion miner can be combined with the output of neural network models to predict Facebook reactions.",
        "There is still much uncharted territory in the area of emotion mining, and this work contributes to this direction.",
        "Refining the architectures of the neural networks used could lead to better performance.",
        "Including the reaction of the page owner in the analysis could be useful information.",
        "Predicting the absolute amount of reactions, rather than just the ratio, is a potential future direction.",
        "Combining images and text could reveal possible synergies in the vision and language domains for sentiment/emotion related tasks."
    ],
    "2330": [
        "We examine the importance of context for the task of reading comprehension.",
        "Our neural module that gates contextual and non-contextual representations shows gains due to context utilization.",
        "Injecting contextual information into our model by integrating a pre-trained language model through our suggested module substantially improves results, reaching state-of-the-art performance on the SQuAD dataset.",
        "Our approach improves results by incorporating contextual information.",
        "Context utilization leads to gains in reading comprehension tasks."
    ],
    "2332": [
        "The proposed method for learning robust dialog policies can handle upstream NLU/ASR errors caused by noisy environments.",
        "The user simulator presented in the paper can mimic realistic conversations and achieve the same success rate with fewer dialog turns compared to fixed rule-based policies in noisy environments.",
        "The simulator can leverage audio signals and white noise to train policies in a production-like environment.",
        "The use of sentiment can be used to further improve the user simulator, such as simulating user frustration or satisfaction.",
        "Other dimensions of dialog, such as per-interaction user satisfaction and efficiency of interactions, can be explored to improve the simulator and RL-based dialog policy learning methods.",
        "The impact of using different kinds of noise on the learned policies can be investigated."
    ],
    "2338": [
        "We proposed a novel method for incorporating aspect information for learning attentions.",
        "Learning attentions from associative memory traces of word and aspects are effective.",
        "Overall, circular convolution remains highly effective for aspect-word fusion.",
        "Simple element-wise multiplications remains a strong baseline, outperforming simple concatenation models such as AT-LSTM and ATAE-LSTM.",
        "Our model shows significant improvement in performance compared to multiple strong neural baselines."
    ],
    "2340": [
        "The proposed hard negative mining approach can effectively reduce inappropriate responses that are identical or too similar to the input context.",
        "The resulting model avoids echoing the input context and selects more appropriate candidates as matching score response.",
        "The HNr+c model achieves better results in terms of Average Precision and Recall@N metrics compared to the models trained without the proposed approach.",
        "The proposed approach can improve the performance of a retrieval-based conversation system.",
        "The use of contexts as possible hard negative candidates can improve the selection of appropriate responses."
    ],
    "2341": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The number of challenges faced by previous work can be improved by our model empirically.",
        "Our model improves on the aspects of text generation and planning.",
        "Future work should apply our model to other dialogue settings, such as cooperative strategic dialogue games or multi-sentence generation tasks."
    ],
    "2347": [
        "The best result for distinguishing hate speech, profanity, and other texts was obtained by a character 4-gram model achieving 78% accuracy.",
        "Distinguishing profanity from hate speech is a very challenging task.",
        "This work is one of the first experiments to detect hate speech on social media in a scenario including non-hate speech profanity.",
        "Previous work (e.g., Burnap and Williams (2015) and Djuric et al. (2015)) has dealt with the distinction between hate speech and socially acceptable texts in a binary classification setting.",
        "The use of more robust ensemble classifiers, a linguistic analysis of the most informative features, and error analysis of the misclassified instances are potential directions for expanding this work."
    ],
    "2349": [
        "The new formulation of any-gram kernels outperformed the original implementation and achieved comparable results to LSTM models across three widely used sentiment analysis data sets.",
        "Our experiments only used the n-grams, but combining the any-gram kernels with other features could be beneficial.",
        "One way to achieve this combination is via auxiliary input to the any-gram kernels, which we described in the context of marking the aspect term in a sentence.",
        "Another way would be to combine the any-gram kernel with other more traditional kernels (e.g. RBF) applied to hand-crafted features."
    ],
    "2350": [
        "The ConMask model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The approach of using a representation extractor with the Clustering Promotion Mechanism achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The tasks in the new dataset are more challenging than existing datasets because of the domain (fiction).",
        "The approach of using a representation extractor with the Clustering Promotion Mechanism is effective in adapting to unsupervised domain adaptation.",
        "The approach of using a cosine annealing strategy to combine the Clustering Promotion Mechanism and Adversarial Distribution Alignment improves the performance of the few-shot classifier."
    ],
    "2354": [
        "The latter performs substantially better than the former\" - This claim suggests that using a multi-label classifier to predict visual entities from action features is more effective than using a simple aggregate action feature vector obtained through maxpooling.",
        "only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline\" - This claim indicates that the proposed method is only slightly worse than a more advanced sequence-to-sequence baseline, despite being much simpler.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\")\" - This claim suggests that providing lexical information to the parsing process is more beneficial than providing syntactic information to the tagging process.",
        "disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality\" - This claim indicates that enabling interactions between tagging and parsing improves the quality of both tasks.",
        "tagging ambiguity is mostly local while dependency parsing heavily depends on POS tags to predict syntactic structures\" - This claim highlights the difference in the nature of the two tasks, with tagging being more local and dependency parsing relying heavily on POS tags."
    ],
    "2361": [
        "The proposed email recipient action annotation scheme is agnostic to specific speech act theories and more suitable for training systems that suggest such actions.",
        "The introduced dataset achieved good inter-annotator agreement levels.",
        "The proposed hierarchical threaded message model RAINBOW is effective in modeling emails.",
        "The RNN reparametrization approach can learn to generalize in a minimally supervised scenario.",
        "The proposed reparametrization framework is extensible and can be used for personalized models or learning the taxonomy automatically with appropriate software."
    ],
    "2363": [
        "'Our findings indicate that improvements can be achieved by training well-known algorithms on very large text datasets.'",
        "'Using certain tricks can provide further gains in quality.'",
        "'De-duplicating sentences in large corpora such as the Common Crawl before training the models is important.'",
        "'Using an algorithm for building the phrases in a preprocessing step is beneficial.'",
        "'Adding position-dependent weights and subword features to the cbow model architecture gives a final boost of accuracy.'"
    ],
    "2364": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We introduce a simple end-to-end automatic speech recognition system that combines a ConvNet acoustic model with Gated Linear Units and a simple beam-search decoder.",
        "The acoustic model is trained to map audio into features, which are then passed through a k-means cluster miner to generate pseudo labels.",
        "We use pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "Our method uses Similarity Entropy Minimization to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The proposed Cosine Annealing Strategy combines two methods to improve domain adaption performance."
    ],
    "2365": [
        "TCNLM outperforms other topic models and language models in capturing global semantic meaning in a document.",
        "The proposed approach infers sensible topics and has the capacity to generate meaningful sentences conditioned on given topics.",
        "Our model has the potential to be extended for machine translation tasks.",
        "Experiments conducted on three corpora validate the superiority of the proposed approach.",
        "The incorporation of inferred topic information into the language model through a Mixture-of-Experts model design improves the learning of local semantic and syntactic relationships between words."
    ],
    "2367": [
        "The use of multilingual BERT encoder brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.",
        "Our novel end-to-end model for joint slot label alignment and recognition requires no external label projection."
    ],
    "2368": [
        "The proposed approach outperforms previous work on DSTC2, as shown by comparing our approach with the baseline in [10].",
        "The proposed approach has the ability to handle slots with unbounded value sets and transfer learning to new slots and domains.",
        "More elaborate DST architectures like NBT-CNN [10] can give better results on DSTC2, but the proposed approach has the advantage of being able to handle slots with unbounded value sets and transfer learning to new slots and domains.",
        "The proposed approach achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The proposed approach outperforms the simple projection baseline using fast-align on most languages."
    ],
    "2369": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Extensive ablation studies confirm the effectiveness of FM layers over FC layers.",
        "Subtractive composition helps in other models such as ESIM."
    ],
    "2371": [
        "The proposed approach achieves state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The use of pseudo labels improves domain adaptation performance.",
        "The approach is limited by the insufficiency of high-quality annotated data.",
        "The proposed method can be combined with distant supervision and reinforcement learning to improve performance without requiring a large, high-quality annotated corpus.",
        "The proposed method is able to handle the harder Samasa problem, which requires semantic information of a word in addition to the characters' context.",
        "Existing Sandhi splitting tools face challenges such as identifying multiple locations of split and validating them based on previous locations."
    ],
    "2373": [
        "The NER system learned using CRF consumes more time while training the model.",
        "The corpus contains three levels of named entities in the case of FIRE, hence three levels separately used for training and testing.",
        "The dataset is not clean for Malayalam and Hindi language like we have Arabic words in between Malayalam words and English letters in between Hindi words in test data which may reduce the performance of the system.",
        "The POS tag is an important feature that helps in deciding the named entity.",
        "The proposed system which uses rich features like linguistic and binary features solves nested named entities using chain classifier.",
        "The specially extracted feature from tweets shows greater performance in NEEL task and CRF along with LBFGS optimization consumes less time while comparing with other.",
        "However, the testing period is low as compared to the NER along with SVM, which ensures CRF can be utilized in real-time streaming applications.",
        "In this work, the second-level tagging is done by considering first-level tag as one feature. This will be overcome by structured output learning in future works.",
        "Collocations and associative measures can be used as a feature to improve nested named entity recognition."
    ],
    "2374": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "The proposed AdaBERT adaptively compresses BERT for various downstream tasks using Neural Architecture Search.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks."
    ],
    "2377": [
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We address the problem of product title compression for E-commerce Apps on phones, where long titles cannot be displayed properly due to the limited screen size.",
        "We propose a pointer network-based sequence-to-sequence model in a multi-task setting to improve user experience and guarantee query-initiated transaction conversion rate.",
        "Our framework not only improves user experience by compressing redundant titles into concise ones, but also guarantees query-initiated transaction conversion rate by prioritizing query-related keywords in the resultant short titles."
    ],
    "2381": [
        "We have shown that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; we hope to see it included in making stronger baseline NMT systems.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem is a very limited form of globally-normalized model.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized models for NMT.",
        "We have contrasted the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Using a logistic normal topic model which incorporates semantic information about synsets as its priors, our proposed system scales linearly with the number of words in the context, allowing us to use the whole document as the context for disambiguation and outperform state-of-the-art knowledge-based WSD systems.",
        "One possible avenue for future research is to use this model for supervised WSD, possibly by using sense tags from the SemCor corpus as training data in a supervised topic model similar to the one presented by (Mcauliffe and Blei 2008).",
        "Another possibility would be to add another level to the hierarchy of the document generating process, allowing us to bring back the notion of topics and then define topic-specific sense distributions."
    ],
    "2382": [
        "'We present a comprehensive comparison study of the existing corpora for selection-based question answering.'",
        "'Our intrinsic analysis provides a better understanding of the uniqueness or similarity between these corpora.'",
        "'Our extrinsic analysis shows the strength or weakness of combining these corpora together for statistical learning.'",
        "'We create a silver-standard dataset for answer retrieval and triggering, which will be publicly available.'",
        "'In the future, we will explore different ways of improving the quality of our silver-standard datasets by fine-tuning the hyper-parameters.'"
    ],
    "2384": [
        "The proposed approach achieves significant improvements over a range of ranking networks for passage retrieval in biomedical question answering.",
        "The use of lean non-parametric weighting schemes is effective in accounting for differences in term distribution between document and question corpora.",
        "The approach is not specific to the biomedical domain and can be applied to a wider range of topical domains.",
        "The use of cosine distances is a light-weight architecture that is of increased interest in academic settings where datasets are often not sufficiently large to robustly fit multitudes of neural network parameters.",
        "The approach demonstrates the potential for improving passage retrieval in biomedical question answering, which can be useful in real-world applications such as medical consulting."
    ],
    "2386": [
        "The brevity problem in neural machine translation can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The solution to the brevity problem is a very limited form of globally-normalized models for NMT.",
        "The brevity problem is an example of label bias.",
        "Our proposed approach can effectively learn entity embeddings, and the learned embeddings are able to help the task of relation extraction.",
        "The proposed approach consistently outperforms both the baseline and the state-of-the-art results.",
        "There is room for further exploration of external knowledge to obtain even better entity embeddings.",
        "The proposed approach can be applied to other datasets or languages."
    ],
    "2394": [
        "The use of pretrained character embeddings in the proposed crowdsourcing learning model significantly outperforms the random initialized embeddings, demonstrating the useful information provided by the pretrained embeddings.",
        "The baseline and worker adversarial models differ in their performance, as shown in the closed test on the training set.",
        "The majority-voting strategy is not effective in capturing the correct NER results, as none of the annotations get the correct result, but our proposed model can capture it.",
        "The LSTM-CRF model fails to recognize the named entity \"Xiexie\" due to not trusting the second annotation, treating it as one noise annotation, while our proposed model is able to recognize it by extracting worker independent features."
    ],
    "2399": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our method significantly outperformed existing transfer learning techniques and the state-of-the-art on six representative text classification tasks.",
        "We hope that our results will catalyze new developments in transfer learning for NLP."
    ],
    "2400": [
        "The proposed approach achieves new state-of-the-art performance on FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The Cosine Annealing Strategy is proposed to combine the two methods.",
        "The approach utilizes pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "Experimental results demonstrate the effectiveness of the proposed approach.",
        "The framework can explicitly capture sentiment-oriented word interaction by learning a set of sentiment-sensitive Word Interaction (SWI) vectors.",
        "The two new models, Contextual Factorization Machine (CFM) and Position-aware Factorization Machine (PFM), benefit fine-grained analysis at the snippet level and simplify the parameter learning."
    ],
    "2401": [
        "Our proposed strategy for a task-completion dialogue agent learns in a more efficient way compared to previous work, using only a small number of real user interactions.",
        "The effectiveness of our proposed method, Deep Dyna-Q (DDQ), is validated by human-in-the-loop experiments, demonstrating that a dialogue agent can efficiently adapt its policy on the fly by interacting with real users via deep reinforcement learning.",
        "One interesting topic for future research is exploration in planning, particularly dealing with the challenge of adapting the world model in a changing environment, as exemplified by the domain extension problem.",
        "The general problem of exploration vs. exploitation in planning is a particular manifestation of the conflict between exploration and exploitation, as pointed out by Sutton and Barto (1998).",
        "Our proposed method, DDQ, integrates planning into dialogue policy learning, which can improve the efficiency of the agent's learning process."
    ],
    "2402": [
        "Multilayer neural network models have gained wide popularity for text classification tasks due to their much better performance than traditional bag-of-words based approaches.",
        "Neural networks can effectively utilize the word order structure present in documents.",
        "However, a potential drawback is that since all neural network approaches are discriminative, they tend to identify key signals in the training data which may not generalize to test data.",
        "We investigate whether these neural network models actually learn to compose the meaning of sentences or just use discriminative keywords.",
        "Our experiments with popular text classifiers show that there is a large drop in test classification accuracy between random and lexicon splits of these datasets.",
        "Simple regularization techniques such as keyword anonymization can substantially improve the performance of text classifiers.",
        "We also observe that adaptive word dropout method which is based on embedding layer's gradient can further improve accuracy and thus reduce the gap between the two dataset splits."
    ],
    "2409": [
        "Our approach outperforms the state-of-the-art relation clustering method by 5.8% pairwise F1 score.",
        "We proposed an approach for unsupervised relation extraction from free text.",
        "Our approach is based on a novel method of re-weighting word vectors according to the dependency parse tree of the sentence.",
        "We use the types of named entities involved in the relations as additional features.",
        "Our method maps similar representation of a relation to the same cluster using a final HAC clustering.",
        "The code for feature building and dimensionality reduction is publicly available."
    ],
    "2412": [
        "We introduce assertion-based question answering (ABQA), an open-domain QA task that answers a question with a semi-structured assertion which is inferred from the content of a document.",
        "We construct a dataset called WebAssertions tailored for ABQA and develop both generative and extractive approaches.",
        "Our ABQA approaches have the ability to infer question-aware assertions from the document.",
        "Incorporating ABQA results as additional features significantly improves the accuracy of a baseline system on passage-based QA.",
        "We plan to improve the question understanding component and the reasoning ability of the approach so that assertions across different sentences could be used to infer the final answer."
    ],
    "2413": [
        "We have published HappyDB, a broad corpus of happy moments expressed in diverse linguistic styles.",
        "We have also derived a cleaned version of HappyDB, added annotations, and presented our analysis of HappyDB based on these annotations.",
        "We believe that HappyDB can spur research of the topic of understanding happy moments and more generally, the expression of emotions in text.",
        "The results of this research can translate to applications that can improve people's lives."
    ],
    "2414": [
        "The hidden representations in NMT models learn linguistic information, and this information can be interpreted to improve the performance of the model.",
        "The effect of layer depth and target language on part-of-speech and semantic tagging can be analyzed to understand how the model learns linguistic information.",
        "Extending this work to other syntactic and semantic tasks, such as dependency relations or predicate-argument structure, could lead to better MT systems.",
        "Understanding how semantic properties are learned in NMT is a key step for creating better MT systems."
    ],
    "2415": [
        "Our proposed framework incorporating user and product information for sentiment classification achieves obvious and consistent improvements compared to other state-of-the-art methods.",
        "Our model can capture user information and product information through visualizations of attention.",
        "Applying two individual hierarchical neural networks to generate two representations, with user attention or with product attention, improves the performance of sentiment classification.",
        "Incorporating user and product information into the sentiment classification framework leads to strong improvements in reading comprehension tasks."
    ],
    "2417": [
        "Claim 6:"
    ],
    "2418": [
        "AMANDA achieves state-of-the-art performance on NewsQA, TriviaQA, and SearchQA datasets, outperforming all prior published models by significant margins.",
        "The proposed components are important for the performance of AMANDA.",
        "AMANDA learns to aggregate meaningful evidence from multiple sentences with deeper understanding and to focus on the important words in a question for extracting an answer span from the passage with suitable answer type.",
        "AMANDA outperforms all prior published models by significant margins.",
        "The proposed question-focused multifactor attention network (AMANDA) learns to aggregate meaningful evidence and focus on important words in a question for extracting an answer span from the passage with suitable answer type."
    ],
    "2419": [
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "Our evaluation on German-English language pairs showed an improvement of up to 0.6 BLEU points.",
        "We also demonstrated gains compared to the previous solution where these models are trained on parts-of-speech tags and word clusters, to address data sparsity and for better generalization.",
        "Our modification to the natural source order and integration of reordering symbols in the training data did not yield improvement."
    ],
    "2420": [
        "Our method achieves significant improvements in performance compared to all previous encoder-decoder neural network approaches for the task of grammatical error correction.",
        "We utilize large English corpora to pre-train and initialize the word embeddings and to train a language model to rescore the candidate corrections.",
        "We make use of edit operation features during rescoring.",
        "Our novel method achieves improved performance on both CoNLL-2014 and JFLEG data sets, significantly outperforming the current leading SMT-based systems.",
        "We have thus fully closed the large performance gap that previously existed between neural and statistical approaches for this task."
    ],
    "2421": [
        "This is the first time that this critical and practicable task has been considered.",
        "Our model reaches the score of 7.3 out of 10, demonstrating the effectiveness.",
        "Directly applying seq2seq model would lead to the repetition problem that lowers the recall rate.",
        "The strong assumption of the order between herb tokens can hurt the performance.",
        "Applying the coverage mechanism and the soft loss function alleviates the repetition problem and results in an improved recall rate."
    ],
    "2427": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "TRANSREV outperforms existing methods in 15 of the data sets under consideration.",
        "TRANSREV is able to approximate, at test time, the embedding of the review as the difference of the embedding of the reviewed item and of the reviewing user.",
        "The approximated review embedding can be used with a sentiment analysis method to predict the review score."
    ],
    "2430": [
        "The proposed model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets for few-shot text classification.",
        "The use of dynamic memory as a learning mechanism is more general than what has been used in this work, and can be applied to other learning problems.",
        "The model significantly outperforms existing HRED models and its attention variants on the Chinese customer services dataset and English Ubuntu dialogue dataset for multi-turn dialogue generation.",
        "The relevant contexts detected by the model are significantly coherent with human judgments.",
        "Introducing topical information or considering detailed content information can further improve the quality of generated responses.",
        "The use of paraphrase information and context in compositionality modeling and scoring is effective and can be applied to downstream tasks.",
        "The improvement of the context-dependent model on an extrinsic machine translation task corroborates the utility of these additional knowledge sources."
    ],
    "2436": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "A good choice of selection criteria can have a strong influence on the model's performance in the low-data regime.",
        "It is not necessary to limit this search to properly defined submodular functions.",
        "The space of raw samples already contains much information that can be exploited.",
        "The information is potentially more useful than the output space information used in the active learning paradigm."
    ],
    "2437": [
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our approach models both view-specific dynamics as well as cross-view dynamics continuously through time.",
        "MARN achieves state-of-the-art results in 6 publicly available datasets and across 16 different attributes related to understanding human communication."
    ],
    "2439": [
        "This work proposes a novel method for consensus-based summarization that takes into account the content of existing summaries, rather than sentence rankings.",
        "The proposed method uses pseudo relevant model summaries to estimate the performance of candidate summaries.",
        "Each candidate is weighted based on their expected performance when generating a meta-ranking.",
        "The proposed C-WCS system outperforms other consensus-based aggregation methods by a large margin and performs at par with state-of-the-art techniques."
    ],
    "2444": [
        "Our proposed decoding-historybased Adaptive Control of Attention (ACA) for the NMT model can transmit significant information in the decoding history to control the output of the attention mechanism adaptively.",
        "With this method, the conflict between the source-side information from the attention and the translated contents can be mitigated.",
        "Our model captures more correct source information with the help of the decoding history and its translation behaves more adaptive to the past translation.",
        "Experiments on Chinese-English translation and English-Vietnamese translation all show that our model outperforms strong baselines, which demonstrate the effectiveness of our model."
    ],
    "2447": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The results for DVQA were more nuanced due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "The current dataset did not include human-generated questions, which limited the potential performance of the models.",
        "Document-level CQA requires information in the rest of the document to answer questions about the chart.",
        "Our model samples latent variables with more flexible distributions without sacrificing recurrent neural network's capability of synthesizing coherent sentences."
    ],
    "2452": [
        "The proposed multi-agent communication game can significantly outperform state-of-the-art methods for training a zero-resource NMT system.",
        "The attention mechanism can be successfully deployed to the target zero-resource NMT model by encouraging the agents to cooperate with each other to win a image-to-target translation game.",
        "The proposed multi-agent learning mechanism can handle generic documents and not limited to the domain where texts can be grounded to visual content.",
        "The method can outperform state-of-the-art methods using automatically crawled noisy multimodal data from the web.",
        "The current method is intrinsically limited to the domain where texts can be grounded to visual content, and it is interesting to explore how to further extend the learned translation model to handle generic documents."
    ],
    "2454": [
        "NAS is an important advance that automatizes the designing process of neural networks, but its computational expense prevents it from being widely adopted.",
        "In this paper, we presented ENAS, a novel method that speeds up NAS by more than 1000x in terms of GPU hours.",
        "ENAS's key contribution is the sharing of parameters across child models during the search for architectures.",
        "We showed that ENAS works well on both CIFAR-10 and Penn Treebank datasets."
    ],
    "2456": [
        "We propose a Description Logics (DL) based formal ontology learning (OL) tool, called DLOL, that works on factual non-negative IS-A sentences.",
        "We argue that OL on IS-A type sentences is non-trivial.",
        "We evaluated the accuracy of DLOL on IS-A datasets, against three state-of-the-art tools, and observed that it had an average LR improvement of 21% (approx.) and an average IIM-Recall improvement of 46% (approx.) with respect to next best performing tool."
    ],
    "2460": [
        "The proposed framework, SparseMAP, outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The SparseMAP model is well-suited for tasks where local ambiguities are common, like many natural language processing tasks.",
        "The SparseMAP loss leads to strong, interpretable networks trained end-to-end.",
        "The modular design of SparseMAP allows it to be applied readily to any structured problem for which MAP inference is available, including combinatorial problems such as linear assignment.",
        "The optimal step size \u03b3 can be computed using the conditional gradient method.",
        "The forward and backward passes of SparseMAP can be efficiently computed."
    ],
    "2462": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The proposed five features are strong enough to be fed into a classifier and beat the performance of most of the state-of-the-art models.",
        "Applying distributional hypothesis to a corpus to build a Distributional Thesaurus (DT) network and computing small number of simple network measures is less computationally intensive compared to preparing vector representation of words.",
        "By applying complex network theory, we can devise an efficient supervised framework for co-hyponymy detection which performs better or at par in some cases, compared to the heavy-weight state-of-the-art models.",
        "Our broad objective is to build a general supervised and unsupervised framework based on complex network theory to detect different lexical relations from a given corpus with high accuracy."
    ],
    "2463": [
        "The beam problem in neural machine translation (NMT) can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "The proposed method for joint slot label alignment and recognition does not require external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Idiom translation is one of the more difficult challenges of machine translation, and neural MT in particular has been shown to perform poorly on idiom translation despite its overall strong advantage over previous MT paradigms.",
        "The proposed data sets for training and testing idiom translation for German\u2192English and English\u2192German can be used to further investigate and improve NMT performance in idiom translation."
    ],
    "2470": [
        "The approach proposed in this paper enables sharing resources between high resource languages and extremely low resource languages.",
        "The proposed approach achieves better performance compared to a strong multilingual baseline system, with a BLEU score of 23 on Romanian-English WMT2016 using a tiny parallel corpus of 6k sentences.",
        "The approach is able to achieve better performance than the multilingual baseline system, despite using a much smaller parallel corpus.",
        "The proposed approach has the potential to improve machine translation for low-resource languages."
    ],
    "2473": [
        "The proposed method, Multinomial Adversarial Networks (MAN), can simultaneously minimize the difference among multiple probability distributions.",
        "MAN can be used to make multiple distributions indistinguishable from one another, making it a versatile tool for various tasks.",
        "The shared-private paradigm of MAN allows for learning domain-invariant features and domain-specific ones, enabling the model to learn only domain-invariant knowledge.",
        "The use of MAN in MDTC leads to outperforming prior art systems and achieving state-of-the-art performance on domains without labeled data when compared to multi-source domain adaptation methods.",
        "The power of MAN lies in its ability to make indistinguishable the shared feature distributions of samples from each domain, enabling the model to learn only domain-invariant knowledge."
    ],
    "2475": [
        "We have introduced an annotation scheme for searching a document collection for arguments relevant to a given topic.",
        "Our annotation scheme is applicable to the information-seeking perspective of argument search and general enough for use on heterogeneous texts.",
        "We have presented a new corpus, including over 25,000 instances over eight topics, that allows for cross-topic experiments using heterogeneous text types.",
        "Our attention-based model better generalizes to unknown topics than vanilla BiLSTM models."
    ],
    "2476": [
        "Our proposed model does not require a heuristic notion of negative training data.",
        "The model is based on a somewhat artificial assumption: L1 words do not necessarily need to have an L2 equivalent, and even when they do, this equivalent need not be realised as a single word.",
        "Our model can induce representations useful to several tasks including but not limited to alignment.",
        "We observed interesting results on semantic natural language processing benchmarks such as natural language inference, lexical substitution, paraphrasing, and sentiment classification.",
        "We are currently expanding the notion of distributional context to multiple auxiliary foreign languages at once, which seems to only require minor changes to the generative story and could increase the model's disambiguation power dramatically.",
        "Another direction worth exploring is to extend the model's hierarchy with respect to how parallel sentences are generated, such as modelling sentence-level latent variables that capture global constraints and expose additional correlations to the model."
    ],
    "2481": [
        "Combining dense count based models with network embedding methods leads to improved word representations.",
        "The combined vector representation of words yields better performance for most similarity and relatedness datasets compared to GloVe and Word2vec representations individually.",
        "Using information from Distributional Thesaurus as a proxy of WordNet can improve the state-of-the-art vector representation.",
        "The combined representation outperforms state-of-the-art embeddings for synonym detection and analogy detection tasks.",
        "In future work, we plan to investigate the effectiveness of the joint representation on other NLP tasks like text classification, sentence completion challenge, etc.",
        "Our ultimate aim is to prepare a better generalized representation of words that can be used across languages in different NLP tasks."
    ],
    "2484": [
        "Our model outperforms both singletask models and traditional multitask architectures.",
        "Evaluating on extremely low-resource settings, our model improves on both speech transcription and translation.",
        "By augmenting our models with regularizers that implement transitivity and invertibility, we obtain further improvements on all low-resource tasks.",
        "These results will hopefully lead to new tools for endangered language documentation.",
        "Our approach can be extended to settings where not all audio is transcribed or translated."
    ],
    "2486": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "The proposed non-autoregressive neural sequence model performs closely to the autoregressive counterpart with significant speedup in decoding.",
        "The iterative refinement indeed refines a target sequence gradually over multiple steps.",
        "The proposed non-autoregressive neural sequence model is outperformed by its autoregressive counterpart in terms of generation quality."
    ],
    "2487": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The approach of joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "The use of a projection-based method for attenuating biases can effectively reduce bias in contextualized embeddings without loss of entailment accuracy.",
        "The proposed ATPL approach for natural language generation and related tasks has a novel architecture based on a rationale derived from the use of Tensor Product Representations for encoding and processing symbolic structure through neural network computation.",
        "Compared to widely adopted LSTM-based models, the proposed ATPL gives significant improvements on all major metrics including METEOR, BLEU, and CIDEr.",
        "The unbinding vectors contain important grammatical information, which allows us to design an effective POS tagger and constituency parser with unbinding vectors as input."
    ],
    "2489": [
        "Our model outperforms strong baselines on implicit argument prediction tasks, especially when using salient entities.",
        "Salience is an important factor for performance in implicit argument prediction, and event knowledge is particularly useful for certain arguments (object and prepositional object).",
        "Our model uses a simple cloze task to support training at scale, and frames implicit argument prediction as the task of selecting the textual entity that completes the event in a maximally narratively coherent way.",
        "The current paper takes a first step towards predicting implicit arguments based on narrative coherence, using a relatively simple model for local narrative coherence.",
        "In the future, we plan to investigate how the extracted implicit arguments can be integrated into a downstream task that makes use of event information, such as reading comprehension."
    ],
    "2491": [
        "The proposed approach based on graphical decomposition and convolutional aggregation outperforms state-of-the-art schemes for long document matching.",
        "The use of a Concept Interaction Graph to organize documents into a graph of concepts is effective in improving the performance of article matching.",
        "The proposed divide-and-conquer approach based on graphical decomposition and convolutional aggregation is critical to the performance improvement in matching article pairs.",
        "The use of professional editors to create two new datasets for long document matching results in more accurate evaluations.",
        "The proposed GCN layers and structural transformation are important factors contributing to the performance improvement in matching article pairs."
    ],
    "2493": [
        "The current lack of proper categorization of MWE in LLOD data sets, such as BabelNet and DBpedia, hinders the linking process with other resources.",
        "The incomplete categorization of MWE in these data sets leads to links with a low level of precision.",
        "The lack of accurate categorization of MWE makes it difficult to link data sources such as LIDIOMS with other resources.",
        "Using declarative link discovery frameworks for computing similarities among MWE without the right classification information becomes a slow task.",
        "A better representation of MWEs, especially idioms, in LLOD will lead to qualitative linked-data driven NLP systems, including better Machine Translation applications."
    ],
    "2495": [
        "Our method obtains state-of-the-art results on two citation recommendation datasets, even without the use of metadata available to the baseline method.",
        "We show that our method obtains state-of-the-art results on two citation recommendation datasets, even without the use of metadata available to the baseline method.",
        "Our system is publicly accessible online.",
        "We introduce a new dataset of seven million scientific articles to facilitate future research on this problem.",
        "Our method remains effective when metadata is missing for query documents.",
        "Researchers can do an effective literature search early in their research cycle or during the peer review process, among other scenarios."
    ],
    "2498": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; more general globally-normalized models can be trained in a similarly inexpensive way.",
        "The model does not assume any universals except independently motivated limits on working memory, which may help address the question of whether universals are indeed necessary for grammar induction.",
        "The distinction this model draws between its learned unbounded grammar G and its derived bounded grammar GD seems to align with Chomsky's (1965) distinction between competence and performance."
    ],
    "2503": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; more general globally-normalized models can be trained in a similarly inexpensive way.",
        "Solving the brevity problem leads to significant BLEU gains, but there is still room for improvement by solving label bias in general.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model can effectively use additional data for a significant improvement in performance, inviting further exploration in semi-supervised/supervised domain adaptation scenarios."
    ],
    "2510": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Leveraging synergies between label spaces sometimes leads to big improvements.",
        "Our analysis showed that the learned label embeddings were indicative of gains from multitask learning.",
        "Auxiliary tasks were often beneficial across domains.",
        "Label embeddings almost always led to better performance.",
        "The dynamics of the label transfer network we use for exploiting the synergies between disparate label spaces were investigated."
    ],
    "2516": [
        "The biased representations in GloVe, ELMo, and BERT embeddings lead to gender, religion, and nationality biases.",
        "The projection-based method can effectively attenuate biases in contextualized embeddings without loss of entailment accuracy.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our bi-affine relation attention network performs well on three datasets, including two standard benchmark biological relation extraction datasets and a new, large, and high-quality dataset introduced in this work.",
        "Our model outperforms the previous state of the art on the Biocreative V CDR dataset despite using no additional linguistic resources or mention pair-specific features.",
        "Our current model predicts only into a fixed schema of relations given by the data, but can be ameliorated by integrating our model into open relation extraction architectures such as Universal Schema.",
        "Our model also lends itself to other pairwise scoring tasks such as hypernym prediction, co-reference resolution, and entity resolution."
    ],
    "2521": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our approach leads to significant improvements compared with strong baseline systems.",
        "In the future work, we plan to extend this method to jointly train multiple NMT systems for 3+ languages using massive monolingual data."
    ],
    "2523": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our proposed recurrent chunking mechanisms outperform benchmark models across different datasets.",
        "The concepts inferred by our data-driven approach are accurate to human judges and show good potential at identifying correct arguments for verbs even though such arguments have never been seen before.",
        "Our work can be seen as mining predicate relations between abstract noun concepts from a taxonomy.",
        "Future work may consider other important NLP tasks such as word sense disambiguation or term similarity using the argument concept representation of verbs."
    ],
    "2529": [
        "Our approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The proposed Cosine Annealing Strategy combines the Clustering Promotion Mechanism and Adversarial Distribution Alignment to improve domain adaptation.",
        "Removing pseudo labels from the parsing process deteriorates both tagging and parsing quality.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "We will explore other types of syntax embeddings and incorporate structural attention mechanism into our tree-based models for future work."
    ],
    "2530": [
        "We demonstrate that unsupervised multi-sense word embedding models can be improved by a simple linear transformation based on Ex-RPCA.",
        "Our method boosts the performance of the baseline by a large margin.",
        "The proposed Ex-RPCA for principal component analysis frames the pseudo multi-sense detection into a dimensionality reduction problem.",
        "The multi-sense word embeddings can be improved by a simple linear transformation based on Ex-RPCA.",
        "Future applications of the proposed method on linguistic analysis for multi-sense word embeddings are expected."
    ],
    "2533": [
        "Our proposed model based on the encoder-decoder framework outperforms the sequence-to-sequence baseline by a significant margin, as evidenced by the BLEU score of 6.3 and 5.5 on two English text simplification datasets, and the ROUGE-2 F1 score of 5.7 on a Chinese summarization dataset.",
        "Our model achieves state-of-the-art performances on all three benchmark datasets.",
        "The experimental results show that our approach significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data."
    ],
    "2534": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our method can yield good translating results."
    ],
    "2535": [
        "CliNER 2.0 achieves state-of-the-art performance in clinical named entity recognition.",
        "CliNER 2.0 has a lower number of false positives compared to other tools like cTakes.",
        "CliNER 2.0 is easy to install and use.",
        "CliNER 2.0 provides an optional flag to disable the deep network and use a simpler CRF model."
    ],
    "2537": [
        "Our EASDRL approach is the first to explore deep reinforcement learning to extract action sequences from texts.",
        "Our EASDRL model outperforms state-of-the-art baselines on three datasets.",
        "Our EASDRL approach could better handle complex action types and arguments.",
        "We showed that our EASDRL approach could better handle complex action types and arguments.",
        "We also exhibited the effectiveness of our EASDRL approach in an online learning environment.",
        "In the future, it would be interesting to explore the feasibility of learning more structured knowledge from texts such as state sequences or action models for supporting planning."
    ],
    "2544": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our approach significantly outperforms current state-of-the-art question generation techniques on both human evaluation and evaluation on common metrics such as BLEU, METEOR, and ROUGE-L."
    ],
    "2547": [
        "Bilingual training can be helpful for grounded language tasks, but the effect of dataset size is non-obvious and follows a U-shaped curve.",
        "The resulting model exhibits human-like sensitivity to contextual difficulty (pragmatics) and language-specific lexical learning in the form of vector relationships between lexical pairs and differences between the two languages in common color-term extensions (semantics).",
        "The introduction of English data can have detrimental effects for languages like Arabic and Spanish, or Waorani and Pirah\u00e3, due to their different placement of modifiers and smaller color term inventories.",
        "Our contribution includes a new dataset of human utterances in a color reference game in Mandarin Chinese, which we release to the public with our code and trained model parameters."
    ],
    "2548": [
        "'We have shown an effective method to encode real-world entities into a language model.'",
        "'Our approach does not require annotated data, and our experiments show an improvement on the heldout and test datasets.'",
        "'One interesting direction to pursue is to incorporate a small amount of transcribed data and use our approach on a combined set of transcribed dataset and artificial training data.'",
        "'There are certain challenges facing this approach when dealing with a mixture of real n-best lists and artificial n-best lists, such as deciding about the proportion of real n-best lists compared to the artificial ones.'",
        "'Future work should consider studying this problem.'"
    ],
    "2556": [
        "the neural lattice language model beats an LSTM-based baseline at the task of language modeling",
        "the latent segmentations generated by the model correspond well to human intuition about multi-word phrases",
        "the varying usage of words with multiple embeddings seems to also be sensible"
    ],
    "2559": [
        "Datasets have become highly influential in driving the direction of research.",
        "Recent datasets for QA have led to impressive advances, but have focused on factoid questions where surface-level cues alone are sufficient to find an answer, discouraging progress on questions requiring reasoning or other advanced methods.",
        "To help the field move towards more difficult tasks, we have presented the AI2 Reasoning Challenge (ARC), consisting of a new question set, text corpus, and baselines.",
        "We find that none of the baseline systems tested can significantly outperform a random baseline on the Challenge set, including two neural models with high performances on SNLI and SQuAD.",
        "Progress on ARC would thus be an impressive achievement, given its design, and be a significant step forward for the community."
    ],
    "2567": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "We believe this dataset will serve the QA and semantic parsing communities, drive research on compositionality, and push the community to work on holistic solutions for QA.",
        "In future work, we plan to train our model directly from weak supervision, i.e., denotations, and to extract information not only from the web, but also from structured information sources such as web tables and KBs."
    ],
    "2568": [
        "The beam problem in neural abstractive summarization can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our proposed method for causality extraction based on self-attention and transfer learning is effective, but still limited by the insufficiency of high-quality annotated data.",
        "The performance of SCITE can be improved by developing annotated datasets from multiple sources based on existing datasets and our causality tagging scheme, and combining our method with distant supervision and reinforcement learning.",
        "The current approach to neural abstractive summarization largely performs extractive summarization, and a future direction would be to investigate ways to enforce abstractiveness of a summary during training."
    ],
    "2574": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model of part-of-speech tagging and dependency parsing improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, leading to improved performance.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "Joining the training sets provided in the English News, the English WikiNews and the English Wikipedia data sets into a single and larger training set can provide better performance.",
        "Adding more features to the current feature set could improve the model's generalization capacity."
    ],
    "2575": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our proposed RPA approach tackles the common generalization issue of the model-free RL when applying to unseen scenes.",
        "Our method can simulate the environment and incorporate the imagined trajectories, making the model more scalable than the model-free agents.",
        "In the future, we plan to explore the potential of the model-based RL to transfer across different tasks, i.e. Vision-and-Language Navigation, Embodied Question Answering [10] etc."
    ],
    "2576": [
        "The performance of existing HRED models and attention variants can be significantly improved by using proper detection methods, such as self-attention.",
        "Our model outperforms existing HRED models and its attention variants in multi-turn dialogue generation.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Our method can be combined with distant supervision and reinforcement learning to achieve better performance without having to build a high-quality annotated corpus for causality extraction.",
        "The use of distributional information in representing noun-compound paraphrases can help mitigate the issue of lexical memorization and increase performance."
    ],
    "2579": [
        "The proposed model for context embeddings modeled as complex networks can induce word senses via community detection algorithms, and presents a significant performance in both single and multiple senses multiple scenarios, without the use of annotated corpora, in a completely unsupervised manner.",
        "A good performance can be obtained when considering only a small context window to generate the embeddings.",
        "A fully-connected and weighted network provides a better representation for the task.",
        "The absence of any annotation allows the use of the proposed method in a range of graph-based applications in scenarios where unsupervised methods are required to process natural languages.",
        "Integrating our methods with other natural language processing tasks can benefit from representing words as context embeddings.",
        "Using community detection algorithms that provide soft communities instead of hard communities can improve the quality of the context representation.",
        "Using neural language models to generate context embeddings can improve the quality of the context representation."
    ],
    "2582": [
        "SegMatch approach to inducing utterance embeddings shows very promising performance.",
        "Word segmentation is a highly non-trivial research problem in itself and the variability of spoken language is a serious and intractable confounding factor.",
        "Even when controlling for speaker identity, there are still superficial features of the speech signal that make it easy for the model to ignore the semantic content.",
        "Some of these may be due to artifacts in datasets and thus care is needed when evaluating unsupervised models of spoken language.",
        "In future work, we want to further explore the effectiveness of enforcing desired invariances via auxiliary classifiers with gradient reversal."
    ],
    "2586": [
        "'ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.'",
        "'Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.'",
        "'We explore the use of a projection-based method for attenuating biases.'",
        "'Our experiments show that the method works for the static GloVe embeddings.'",
        "'Providing the translation as an additional input signal is beneficial to the transcription task.'",
        "'Sharing the decoder and the attention parameters leads to lower character error rate over either a coupled ensemble architecture or simple attention mechanisms without parameter sharing.'",
        "'This work will provide a concrete basis for leveraging translations in a language documentation pipeline.'"
    ],
    "2587": [
        "Our method has the advantage of being simple yet powerful and allows to keep semantic information in binary vectors.",
        "Our binary embeddings exhibit almost the same performances as the original real-valued vectors on both semantic similarity and document classification tasks.",
        "Furthermore, since the binary representations are smaller (a 256-bit binary code is 37.5 smaller than a traditional 300-dimensional word vector), it allows one to run a top-K query 30 times faster than with real-valued vectors.",
        "Additionally, it is possible to recontruct real-valued vectors from the binary representations using the model decoder."
    ],
    "2590": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The word-level models can be trained much faster and give reasonable performance on smaller training sets.",
        "Models trained on only 20 hours of translated speech achieve precision and recall of around 30% for content words.",
        "Our extensive analyses in this work will contribute to better decision-making for architectural choices in computationand data-limited settings.",
        "Sub-word modelling could balance the trade-off between training costs and translation performance.",
        "Speech features targeted to low-resource multi-speaker settings and speaker normalization could improve translation performance."
    ],
    "2591": [
        "We propose a new perspective and solution to the task of parsing scene graphs from textual descriptions.",
        "We move the labels/types from the nodes to the edges and introduce an edge-centric view of scene graphs.",
        "We show that the gap between edge-centric scene graphs and dependency parses can be filled with a careful redesign of label and action space.",
        "We train a single, customized, end-to-end neural dependency parser for this task, as opposed to prior approaches that used generic dependency parsing followed by heuristics or simple classifier.",
        "Our trained parser is validated in terms of both SPICE similarity to the ground truth graphs and recall rate/median rank when performing image retrieval.",
        "We hope our paper can lead to more thoughts on the creative uses and extensions of existing NLP tools to tasks and datasets in other domains.",
        "In the future, we plan to tackle more computer vision tasks with this improved scene graph parsing technique in hand, such as image region grounding.",
        "We also plan to investigate parsing scene graph with cyclic structures, as well as whether/how the image information can help boost parsing quality."
    ],
    "2594": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We highlight interpretability as an advantage over conventional models.",
        "Future work includes investigation of self-attention with other sequences of low-information states such as characters, and of transferring results on controlling context range and interpretability to text modeling."
    ],
    "2600": [
        "Our approach produces natural language explicitly grounded in entities detected in images.",
        "We introduce a two-stage approach that first generates a hybrid template and then fills slots based on object categories recognized by detectors in image regions.",
        "We reorganize the train and val splits of the COCO dataset for robust image captioning.",
        "Experimental results validate the effectiveness of our proposed approach on standard, robust, and novel object image captioning tasks."
    ],
    "2601": [
        "We introduced a simple retrofitting-like extension to the original GloVe model and showed that the resulting representations were effective in a number of tasks and models.",
        "The resulting representations were effective in a number of tasks and models, provided a substantial (unsupervised) dataset in the same domain is available to tune the representations.",
        "The most natural next step would be to study similar extensions of other representation-learning models."
    ],
    "2604": [
        "The proposed MMDA framework exposes the end-to-end ASR system to a much wider range of training data, which is the first attempt at truly end-to-end multi-modal data augmentation for ASR.",
        "Experiments show promising results for the MMDA architecture, and possible extensions and future research in this area are highlighted.",
        "The MMDA framework exposes the end-to-end ASR system to a much wider range of training data than previous sequence-to-sequence approaches.",
        "The proposed MMDA architecture outperforms sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that the section titles generated by the approach lead to strong improvements across multiple reading comprehension tasks."
    ],
    "2606": [
        "The brevity problem in neural machine translation can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our proposed solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The combination of relevant document retrieval techniques with semantic similarity, sentiment analysis, and source reliability of articles is effective for credibility analysis in an open-domain setting.",
        "The most important module is semantic similarity, followed by sentiment analysis, and then author, website scores.",
        "The choice of the classifier plays a major role in the output.",
        "Support Vector Methods with RBF kernel and Neural Network architecture (Multilayer Perceptron) give the best results for the problem.",
        "Applying this system to domains like social media and enhancing CREDO with more handcrafted features like the writing style are potential future work."
    ],
    "2609": [
        "We conducted an extensive empirical study of semantic divergences in parallel corpora.",
        "Our crowdsourced annotations confirms that correctly aligned sentences are not necessarily meaning equivalent.",
        "We introduced an approach based on neural semantic similarity that detects such divergences much more accurately than shallower translation or alignment based models.",
        "Importantly, our model does not require manual annotation, and can be trained for any language pair and domain with a parallel corpus.",
        "Filtering out divergent examples helps speed up the convergence of neural machine translation training without loss in translation quality.",
        "These findings open several avenues for future work, such as how to improve divergence detection further, characterize the nature of the divergences beyond binary predictions, and study the impact of divergent examples on other applications."
    ],
    "2611": [
        "Transfer learning leads to performance improvements on many tasks.",
        "Using transfer learning is more critical when less training data is available.",
        "When task performance is close, the correct modeling choice should take into account engineering trade-offs regarding the memory and compute requirements.",
        "The transformer and DAN based universal encoding models provide strong transfer performance on a number of NLP tasks.",
        "Sentence level embeddings surpass the performance of transfer learning using word level embeddings alone.",
        "Models that make use of sentence and word level transfer achieve the best overall performance.",
        "Transfer learning is most helpful when limited training data is available for the transfer task.",
        "The encoding models make different trade-offs regarding accuracy and model complexity that should be considered when choosing the best model for a particular application."
    ],
    "2612": [
        "Our proposed model DCMTL achieves best F1 score comparing to several strong baselines on the real-world Chinese E-commerce dataset ECSA.",
        "Our model meets better understanding of users' utterances and improves customers' shopping experience.",
        "Our future research may include a joint model for category classification and slot filling.",
        "Active learning for slot filling can also be investigated by involving human-beings interaction with our system."
    ],
    "2613": [
        "This is the first piece of work that focuses on extractive summarization for E-commerce product titles.",
        "Our proposed model outperforms several popular summarization models, achieving an ROUGE-1 F1 score of 0.725.",
        "The result of online A/B testing shows substantial benefits of our model in real online shopping scenarios.",
        "Possible future works include handling similar terms that appear in the short titles generated by our model."
    ],
    "2617": [
        "The proposed method can transform audio from one style to another with high quality.",
        "The method is an end-to-end sequenceto-sequence model that combines convolutional and recurrent neural networks.",
        "The method can handle unseen words and musical notes by conditioning on the speaker or instrument.",
        "Subjective tests confirm the quality of the generated audio.",
        "This work alleviates the need for complex audio processing pipelines.",
        "The work sheds new insights on the capabilities of end-to-end audio transformations.",
        "The method has the potential to be built upon and extended by others."
    ],
    "2619": [
        "The Transformer model (Vaswani et al., 2017) for English-to-Czech neural machine translation achieves better translation quality with larger batch sizes and more GPUs.",
        "The best performing model obtained on 8 GPUs trained for 8 days outperforms the WMT17 winner in a number of automatic metrics.",
        "Limiting exploration to the more or less basic parameter settings, the report can be useful for other researchers.",
        "The Transformer model and its implementation in Tensor2Tensor are best fit for \"intense training\": using as many GPUs as possible and running experiments one after another should be preferred over running several single-GPU experiments concurrently.",
        "With at least a day and a 11GB GPU for training, the larger setup (BIG) should be always preferred."
    ],
    "2625": [
        "The proposed NIHRIO system achieved a high ranking in the Semeval-2018 Task 3 on \"Irony detection in English tweets\".",
        "The system used Multilayer Perceptron to handle the task, utilizing various features such as lexical, syntactic, semantic, and polarity features.",
        "The system achieved a high F1 score in both subtasks of binary and multi-class irony detection in tweets.",
        "The proposed approach using Multilayer Perceptron and various features outperformed other systems, indicating the effectiveness of the proposed method."
    ],
    "2626": [
        "'Extensive experiments on six text classification benchmarks show the effectiveness of capsule networks in text classification.'",
        "'Capsule networks also show significant improvement when transferring single-label to multi-label text classifications over strong baseline methods.'",
        "'Three strategies were proposed to boost the performance of the dynamic routing process to alleviate the disturbance of noisy capsules.'",
        "'The use of dynamic routing in capsule networks can improve the performance of text classification.'"
    ],
    "2628": [
        "The performance of the model can be improved significantly by pre-training with automatically constructed cloze questions.",
        "Pretraining helps with questions that ask for factual information located in a specific part of the context.",
        "Fine-tuning is necessary to achieve good performance, even when the model is trained only on cloze questions.",
        "Exploring active learning setups and adapting cloze style pretraining to NLP tasks other than QA is a promising area for future work."
    ],
    "2631": [
        "\"We have incorporated the well-known word2vec model into the ODP-based classification framework.",
        "\"Our approach involves two tasks: generating category vectors that represent the semantics of ODP categories, and developing a new semantic similarity measure that utilizes both category and word vectors.",
        "\"We have verified the large-scale classification performance of the proposed methodology using real-world datasets, and the performance evaluation results confirm that our scheme significantly outperforms baseline methods.",
        "\"Our proposed methodology can be applied to different applications, including contextual and mobile advertising.",
        "\"The performance evaluation results confirm that our scheme significantly outperforms baseline methods."
    ],
    "2634": [
        "The lack of resources in French leads to generation of low quality titles, but this can be drastically improved upon with transfer learning between French and English and/or German.",
        "In case of low-resource languages, copying monolingual data (even if out-of-domain) improves the performance of the system.",
        "Normalization with placeholders usually helps for languages with relatively easy morphology.",
        "It is important to over-sample the low-resourced languages in order to balance the high-& low-resourced data, thereby, creating a stable NLG system.",
        "For French, a low-resource language in our use case, the hybrid system which combines manual rules and SMT technology is still far better than the best neural system.",
        "The multi-lingual model has the best trade-off, as it achieves the best results among the neural systems in all three languages and it is one single model which can be deployed easily on a single GPU machine."
    ],
    "2636": [
        "The cui2vec embeddings achieve state-of-the-art performance in many instances, outperforming previous results.",
        "The ability to extract meaningful structure in an unsupervised manner is extremely important for healthcare data.",
        "Most sources of healthcare data are not easily shareable, limiting researchers to small sources of local data.",
        "The provided cui2vec embeddings were created using large and national sources of healthcare data, addressing the barrier of limited access to data.",
        "The embeddings will be generally useful for a variety of clinically oriented machine learning tasks."
    ],
    "2637": [
        "We have presented a novel semantic-visual embedding pipeline that leverages recent architectures to produce rich, comparable descriptors for both images and texts.",
        "The use of a selective spatial pooling at the very end of the fully convolutional visual pipeline allows us to equip our system with a powerful mechanism to locate in images the regions corresponding to any text.",
        "Extensive experiments show that our model achieves high performance on cross-modal retrieval tasks as well as on phrases localization.",
        "We also showed first qualitative results of zero-shot learning, a direction towards which our system could be pushed in the future with, among others, a deeper exploitation of language structure and of its visual grounding."
    ],
    "2640": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "The solution to the brevity problem is a very limited form of globally-normalized models for NMT.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The ConMask model can capture the correct relationship between entities even when the indicator words are not highly correlated with the relationship name from the model's perspective."
    ],
    "2645": [
        "We have proposed two neural models of event factuality prediction -a bidirectional linear-chain LSTM (L-biLSTM) and a bidirectional childsum dependency tree LSTM (T-biLSTM) -which yield substantial performance gains over previous models based on deterministic rules and handengineered features.",
        "We found that both models yield such gains, though the L-biLSTM generally outperforms the T-biLSTM; for some datasets, a simple ensemble of the two (H-biLSTM) improves over either alone.",
        "We have also extended the UDS-IH1 dataset, yielding the largest publicly-available factuality dataset to date: UDS-IH2.",
        "In experiments, we see substantial gains from multi-task training over the three factuality datasets unified by Stanovsky et al. (2017) , as well as UDS-IH2.",
        "Future work will further probe the behavior of these models, or extend them to learn other aspects of event semantics."
    ],
    "2646": [
        "The need to report accuracy on unseen tokens and to compare performance to a na\u00efve memorization baseline when evaluating historical text normalization systems.",
        "Both models greatly outperformed the baseline on unseen tokens, with the soft attention model performing slightly better for smaller data sizes, and the hard attention model doing a bit better for larger ones.",
        "The improvements in performance did not translate into clearly better POS tagging downstream.",
        "The evaluation guidelines presented in the paper can help promote work in this area to provide better tools for working with historical text collections."
    ],
    "2647": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "Our method improved NMT translation results up to 6 BLEU points on three narrow domain translation tasks, while causing little increase in the translation time.",
        "Our approach outperforms sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks."
    ],
    "2648": [
        "No parallel data are needed\" - This claim suggests that the proposed approach for voice conversion does not require parallel data, which can be a time-consuming and costly requirement for many existing methods.",
        "Conversion to multiple target speakers can be achieved by a single model\" - This claim highlights the flexibility of the proposed approach, which can convert speech into multiple target speaker voices using a single model, rather than requiring separate models for each target speaker.",
        "Adding a residual signal can improve significantly the quality of converted speech\" - This claim suggests that the proposed approach can improve the quality of converted speech by adding a residual signal, which can be an effective way to enhance the quality of the converted speech.",
        "Sharp voice spectra can be produced with this approach\" - This claim highlights the effectiveness of the proposed approach in producing sharp voice spectra, which is an important aspect of high-quality speech conversion.",
        "Subjective human evaluation verifies the quality of the converted speech\" - This claim suggests that the proposed approach has been evaluated subjectively by humans and has been found to produce high-quality converted speech."
    ],
    "2649": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "Our QA model solves the most difficult step 3 by transforming it to a QA problem and achieving great improvement over baseline systems.",
        "Our QA4IE benchmark is the largest document level IE benchmark, consisting of 293K documents and 2 million golden relation triples with 636 different relation types.",
        "Existing best IE baseline systems achieve a great improvement over our system.",
        "Future work includes solving the triples with multiple entities as the second entity, processing longer documents, and improving the quality of our benchmark."
    ],
    "2650": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "our approach achieves new state-of-the-art on FewRel 2.0 dataset",
        "degeneration problem in existing VAE models such as the VHRED is persistent",
        "hierarchical latent variable modeling improves the performance of conversation modeling",
        "human evaluation via AMT shows that our approach outperforms previous work on joint POS tagging and dependency parsing"
    ],
    "2657": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptive search method can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The proposed SHAPED and Mix-SHAPED model architectures significantly outperform models that are either trained in a manner that ignores style characteristics or single-style models.",
        "Even for single-style models trained on over 1M examples, their performance is inferior to the performance of SHAPED models on that particular style.",
        "The proposed architectures are both efficient and effective in modeling both generic language phenomena and particular style characteristics, and are capable of producing higher-quality abstractive outputs that take into account style characteristics."
    ],
    "2659": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing",
        "the discovered rules are used to extract Semitic roots, which are the basic units of these languages",
        "EventKG V1.1 includes over 690 thousand event resources and over 2.3 million temporal relations",
        "unique features of EventKG include light-weight integration and fusion of structured and semi-structured multilingual event representations and temporal relations in a single knowledge graph",
        "the light-weight integration enables to significantly increase the coverage and completeness of the included event representations, in particular with respect to times and locations"
    ],
    "2661": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model",
        "solving the brevity problem leads to significant BLEU gains",
        "our solution to the brevity problem requires globally-normalized training on only a small dataset",
        "incorporating dictionaries into neural network could significantly enhance the performance of deep neural network for the Chinese CNER",
        "the proposed approaches could process rare and unseen entities better than previous methods"
    ],
    "2662": [
        "The proposed NMT engine achieves faster speeds without sacrificing accuracy.",
        "The faster NMT engine enables the use of small ensembles of models.",
        "The speedup allows for higher quality translation while maintaining adequate fluency.",
        "The 8-bit decoder does not result in significant degradation compared to the 32-bit decoder.",
        "The faster NMT engine enables the use of smaller models for translation."
    ],
    "2663": [
        "typographic markers such as emojis and emoticons are most frequent for Twitter",
        "tag questions, exclamation, metaphors are frequent for Reddit",
        "analysis across different topical subreddits",
        "planning to experiment with other markers (e.g., ironic echo, repetition, understatements)"
    ],
    "2664": [
        "ClassiNets significantly improves the classification accuracy of a sentence-level sentiment classification task, outperforming previously proposed methods such as structural correspondence learning (SCL), frequent term sets (FTS), Skip-thought vectors, FastSent, and Paragraph2Vec on multiple datasets.",
        "ClassiNets uses an efficient method for feature expansion using locality sensitive hashing to approximately compute the neighborhood of a vertex, avoiding all-pair computation of confusion matrices.",
        "ClassiNets can be applied to other tasks that require missing feature prediction, such as recommendation systems.",
        "The global feature expansion method significantly improves the classification accuracy of a sentence-level sentiment classification task, outperforming previously proposed methods such as structural correspondence learning (SCL), and frequent term sets (FTS), Skip-thought vectors, FastSent, and Paragraph2Vec on multiple datasets."
    ],
    "2665": [
        "We have presented an argument for averaging as a valid meta-embedding technique.",
        "Experimental performance is close to, or in some cases better than that of concatenation, with the additional benefit of reduced dimensionality.",
        "When conducting meta-embedding, both concatenation and averaging should be considered as methods of combining embedding spaces, and their individual advantages considered."
    ],
    "2667": [
        "GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing."
    ],
    "2668": [
        "Our proposed coarse-to-fine approach alleviates the additional computational cost of higher-order inference, while maintaining the end-to-end learnability of the entire model.",
        "We modeled higher order interactions between spans in predicted clusters.",
        "Our state-of-the-art coreference resolution system models higher order interactions between spans in predicted clusters.",
        "Our approach maintains the end-to-end learnability of the entire model.",
        "Our coarse-to-fine approach alleviates the additional computational cost of higher-order inference."
    ],
    "2669": [
        "There is still room for improvement in automated cognate detection methods for phylogenetic inference.",
        "Expert-annotated cognate sets are also not free from errors.",
        "It seems useful to investigate how the consistency of cognate coding by experts could be further improved.",
        "Our future work intends to create a cognate identification system that combines the output of different algorithms in a more systematic way.",
        "We intend to infer cognate sets from the combined system and use them to infer phylogenies and evaluate the inferred phylogenies against the gold standard trees."
    ],
    "2671": [
        "We have shown how this can be done with two key contributions: (1) a VerbNet-derived rulebase (the Semantic Lexicon), describing how events affect the world and (2) an integration of state-based reasoning with language processing, allowing PROCOMP to infer the states that arise at each step.",
        "We have shown how this outperforms two state-of-the-art systems that rely on surface cues alone.",
        "The Semantic Lexicon is available from the authors on request.",
        "Since this work was performed, we have subsequently developed two neural systems that outperform PROCOMP, described elsewhere (Dalvi et al., 2018).",
        "An integration of PRO-COMP's Semantic Lexicon with a neural system remains a currently unexplored opportunity for further improvements in machine reading about processes."
    ],
    "2672": [
        "Our generic architecture for video captioning learns the aligned cross-modal attention globally and locally, which can further boost the performance when plugged into existing reinforcement learning methods.",
        "Incorporating features from other modalities, such as optical flow and C3D features, into our HACA framework can further improve the performance of video captioning.",
        "The proposed generic architecture for video captioning can be plugged into existing reinforcement learning methods to further boost the performance.",
        "Our HACA framework learns the aligned cross-modal attention globally and locally, which enables the integration of features from other modalities to improve the performance of video captioning."
    ],
    "2673": [
        "The task of community member retrieval based on their tweets can be defined using a person re-identification task with a small number of examples.",
        "The method introduced in this paper gives very good results compared to word2vec and LDA baselines.",
        "The user embeddings learned efficiently represent user interests.",
        "The text embeddings are largely complementary to the social network features used in other studies, so performance gains can be expected from feature combination.",
        "The reidentification training objective proposed in this paper can easily be used with other methods for deriving document embeddings, such as (Le and Mikolov, 2014; Kim, 2014)."
    ],
    "2674": [
        "Learning a self-training strategy automatically can release the burden of human efforts in strategy design and is more flexible in choosing the most useful data.",
        "Our approach outperforms the baseline solutions in terms of a better tagging performance and stability.",
        "The current solution can be improved in several directions, such as incorporating global tagging information of all the unlabeled instances into the input to the deep neural network.",
        "Using neural network structures other than CNN to represent test sentences may also improve the current solution."
    ],
    "2676": [
        "The proposed paragraph-level neural network model outperforms previous models for implicit discourse relation recognition on the PDTB dataset.",
        "The model takes a sequence of discourse units as input and captures the overall discourse structure to predict a sequence of discourse relations in a paragraph.",
        "The model uses inter-dependencies between discourse units and discourse relation continuity to recognize implicit discourse relations.",
        "The model achieves competitive performance compared to previous systems trained using the full dataset by using less than 1% of the training data.",
        "The model significantly outperforms previous methods, reducing the error by 21% on English Switchboard."
    ],
    "2677": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The layer provides a useful inductive bias for solving problems of reading comprehension that require aggregating information from multiple mentions of the same entity.",
        "Noise in the coreference annotations has a detrimental effect on the performance.",
        "Joint models that learn to do coreference resolution and reading together may be explored in future work."
    ],
    "2680": [
        "There are no improvements when learning from user star ratings, unless the noisy ratings themselves are stripped off in a bandit-to-supervised conversion.",
        "Implicit task-based feedback can be used successfully as a reward signal for NMT optimization, leading to improvements both in terms of enforcing individual word translations and in terms of automatic evaluation measures.",
        "In the future, we plan transfer these findings to production settings by performing regular NMT model updates with batches of collected user behavior data, especially focusing on improving translation of ambiguous and rare terms based on rewards from implicit partial feedback."
    ],
    "2681": [
        "We presented a novel multitask approach to learning semantic parsers from disjoint corpora with structurally divergent formalisms.",
        "Our experiments show that joint learning and prediction can be done with scoring functions that explicitly relate spans and dependencies, even when they are never observed together in the data.",
        "We handled the resulting inference challenges with a novel adaptation of graphical model structure learning to the deep learning setting.",
        "Our selection of factors is specific to spans and dependencies, but our general techniques could be adapted to work with more combinations of structured prediction tasks."
    ],
    "2683": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "PReFIL outperformed previous state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL's performance on DVQA is affected by OCR model variations, while it exceeds human baseline for FigureQA.",
        "All OCR versions performed better than humans for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The proposed model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets."
    ],
    "2684": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our approach outperforms benchmark models across different datasets.",
        "We have performed extensive experiments on three public datasets of machine reading comprehension: CoQA, QuAC, and TriviaQA.",
        "The texts generated by ED RL are shorter compared to the other two neural systems, which might affect BLEU-4 scores and also the ratings provided by the annotators.",
        "ED RL drops information pertaining to dates or chooses to just verbalise birth place information.",
        "Overall, ED MTL seems to be more detail-oriented and faithful to the facts included in the infobox.",
        "The template system manages in some specific configurations to verbalize appropriate facts, however, it often fails to verbalize infrequent properties or focuses on properties which are very frequent in the knowledge base but are rarely found in the abstracts."
    ],
    "2687": [
        "The proposed model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptive approach can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The proposed model outperforms sequence-to-sequence approaches in low-resource domains.",
        "The human evaluation showed strong improvements across multiple reading comprehension tasks.",
        "The novel reward functions improve the saliency and directed logical entailment aspects of a good summary.",
        "The multi-reward approach of optimizing multiple rewards simultaneously in alternate minibatches is effective."
    ],
    "2688": [
        "We demonstrate that we can overcome model overstability and increase their robustness by training on diverse adversarial data that eliminates latent data correlations.",
        "We further show that adversarial training is more effective when we jointly add useful semantic-relations knowledge to improve model capabilities.",
        "We hope that these robustness methods are generalizable to other insertion-based adversaries for Q&A tasks."
    ],
    "2690": [
        "DURel shows empirical validity in our annotation study with high inter-annotator agreement.",
        "It relies on an intuitive notion of semantic relatedness and needs no definition of word senses.",
        "We proposed two measures of lexical semantic change that predict various semantic change constellations.",
        "One measure successfully distinguishes between innovative and reductive meaning change.",
        "The annotated test set for German is publicly available and can be used to compare computational models of semantic change, and more generally to evaluate models of lexical variation in corpora across times, domains, etc.",
        "Further test sets across languages can be obtained by applying DURel to the respective language uses."
    ],
    "2692": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Cross-lingual and multilingual classifiers yield comparable performance to individual language models.",
        "Future plans include a systematic exploration of feature representations which are meaningful for the AES context while being portable across languages.",
        "Modeling proficiency classification as a domain adaptation problem (where the domain is another language), and doing multi-task learning by considering other annotation dimensions are interesting directions to pursue in future.",
        "Considering that we have publicly available CEFR graded corpora for other languages such as Estonian, it would be interesting to extend this approach to new languages.",
        "Investigating questions such as the relationship between genetic/typological similarities between languages and cross/multi-lingual CEFR classification task in future.",
        "Researchers express concerns about the validity of the chosen feature constructs, and bias and fairness in models."
    ],
    "2693": [
        "'The most successful conversations (in both rating and duration) are about the topics which are handled by Structured Topic Dialogues.'",
        "'These dialogues incorporate interesting facts about the corresponding entities and additionally, they suggest a related entity or recommend to switch a topic.'",
        "'The rule-based topic-level DM is a good solution how to implement a new topic without a corresponding dataset.'",
        "'Additionally, we use the DMs to collect data for each topic which can be used as a training set for a neural network DM.'",
        "'The advantage of the neural network-based DM that it can be partially transferred from one topic to another.'",
        "'Rule-based systems, on the other hand, need to be rewritten almost from scratch.'"
    ],
    "2695": [
        "Unspeech context embeddings contain and embed speaker characteristics, but supervised speaker embeddings like i-vectors would be better suited for tasks like speaker recognition or authentication.",
        "Clustering utterances according to Unspeech contexts and using the cluster IDs for speaker adaptation in HMM-GMM/TDNN-HMM models is a viable alternative if no speaker information is available.",
        "Using Unspeech context embeddings as additional input features did not yield significant WER improvements compared to an i-vector baseline on TED-LIUM dev and test, but we observed consistent WER reductions with out-of-domain data from the Common Voice corpus.",
        "Modifying the training objective to see if phonetic Unspeech embeddings can be trained using a similar unsupervised training procedure is a potential future direction.",
        "The current implementation of TEDx Unspeech models scored higher EERs, but were at the same time better context vectors in the acoustic models."
    ],
    "2697": [
        "Our model outperforms other KGC models on metrics such as Mean Rank and MRR. (based on the results of the evaluation)",
        "ConMask is able to capture the correct relationship even when the name of the entity does not appear in the description. (based on the example of David Duncan being ranked as the 2nd candidate)",
        "Our model is relatively insensitive to interaction length and is able to recover both explicit and implicit references to previously-mentioned entities and constraints. (based on the analysis of user focus change)",
        "There is room for improvement in the model, as it still has some limitations and can be affected by entities with similar names to the given relationships. (based on the example of entities with similar names being ranked highly)",
        "Our segment-copying models suffer from error propagation when extracting segments from previously-generated queries. (based on the observation that the model can be affected by erroneous segments)"
    ],
    "2698": [
        "Bias in NLP systems can mimic and amplify stereotypes in society.",
        "The dataset used for coreference detection includes gender-neutral pronouns and examples with one job title instead of two.",
        "Three systems are significantly gender-biased.",
        "Systems can ignore their bias given sufficient cues.",
        "Methods can be used to make coreference models more robust to spurious, gender-biased cues without incurring significant penalties on performance."
    ],
    "2699": [
        "Our experiments suggest that both methods work equally well in a realistic end-to-end setting.",
        "While in general, recall is still low, the oracle experiments suggest that both methods can recover elided predicates from correct dependency trees.",
        "As parsers become more and more accurate, the gap recovery accuracy should also increase.",
        "Our method can be used to automatically add the enhanced UD representation to UD treebanks in other languages than English.",
        "Apart from being useful in a parsing pipeline, we therefore also expect our method to be useful for building enhanced UD treebanks."
    ],
    "2702": [
        "The proposed Reinforced Neural Extractive Summarization (RNES) model can effectively balance cross-sentence coherence and sentence importance, achieving state-of-the-art performance on a benchmark dataset.",
        "The RNES model outperforms other neural coherence models.",
        "The proposed method can extract a coherent and informative summary from a single document.",
        "The model's performance can be improved by improving the neural coherence model and introducing human knowledge into the RNES."
    ],
    "2711": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Our proposed method can effectively preserve salient source relations in summaries.",
        "Structural models are on-par with or surpass state-of-theart published systems.",
        "A machine learning model needs to focus on the relevant paragraphs of the evidence to perform stance detection.",
        "Memory networks are well-suited for the task of stance detection, as they can model complex dependencies such as semantic relationships with respect to entire previous paragraphs.",
        "Our proposed extension of memory networks offers sizable performance gains, making them competitive.",
        "Our model can extract meaningful snippets from documents that can explain the factuality of a given claim.",
        "In future work, we plan to extend the inference component to select an optimal set of explanations for each prediction, and to explain the model as a whole."
    ],
    "2712": [
        "The proposed method of combining pre-trained neural and n-gram language models is effective.",
        "The gating network used in the proposed method is small, fast to train, and run.",
        "The gating network takes as input handcrafted features found by analyzing where n-grams outperform neural models.",
        "The proposed method can achieve better results than ensembling.",
        "The use of a per-time step predictor of the optimal weight between the two models is effective."
    ],
    "2715": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The basic conversation model learned from Reddit conversations is competitive with existing sentence-level encoders on public STS tasks.",
        "A multitask model trained on Reddit and SNLI classification achieves the state-of-the-art for sentence encoding based models on the STS Benchmark task.",
        "Even without any task-specific training, the Reddit and Reddit+SNLI models are already competitive on CQA subtask B."
    ],
    "2718": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our final model outperforms an existing state-of-the-art model on a large scale WIKIBIO dataset by 21%.",
        "We introduce datasets for French and German and demonstrate that our model gives state-of-the-art results on these datasets.",
        "Fine-tuning with small amounts of in-domain data can improve the performance of an out-of-domain model on the target domain.",
        "Jointly learning to generate natural language descriptions from structured data in multiple languages is a promising future work.",
        "Replacing concepts in the input infobox with Wikidata concept IDs, which are language-agnostic, could facilitate joint learning and share a large amount of input vocabulary across languages."
    ],
    "2720": [
        "The proposed framework, LD-Net, achieves efficient contextualized representation for a specific task without requiring gradient oracles of large language models (LMs) nor costly retraining.",
        "LD-Net is able to conduct layer-wise pruning for a specific task, which can improve the efficiency of the model.",
        "The proposed method does not require the gradient oracle of LMs, which can save time and resources.",
        "The method can be applied to other applications in the future."
    ],
    "2721": [
        "The proposed scheme for TempRel annotation between events simplifies the task by focusing on a single time axis at a time.",
        "End-points of events are a major source of confusion during annotation, and focusing on start-points only can help handle this issue.",
        "The pilot study shows significant IAA improvements compared to literature values, indicating a better task definition under the proposed scheme.",
        "The usage of crowdsourcing to collect a new dataset, MATRES, at a lower time cost is enabled by the proposed scheme.",
        "MATRES, albeit crowdsourced, has achieved a reasonably good agreement level with the gold set and existing datasets.",
        "Existing schemes suffer from low IAAs and lack of data, making the findings in this work a good start towards understanding more sophisticated semantic phenomena in this area."
    ],
    "2724": [
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection."
    ],
    "2728": [
        "The performance benefits from dynamic reward values\" - This claim suggests that the use of dynamic reward values in imitation learning improves the performance of the event extraction framework.",
        "the proposed framework reduces the requirement of linguistic feature engineering\" - This claim indicates that the proposed framework for imitation learning reduces the need for manual feature engineering, which can be time-consuming and difficult to obtain.",
        "Our current framework is built upon model-free RL approaches\" - This claim states that the current implementation of the event extraction framework uses model-free reinforcement learning techniques.",
        "Our future work will focus on exploring model-based RL frameworks\" - This claim suggests that the authors plan to explore the use of model-based reinforcement learning frameworks in the future, which could potentially improve the performance of the event extraction framework."
    ],
    "2729": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We contrast the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Our state-of-the-art results on four benchmarks (SNLI, MultiNLI, SciTail, Quora Question Pairs) show the effectiveness of this multi-step inference architecture.",
        "In future, we would like to incorporate the pertrained contextual embedding, e.g., ELMo (Peters et al., 2018) and GPT (Radford et al., 2018) into our model and multi-task learning (Liu et al., 2019)."
    ],
    "2731": [
        "Our unsupervised approach can generate correct English sentences from structured data, with better performance compared to two fully supervised systems.",
        "Our approach does not require any labeled data, and incorporating out-of-domain data into the training phase further improves the quality.",
        "Our unsupervised setup outperforms both supervised baselines in terms of fluency and grammatical correctness, as measured by a human evaluation.",
        "Our approach can generate output in any language from the same structured data, making it suitable for NLG problems where the goal is to include all the information from the structured data in the output.",
        "In future work, we will focus on the semi-supervised approach to make the DAE suitable for problems where only a subset of the structured information should be included in the output."
    ],
    "2732": [
        "'Experiments on two categories of MTL frameworks demonstrate that multi-task learning outperforms single-task learning both on the source tasks and most of the transfer tasks.'",
        "'In addition, we analyze what linguistic information is captured by the sentence representations.'",
        "'We will explore advanced techniques to model domains...'",
        "'We will study whether multi-task learning can be benefited from proper modeling of domains.'"
    ],
    "2733": [
        "The proposed method can significantly improve the performance of an arbitrary neural sequence decoder on any reasonable translation metric in either greedy or beam-search decoding, with only a few trained parameters and minimal additional training time.",
        "The training strategy based on an automatically-generated pseudo-parallel corpus makes it possible to improve the performance of the decoder on any reasonable translation metric.",
        "The model is agnostic to both the model architecture and the target metric, allowing for exploration of more diverse and ambitious model-target metric pairs in future work.",
        "The method requires only a few trained parameters and minimal additional training time, making it a promising approach for improving the performance of neural sequence decoders.",
        "The proposed method has the potential to improve the performance of neural sequence decoders on a wide range of translation metrics, including greedy and beam-search decoding."
    ],
    "2734": [
        "'ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.'",
        "'We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.'",
        "'Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.'",
        "'Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.'",
        "'We would like to automatically learn a delexicalizer from data.'",
        "'We tackle zero-shot parsing when the structure distribution in the target domain is very different from the source domains.'",
        "'We apply our framework to datasets where only denotations are provided.'"
    ],
    "2735": [
        "The authors have introduced a large-scale dataset of human-generated QA pairs, called DuoRC, which includes 186K pairs of questions and answers.",
        "The dataset is designed to ensure little to no lexical overlap between questions and answers in different versions.",
        "The authors hope to introduce new research challenges on QA requiring external knowledge and common-sense driven reasoning, deeper language understanding, and multiple-sentence inferencing.",
        "The state-of-the-art RC models perform poorly on the DuoRC dataset, highlighting the need for further research."
    ],
    "2736": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed generative model can produce reasonable answers with separate components to explicitly process numbers.",
        "A hybrid word-character model can greatly alleviate the generic answer problem.",
        "Our study shows that a hybrid retrieval-generation model can significantly improve the diversity of informativeness of the generated answers."
    ],
    "2737": [
        "Our approach based on combining string kernels and word embeddings for automatic essay scoring attains the best performance on the task, both in in-domain and cross-domain settings.",
        "Using a shallow approach, we report better results compared to recent deep learning approaches.",
        "String kernels, both alone and in combination with word embeddings, attain the best performance on the automatic essay scoring task.",
        "In-domain and cross-domain comparative studies indicate that string kernels and word embeddings are effective for automatic essay scoring.",
        "Our approach outperforms recent deep learning approaches for automatic essay scoring."
    ],
    "2738": [
        "Our system obtains the highest accuracy on CTB, regardless of the oracle used for training.",
        "Except the in-order parser by Liu and Zhang [37] on the WSJ, it outperforms all other greedy shift-reduce parsers in terms of accuracy with just static training, and matches the second best result on the WSJ when we use a dynamic oracle for training.",
        "Unlike traditional bottom-up systems, this novel algorithm can be applied to any language without the need of further additional resources.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "The parser's source code will be freely available after acceptance.",
        "Our algorithm is the fastest transition system developed so far for constituent parsing, as it consumes the shortest sequence of transitions to produce phrase-structure trees.",
        "In practice, it outspeeds other approaches in a comparison under homogeneous conditions and it will certainly alleviate the bottleneck caused by parsers in NLP applications that rely on syntactic representations.",
        "The novel non-binary algorithm excels in building trees with a large number of children."
    ],
    "2740": [
        "Our approach achieves new state-of-the-art results on three benchmarks.",
        "We propose a neural architecture that learns a distributional semantic representation, leveraging both document and sentence level information.",
        "We find that context increased with document-level information improves performance.",
        "We utilize adaptive classification thresholds to further boost the performance."
    ],
    "2746": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our approach is efficient and scalable for largescale domain classification.",
        "We have shown the effectiveness of our approach with 1,500 domains in a real IPDA system.",
        "Our novel reranking models are effective in terms of pointwise, pair-wise, and list-wise ranking approaches.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification achieves new state-of-the-art results.",
        "The model can be applied to other learning problems."
    ],
    "2749": [
        "The system presented in the paper surpasses human accuracy on two datasets (DVQA and FigureQA).",
        "PReFIL uses an unsupervised method for discovering root-and-pattern morphology in Semitic languages, which allows for the extraction of Semitic roots.",
        "The discovered rules are used to validate the pattern discovery method and root extraction method (JZR).",
        "The results show that better OCR methods lead to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results suggest that the community is ready for more difficult CQA datasets.",
        "The system has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrval, table reconstruction, and enabling better understanding of charts by people with visual impairments.",
        "The use of transfer learning and more training data could be promising directions for future research in representation learning-based sarcasm detection."
    ],
    "2751": [
        "PReFIL surpasses prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL has better performance on FigureQA than on DVQA.",
        "PReFIL using oracle OCR exceeds human performance on structure questions.",
        "Better OCR methods lead to better results for DVQA.",
        "Future developments in OCR technology will likely improve PReFIL."
    ],
    "2752": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "Adding noise on the word embedding layer can in general improve the model's performance.",
        "Certain types of noise could constantly perform better from the empirical results.",
        "Employing task-related constraints such as word sentiment polarity or the distances between current word and two entities is promising but more challenging to accomplish.",
        "Leveraging knowledge bases and language model features for multinomial token replacement is a promising future work."
    ],
    "2753": [
        "LISA outperforms the state-of-the-art on two benchmark SRL datasets, including out-of-domain.",
        "Viterbi decoding has nearly the same impact for LISA, D&M, and gold syntax trees.",
        "Gold parses provide little improvement over predicted parses in terms of BIO label consistency.",
        "Providing LISA with gold parses is particularly helpful for sentences longer than 10 tokens.",
        "The tendency of syntactic parsers to perform worse on longer sentences."
    ],
    "2755": [
        "We have developed a method for recasting a wide range of semantic phenomena from many NLP datasets into labeled NLI sentence pairs, providing a diverse NLI framework for diagnosing NLU models' ability to capture and perform distinct types of reasoning.",
        "Our experiments demonstrate how to use this framework as an NLU benchmark, and the DNC is actively growing as we continue recasting more datasets into labeled NLI examples.",
        "We encourage dataset creators to recast their datasets in NLI and invite them to add their recast datasets into the DNC, which is available online at http://www.decomp.net.",
        "The collection of labeled NLI examples, along with baselines and trained models, is available online and can be used to evaluate the performance of NLU models in capturing and performing distinct types of reasoning."
    ],
    "2756": [
        "We study the problem of parsing tweets into Universal Dependencies.",
        "Our annotators disagree on the annotation, and inherent ambiguity in this genre makes consistent annotation a challenge.",
        "Using this new treebank, we build a pipeline system to parse tweets into UD.",
        "We propose a new method to distill an ensemble of 20 greedy parsers into a single one to overcome annotation noise without sacrificing efficiency.",
        "Our parser achieves an improvement of 2.2 in LAS over a strong baseline and outperforms other state-of-the-art parsers in both accuracy and speed."
    ],
    "2757": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "We avoided using traditional NLP features like linguistic features and emotion/sentiment lexicons by substituting them with continuous vector representations learned from huge corpora.",
        "Emoji sentence representations and emotional word vectors trained from neural networks can be used with tweet-specific features as input for other traditional regression models, such as SVR and Kernel Regression, to solve the task of regression and ordinal classification.",
        "We proved the effectiveness of finding the mapping of the relationship between regression and ordinal labels from the training set to perform ordinal classification.",
        "Our system ranked among the top three in every subtask of the competition we participated.",
        "For future work, we want to work further on employing these emotion representations on other tasks, such as text generation, while we gather more data and improve the model to train the representations."
    ],
    "2768": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but the results for DVQA were more nuanced due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results suggest that the community is ready for more difficult CQA datasets.",
        "Charts in the wild may contain variations that are not captured by current datasets, and human-generated questions should be included in future datasets.",
        "Document-level CQA requires document question answering, page segmentation, and more.",
        "The potential applications of improving chart understanding include automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments."
    ],
    "2769": [
        "Improving data quality is an important step in improving emotion classification results (achieves an F1-score of 83%).",
        "Our dataset is more imbalanced, but even when balancing the results did not improve significantly (average F1-score of 68%).",
        "The authors were still working on making their datasets publicly available, so unfortunately we couldn't compare directly with their method.",
        "We proposed an enriched graph-based feature extraction mechanism to extract emotion-rich representations.",
        "Our patterns capture implicit emotional expressions which improves emotion recognition results and helps with interpretability.",
        "We hope to improve the pattern weighing mechanism so as to improve the performance on emotion recognition tasks and minimize trade-off between pattern coverage and performance."
    ],
    "2770": [],
    "2778": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; more general globally-normalized models can be trained in a similarly inexpensive way.",
        "The discovery of root-and-pattern morphology in Semitic languages using an unsupervised method is presented in this work.",
        "The extracted Semitic roots are the basic units of these languages, and intrinsic and extrinsic evaluations validate the pattern discovery method and root extraction method (JZR).",
        "There is still lots of improvement space in narrative paragraph generation tasks, such as how to better simulate human imagination to create more vivid and diversified stories."
    ],
    "2783": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We defined two novel tasks based on PeerRead: (i) predicting the acceptance of a paper based on textual features and (ii) predicting the score of each aspect in a review based on the paper and review contents.",
        "Certain properties of a paper, such as having an appendix, are correlated with higher acceptance rate.",
        "Our primary goal is to motivate other researchers to explore these tasks and develop better models that outperform the ones used in this work.",
        "We hope that other researchers will identify novel opportunities which we have not explored to analyze the peer reviews in this dataset.",
        "As a concrete example, it would be interesting to study if the accept/reject decisions reflect author demographic biases (e.g., nationality)."
    ],
    "2787": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "The method works for the static GloVe embeddings.",
        "Applying these new techniques to RNMT models yields RNMT+, an enhanced RNMT model that significantly outperforms the three fundamental architectures on WMT'14 En\u2192Fr and En\u2192De tasks.",
        "The Transformer encoder and the RNMT+ decoder are superior to their counterparts.",
        "Horizontally and vertically mixing components borrowed from these architectures leads to hybrid architectures that obtain further improvements over RNMT+.",
        "Our work will motivate NMT researchers to further investigate generally applicable training and optimization techniques.",
        "Our exploration of hybrid architectures will open paths for new architecture search efforts for NMT."
    ],
    "2795": [
        "The dense approach to word sense disambiguation substantially boosts the performance of the sparse approach on three different sense inventories for Russian.",
        "The dense approach has smoothing capabilities that reduce sparseness, making it a better option than the sparse approach.",
        "Watasense has a simple API for integrating different algorithms for WSD and requires only a basic set of language processing tools to be available, making it accessible to low-resourced languages.",
        "The dense approach is recommended for further studies due to its performance benefits.",
        "The sparse approach and the dense approach have different strengths and weaknesses, and both approaches have their own advantages and disadvantages."
    ],
    "2796": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our approach outperforms benchmark models across different datasets.",
        "The annotation properties that describe details about an ontology concept in natural language, in particular the labels and descriptions, contribute most to the feature vectors.",
        "Transfer learning can further significantly improve OPA2Vec performance.",
        "OPA2Vec can comprehensively encode for information in ontologies.",
        "OPA2Vec has the potential to become a highly useful, standard analysis tool in the biomedical domain, supporting any application in which ontologies are being used."
    ],
    "2797": [
        "The proposed algorithms for neural sequence prediction combine the idea of fine-grained credit assignment and entropy regularization, leading to positive empirical results.",
        "The current oracle Q-function obtained is far from perfect, with the bottleneck lying in the RL algorithm.",
        "The ground-truth reference contains sufficient information for an oracle, and improving its accuracy will be a promising future direction.",
        "The proposed algorithms have numerous potential applications, including the use of an oracle."
    ],
    "2798": [
        "Our method combines acoustic signal processing techniques with deep learning-based approaches to recognize ICD codes.",
        "There is future research to be done on handling multiple speakers and tonal languages such as Chinese.",
        "Recent work on style and speaker tokens may prove beneficial.",
        "This work shows the potential of modern automatic speech recognition to provide efficient, accurate, and cost-effective healthcare documentation."
    ],
    "2801": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "The NEWSROOM dataset is the largest summarization dataset available to date, and exhibits a wide variety of human summarization strategies.",
        "Using NEWSROOM to train an existing state-of-art mixed-strategy summarization model results in performance improvements on out-of-domain data."
    ],
    "2806": [
        "The proposed approach achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptive approach can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The crowd annotation achieves an F1 score greater than 0.67 compared to expert linguists, and an accuracy that is comparable to the state of the art.",
        "The aggregated crowd annotations can provide an ambiguity-enhanced dataset for experimentation.",
        "The approach can capture and interpret inter-annotator disagreement as an indication of ambiguity.",
        "The modified CrowdTruth metrics can capture frame-sentence agreement (FSS), sentence quality (SQS) and frame quality (FQS).",
        "The results show a clear link between inter-annotator disagreement and ambiguity, either in the sentence, frame, or the task itself.",
        "Collapsing such cases to a single, discrete truth value is inappropriate, creating brittle, incomplete datasets, and therefore arbitrary targets for machine learning.",
        "Ranking examples by a score is informative, and the crowd offers alternate interpretations that are often sensible."
    ],
    "2809": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "our model can successfully diversify the image description by reflecting the user's choice, and that user's interest learned can be further applied to new images."
    ],
    "2815": [
        "The proposed MTSA model is highly parallelizable and has more expressive power due to its efficient capture of pairwise dependency at token level and delicate modeling of global dependency at feature level.",
        "The MTSA model achieves a sweet spot between performance and efficiency, outperforming previous self-attention mechanisms while being as memory-efficient as CNN and scalable to long sequences.",
        "The experiments conducted on nine NLP tasks verify that the MTSA can reach state-of-the-art performance with appealing efficiency.",
        "The use of Dropout in the MTSA model can improve its performance by applying it to each of two matrices composing the dividend in Algorithm 1 Step 7, with a keep probability of \u221apad."
    ],
    "2817": [
        "Our baseline reduces the task from labeling the relationship between two sentences to classifying a single hypothesis sentence.",
        "In six of the ten datasets, always predicting the majority-class label is not a strong baseline, as it is significantly outperformed by the hypothesis-only model.",
        "Our analysis suggests that statistical irregularities, including word choice and grammaticality, may reduce the difficulty of the task on popular NLI datasets by not fully testing how well a model can determine whether the truth of a hypothesis follows from the truth of a corresponding premise.",
        "We hope our findings will encourage the development of new NLI datasets which exhibit less exploitable irregularities, and that encourage the development of richer models of inference.",
        "As a baseline, new NLI models should be compared against a corresponding version that only accesses hypotheses."
    ],
    "2818": [
        "The choice of encoder can have a substantial effect on parser performance.",
        "Incorporating more information, such as subword features or externally-trained word representations, can lead to state-of-the-art parsing results.",
        "Structuring the architecture to separate different kinds of information from each other can also lead to improvements in parsing and other natural language processing tasks.",
        "Further research into different ways of encoding utterances can lead to additional improvements in parsing and other natural language processing tasks."
    ],
    "2819": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses a combination of Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The use of pseudo labels improves the domain adaptation performance.",
        "The proposed approach is effective in reducing the noise of pseudo labels to improve the domain adaption performance.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "The linguistic evaluation of modern sentence encoders shows that they capture a wide range of properties.",
        "The probing tasks can help explore what information is captured by different pre-trained encoders.",
        "Different encoder architectures trained with the same objective can result in different embeddings, highlighting the importance of architecture prior for sentence embeddings.",
        "BiLSTM-max embeddings are already capturing interesting linguistic knowledge before training, and they detect semantic acceptability without having been exposed to anomalous sentences before.",
        "The publicly available probing task set will become a standard benchmarking tool for the linguistic properties of new encoders."
    ],
    "2825": [
        "We introduced a scenario for improving a neural semantic parser from logged bandit feedback.",
        "Our approach allows for stochastic gradient optimization, which is crucial in working with neural networks.",
        "It is essential to obtain reward signals at the token-level in order to learn from partially correct queries.",
        "We presented experimental results using feedback collected from humans and a larger scale setup with simulated feedback.",
        "Our approach can significantly outperform a strong baseline using a bandit-to-supervised conversion.",
        "Our approach can be transferred to other domains, such as collecting feedback for other types of queries."
    ],
    "2830": [
        "The proposed E2E dialogue state tracker based on the pointer network achieves state-of-the-art accuracy on the DSTC2 dataset.",
        "The hybrid architecture of the model, combining the pointer network with a jointly trained classification component, is able to handle unknown slot values, which is a valuable feature in real-world situations.",
        "The feature dropout trick proved to be particularly effective in improving the performance of the model.",
        "The existing methods for handling slot values are not able to handle unknown slot values, which is a common problem in real-world scenarios.",
        "The proposed method is able to overcome this limitation and achieve better results."
    ],
    "2833": [
        "The problem of inconsistent experimental setups in Chinese-to-English MT hinders the evaluation of new approaches and should be addressed by using standardized training, development, and test sets, as well as a common benchmark system setup.",
        "Our proposed MT approach can be used to build a competitive system that outperforms systems in almost all 403 published papers in the past 11 years on the NIST OpenMT test sets.",
        "We encourage Chinese-to-English MT experiments to use our common benchmark consisting of standard data and evaluation.",
        "The NIST dataset from LDC is widely used in the Chinese-to-English MT research community, and we have put up a scoreboard listing the scores achieved by all prior published papers when evaluated on the NIST dataset.",
        "By releasing the source code and translation output of our best NMT system12, we hope future work can make more meaningful comparisons to previous MT research."
    ],
    "2834": [
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "We cast this problem as an extreme form of domain adaptation and showed that, even when adapting a small proportion of parameters (the softmax bias, < 0.1% of all parameters), allowed the model to better reflect personal linguistic variations through translation.",
        "The number of parameters specific to any person could be reduced to as low as 10 while still retaining better scores than a baseline for some language pairs, making it viable in a real-world application with potentially millions of different users."
    ],
    "2836": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our approach showed significant and consistent improvements over a variety of languages with different morphological typologies, making it a competitive solution for NMT of low-resource and morphologically-rich languages.",
        "In the future, we plan to develop a more efficient implementation of our approach and to test its scalability on larger data sets."
    ],
    "2838": [
        "Our proposed end-to-end framework outperforms baseline models by a large margin and achieves state-of-the-art performance on two challenging datasets.",
        "Our model is designed with three different modules to tackle the multi-passage MRC task, including finding answer boundaries, modeling answer content, and conducting cross-passage answer verification.",
        "All three modules can be trained with different forms of answer labels, and joint training can provide further improvement.",
        "Our model achieves competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard."
    ],
    "2840": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our lexicalized grid model yields state of the art results on standard coherence assessment tasks in monologue and conversations.",
        "We also show a novel application of our model in forum thread reconstruction.",
        "Our future goal is to use the coherence model to generate new conversations."
    ],
    "2843": [
        "The proposed semi-supervised model for noun-compound paraphrasing improves generalization abilities compared to previous models.",
        "Training the model to predict both a paraphrase and a missing constituent given the paraphrase and other constituent leads to better performance in two noun-compound interpretation tasks.",
        "The model's ability to generate new paraphrase templates unseen during training is a potential area for future exploration."
    ],
    "2844": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "S-LSTM outperforms BiLSTMs using the same number of parameters, demonstrating that S-LSTM can be a useful addition to the neural toolbox for encoding sentences.",
        "The structural nature in S-LSTM states allows straightforward extension to tree structures, resulting in highly parallelisable tree LSTMs.",
        "Investigating S-LSTM to more NLP tasks, such as machine translation, is a potential future direction."
    ],
    "2845": [
        "The proposed model based on gaze features and part-of-speech information achieves accuracy similar to that of linguistic-based models and state-of-the-art systems, without the need for text processing.",
        "Late gaze features are the most discriminative ones for disambiguating categories.",
        "The model learns highly interpretable attention weights, enabling more explainable sarcasm detection methods.",
        "The proposed MI-ARN model outperforms strong state-of-the-art baselines such as GRNN and CNN-LSTM-DNN.",
        "Intra-sentence similarity can be used to detect contrastive sentiment, situations, and incongruity."
    ],
    "2846": [
        "The proposed method of adversarial training based on perturbations in the word embedding space is interpretable and can generate reasonable adversarial texts.",
        "The method maintained or improved the state-of-the-art performance in well-studied sentiment classification (SEC), category classification (CAC), and grammatical error detection (GED) benchmark datasets.",
        "The use of perturbations in the word embedding space provides interpretable visualizations of perturbations, which can help researchers analyze a model's behavior."
    ],
    "2851": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The beam problem can largely be explained by the brevity problem.",
        "Using Wubi encoding for character-level Chinese-English translation is suitable and can lead to performance improvements.",
        "All of our models trained using the Wubi encoding achieve comparable or better performance to the baselines trained directly on raw Chinese.",
        "On the character-level, using Wubi yields BLEU improvements when translating both to and from English, despite the increased length of the input or output sequences.",
        "There are also improvements on the subword-level when translating from English.",
        "Making use of the semantic structure of the Wubi encoding scheme to develop architectures tailored to utilize it is a future direction.",
        "Multilingual many-to-one character-level translation from Chinese and several Latin languages simultaneously is possible using encodings such as Wubi."
    ],
    "2852": [
        "The proposed approach outperforms other baselines by effectively leveraging the information from unobserved word pairs.",
        "We would like to conduct experiments on other languages where available text corpora are relatively hard to obtain.",
        "We plan to study how to leverage other information to facilitate the training of word embeddings under the low-resource setting.",
        "The proposed approach outperforms other baselines by effectively leveraging the information from unobserved word pairs.",
        "We are also interested in applying the proposed approach to domains, such as legal documents and clinical notes, where the amount of accessible data is small."
    ],
    "2853": [
        "We propose an end-to-end trainable joint model combining autoencoder with neural random forest to detect fake reviews.",
        "Our method uses quality feature analysis to mine efficient features for detecting opinion fraud, and those efficient features are employed as input in our model.",
        "Our model starts with an autoencoder, and then fully connected layers and ends in a differentiable random forest, where the differentiable random forest is trained back propagation.",
        "The extensive experimental results demonstrate that our method beats a series of state-of-the-art methods yielding 96% of accuracy."
    ],
    "2856": [
        "We propose AdaBERT, an effective and efficient model that adaptively compresses BERT for various downstream tasks.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "We found eigenvector similarity of sampled nearest neighbor subgraphs to be predictive of unsupervised BDI performance.",
        "We hope that this work will guide further developments in this new and exciting field."
    ],
    "2857": [
        "The model comparison results suggest that both Uniform and FBD priors show better fit to the datasets of the Indo-European language family than the coalescent prior.",
        "Therefore, based on the Bayes Factor analysis, I conclude that the Steppe hypothesis is supported by FBD and Uniform priors for majority of the datasets.",
        "The FBD tree prior does not infer any ancestry relation from any of the datasets, suggesting that the lexical datasets used in the paper do not have signal for ancestry relations.",
        "I also observe that the Bayesian inference program can infer well-established subgroups correctly from the data and need not be supplied beforehand.",
        "The experiments reported in the paper suggest that right tree priors and corrected cognacy judgments are important for estimating the phylogeny and the age of Indo-European language family."
    ],
    "2861": [
        "Our approach enables reduced output duplication and better constraint placement compared to existing methods.",
        "Terminology constraints as provided by customers can be respected during NMT decoding while maintaining the overall translation quality.",
        "Our improvements in computational complexity translate into faster decoding speeds.",
        "Future work includes the application of our approach to more recent architectures such as Vaswani et al. (2017)."
    ],
    "2862": [
        "Our proposed approach outperforms benchmark models across different datasets.",
        "We introduce two neural networks that score a sequence's adherence to discourse structure in long text.",
        "The teachers are used to compute rewards for a self-critical reinforcement learning framework.",
        "Our teacher-trained generator better models the latent event sequences of cooking recipes.",
        "Maintaining semantic coherence in longer recipes is the main reason for the improvement."
    ],
    "2868": [
        "The proposed application of QE to evaluate interpreter output is novel and effective.",
        "The use of METEOR to evaluate interpreter output is a useful starting point, but there is room for improvement by creating fine-grained measures to evaluate various aspects of interpreter performance.",
        "The proposed approach can be immediately applied to allow CAI systems to selectively offer assistance to struggling interpreters.",
        "There is potential for future work in creating fine-grained measures to evaluate various aspects of interpreter performance."
    ],
    "2874": [
        "The proposed model is based on the hypothesis that multi-level visual features and associated attention can provide additional information for deep visual understanding.",
        "The VQA model learns to capture the bimodal feature representation from visual and language domains.",
        "Employing state-of-the-art CNN architectures to obtain visual features for local regions on the image-grid and object proposals.",
        "Developing a hierarchical co-attention scheme that learns the mutual relationships between objects, object-parts, and given questions to predict the best response.",
        "Validating the hypotheses by evaluating the proposed model on two large-scale VQA dataset servers followed by an extensive ablation study reporting state-of-the-art performance."
    ],
    "2883": [
        "The proposed CNN-based aspect extraction model with double embeddings mechanism outperforms state-of-the-art methods with a large margin.",
        "The use of double embeddings mechanism without extra supervision improves the performance of the aspect extraction model.",
        "The proposed method does not require extra supervision, which makes it more efficient and effective compared to state-of-the-art methods."
    ],
    "2885": [
        "We presented SPIGOT, a novel approach to backpropagating through neural network architectures that include discrete structured decisions in intermediate layers.",
        "Our method employs a proxy for the gradients with respect to argmax's inputs, employing a projection that aims to respect the constraints in the intermediate task.",
        "Experiments show that SPIGOT achieves stronger performance than baselines under both settings, and outperforms state-of-the-art systems on semantic dependency parsing."
    ],
    "2886": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our corpora provide effective training ground for supervised WSD system, especially in a multilingual setting where sense annotated data is scarce.",
        "The performance of supervised systems trained on this data is better or comparable to those trained on semi-automatically and, in some cases, manually-curated data.",
        "Our corpora are able to address the need for sense-annotate data in low-resources languages."
    ],
    "2888": [
        "'We presented the first application of triclustering for unsupervised frame induction.'",
        "'Our graph-based triclustering algorithm yields state-of-the-art results.'",
        "'A promising direction for future work is using the induced frames in applications, such as Information Extraction and Question Answering.'",
        "'The source code and the data are available online under a permissive license.'",
        "'We tested several triclustering methods as the baselines.'",
        "'We designed a dataset based on the FrameNet and SVO triples to enable fair corpus-independent evaluations of frame induction algorithms.'"
    ],
    "2892": [
        "Experimental results show that our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The type information of named entities captures inherent text features, leading to improved performance of the overall language model.",
        "Using entity type information as prior can significantly increase the prediction accuracy of the language model.",
        "The vocabulary size of a type-based language model can be reduced significantly, leading to improved performance."
    ],
    "2894": [
        "Our proposed model outperforms the sequence-to-sequence baseline by a large margin.",
        "Our model achieves state-of-the-art performances on a Chinese social media dataset.",
        "The adversarial learning approach improves the supervision of the autoencoder.",
        "The autoencoder learns a better internal representation for abstractive summarization."
    ],
    "2898": [
        "The beam problem in RNN language models can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Developing annotated datasets from multiple sources based on existing datasets and our causality tagging scheme can improve the performance of SCITE.",
        "Combining our method with distant supervision and reinforcement learning can achieve better performance without having to build a high-quality annotated corpus for causality extraction."
    ],
    "2899": [
        "We propose constructing an event graph to solve the script event prediction problem based on network embedding.",
        "To better utilize the dense connections information among events, we construct a narrative event evolutionary graph (NEEG) based on the extracted narrative event chains.",
        "We present a scaled graph neural network (SGNN) to model the events interactions and learn better event representations for choosing the correct subsequent event.",
        "Experimental results show that event graph structure is more effective than event pairs and event chains, which can help significantly boost the prediction performance and make the model more robust."
    ],
    "2901": [
        "We have presented a new parallel corpus in the news domain that aims at improving the MT of Turkish and Kurdish, two very low-resource languages.",
        "Our parallel corpus is a collection of news articles retrieved from the online news magazine Bianet.",
        "The experimental findings show that the addition of the Bianet corpus yields a significant improvement on the overall translation quality.",
        "The addition of the Bianet corpus proves that it could be useful for building MT systems in the given language pairs.",
        "Our corpus is available online for public use."
    ],
    "2903": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets."
    ],
    "2904": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The baseline model alone appears to be reasonably strong, despite the fact that the input features require very little preprocessing and linguistic knowledge.",
        "Adding word embeddings provides information that the model can use to learn useful features that correlate with pitch accents.",
        "The challenge lies in that such strong features tend to lead to overfitting.",
        "The fact that the more textual overlap is found in training and test data, the better the increase in performance must be kept in mind when using word embeddings in such a setting.",
        "Possible future directions include the exploration of how the acoustic and lexical information interact during training, and what lexical information the model is learning to exploit."
    ],
    "2908": [
        "The AMORE-UPF model performs well on rare entities, which is attributed to the beneficial effect of the entity library.",
        "Having an entity library requires the LSTM of our model to output a meaningful entity representation, which is easy in the case of first person pronouns and nominal mentions.",
        "The LSTM can learn to simply forward the speaker embedding unchanged in the case of pronoun I, and the token embedding in the case of nominal mentions.",
        "The induced entity representations may be useful in others, such as encoding entities' attributes and relations.",
        "A module like our entity library can be employed elsewhere, in natural language processing and beyond."
    ],
    "2910": [
        "The approach of using \u00e0 la carte embedding achieves strong performance on several tasks and promises to be useful in many linguistic settings.",
        "Replacing simple window contexts with other structures, such as dependency parses, could yield results in domains such as question answering or semantic role labeling.",
        "Using word weighting when building context vectors or of spectral information along the lines of Mu and Viswanath (2018) is worthy of further study.",
        "The Contextual Rare Words (CRW) dataset provided will support research on few-shot learning of word embeddings.",
        "Our approach can be combined with compositional approaches to handle settings such as zero-shot learning.",
        "The method is useful for representing (potentially cross-lingual) entities such as synsets, whose embeddings have found use in enhancing WordNet and related knowledge bases.",
        "There remain many language features, such as named entities and morphological forms, whose representation by our method remains unexplored."
    ],
    "2911": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "Our approach can provide users with effective device operation schedules and reduce the energy cost of at least 30% in both Spot and Regulation market, while maintaining a scheduling acceptance of at least 60% on a real dataset.",
        "The high stochasticity in the user behavior can lead to incorrect predictions of the user energy needs. However, generic prediction models can lead to scheduling acceptance and nancial savings that are close to the optimal, when the user provides sufficient exibility."
    ],
    "2912": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "This corpus, to the best of our knowledge, is the first manually annotated Chinese dataset for non-task-annotated dialogue systems.",
        "Therefore, it is more reliable than automatic collected data and thus potentially beneficial to chatbot training and evaluation.",
        "Benchmark experiments on this corpus comparing various response selection models confirm the usefulness of the proposed corpus for dialogue systems."
    ],
    "2913": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We propose PReFIL, a new CQA system that improves the state-of-the-art and surpasses human accuracy on two datasets.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments."
    ],
    "2917": [
        "The improved story flow is a result of the inclusion of the previous sentence-story encoder.",
        "The image-sequence encoder successfully learned the dependencies between the images and the proposed architecture was able to model the complex relations between the images and the stories.",
        "With the help of human evaluation, we concluded that most of the generated stories from our model made sense and looked like a story a human would tell.",
        "In order to improve our solution, in the future, we will focus on 3D convolutional neural networks for modelling the image sequences.",
        "We will focus on the use of attention-based models, because they will produce better alignment between the previous-sentence encoder and the decoder in our architecture."
    ],
    "2918": [
        "With an Adversarially Regularized Autoencoder, a continuous text representation is learned of medical captions that can be useful in further applications.",
        "GANs are models that learn the underlying distribution while generating detailed continuous data.",
        "The successful training of a GAN on discrete data in the ARAE setup forebodes success for text generation as well.",
        "We illustrate the potential of GANs for discrete inputs by extending the ARAE architecture to create text conditioned on simple class labels, similar to conditional GANs.",
        "A quantitative evaluation shows that the conditional ARAE achieves a lower perplexity than both the unconditional ARAE and an LSTM baseline."
    ],
    "2920": [
        "Our proposed model outperforms state-of-the-art baselines in paragraph-level question generation.",
        "The ablation study shows the effectiveness of different components in our model.",
        "Our question generation framework produces a large corpus of high-quality question-answer pairs.",
        "Our approach could be applied to other text generation tasks."
    ],
    "2921": [
        "The model achieves on par with or strictly better than four baselines on three text classification tasks, while requiring fewer parameters than the stronger baselines.",
        "On smaller training sets, SoPa outperforms all four baselines.",
        "SoPa is an extension of a one-layer CNN that naturally models flexible-length spans with insertion and deletion.",
        "SoPa can be easily customized by swapping in different semirings.",
        "SoPa performs on par with or strictly better than four baselines on three text classification tasks, while requiring fewer parameters than the stronger baselines.",
        "As a simple version of an RNN, which is more expressive than one-layer CNNs, we hope that SoPa will encourage future research on the bridge between these two mechanisms."
    ],
    "2923": [
        "Our proposed framework overcomes some common limitations of RNNs as text generation models.",
        "The quality of the text produced by our model exceeds that of competitive baselines by a large margin.",
        "The decoding objective suitable for generation is learned through a combination of sub-models that capture linguistically-motivated qualities of good writing."
    ],
    "2925": [
        "We proposed a novel method for removing model biases by explicitly protecting private author attributes as part of model training.",
        "Our method results in increased privacy, while also maintaining, or even improving, task performance, through increased model robustness.",
        "Our approach demonstrates the effectiveness of incorporating adversarial learning into deep learning for privacy-preserving NLP tasks.",
        "We evaluate our methods with POS tagging and sentiment classification, showing improved privacy and task performance.",
        "Our method achieves increased privacy while maintaining or improving task performance, which is a key contribution of this work."
    ],
    "2926": [
        "The proposed model achieves state-of-the-art performance on the task of ROC Story Cloze ending prediction, outperforming a collection of strong baselines.",
        "The model requires less domain-specific training data to achieve high performance.",
        "The proposed model is able to track various semantic aspects with external memory chains.",
        "The model demonstrates the effectiveness of using external memory chains for tracking semantic aspects in natural language processing tasks.",
        "The proposed approach improves upon previous state-of-the-art models, indicating the potential for further advancements in this area."
    ],
    "2927": [
        "We have proposed adversarial stability training to improve the robustness of NMT models.",
        "The basic idea is to train both the encoder and decoder robust to input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart.",
        "Experiments on Chinese-English, English-German and English-French translation tasks show that the proposed approach can improve both the robustness and translation performance.",
        "Our training framework is not limited to specific perturbation types, it is interesting to evaluate our approach in natural noise existing in practical applications, such as homonym in the simultaneous translation system.",
        "It is also necessary to further validate our approach on more advanced NMT architectures, such as CNN-based NMT (Gehring et al., 2017) and Transformer (Vaswani et al., 2017)."
    ],
    "2928": [
        "The proposed two-stage process for RC, which first generates candidates with an extraction model and then selects the final answer by combining the information from all the candidates, is effective.",
        "Treating candidate extraction as a latent variable and jointly training both stages with RL leads to significant improvements in performance.",
        "Introducing a selection model is necessary for improving RC performance.",
        "Fusing candidates' information when modeling leads to better performance.",
        "The proposed joint training strategy is effective in improving RC performance."
    ],
    "2929": [
        "We proposed a novel data augmentation using numerous words given by a bi-directional LM.",
        "Our method produced various words compatibly with the labels of original texts and improved neural classifiers more than the synonym-based augmentation.",
        "Our method is independent of any task-specific knowledge or rules, and can be generally and easily used for classification tasks in various domains.",
        "The improvement by our method is sometimes marginal.",
        "Future work will explore comparison and combination with other generalization methods exploiting datasets deeply as well as our method."
    ],
    "2932": [
        "The proposed model combines the strength of extractive and abstractive summarization.",
        "A novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions.",
        "The inconsistency loss enables extractive and abstractive summarization to be mutually beneficial.",
        "By end-to-end training of our model, we achieve the best ROUGE-recall and ROUGE while being the most informative and readable summarization on the CNN/Daily Mail dataset in a solid human evaluation."
    ],
    "2933": [
        "The proposed model achieves higher accuracy with context compared to without context on the Switchboard Dialogue Act corpus, with an improvement of 3.4% (claim: 'We evaluated the proposed model on the Switchboard Dialogue Act corpus and show the results with and without context. For this corpus, our model achieved an accuracy of 77.34% with context compared to 73.96% without context.')",
        "The use of context-based learning approach for DA identification task improves the accuracy (claim: 'We argue to generalise the discourse modelling for conversation within the context of communication. Hence, we propose to use the context-based learning approach for the DA identification task.')",
        "The proposed model uses minimal information, such as the context of a few preceding utterances, which can be adapted to an online learning tool such as a spoken dialogue system (claim: 'Our model uses minimal information, such as the context of a few preceding utterances which can be adapted to an online learning tool such as a spoken dialogue system.')",
        "The model achieves comparable performance with significantly less computational cost (claim: 'We used simple RNN to model the context of preceding utterances. We used the domain-independent pre-trained character language model to represent the utterances.')"
    ],
    "2936": [
        "The proposed model, Contextual Sarcasm Detector (CASCADE), leverages both content and contextual information for sarcasm classification.",
        "Using user profiling and discourse modeling along with a CNN-based textual model can achieve state-of-the-art performance on a large-scale Reddit corpus.",
        "Discourse features and user embeddings play a crucial role in the performance of sarcasm detection.",
        "The proposed model is effective in detecting sarcasm in online discussions.",
        "The use of contextual information, such as user profiling and discourse modeling, can improve the performance of sarcasm detection."
    ],
    "2941": [
        "Recent developments in neural natural language processing have made it very easy to build custom parsers.",
        "Contextualized word representations help parsers learn the syntax of new domains with very few examples.",
        "They also work extremely well with parsing models that correspond directly with a granular and intuitive annotation task (like identifying whether a span is a constituent).",
        "This allows you to train with either full or partial annotations without any change to the training process.",
        "With a couple hours of effort (and a layman's understanding of syntactic building blocks), they can get significant performance improvements.",
        "We envision an iterative use case in which a user assesses a parser's errors on their target domain, creates some partial annotations to teach the parser how to fix these errors, then retrains the parser, repeating the process until they are satisfied."
    ],
    "2944": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "New datasets and models are required to take reading comprehension to a deeper level of machine understanding.",
        "The ProPara dataset presents new challenges for previous models, and our new models exploit ideas from surface-level QA, producing performance gains."
    ],
    "2946": [
        "Our approach, SuBiLSTM and SuBiLSTM-Tied, improves the performance of the BiLSTM model by capturing long-range dependencies.",
        "We demonstrate gains in performance by replacing BiLSTMs in existing models for several sentence modeling tasks.",
        "The main drawback of our method is the quadratic time complexity required to compute the representations in a SuBiLSTM.",
        "As future direction of work, we intend to explore variants of SuBiLSTM, where only suffixes of fixed or small random lengths are computed.",
        "We also plan to utilize the information exposed by SuBiLSTM in more novel ways."
    ],
    "2948": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The discovery of root-and-pattern morphology in Semitic languages using an unsupervised method is presented in this work.",
        "The extracted roots are the basic units of these languages, and intrinsic and extrinsic evaluations validate the method.",
        "Universal sentence representations can consider information more abundantly than a complex model.",
        "Universal sentence representations trained on a large-scale dataset are more effective for MTE tasks than sentence representations trained on a small or limited in-domain dataset."
    ],
    "2950": [
        "The proposed ensemble of a novel two-layered attention network and classical supervised Support Vector Regression for sentiment analysis performs remarkably well on benchmark datasets.",
        "The two-layered attention network builds representation hierarchically from word to sentence level using knowledge bases, leading to improved performance.",
        "The proposed system outperforms existing top systems for both sub-tracks comfortably, with an improvement of 1.7 and 3.7 points for sub-tracks 1 and 2, respectively.",
        "The system is robust and can be effectively used as a submodule in an end-to-end stock market price prediction system."
    ],
    "2952": [
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Our approach does not require handcrafted features (e.g., word alignment, syntactic features) as well as transfer learning knowledge.",
        "In addition, it allows using several pretrained word embeddings with different dimensions.",
        "Future work could apply our multiple word embeddings approach for transfer learning tasks."
    ],
    "2953": [
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The proposed AdaBERT achieves comparable performance while significantly improves the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The dependency syntactic information may also be used in aspect term and aspect opinion co-extraction, and other sequence labeling tasks.",
        "Additional linguistic features (e.g., POS) and char embeddings can further boost the performance of the proposed model."
    ],
    "2955": [
        "Morse outperforms state-of-the-art results on all nine languages.",
        "Producing morphological features as a sequence outperforms methods that produce whole tags or feature sets, and the advantage is more significant in low resource settings.",
        "Morse is the first deep learning model that performs joint lemmatization and tagging.",
        "Morse can handle unknown and rare wordforms and tags.",
        "Morse can produce a variable number of features in multiple inflectional groups to represent derivations in morphologically complex languages."
    ],
    "2956": [
        "We have developed a synthetic data generation framework that can be used to provide an unlimited dataset perception-instruction-action instances to train and evaluate grounded language learning models.",
        "Our proposed evaluation metric measures the learning efficiency of a model using the number of instances to reach a particular performance rather than the accuracy reached on a fixed-sized dataset.",
        "We developed a novel grid-based representation for perceptual states, where spatial relations are missing in previous approaches.",
        "Our model resulted in state-of-the-art accuracy in Single-Sentence and achieved comparable results in Paragraph without using any external resources."
    ],
    "2957": [
        "PReFIL surpasses prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL exceeds human baseline for FigureQA, but results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceed the human baseline for structure questions, and only PRe-FIL using oracle OCR exceeds humans across all question types.",
        "Better OCR methods lead to better results for DVQA.",
        "The proposed affect-enriched word embeddings outperform state-of-the-art in benchmark intrinsic evaluations and extrinsic applications, including sentiment, personality, and affect prediction."
    ],
    "2958": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "Our QA system is efficient, robust, and scalable to large documents.",
        "Our approach can handle adversarial inputs more effectively than existing methods.",
        "Most questions can be answered using a small set of sentences.",
        "Our sentence selector selects a minimal set of sentences to answer the question, leading to efficiency and effectiveness.",
        "Our method achieves training and inference speedup of up to 15\u00d7 and 13\u00d7, respectively, while maintaining accuracy comparable to or better than existing state-of-the-art."
    ],
    "2959": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The interaction between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Disabling the interactions between tagging and parsing leads to a significant decrease in both tagging and parsing accuracy.",
        "Tagging ambiguity is mostly local while dependency parsing heavily depends on POS tags to predict syntactic structures.",
        "The ConMask model was able to capture the correct relationship because the words \"The Time Machine\" appeared in the description of David Duncan as one of his major works.",
        "Applying a filter to modify the list of predicted target entities so that entities that are same as the relationship will be rearranged is a possible future work."
    ],
    "2960": [
        "We proposed a novel openvocabulary generative model based on a continuous probability density function for modelling numerals.",
        "Our approaches in modelling and evaluation can be used in future work in tasks such as approximate information extraction, knowledge base completion, numerical fact checking, numerical question answering, and fraud detection.",
        "We found that modelling numerals separately from other words through a hierarchical softmax can substantially improve the perplexity of LMs.",
        "Using a continuous probability density function can improve prediction accuracy of LMs for numbers by substantially reducing the mean absolute percentage metric.",
        "We provided the first thorough evaluation of LMs on numerals on two corpora, taking into account their high out-of-vocabulary rate and numerical value (magnitude)."
    ],
    "2966": [
        "We present a joint system which generates question-related captions with question features as heuristics and uses the generated captions to provide additional common knowledge to help the VQA system.",
        "Our approach produces more informative captions while outperforming the current state-of-the-art in terms of VQA accuracy.",
        "Including additional common knowledge besides images in the VQA tasks and more heuristics in image captioning tasks is necessary and important.",
        "Our approach demonstrates the complementarity of the image captioning task and the VQA task.",
        "The generated captions provide additional common knowledge to help the VQA system."
    ],
    "2971": [
        "Our approach obtains results close to the state of the art and a good behavior on re-entrant edges.",
        "AMR-COVINGTON produces sequences of NO-ARCs which could be shortened by using non-local transitions.",
        "Fewer hooks and lookup tables are needed to deal with the high sparsity of AMR."
    ],
    "2972": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The ConMask model is able to capture the correct relationship between entities, even when the name of the entity does not appear in the description.",
        "The ConMask model has some limitations and room for improvement, such as the ability to filter out entities that are similar to the given relationships.",
        "The model is able to perform well on cross-lingual sentiment analysis, with an average of 14 percentage points in F1 on binary and 4 pp on 4-class crosslingual sentiment analysis.",
        "The model is better than ARTETXE and BARISTA at transferring sentiment, but assigns too much sentiment to functional words.",
        "The model will be extended in the future to project multi-word phrases, as well as single words, which could help with negations and modifiers."
    ],
    "2973": [
        "We made use of MT data to build NLU models to reduce time and costs needed to bootstrap an NLU model for a new language.",
        "Using MT data showed a large improvement in performance compared to a grammar-based baseline and outperformed a baseline using an inhouse data collection.",
        "Applying filtering and post-processing techniques improved results further over using MT data as they are.",
        "Our approach can be applied to further languages and explore bootstrapping new domains for an existing NLU system."
    ],
    "2974": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Using RDF embeddings to represent changes is a promising approach to the mapping adaption problem, as it can achieve precision around 0.8 without requiring sophisticated change models or expert knowledge of ontology engineering or the application domain.",
        "The approach does not require any information regarding the nature of changes, such as distinguishing between correction of typos and major structural changes.",
        "The approach can learn new patterns autonomously, without the need for constantly adapting to evolving domains.",
        "The approach is applicable to other data sets and different application domains."
    ],
    "2977": [
        "We proposed a novel Working Memory Network architecture that introduces improved reasoning abilities to the original MemNN model.",
        "By augmenting the MemNN architecture with a Relation Network, the computational complexity of the RN can be reduced, without loss of performance.",
        "This opens the opportunity for using RNs in larger problems, something that may be very useful, given the many tasks requiring a significant amount of memories.",
        "Our model can be easily adapted for visual question answering.",
        "Evidence from cognitive sciences seems to show that all these abilities are needed in order to achieve human-level complex reasoning."
    ],
    "2978": [
        "Our model uses 3\" - This claim highlights the innovative aspect of the proposed neural architecture, which utilizes three different types of supervision to map and specialize a vector space. Prior work has focused only on optimizing individual word embeddings available in external resources.",
        "limited supervision\" - The claim emphasizes that the proposed model uses limited supervision to map and specialize the vector space, which is a departure from prior work that has focused only on optimizing individual word embeddings available in external resources.",
        "three different types of supervision\" - This claim highlights the novelty of the proposed model, which utilizes three different types of supervision to map and specialize the vector space. Prior work has focused only on optimizing individual word embeddings available in external resources.",
        "improved performance\" - The claim suggests that the proposed model achieves improved performance compared to prior work, as it uses limited supervision to map and specialize the vector space.",
        "same model size and speed\" - This claim highlights the efficiency of the proposed model, which achieves improved performance while maintaining the same model size and speed as prior work."
    ],
    "2979": [
        "The approach of considering co-occurrence counts of word triples in sentences and representing them as a third-order tensor has not been effective due to the size of the tensor and the dynamic range of entries.",
        "Restricting word triples to the scenario when one of the words is a preposition is linguistically justified and numerically justified, as prepositions are very frequent and co-occur with essentially every word in the vocabulary.",
        "The tensor-based approach to prepositional representation is effective, as shown by intrinsic evaluations and new state-of-the-art results in downstream evaluations.",
        "The vector representations of prepositions are expected to be widely used in more complicated downstream NLP tasks where prepositional role is crucial, including \"text to programs\" (Guu et al., 2017)."
    ],
    "2981": [
        "The results of our investigation on L1 cognate effects on the productions of advanced non-native Reddit authors are accompanied by a large dataset of native and non-native English speakers, annotated for author country (and, presumably, also L1) at the sentence level.",
        "We plan to more carefully investigate productions of speakers from multilingual countries, like Belgium and Switzerland.",
        "Another extension of this work may broaden the analysis to include additional language families.",
        "The tendency to choose an English cognate is more powerful in L1s with both phonetic and orthographic similarity to English (Roman script) than in L1s with phonetic similarity only (e.g., Cyrillic script).",
        "We would like to extend this work by studying whether the tendency to choose an English cognate is more powerful in L1s with both phonetic and orthographic similarity to English (Roman script) than in L1s with phonetic similarity only (e.g., Cyrillic script)."
    ],
    "2985": [
        "We have defined a new evaluation framework for crosslingual document classification in eight languages.",
        "Our corpus largely extends previous corpora which were also based on the Reuters Corpus Volume 2, but mainly considered the transfer between English and German.",
        "We provide detailed baseline results using two competitive approaches (multilingual word and sentence embeddings, respectively), for cross-lingual document classification between all eight languages."
    ],
    "2986": [
        "A simple cosine distance in a joint multilingual sentence embedding space can be used to filter noisy parallel data and mine for bitexts in large news collections, improving a competitive baseline on the WMT'14 English to German task by 0.3 BLEU.",
        "The proposed multilingual sentence distance could be used in MT confidence estimation or to filter back-translations of monolingual data.",
        "The filtered and extracted data will be made freely available, along with a tool to filter noisy bitexts in nine languages.",
        "There are many directions to extend this research, including scaling up to larger corpora and applying it to the data mined by the European ParaCrawl project."
    ],
    "2989": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our goal is to teach the reinforcement agent to optimize the selection/redistribution strategy that maximizes the reward of boosting the performance of relation classification.",
        "Our framework does not depend on a specific form of the relation classifier, meaning that it is a plug-and-play technique that could be potentially applied to any relation extraction pipeline."
    ],
    "2991": [
        "Improvements in both vocabulary coverage and translation performance by using Japanese predicate conjugation information.",
        "It is important for NMT systems to retain the grammatical property of the target language when injecting linguistic information as a special token.",
        "The proposed method is effective not only for OOV but also for unknown words."
    ],
    "2993": [
        "The proposed method achieves a translation quality comparable to state-of-the-art preordering methods that require manual feature design.",
        "The proposed method does not require a manual feature design for MT.",
        "Future work includes developing a model that jointly parses and preorders a source sentence.",
        "Future work includes integrating preordering into the NMT model."
    ],
    "2994": [
        "The Snips Voice Platform offers state-of-the-art performance while never sending user queries to the cloud.",
        "Small-sized neural networks can be trained that enjoy near state-of-the-art accuracy while running in real-time on small devices.",
        "The acoustic modeling side has shown how small-sized neural networks can be trained that enjoy near state-of-the-art accuracy while running in real-time on small devices.",
        "The language modeling side has described how to train the language model of the ASR and the NLU in a consistent way, efficiently specializing them to a particular use case.",
        "The resulting SLU engine has been demonstrated to be accurate on real-world assistants.",
        "Sufficient, high-quality training data can be obtained without compromising user privacy through a combination of crowdsourcing and machine learning.",
        "Future research directions include private analytics, allowing to receive privacy-preserving feedback from assistant usage, and federated learning, as a complement to data generation."
    ],
    "2995": [
        "The proposed model can reason about context-dependent instructional language that displays strong dependencies on both the history of the interaction and the state of the world.",
        "Future modeling work may include using intermediate world states from previous turns in the interaction, which is required for some of the most complex references in the data.",
        "The proposed learning approach using SESTRA can overcome learned biases in on-policy learning.",
        "The additional reward observations are particularly useful in recovering from biases acquired early during learning, such as due to biased action spaces, which is likely to lead to incorrect blame assignment in neural network policies.",
        "When the domain and model are less susceptible to biases, the benefit of the additional reward observations is less pronounced.",
        "One possible direction for future work is to use an estimator to predict rewards for all actions, rather than observing them."
    ],
    "2996": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our neural argument generation framework enhanced with evidence retrieved from Wikipedia produces more informative arguments than popular sequence-to-sequence-based generation models.",
        "Separate decoders were designed to first produce a set of keyphrases as talking points, and then generate the final argument.",
        "Our model produced more informative arguments than popular sequence-to-sequence-based generation models, as shown by both automatic evaluation against human arguments and human assessment."
    ],
    "2997": [
        "The proposed approach of zero-shot machine translation using reinforcement learning with monolingual data can come close to the performance of the corresponding supervised setting.",
        "Zero-shot dual learning outperforms the multilingual NMT baseline model even when a small parallel corpus for the zero-shot language pair is available.",
        "The model can easily scale up to improve the zero-shot translation of multiple language pairs, yielding an improvement of up to 15.19 BLEU points over the base NMT model.",
        "This framework is promising for machine translation for low-resource languages, especially when combined with techniques like bridging.",
        "The relation between model capacity, number of languages, and language families needs to be better understood to optimize information sharing.",
        "Exploiting more explicitly cross-language generalization, such as in combination with recent unsupervised methods for embeddings optimization, is promising.",
        "The reinforcement learning approach can be extended to include other reward components, such as linguistically motivated ones."
    ],
    "2998": [
        "This work seeks to build computational models that learn to discriminate \"answerable\" questions from those that are not.",
        "The proposed approach considers both context length and context variety for predicting question answerability.",
        "Other model architectures such as recurrent neural networks can be explored in the future.",
        "This study presents a first step toward using data-driven approaches for studying question effectiveness."
    ],
    "2999": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Supervised summarization approaches provide a promising avenue for scoring sentences.",
        "The incorporation of a redundancy removal step to supervised models is the key contributor to the results."
    ],
    "3000": [
        "Our approach is effective, surpassing state-of-the-art systems.",
        "We use reinforcement learning to exploit the space of extractive summaries and promote summaries that are concise, fluent, and adequate for answering questions.",
        "Our system converts human abstracts to a set of question-answer pairs.",
        "The approach is based on converting human abstracts to a set of question-answer pairs.",
        "The results show that our approach is effective."
    ],
    "3001": [
        "The proposed framework for statistical abstractive summarization based on the Abstract Meaning Representation (AMR) shows promise and provides direction for future research.",
        "The approach transforms semantic graphs of the input into a single summary semantic graph using a structured prediction algorithm, which is the centerpiece of the framework.",
        "The approach has the potential to improve the quality of summaries and provide new directions for future research.",
        "The use of AMR enables the representation of abstract meaning in a more explicit and formal way, which can help improve the accuracy and coherence of the generated summaries.",
        "The proposed framework has the potential to be applied to a wide range of domains and languages, making it a versatile and valuable tool for summarization tasks."
    ],
    "3002": [
        "The proposed neural network architecture for hypernym discovery can yield satisfying results on various tasks.",
        "Word embedding and sense embedding have different performance in domain-specific tasks, with sense embedding being more volatile.",
        "The neural network models can model the representations in latent space for words and phrases.",
        "The proposed neural network architecture for hypernym discovery can yield satisfying results on various tasks.",
        "Word embedding and sense embedding have different performance in domain-specific tasks, with sense embedding being more volatile.",
        "The neural network models can model the representations in latent space for words and phrases."
    ],
    "3003": [
        "Short textual descriptions of entities facilitate instantaneous grasping of key information about entities and their types.",
        "Generating them from facts in a knowledge graph requires not only mapping the structured fact information to natural language, but also identifying the type of entity and then discerning the most crucial pieces of information for that particular type.",
        "This is very challenging in light of the very heterogeneous kinds of entities in our data.",
        "We have introduced a novel dynamic memory-based neural architecture that updates its memory at each step to continually reassess the relevance of potential input signals.",
        "Our approach outperforms several competitive baselines.",
        "In future work, we hope to explore the potential of this architecture on further kinds of data, including multimodal data."
    ],
    "3005": [
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "Our rating study, comparing 5-point and preference ratings, showed that their reliability is comparable, whilst cardinal ratings are easier to learn and to generalize from, and also more suitable for RL in our experiments.",
        "Improvements of over 1 BLEU are achievable by learning from a dataset that is tiny in machine translation proportions.",
        "Our results bear a great potential for future applications on larger scale."
    ],
    "3006": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The creation of stepwise ensembles did not result in better performance compared to simply averaging the models.",
        "Some signs of overfitting on the dev set were found.",
        "In future work, we would like to apply the methods (translation and semi-supervised learning) used on Spanish on other low-resource languages and potentially also on other tasks."
    ],
    "3007": [
        "Our proposed structured self-attention encoder for NMT shows significant gains in performance over a strong baseline on standard WMT benchmarks.",
        "The models presented do not access any external information such as parsetrees or part-of-speech tags, yet appear to use and induce structure when given the opportunity.",
        "Our induction performance is language pair dependent, which invites an interesting research discussion as to the role of syntax in translation and the importance of working with morphologically rich languages."
    ],
    "3009": [
        "The proposed denoising distant supervised method for RE via instance-level adversarial training achieves efficient noise reduction in finer granularity and significantly outperforms the state-of-the-art baseline.",
        "The method is robust for those long-tail entity pairs with few instances.",
        "Inspired by (Ji et al., 2017), adopting external knowledge, from either KBs or text, to help train more efficient samplers and discriminators for adversarial training may further improve the robustness of RE models.",
        "Extending the instance-level adversarial training to the entity-pair level may further improve the robustness of RE models."
    ],
    "3012": [
        "The proposed sentence-level RL model for abstractive summarization makes the model aware of the word-sentence hierarchy.",
        "The proposed model achieves a new state-of-the-art on both CNN/DM versions and generalizes better on test-only DUC-2002.",
        "The proposed model has a significant speed-up in training and decoding."
    ],
    "3013": [
        "As RNNs are not good at remembering the old history and cannot consider word relationship either, sometimes conventional NMT cannot get enough source information and hence emphasizes too much on the fluency of the target.",
        "Our method can get better translation on the NIST Zh-En dataset and the WMT En-De dataset and can even outperform the system with supervised syntactic knowledge.",
        "The RNs employs CNNs to collect information around one word and explicitly connect each word with all the other words, providing opportunities for NMT to capture relationship between source words.",
        "Incorporating RNNs into attentional NMT leads to a better source representation and can improve translation performance."
    ],
    "3016": [
        "We propose a novel technique for bi-directional neural machine translation.",
        "A single model with a standard NMT architecture performs both forward and backward translation, allowing it to back-translate and incorporate any source or target monolingual data.",
        "By continuing training on augmented parallel data, bi-directional NMT models consistently achieve improved translation quality, particularly in low-resource scenarios and crossdomain tasks.",
        "These models also reduce training and deployment costs significantly compared to standard uni-directional models."
    ],
    "3017": [
        "We present a general approach to align embeddings in high dimensional space.",
        "Our approach can be efficiently solved using a stochastic algorithm.",
        "We develop a convex relaxation that can be used to initialize our approach.",
        "Our method achieves performances on par with the state-of-the-art on a real application, namely unsupervised word translation.",
        "There is a link between graph matching and point cloud alignments that can be further investigated.",
        "Our approach is guaranteed to work for certain types of problems.",
        "It is possible to improve the relaxation procedure."
    ],
    "3018": [
        "Distillation of an ensemble into a single model can significantly improve the single model's performance, as shown by experiments on transition-based dependency parsing and machine translation.",
        "The distillation method proposed in this paper provides empirical guarantees for its effectiveness, as demonstrated by comparison analysis.",
        "The proposed distillation method can be applied to both reference and exploration states, allowing for more flexible use of the ensemble.",
        "The use of distillation can improve the performance of a single model, as shown by experiments on two tasks (dependency parsing and machine translation)."
    ],
    "3019": [
        "The model based on gaze features and part-of-speech information achieves accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerge as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The novel end-to-end model for joint slot label alignment and recognition requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The encoder-decoder approach for generating a sentence to describe a table row extends the encoder-decoder framework by integrating the semantics of a table.",
        "A flexible copying mechanism is developed to replicate contents from cells, attributes, and captions in the output sentence."
    ],
    "3021": [
        "The proposed model, DRCN, outperforms previous deep RNN models using residual connections.",
        "The use of intact features over multiple layers composes a community of semantic knowledge that improves performance.",
        "Bottleneck components are inserted to reduce the size of the network.",
        "The model is the first generalized version of DenseRNN, expandable to deeper layers with controllable feature sizes using an autoencoder.",
        "The interpretability of the model is demonstrated through attentive weights and the rate of maxpooled positions.",
        "The model achieves state-of-the-art performance on multiple datasets for three highly challenging natural language tasks.",
        "The collective semantic knowledge used in the proposed method is expected to be applied to various other natural language tasks."
    ],
    "3022": [
        "Representation matters\" for neural relation classification, and different representations have \"clear consequences\" on downstream processing.",
        "The use of dependency representations for neural relation classification has been examined, and three widely used representations have been compared.",
        "Future work will extend the study to neural dependency parsers and other relation classification data sets.",
        "The choice of representation can have a significant impact on the performance of downstream processing tasks.",
        "The use of dependency representations can improve the performance of neural relation classification."
    ],
    "3023": [
        "The proposed AMR parser uses methods from supertagging and dependency parsing to map a string into a well-typed AM term, which is then evaluated into an AMR.",
        "The AM term represents the compositional semantic structure of the AMR explicitly, allowing for the use of standard treebased parsing techniques.",
        "The parser currently computes the complete parse chart, but future work will speed it up through the use of pruning techniques.",
        "The method will also look into more principled methods for splitting the AMRs into elementary as-graphs to replace hand-crafted heuristics.",
        "Advanced methods for alignments, such as those in Lyu and Titov (2018), seem promising for overcoming the need for heuristics.",
        "The method has the potential to be applied to other semantic representations, once the need for heuristics is overcome."
    ],
    "3025": [
        "We have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages.",
        "This allows sharing without crosslingual alignments, shared annotation, or parallel data.",
        "A polyglot model can outperform a monolingual one for semantic analysis, particularly for languages with less data."
    ],
    "3027": [
        "We showed that by using language models as discriminators, we could outperform traditional binary classifier discriminators in three unsupervised text style transfer tasks.",
        "A language model can provide a more stable and more informative training signal for training generators than a binary classifier discriminator.",
        "It is possible to eliminate adversarial training with negative samples if a structured model is used as the discriminator.",
        "We plan to explore and extend our model to semi-supervised learning in the future."
    ],
    "3029": [
        "Exploiting bias in the data selection process\" can lead to high performance on a dataset.",
        "Simplifying the decision of picking an object to a smaller set of candidates may improve performance.",
        "Removing all words except for nouns and adjectives only marginally hurts performance for certain systems.",
        "Focusing only on simple properties like the category of the target object may be possible to further reduce the set of candidates.",
        "Careful analysis is important when constructing new datasets and models for grounded language tasks.",
        "The techniques used in this study can be applied more generally to other tasks to gain insight into what our models are learning and whether our datasets contain exploitable bias."
    ],
    "3030": [
        "'The proposed method generates captions that draw laughter.'",
        "'We built the Boket-eDB, which contains pairs comprising a theme (image) and a corresponding funny caption (text).'",
        "'We effectively trained a funny caption generator with the proposed Funny Score by weight evaluation.'",
        "'The NJM was much funnier than the baseline STAIR caption.'"
    ],
    "3031": [
        "Character-level neural models cannot yet match the performance of morphology-level models on in-domain data, but they provide considerable advantages over whole-word models.",
        "The shortcomings of character-level models depend on the morphology type, with limitations in agglutinative languages for data with rich derivational morphology and high contextual ambiguity, and struggles in fusional languages for tokens with high number of morphological tags.",
        "The similarity between character-level and morphology-level models is higher than the similarity within character-level models on languages with high OOV%.",
        "Character-level models rely relatively more on training data size, and their performance will improve faster than morphology-level models with more training data.",
        "Out-of-domain performance of character-level models surpasses all morphology-level models, but the improvement is less expected when model complexity is increased.",
        "Character-level models generally perform better than models that only have access to predicted/silver morphological tags."
    ],
    "3032": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "This end-to-end approach in starred mode with training augmentation provides better results than a pipeline approach to detect named entity categories.",
        "The performances of this end-to-end approach to extract named entity values are worse than the ones got by the pipeline process.",
        "This study presents promising results in a first attempt to experiment an end-to-end approach to extract named entities, and constitutes an interesting start point for future work."
    ],
    "3034": [
        "Neural machine translation is less robust to many types of noise than statistical machine translation.",
        "In the most extreme case, when the reference is an untranslated copy of the source data, neural machine translation may learn to excessively copy the input.",
        "These findings should inform future work on corpus cleaning.",
        "There are five types of noise in parallel data that can affect the performance of machine translation systems.",
        "Raw web crawl data can be a source of motivation for studying these types of noise."
    ],
    "3039": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "LSTMs are the most effective low-bias learners for acceptability classification.",
        "Compared to humans, though, their absolute performance is underwhelming.",
        "Our models with unsupervised pretraining have an advantage over similar models without pretraining.",
        "The supervised models universally see a substantial drop in performance from the in-domain test set to the out-of-domain test set."
    ],
    "3041": [
        "The task becomes easier when a broader context is given\" for humans performing quantifier inference.",
        "The best-performing LSTMs show a decrease in performance when given a broader context.",
        "Human performance with proportional quantifiers is boosted by a broader context, while logical quantifiers do not show this effect.",
        "Humans are able to grasp the magnitude of the missing quantifier even when guessing the wrong one.",
        "The overlapping meaning and use of expressions involving quantifiers is reflected in human performance.",
        "There is an ordered mental scale of quantifiers, as evidenced by the differences in human performance across different types of quantifiers.",
        "The reason why models fail with certain quantifiers and not others is yet not clear, and may be due to engineering issues."
    ],
    "3045": [
        "The current NLI models struggle with certain phenomena, despite reporting high accuracy on NLI tasks.",
        "The MultiNLI dataset is a valuable resource for the NLP community, but the models should be evaluated using stress tests to ensure they are not exploiting simple idiosyncrasies of the training data.",
        "The proposed stress test evaluation paradigm can be updated in the future to cover problems of future models.",
        "The release of the stress tests and associated resources will promote work on models that get us closer to true natural language understanding.",
        "The current NLI models are not robust to random perturbations (noise tests) and susceptible to shallow lexical cues (distraction tests).",
        "The ability of NLI models to reason about quantities and antonymy (competence tests) should be evaluated using stress tests."
    ],
    "3046": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "We explore the use of a projection-based method for attenuating biases in word representations using the task of natural language inference.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion, and nationality biases."
    ],
    "3048": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model",
        "solving the brevity problem leads to significant BLEU gains",
        "our solution to the brevity problem requires globally-normalized training on only a small dataset",
        "sentiment analysis models should be able to explain not only whether the data contains negative or positive meaning but also whether the person is negative or positive",
        "we suggest a framework as objective methods to evaluate sentiment analysis model"
    ],
    "3050": [
        "Our proposed sentence embedding method using a sequential encoder-decoder with a pairwise discriminator outperforms previous state-of-art methods for NLP tasks.",
        "Experimental analysis justifies that our method outperforms all previous state-of-art methods in terms of BLEU, METEOR and TER scores.",
        "Our method significantly outperforms all other methods in terms of BLEU, METEOR and TER scores.",
        "We plan to generalize this to other text understanding tasks and also extend the same idea in vision domain."
    ],
    "3053": [
        "The proposed Japanese PAS analysis model exploits a semi-supervised adversarial training to learn Japanese PAS and selectional preferences.",
        "The generator neural network is trained from raw corpora and enhanced with external knowledge using a validator.",
        "The semi-supervised training method can be applied to other NLP tasks in the future.",
        "The proposed model improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\")."
    ],
    "3057": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy."
    ],
    "3058": [
        "The proposed fixed-size encoding of text sequence with dynamic routing mechanism outperforms other encoding models by a significant margin, as shown by experimental results on five text classification tasks.",
        "The use of LSTM hidden states as word encoding in this paper is one possible alternative to other word encodings such as convolved n-gram.",
        "The dynamic routing mechanism could also be useful to improve the encoder in sequenceto-sequence tasks.",
        "The authors plan to investigate more sophisticated routing policies for better encoding the text sequence in the future."
    ],
    "3061": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We propose Dynamic Memory Induction Networks (DMIN) for few-shot text classification, which builds on external working memory with dynamic routing.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our proposed approach can efficiently leverage label embeddings to deal with heterogeneous schemas.",
        "Our neural network formulation outperforms a strong rule-based baseline.",
        "We quantified the gains from various components of the proposed approach."
    ],
    "3064": [
        "When all source sentences are present in the test data, multi-encoder NMT has better performance than mixture of NMT experts except for {En, Es, Pt (br)}-to-Fr.",
        "However, when the input is incomplete, mixture of NMT experts achieves performance better than or equal to multi-encoder NMT.",
        "It's possible that if we designed a better attention strategy for multi-encoder NMT we may be able to resolve this problem.",
        "The experimental results with simulated and actual incomplete multilingual corpora show that this simple modification allows us to effectively use all available translations at both training and test time.",
        "The performance of multi-source NMT depends on source and target languages, and the size of missing data.",
        "As future work, we will investigate the relation of the languages included in the multiple sources and the number of missing inputs to the translation accuracy in multi-source scenarios."
    ],
    "3068": [
        "We introduce a simple unsupervised method for Commonsense Reasoning tasks.",
        "The resulting systems outperform previous best systems on both Pronoun Disambiguation Problems and Winograd Schema Challenge.",
        "Remarkably on the later benchmark, we are able to achieve 63.7% accuracy, comparing to 52.8% accuracy of the previous state-of-the-art.",
        "Our system discovers key features of the question that decides the correct answer, indicating good understanding of the context and commonsense knowledge.",
        "We also demonstrated that ensembles of models benefit the most when trained on a diverse set of text corpora.",
        "This simple technique will be a strong building block for future systems that utilize reasoning ability on commonsense knowledge."
    ],
    "3071": [
        "The claim that the proposed Relational Tensor Network model for multimodal Affect Recognition takes into account the context of a segment in a video based on the relations and interactions with its neighboring segments within the video.",
        "The claim that the model shows the best performance as compared to state-of-the-art techniques for sentiment and emotion recognition on the CMU-MOSEI dataset.",
        "The claim that the model meticulously adds various feature sets on the word level, including language model based embeddings and segment level sentiment features."
    ],
    "3078": [
        "Automatic evaluation metrics for MRC tasks, such as BLEU and ROUGE, may be biased towards human judgment, especially for yes-no and entity questions.",
        "The proposed adaptations to ROUGE and BLEU metrics can better evaluate yes-no and entity answers by introducing two bonus terms based on lexical overlap.",
        "The statistical analysis shows that the proposed adaptations achieve higher correlation to human judgment compared with original ROUGE-L and BLEU.",
        "The methodology proposed in this paper is effective in evaluating MRC systems, and future work will cover more question types and datasets.",
        "The exploration of the design of MRC evaluation metrics can bring more research attention to the field."
    ],
    "3080": [
        "We addressed the problem of simultaneous translation by modifying the architecture in Neural MT decoder.",
        "Our results showed improvements over previously established WIW and WID methods.",
        "We additionally modified the Neural MT training to match the incremental decoding, which significantly improved the chunk-based decoding.",
        "The code for our incremental decoder and agents has been made available.",
        "In the future, we would like to change the training model to dynamically build the encoder and the attention model in order to match our incremental decoder."
    ],
    "3082": [
        "Our proposed reinforcement learning framework for Chinese zero pronoun resolution achieves an F-score of 67.2% on the test dataset, surpassing all existing models by a considerable margin.",
        "Our model learns the policy on selecting antecedents in a sequential manner, leveraging effective information provided by earlier predicted antecedents.",
        "The strategy of our model contributes to predicting for later antecedents, bringing a natural view for the task.",
        "Our reinforcement learning model integrates anaphoric zero pronoun determination and resolution jointly in a hierarchical architecture without using parser or anaphoric zero pronoun detector.",
        "We plan to explore neural network models for efficaciously resolving anaphoric zero pronoun documents and research on some specific components which might influence the performance of the model, such as the embedding.",
        "We plan to research on the possibility of applying adversarial learning (Goodfellow et al., 2014) to generate better rewards than human-defined reward functions."
    ],
    "3084": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our method can resolve ambiguity in unigram counts, notably due to syncretism.",
        "Given a lexicon, an unsupervised model partitions the corpus count for each ambiguous form among its analyses listed in a lexicon.",
        "We empirically evaluated our method on 5 languages under two evaluation metrics."
    ],
    "3085": [
        "The novel generative model for morphological inflection generation in context allows for the exploitation of unlabeled data in the training of morphological inflectors.",
        "The model's rich parameterization prevents tractable inference, and a variational inference procedure based on the wake-sleep algorithm is used to marginalize out the latent variables.",
        "Experimental validation on 23 languages shows that the model improves by large margins over baselines, particularly in lower-resource conditions."
    ],
    "3086": [
        "The authors have created a parallel corpus of 114 narratives in an endangered language, Griko, with translations in Italian.",
        "The corpus is currently annotated with Part-of-Speech tags, but the authors plan to enrich the resource with additional annotations, such as syntactic and morphological annotations, in the future.",
        "The authors intend to contribute their corpus to the Universal Dependencies treebanks, as Griko is currently absent from the supported languages.",
        "The authors have used a projection-based method for attenuating biases in word representations, and their experiments show that this approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy."
    ],
    "3087": [
        "The embeddings of GloVe, ELMo, and BERT encode gender, religion, and nationality biases.",
        "A projection-based method can be used to attenuate these biases.",
        "The method works for static GloVe embeddings.",
        "Extending the approach to contextualized embeddings (ELMo, BERT) can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The MultiATIS++ corpus is a multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families.",
        "The use of multilingual BERT encoder, machine translation, and label projection can be used for cross-lingual transfer.",
        "A novel end-to-end model for joint slot label alignment and recognition that requires no external label projection can be introduced.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "The simple projection baseline using fast-align can be outperformed by the novel end-to-end model.",
        "The state-of-the-art label projection approach can be achieved with only half of the training time."
    ],
    "3091": [
        "The proposed two-step recommendation system for enhancing the reference section of Wikipedia entity pages achieves significant improvements over competing baselines.",
        "The first step of the system uses a supervised classification approach to obtain relevant wikilinks for an entity page.",
        "The second step recommends a ranked list of references from the relevant wikilinks obtained in the first stage.",
        "WikiRef achieves an overall precision@1 of 0.44 for the CS dataset and 0.45 for the PH dataset, which are very significant improvements over competing baselines.",
        "Manual evaluations over 25 wikipages show that WikiRef also recommends the most relevant references that are absent in the gold standard dataset.",
        "The proposed approach can be adapted to filter any type of entity pages' reference section.",
        "Future work includes further evaluations on various datasets including biology and medicine, and preparing an online tool that triggers WikiRef to recommend references as soon as relevant wikilinks are added to a target Wikipedia article."
    ],
    "3093": [
        "The proposed constituency parsing scheme based on predicting real-valued scalars can identify the sequence of top-down split decisions.",
        "The neural network model that predicts syntactic distances and constituent labels can be employed to build an unambiguous mapping between each (d, c, t) and a parse tree.",
        "The model predicts split decisions in parallel, leading to strong performance compared to previous models while being significantly more efficient.",
        "The architecture of the model is no more than a stack of standard recurrent and convolution layers, making deployment straightforward."
    ],
    "3094": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model outperforms the CNN and LSTM, and does not add any additional parameters over the standard LSTM model.",
        "The promise of the model in classification tasks involving capturing and combining relevant information from multiple points in the previous context.",
        "Future work will focus on designing models that can deal with and be optimized for scenarios with severe data imbalance.",
        "Explore various applications of presupposition trigger prediction in language generation applications, as well as additional attention-based neural network architectures."
    ],
    "3095": [
        "The sensitivity to dictionary size is an important factor to be considered in practice.",
        "Models that rely on dictionaries such as voclink and softlink are more applicable to low-resource languages than hardlink.",
        "Softlink can outperform voclink with only a limited amount of lexical information.",
        "Softlink relaxes and generalizes hardlink to be adaptable to more situations, while using dictionary information more efficiently than voclink.",
        "Using dictionary information more efficiently than voclink, softlink may be able to transfer knowledge to low-resource languages more effectively than other approaches."
    ],
    "3097": [
        "We analyzed five different neural models for sentence pair modeling and conducted a series of experiments with eight representative datasets for different NLP tasks.",
        "We quantified the importance of the LSTM encoder and attentive alignment for inter-sentence interaction, as well as the transfer learning ability of sentence encoding based models.",
        "We showed that the SNLI corpus of over 550k sentence pairs cannot saturate the learning curve.",
        "We systematically compared the strengths and weaknesses of different network designs and provided insights for future work."
    ],
    "3098": [
        "The joint model significantly improves the quality of transfer between formal and informal styles in both directions compared to prior work (Rao and Tetreault, 2018).",
        "The joint model learns to perform FSMT without being explicitly trained on style-annotated translation examples.",
        "Our model outperforms previously proposed phrase-based MT model (Niu et al., 2017) on the FSMT task.",
        "The joint model performs on par with a neural model with side-constraints, which requires more involved data selection.",
        "Multi-task learning shows promise for controlling style in language generation applications.",
        "Future work will investigate other multi-task architectures and objective functions to better capture the desired output properties and address current weaknesses such as meaning errors revealed by manual analysis."
    ],
    "3100": [
        "We proposed a framework that leverages dialogue state representation, which is tracked by an attention-based method.",
        "Our framework performed an entry-level soft lookup over the knowledge base, and applied copying mechanism to retrieve entities from knowledge base while decoding.",
        "This framework was trained in an end-to-end fashion with only the dialogue history, and get rid of other annotation.",
        "Experiments showed that our model outperformed other Seq2Seq models on both automatic and human evaluation.",
        "The visualization and case study demonstrated the effectiveness of dialogue state representation and entity retrieval."
    ],
    "3101": [
        "The ensemble model of n-gram based probabilistic model (MNB) and char-trigram based deep learning model(LSTM) outperforms deep learning models on small multilingual codemixed data for sentiment analysis.",
        "There is a need for analyzing the sentiments of social media texts due to the increase in popularity and impact of social media.",
        "The proposed ensemble model can be extended to other language pairs of code-mixed data in future work.",
        "Individual languages have rich features that can be utilized to help identify sentiments in their code-mixed versions."
    ],
    "3103": [
        "We built a unified neural sequence labeling framework to reproduce and compare recent state-of-the-art models with different configurations.",
        "Experiments show that character information helps to improve model performances, especially on disambiguating OOV words.",
        "In most cases, models with word-level LSTM encoders outperform those with CNN, at the expense of longer decoding time.",
        "We observed that the CRF inference algorithm is effective on NER and chunking tasks, but does not have the advantage on POS tagging.",
        "With controlled experiments on the NER dataset, we showed that BIOES tags are better than BIO.",
        "Besides, pretrained GloVe 100d embedding and SGD optimizer give significantly better performances compared to their competitors."
    ],
    "3105": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "Incorporating knowledge from textual sources by initializing the entity embeddings with distributional representation of entities could improve our results further, which we will explore in the future."
    ],
    "3109": [
        "The performance of existing HRED models and attention variants can be significantly improved by using proper detection methods, such as self-attention.",
        "The proposed ReCoSa model outperforms existing HRED models and attention variants in multiturn dialogue generation.",
        "The relevant contexts detected by the ReCoSa model are coherent with human judgments.",
        "The insufficiency of high-quality annotated data is a limiting factor for the performance of the SCITE method.",
        "The use of multihead self-attention can improve the learning of dependencies between cause and effect.",
        "The proposed method can be further improved by developing annotated datasets from multiple sources based on existing datasets and the causality tagging scheme, and combining the method with distant supervision and reinforcement learning.",
        "Domain adaptation techniques, such as speech enhancement, data augmentation, and autoencoders, can improve the robustness of close-talking speech recognition models to distant speech recognition.",
        "Multi-condition training can produce models that are more robust than the baseline, but it also has the most stringent requirement.",
        "Speech enhancement and data augmentation have the potential to match the performance of multi-condition training, but they require specific conditions to be met.",
        "Unsupervised domain adaptation with autoencoders is promising and can achieve better results than data augmentation with simulated reverberation, even when only requiring independent unlabeled data from both domains."
    ],
    "3110": [
        "We have proposed a new approach called double path networks for sequence-to-sequence learning, which achieves state-of-the-art performance in several sequence-to-sequence tasks.",
        "Our approach uses a cross attention module to leverage the advantages of two different models, and we plan to explore more language pairs and other sequence-to-sequence tasks in the future.",
        "We will investigate how to design better structures for information fusion in the future.",
        "We would like to extend the current double path model to multiple paths."
    ],
    "3111": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The results of PReFIL on DVQA are more nuanced due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The novel end-to-end model for joint slot label alignment and recognition requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "VKMN achieves promising results on VQA v1.0 and v2.0 benchmarks, and outperforms state-of-the-art methods on the knowledgereasoning related questions.",
        "The influence of target object size is studied by comparing VKMN and MLB performance on VQA 2017 test-dev set at different object-size groups.",
        "For both MLB and VKMN, larger targets show higher overall accuracy than smaller targets, especially on the \"Yes/No\" answer-type.",
        "VKMN outperforms MLB on the \"other\" answer-type significantly on all the object-size groups."
    ],
    "3112": [
        "The proposed visually grounded speech model achieves a P@10 of almost 60% for crosslingual keyword spotting.",
        "The majority of errors are due to semantically related retrievals, and taking these into account, the approach comes close to a supervised model trained on parallel speech with text translations.",
        "The use of multitask learning implicitly predicting tags not in the keyword set provides a small benefit.",
        "Using soft targets from the visual tagger is better than oracle hard targets, and this aligns with findings in student-teacher knowledge distillation studies.",
        "Future work will consider error analyses at a larger scale and applications on truly low-resource (e.g. unwritten) languages."
    ],
    "3113": [
        "We presented a unified graph framework to conduct event coreference and sequencing.",
        "We achieved state-of-the-art results on event coreference and report the first attempt at event sequencing.",
        "While we only studied two types of relations, we believe the method can be adopted in broader contexts.",
        "In the future, we plan to build a joint model to allow the tasks to mutually improve each other.",
        "Analyzing event structure can bring new aspects of knowledge from text.",
        "Event Coreference systems can help group scattered information together.",
        "Understanding Event Sequencing can help clarify the discourse structure, which can be useful in other NLP applications, such as solving entity coreference problems.",
        "However, in our investigation, we find that the linguistic theory and definitions for events are not adequate for the computational setting.",
        "Proper theoretical justification is needed to define event coreference, which should explain the problems, such as argument mismatches.",
        "In addition, we also need a theoretical basis for script boundaries."
    ],
    "3116": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "We introduce the Attentive Unsupervised Text (W)riter (AUTR), a latent variable model which uses an external memory and a dynamically updated attention mechanism to write natural language sentences.",
        "The model achieves a higher ELBO on the Book Corpus dataset, and relies more heavily on the latent representation compared to a purely RNN-based generative model.",
        "AUTR is also computationally efficient, requiring fewer RNN time steps than the sentence length.",
        "We find that it is able to generate coherent sentences, as well as impute missing words effectively.",
        "The idea of using a canvas-based mechanism to generate text is very promising and presents plenty of avenues for future research.",
        "Using an RNN with a similar attention mechanism to parametrise the variational distribution would also facilitate extending the model to other natural language tasks such as document classification and translation."
    ],
    "3119": [
        "The SMHD dataset is the largest dataset of Reddit users with diverse mental health conditions and matched control users, and it is up to two orders of magnitude larger than the largest published similar resource.",
        "The SMHD dataset supports a variety of mental health conditions and is available for research purposes.",
        "The language use of diagnosed users (with mental health conditions) differs from that of control groups, as measured by various linguistic and psychological signals.",
        "FastText is the most effective approach for identifying diagnosed users using text classification methods.",
        "The SMHD dataset is available to the research community for further study and reproducibility of suggested approaches."
    ],
    "3120": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The combination of the Clustering Promotion Mechanism and Cosine Annealing Strategy achieves better domain adaption performance.",
        "The proposed method can be used to train a few-shot classifier using pseudo-labeled target-domain data and labeled source-domain data.",
        "Examining factors that influence legislative floor action success can help elucidate the similarities and differences of the policymaking processes."
    ],
    "3122": [
        "The proposed conversation model based on Multi-Turn hybrid CNN (MT-hCNN) is effective and efficient.",
        "The model can adapt knowledge learned from a resource-rich domain.",
        "The model is transferable to other domains.",
        "The online deployment of the proposed model in AliMe E-commerce chatbot showed its effectiveness."
    ],
    "3124": [
        "Our linguistically motivated techniques do not perform better than current state-of-the-art agnostic methods in machine translation.",
        "Our new methods are not accurate enough in splitting words to morphs, leading to a high amount of resulting morphs, most of which are rare in the data.",
        "The addition of a zero suffix mark after all but final tokens in the sentence showed a big improvement in linguistically adequate splits.",
        "Explicit sentence ends may benefit Tensor2Tensor NMT model more than better segmentation.",
        "Common linguistically non-informed word segmentation methods, such as BPE and SubwordTextEncoder, are still the best choice despite our analysis showing an important difference in STE and BPE leading to considerably better performance.",
        "The same feature (support for zero suffix) can be utilized in BPE, giving similar gains."
    ],
    "3126": [
        "We proposed to leverage on linked entities to improve the performance of sequence-to-sequence models on neural abstractive summarization task.",
        "Our proposed method, Entity2Topic (E2T), is easily attachable to any model using an encoder-decoder framework.",
        "E2T applies linked entities into the summarizer by encoding the entities with selective disambiguation and pooling them into one summary topic vector with firm attention mechanism.",
        "We showed that by applying E2T to a basic sequence-to-sequence model, we achieve significant improvements over the base model and consequently achieve a comparable performance with more complex summarization models."
    ],
    "3127": [
        "Our model performs significantly better than previous models.",
        "The experimental results show that our model can handle the cold-start problem effectively.",
        "The improvements increase when the level of sparsity in data increases, which confirms that HCSC is able to deal with the cold-start problem.",
        "Our fast word encoder contextualizes words to contain both short and long range word dependency features.",
        "Our attention mechanism called Cold-start Aware Attention (CSAA) considers the existence of the cold-start problem among users and products by using a shared vector and a frequency-guided selective gate."
    ],
    "3128": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our method improves the classification performance and achieves state-of-the-art perfor-mance on multiple data sets."
    ],
    "3130": [
        "NCRF++ achieves state-of-the-art results with efficient running speed.",
        "NCRF++ supports flexible feature utilization.",
        "NCRF++ can generate nbest label sequences rather than the best one.",
        "NCRF++ is an open-source neural sequence labeling toolkit."
    ],
    "3131": [
        "The AMR formalism has feasibility for multi-document summarization.",
        "The proposed approach for generating abstractive summaries from multiple source documents is full-fledged.",
        "The AMR summarization framework performs competitively with state-of-the-art abstractive approaches.",
        "The abstract meaning representation is a powerful semantic formalism for the task of abstractive summarization."
    ],
    "3132": [
        "Our approach can effectively preserve salient source relations in summaries.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "We showed that our models can effectively combine source syntactic structure with the copy mechanism of an abstractive summarization system.",
        "Our investigated structure-infused copy mechanisms can preserve salient source relations in summaries.",
        "The combination of source syntactic structure and abstractive summarization system can achieve competitive performance with state-of-the-art published systems."
    ],
    "3135": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "The proposed PReFIL system has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments."
    ],
    "3139": [
        "Our proposed joint input-label embedding model generalizes over existing models and addresses their limitations while preserving high performance on both seen and unseen labels.",
        "Compared to baseline neural models with a typical output layer, our model is more scalable and has better performance on the seen labels.",
        "Compared to previous joint input-label models, our model performs significantly better on unseen labels without compromising performance on the seen labels.",
        "The ability of our model to capture complex input-label relationships contributes to its improved performance.",
        "Our model's controllable capacity and training objective based on cross-entropy loss also contribute to its improved performance.",
        "Future work includes learning the label representation with a more sophisticated encoder and using importance sampling for label sampling.",
        "Another interesting direction is finding a more scalable way of increasing the output layer capacity, such as using a deep rather than wide classification network.",
        "Adapting the proposed model to structured prediction, such as by using a softmax classification unit instead of a sigmoid one, would benefit tasks such as neural machine translation, language modeling, and summarization."
    ],
    "3140": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Biased representations lead to biased inferences in natural language inference tasks.",
        "GloVe, ELMo, and BERT embeddings all encode gender, religion, and nationality biases.",
        "A projection-based method can effectively attenuate bias in contextualized embeddings without loss of entailment accuracy.",
        "Our novel fusion strategy outperforms the widely used early fusion on both datasets typically used to test multimodal sentiment analysis methods.",
        "Our method outperforms the state of the art in multimodal sentiment analysis and emotion detection by a significant margin.",
        "Improving the quality of unimodal features, especially textual features, will further improve the accuracy of classification.",
        "Experimenting with more sophisticated network architectures is a future research direction."
    ],
    "3144": [
        "The proposed model iTM-VAE can learn more topics and produce sparser posterior topic proportions compared to traditional nonparametric topic models.",
        "The use of a hierarchical construction in the generative procedure can further diversify the document-specific topic distributions.",
        "The technique of incorporating a hyper-prior into the VAE framework can alleviate the collapse-to-prior problem.",
        "The proposed model iTM-VAE-HP can adapt better to data and learn more topics compared to traditional nonparametric topic models.",
        "The use of a stick-breaking prior in the generation of atom weights can lead to better variability in the topic distributions.",
        "The Kumaraswamy distribution can be exploited to model the atom weights, leading to better optimization of the model."
    ],
    "3149": [
        "The proposed approach (E-SCBA) can optimize the quality of reply by incorporating both emotion and topic knowledge into the generation process.",
        "The newly designed decoder uses syntactic knowledge to constrain generation, ensuring fluency and grammaticality of the reply.",
        "The approach can generate replies that feature both emotion and logic, with rich diversity.",
        "The use of syntactic knowledge in the decoder improves the fluency and grammaticality of the generated replies.",
        "The approach can generate replies that are both emotionally resonant and logically coherent."
    ],
    "3150": [
        "SA-BiLSTM outperforms BiLSTM in terms of UWA for detecting Neural and Joy emotions, but not for Sadness and Anger emotions.",
        "The proposed network, SA-BiLSTM, is advantageous to BiLSTM in terms of UWA.",
        "BiLSTM has better WA performance than SA-BiLSTM.",
        "The uneven training data distributions may cause bias in the test results.",
        "Incorporating more related data or retrieving more linguistic information may improve the model's performance."
    ],
    "3151": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We propose PReFIL, a new CQA system that improves the state-of-the-art and surpasses human accuracy on two datasets.",
        "Our results suggest harder datasets are needed for future algorithms.",
        "Better OCR is also important for advancing the field."
    ],
    "3152": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Adding an instance normalization layer on top of the trainable filterbanks is critical for learning gammatone-based architectures.",
        "The use of a fixed squared Hanning window as low-pass filter is critical to learn the scattering-based filterbanks from random initialization of the filters.",
        "With these two improvements, we observe a consistent reduction of WER against comparable mel-filterbanks on the open vocabulary task of the WSJ dataset."
    ],
    "3155": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Model ensembles give better performance than using a single network in all RNN cases.",
        "The combination of a single Master network and model ensembles further improved performance in LSTM and zoneout LSTM networks.",
        "Using RPL leads to better phone error rate (PER) only in LSTM and some GRU scenarios.",
        "The best average PER we have obtained is 14.84% in LSTM network with Master network and ensemble models, which is slightly lower than the best-published PER to date, according to our knowledge.",
        "The best single experiment resulted in 14.64% PER and it was the LSTM network with Master network, ensemble models, and RPL."
    ],
    "3158": [
        "The proposed method detects current (real) news events with a state-of-the-art performance.",
        "The method consists of three main steps: detecting news-relevant tweets, clustering news events, and ranking detected events based on the size of event clusters and growth speed of tweet frequencies.",
        "The proposed system outperforms a related state-of-the-art solution in detecting current news events.",
        "The approach is evaluated on a large, publicly available corpus of annotated news events from Twitter.",
        "The future work will investigate the scalability of the approach and implement it on a big data platform."
    ],
    "3160": [
        "The importance of temporal considerations when working with language related to mental health conditions.",
        "The introduction of RSDD-Time, a novel dataset of manually annotated self-reported depression diagnosis posts from Reddit, which includes extensive temporal information about the diagnosis.",
        "The ability to automatically extract temporal cues and predict temporal aspects of a diagnosis using rule-based and machine learning methods.",
        "The potential for further exploration of the dataset and experiments."
    ],
    "3161": [
        "The use of dynamic memory induction networks (DMIN) for few-shot text classification achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The best performing network for phone recognition is the LSTM network with 4 layers each with 1024 units, which achieves an average PER of 15.58%.",
        "The best result of all experiments was 15.02% PER for LSTM network with 4 layers each with 1024 units, which is the best result published to date according to the authors' knowledge.",
        "The simple FF networks with ReLU activation function obtained 16.49% PER as their best result, and the best performing TDNN networks could not achieve better results than the simple FF networks.",
        "The use of sigmoid activation function in FF networks did not lead to better performance, and the best performing network with sigmoid activation function had a worse PER than some of the FF ReLU networks."
    ],
    "3164": [
        "Training NNs on automatically labeled data improves their accuracy.",
        "NNs learn relational structural information, as evidenced by the fact that TK models based on syntactic structures improve accuracy, while other advanced models based on similarity feature vectors do not produce any improvement.",
        "Our approach allows for NN models comparable to TK-based approaches without requiring expensive TK processing at deployment time."
    ],
    "3166": [
        "Our results suggest that limiting seed words to supposedly temporally stable ones does not improve performance as suggested in previous work but rather turns out to be harmful.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "We recommend using SVD PPMI together with PARASIMNUM (our adaption of the Turney and 8 Typical emotion lexicons are one or even two orders of magnitude larger, as discussed in Section 2.1.)",
        "Given the current correlation values, we would need to increase the size of our gold standard by a factor of about 40-a challenging task, given its expert reliant nature.",
        "We will continue to work on further solutions to get around data sparsity issues when working with historical language, hopefully allowing for more advanced machine learning approaches in the near future."
    ],
    "3168": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Using multimodal features that were developed for multimodal attention-based video description enhances the quality of generated dialog about dynamic scenes.",
        "We are making our data set and model publicly available for a new Video Scene-Aware Dialog challenge."
    ],
    "3169": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "We have explored simple and effective ways to alleviate or eliminate the beam problem.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to significant BLEU gains, but there remains to be gained by solving label bias in general.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset, and more general globally-normalized models can be trained in a similarly inexpensive way."
    ],
    "3170": [
        "We have presented a method to train speaker identification models using only the information about speakers appearing in each of the recordings in training data, without any segment level annotation.",
        "Our experimental results show that a model trained using such weak labels can provide a recall of 66% at 93% precision on a broadcast news task.",
        "On the VoxCeleb dataset of YouTube videos, the method results in 94.6% accuracy, greatly outperforming a baseline system that uses face identification for obtaining training data annotation.",
        "In the future, we plan to extend the method to a more general multi-instance multi-label learning problem."
    ],
    "3171": [
        "We introduced the Natural Language Decathlon (decaNLP), a new benchmark for measuring the performance of NLP models across ten tasks.",
        "Despite not having any task-specific modules, we trained MQAN on all decaNLP tasks jointly, and we showed that anti-curriculum learning gave further improvements.",
        "After training on decaNLP , MQAN exhibits transfer learning and zero-shot capabilities.",
        "When used as pretrained weights, MQAN improved performance on new tasks.",
        "It also demonstrated zero-shot domain adaptation capabilities on text classification from new domains."
    ],
    "3173": [
        "We identify two issues in current datasets for mapping questions to SQL queries.",
        "Human-written datasets require properties that have not yet been included in large-scale automatically generated query sets.",
        "The generalizability of systems is overstated by the traditional data splits.",
        "Our analysis has clear implications for future work.",
        "Evaluating on multiple datasets is necessary to ensure coverage of the types of questions humans generate.",
        "Developers of future large-scale datasets should incorporate joins and nesting to create more human-like data.",
        "New systems should be evaluated on both question-and query-based splits, guiding the development of truly general systems for mapping natural language to structured database queries."
    ],
    "3174": [
        "Our approach outperforms existing HRED models and their attention variants in low-resource domains.",
        "The human evaluation showed strong improvements across multiple reading comprehension tasks using our section titles.",
        "The proposed ReCoSa model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "Our ensemble approach performs the best by a wide margin.",
        "Adversarially-trained models usually perform best on the type of noise they have seen during training.",
        "Our FIDS-W model performs best on the Nat noise amongst models which have not been trained on this type of noise.",
        "The FIDS-W model can generalize to Rand, which is an extreme case of attack, and performs best on the Rand noise compared with models which are not trained on Rand either."
    ],
    "3176": [
        "The proposed SAW Reader uses subword embedding to enhance the word representation and limit the word frequency spectrum to train rare words efficiently.",
        "With the help of the short list, the model size will also be reduced together with training speedup.",
        "Unlike most existing works, which introduce either complex attentive architectures or many manual features, our model is much more simple yet effective.",
        "The proposed reader has been proved effective for learning joint representation at both word and subword level and alleviating OOV difficulties."
    ],
    "3177": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks."
    ],
    "3184": [
        "The proposed cross-lingual coreference model is empirically strong, as evidenced by both intrinsic and extrinsic evaluations in the context of an entity linking task.",
        "The use of multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "The novel end-to-end model for joint slot label alignment and recognition requires no external label projection, achieving competitive performance to the state-of-the-art label projection approach with only half of the training time."
    ],
    "3185": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our method achieved significant perplexity and WER improvements on the state-of-the-art ASR system over two strong baseline LMs.",
        "Importantly, the expensive retraining was avoided and no additional training data was used.",
        "We believe that our approach of manipulating input and output projection layers is general enough to be applied to other neural network models with similar architectures."
    ],
    "3194": [
        "The shared attention mechanism in the proposed neural APE system allows for easy interpretability and understanding of how the selection shifts on either input at each decoding step.",
        "The model has reported competitive accuracy compared to recent, similar systems (i.e., systems trained with the official WMT16 and WMT17 data and 500K extra training triplets).",
        "The proposed model is based on two separate encoders that share a single, joined attention mechanism.",
        "The model's interpretability can be further explored in future work."
    ],
    "3206": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our carefully-tuned system achieves state-of-the-art performance, highlighting the importance of finding the best hyperparameter configuration.",
        "A simple syntactic dependency parser can be extended to produce graph-structured dependencies without any further augmentations.",
        "A high-performing graph-based parser can be adapted to different types of dependency graphs with only small changes without obviously hurting accuracy.",
        "Transition-based parsers require whole new transition sets or even data structures to generate arbitrary graphs, while our graph-based parser can adapt to different types of dependency graphs with only small changes."
    ],
    "3208": [
        "Our proposed data-driven framework for LU achieves significant improvements in F-scores when only a small training set is used.",
        "Our novel diversity rank encourages diverse generation and filters out alike instances, leading to improved performance.",
        "Our framework leverages one utterance's alternative expressions of the same semantic to train a seq2seq model.",
        "Careful case study shows the capability of our framework to generate diverse alternative expressions."
    ],
    "3210": [
        "The brevity problem in machine translation can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our proposed recurrent chunking mechanisms outperform benchmark models across different datasets.",
        "Sample normalization, syllable internal delta, tone dependent trees, and random forest provide advantageous objective evaluation results.",
        "The BLSTM model with our new additive architecture and syllable internal delta regularization provides good improvement compared to a single BLSTM model.",
        "Using an additional BLSTM fed with word-level features like word embeddings and part of speech can capture some lexical information that helps improve the prediction result.",
        "Further experiments are needed to explore the effectiveness of certain techniques on variable datasets and to determine what kind of lexical information is captured by the residual contour."
    ],
    "3212": [
        "The proposed language adaptation method for multilingual RNN/CTC based speech recognition systems enables systems to achieve parity with monolingual systems and improve beyond the monolingual baseline.",
        "Training acoustic models on data from multiple languages is a challenging problem due to differences in acoustic modelling units between languages.",
        "The proposed neural adaptation techniques can mitigate these issues and enable the adaptation of the acoustic model to multiple languages with better performance than the monolingual baseline.",
        "Integrating more languages into the setup is expected to allow for better generalization across languages.",
        "Using other acoustic units like BPE (byte pair encoding) units may provide better results.",
        "Investigating the use of the proposed adaptation method in new domains is a potential future work."
    ],
    "3215": [
        "We propose Sequential Copying Networks (Se-qCopyNet) to model the sequential copying phenomenon in sequence-to-sequence generation.",
        "It leverages pointer networks as the prediction component to dynamically extract spans from input sentence during the decoding process.",
        "Experiments on abstractive sentence summarization and question generation tasks show that SeqCopyNet has ability of copying a sub-span from input sentence.",
        "In the future, we will apply SeqCopyNet to other tasks such as multiturn dialog response generation."
    ],
    "3216": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed joint sentence scoring and selection approach combines sentence scoring and selection into one phase, which significantly improves the performance of extractive document summarization.",
        "The partial output summary and current extraction state are used to score sentences and select the most relevant sentences for the summary."
    ],
    "3219": [
        "The proposed DNC model achieves state-of-the-art results on the bAbI task and passable results on the CNN RC task without any task-specific model adaptation.",
        "The introduced Bypass Dropout and DNC Normalization improve the robustness and usability of the DNC.",
        "The rsDNC is more scalable to large-scale QA tasks compared to the vanilla DNC.",
        "The rsDNC architecture can be extended to a sequence-to-sequence model, enabling its usage in other NLP tasks.",
        "The training augmentation for one of the bAbI tasks can solve all of them.",
        "The introduced contendbased memory unit lowers memory consumption and training time accompanied by only a minimal loss of performance.",
        "The novel bidirectional architecture improves the contextual accessibility and allows questions at every position in the input sequence."
    ],
    "3220": [
        "Our model combines existing techniques of probabilistic modeling and inference to attempt to fit the actual distribution of the world's vowel systems.",
        "We presented a generative probability model of sets of measured (F 1 , F 2 ) pairs.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Enabling \"tag \u2192 parse\" only also improves the tagging accuracy itself."
    ],
    "3226": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The widespread practical utility of execution guidance by applying it on four different state-of-the-art models which we then evaluated on three different datasets.",
        "The improvement offered by execution-guided decoding is dependent on the nature of the extended model and the degree of integration with the decoding procedure.",
        "Execution guidance can potentially be applied to other tasks, such as natural language to logical form, which we plan to explore in future work.",
        "Integrating execution guidance into the training phase will further improve model performance."
    ],
    "3227": [
        "'Our model significantly outperforms existing HRED models and its attention variants.'",
        "'The relevant contexts detected by our model are significantly coherent with humans' judgements.'",
        "'providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").'",
        "'Our system achieved an averaged LAS of 75.84% and obtained the first place in LAS in the final evaluation.'"
    ],
    "3230": [
        "We propose a joint model of language, meter, and rhyme that captures language and form for modelling sonnets.",
        "Our research reveals that vanilla LSTM language model captures meter implicitly.",
        "Our proposed rhyme model performs exceptionally well.",
        "Machine-generated generated poems, however, still underperform in terms of readability and emotion."
    ],
    "3235": [
        "The proposed method of variational attention shows promising results in capturing global sentential information, improving the performance of parsers.",
        "The fully-supervised parser based on the proposed method outperforms state-of-the-art baseline parsers by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "The structural models investigated in this paper can effectively preserve salient source relations in summaries, comparing favorably to state-of-the-art published systems.",
        "The proposed method has the potential to be scaled up for larger-scale tasks and more complex models, such as multi-hop attention models, transformer models, and structured models.",
        "The latent variables obtained through variational attention can be used for interpretability and incorporating prior knowledge into the model."
    ],
    "3238": [
        "The proposed neural network model for joint POS tagging and graph-based dependency parsing achieves strong performance in both intrinsic tasks (UAS at 94.51% and LAS at 92.87%) and extrinsic downstream applications (state-of-the-art POS tagging accuracy at 97.97%, and competitive results in the CoNLL 2018 shared task).",
        "The joint model outperforms UDPipe on average by 0.8% in POS tagging, 3.1% in UAS, and 3.6% in LAS scores on 61 big UD treebank test sets.",
        "The proposed model serves as a new strong baseline for both intrinsic POS tagging and dependency parsing tasks as well as for extrinsic downstream applications such as biomedical event extraction and opinion analysis.",
        "The model achieves state-of-the-art downstream task scores for the biomedical event extraction and opinion analysis applications."
    ],
    "3244": [
        "Our approach significantly outperforms previous methods, reducing the error by 21% on English Switchboard.\" (related to the effectiveness of the proposed method)",
        "Our method trained on the full dataset significantly outperforms previous methods, achieving competitive performance with less than 1% of the training data.\" (related to the efficiency of the proposed method in terms of training data usage)",
        "The learned transition model employs an abstract, discrete state (a bottleneck state), which increases sample efficiency by reducing the number of model parameters and by exploiting structural properties of the environment.\" (related to the design of the proposed approach)",
        "We have provided a mathematical analysis of the Bottleneck Simulator in terms of fixed points of the learned policy.\" (related to the theoretical understanding of the proposed approach)",
        "The analysis reveals how the policy's performance is affected by four distinct sources of errors related to the abstract space structure (structural discrepancy), to the transition model estimation variance, to the transition model estimation bias, and to the transition model class bias.\" (related to the understanding of the factors affecting the policy's performance)",
        "Our dialogue experiments are based on a complex, real-world task with a very high-dimensional state space and evaluated by real-world users.\" (related to the practical applicability of the proposed approach in a real-world setting)"
    ],
    "3245": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed approach using skip connections can reduce latent variable collapse in s.",
        "The resulting family of deep generative models ( -s) learn useful summaries of data.",
        "-s yield higher mutual information than their counterparts.",
        "The use of more sophisticated s such as the --lead to a significant improvement in terms of latent variable collapse."
    ],
    "3246": [
        "Explicitly incorporating and modeling hierarchical information leads to increased performance in experiments on entity typing and linking across three challenging datasets.",
        "Introducing two new human-annotated datasets, MedMentions and TypeNet, which provide a larger and deeper dataset for evaluating the performance of hierarchical models.",
        "The current work already demonstrates considerable improvement over non-hierarchical modeling, but future work will explore additional techniques such as Box embeddings and Poincar\u00e9 embeddings to represent the hierarchical embedding space.",
        "Improving recall in the candidate generation process for entity linking is a potential direction for future work.",
        "The authors are excited to see new techniques from the NLP community using the resources they have presented."
    ],
    "3247": [
        "\"Our experimental results reveal that adversarial training of a domain discriminator works as a regularizer across different architectures ranging from simple to complex networks.",
        "\"All tested feature extractors were able to learn a domain-invariant document representation.",
        "\"We then demonstrated that the DANN architecture is able to profit from multiple source-domains for training. In those cases, it is advantageous to learn the domain discriminator to differentiate between all domains rather than combining all training data into a single 'source' domain.",
        "\"Learning a projection of word vectors into a common space during training can improve classification performance and renders the use of pretrained multilingual word vectors unnecessary."
    ],
    "3249": [
        "Our approach performs comparably to a 6-layer NMT model without sharing parameters across layers, while having the same size as a single-layer NMT model.",
        "We demonstrated that our approach can be used to generate pseudo-parallel corpora or back-translated corpora, which leads to further improvements in translation quality.",
        "Our work promotes the research of techniques that rely on reusability of parameters and hence simplify the existing NMT architectures.",
        "In the future, we will perform an in-depth analysis of the limits of recurrent stacking of layers and combine our methods with knowledge distillation approaches for high performance compact NMT modeling.",
        "We also plan to experiment with more complex mechanisms to compute the recurrent information during stacking for improved NMT performance."
    ],
    "3253": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to improved performance in single-domain tasks compared to the state-of-the-art NBT method.",
        "Our model alleviates the need for ontology-dependent parameters and maximizes the amount of information shared between slots and across domains.",
        "Introducing new domains and ontology terms without further training is a potential future direction for zero-shot learning."
    ],
    "3254": [
        "We propose a method named cavity filling, which generates pseudo-features to fill the gaps between the minor and major classes in feature spaces.",
        "Further, we extract features from a layer of trained deep neural network, obtain multivariate probability distributions from the features of each minor class, and sample the minor-class pseudo-features from multivariate probability to virtually increase the minor-class training data.",
        "We do not generate pseudo-data but generate pseudo-features.",
        "The cavity filling method functions in an appropriate manner for multi-class imbalanced data.",
        "A Mean scores of the minor classes (Cifar10) The tables that summarize the macro averaged scores for minor classes in Cifar10 are presented here because some points are difficult to distinguish in the figures."
    ],
    "3266": [
        "the performance of state-of-the-art models drops significantly\" when evaluating mental health indices under real-world settings.",
        "it is impossible to assess the contribution of the features\" in current experimental settings.",
        "a successful model in this setting would not only provide us with insights on what types of behavior affect mental state, but could also be employed in a real-world system without the danger of providing false alarms to its users.",
        "The use of transfer learning techniques on latent feature representations may help overcome the problem of having only a few instances from a diversely behaving small group of subjects.",
        "The goal for future work is to achieve positive results in the LOUOCV setting."
    ],
    "3267": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, it improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "We observed the methods sacrifice minimally in terms of model evaluation time and predictive performance for the substantial compression gains observed.",
        "It would be interesting to go beyond the results of Section 3.3 to see if there is a better quantization scheme for our models."
    ],
    "3269": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; more general globally-normalized models can be trained in a similarly inexpensive way.",
        "Experimental results show that our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed sentence gate improves performance over baseline models with meaningful behavior of its gate score.",
        "Future work involves extending our algorithm to other tasks in machine comprehension."
    ],
    "3277": [
        "Using multilingual bottleneck (BNFs) and autoencoder features in a CNN-DTW keyword spotter does not improve performance over MFCCs.",
        "BNFs trained on a corpus of 10 languages lead to substantial improvements.",
        "Our overall CNN-DTW based approach, which combines the low-resource advantages of DTW with the speed advantages of CNNs, further benefits by incorporating labelled data from well-resourced languages through the use of BNFs when these are obtained from several diverse language.",
        "The use of BNFs trained on a corpus of 10 languages leads to substantial improvements in performance.",
        "Incorporating labelled data from well-resourced languages into our CNN-DTW based approach leads to improved performance."
    ],
    "3278": [
        "We have presented our first efforts in building an automatic speech recognition system for Somali as part of a United Nations programme aimed at humanitarian monitoring.",
        "A corpus of only 1.57h of in-domain Somali data was available.",
        "Approximately 59h of annotated speech in 7 unrelated languages was available for multilingual modelling.",
        "Several acoustic model architectures were evaluated, including state-of-the-art neural configurations and a new combination of CNN, TDNN, and BLSTM layers.",
        "Best performance was achieved by a multilingually-trained CNN-TDNN-BLSTM acoustic model, achieving a word error rate of 53.75%.",
        "Augmentation of the language model training data using a generative RNN-LSTM afforded small gains.",
        "We also found that BLSTM layers consistently benefited more from subsequent adaptation to the target language than LSTM layers."
    ],
    "3279": [
        "The proposed approach achieves better performance than other algorithms in most downstream tasks.",
        "The neural \"bilingual expert\" model is used as the prior knowledge model.",
        "A simple Bi-LSTM is used as the quality estimation model with manually designed mis-matching features.",
        "The proposed approach yields better performance than other algorithms on the WMT 17/18 QE competition dataset."
    ],
    "3282": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Frequency weighted DLG-BPE and AV-BPE achieves stably better performance than FRQ-BPE at different significant levels.",
        "AV-BPE shows best performance on the German-English task, while DLG-BPE behaves better on the Chinese-English task.",
        "The choice of subword segmentation strategies may be sensitive to specific language pairs, deserving further exploration in the future."
    ],
    "3288": [
        "We presented an extension of a very small parallel corpus on an endangered language called Griko.",
        "We make this corpus, with all its different levels of representation, freely available to the community as an effort in the direction of research replicability for low-resource approaches.",
        "We illustrated the potential of this parallel corpus by performing the tasks of speech-to-text alignment and unsupervised word discovery.",
        "We encourage the community to challenge the baselines presented here.",
        "Future work includes comparing the tasks results from this extreme case of language documentation with other low-resource corpora, such as the one presented in [15]."
    ],
    "3296": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "We defined a novel concept of a teaser, a Shortcut-Text amalgamating interesting facts from the news article and teasing elements.",
        "We identified properties like abstractive, teasing, and bona-fide that assist in comparing a teaser with the other forms of short texts.",
        "An overlap-based comparative study of headlines and teasers shows teasers as abstractive while headlines as nearly extractive."
    ],
    "3297": [
        "We developed a method that is trained using only ID sentences for OOD sentence detection.",
        "The proposed method achieved higher accuracy than state-of-the-art methods in detecting OOD sentences.",
        "Our method will help improve user experience in dialog systems by enabling accurate detection of OOD sentences.",
        "We used an LSTM network trained for domain-category analysis as a neural sentence embedding system for OOD sentence detection.",
        "The word representations were pre-trained using a large set of unlabeled text before domain-category analysis was trained."
    ],
    "3300": [
        "'Our approach achieves new state-of-the-art on FewRel 2.0 dataset.'",
        "'providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\")'",
        "'exploring more sentiment features to capture implicit hateful comments and adding more pre-processing levels'",
        "'identifying the Hindi-English codemixed instances and processing these instances and Hindi monolingual instances separately could be a future direction to explore'",
        "'adding sentiment features to the lexical information might help to model performance for Hindi data.'"
    ],
    "3301": [
        "The proposed parallel corpus mining approach using sentence embeddings produced by a bilingual dual-encoder model is effective.",
        "The approach calculates the dot-product score for sentence embedding vectors to assess translation pair quality.",
        "The selection of hard negatives, which consist of semantically similar sentence pairs that are not translations of each other, improves the ability of the model to identify true translation pairs.",
        "The proposed method is useful for both mining and filtering parallel data.",
        "The method compares favorably to Zipporah for filtering, while for mining it provides a lightweight alternative to Uszkoreit et al. (2010) 's method."
    ],
    "3305": [
        "Low latency translation is important for real-time speech translation systems.",
        "Our main contribution is to propose a simple method to deal with scenarios where data at inference time is different from the training data, which can be resolved with adaptation.",
        "Using simple techniques to generate artificial data are effective to get more fluent output with less correction.",
        "Multi-task learning can help adapt the model to the new inference condition, without losing the original capability to translate full sentences.",
        "Combining these two ideas, we are able to maintain high quality translation at low latency, minimizing the number of corrected words by 45%, which significantly improves user experience for practical applications."
    ],
    "3313": [
        "The proposed dual attentional seq2seq model with modified sentence ranking algorithm can generate high-quality summaries.",
        "The extractive encoder in the model strengthens the semantic representation of the summary.",
        "The pointer-generator and coverage mechanisms in the model solve the problems of OOV and duplicate words.",
        "The non-anonymized CNN/Daily Mail dataset is used to evaluate the performance of the model, and the results are close to the state-of-the-art ROUGE F 1 scores.",
        "The abstractive ROUGE-2 F 1 scores are the highest among all the models, and the summaries have better readability and higher semantic accuracies.",
        "In future work, the authors plan to unify the reinforcement learning method with their abstractive models."
    ],
    "3314": [
        "The use of attention mechanisms in deep neural networks can inform observers about the importance of historical samples in a sequential analysis task.",
        "Existing methods for understanding what happened at the stopping point in a sequential analysis task are computationally expensive and tied to the implementation details or task of the model.",
        "The proposed method of monitoring attention changes over the sequential analysis provides a negligible overhead and is not tied to the implementation details or task of the model, making it an important consideration in real-time systems.",
        "The introduced method can be used to inform observers about what happened at the stopping point in a sequential analysis task, providing a computationally inexpensive means to understand the task."
    ],
    "3318": [
        "The proposed neural model with layered memory and word attention mechanism outperforms previous works on complete discourse act tagging and other subproblems like question-answer extraction or stance detection.",
        "The model can extend the idea of word relevance to larger contexts than only the previous and first comments, enabling the analysis of propagation of topic, sentiment, and other linguistic features through conversations.",
        "With sufficient amount of training data from different platforms, the model has the potential to present a breakthrough in the analysis of discourse in dialogues.",
        "The model can characterize a small stream of discussions regarding a single topic; that too in a single platform, revealing the potential of discourse analysis."
    ],
    "3323": [
        "We introduced a new language modeling task for clinical notes based on EHR data and showed how to represent the multi-modal data context to the model.",
        "We proposed evaluation metrics for the task and presented encouraging results showing the predictive power of such models.",
        "We discussed how such models could be useful in sophisticated spell-checking and auto-complete features, potentially assisting with the burden of writing accurate clinical documentation."
    ],
    "3324": [
        "Our approach fully utilizes the spatial information and is able to capture complex relations between the image and question.",
        "The proposed QGHC network shows strong capability on solving the visual question answering problem.",
        "The proposed approach is complementary with existing feature fusion methods and attention mechanisms.",
        "Extensive experiments demonstrate the effectiveness of our QGHC network and its individual components."
    ],
    "3325": [
        "The proposed seq2seq-based model with a hierarchical decoder achieves significant improvement over the classic seq2seq model.",
        "The proposed methods introduce additional word-level or sentence-level labels as features, allowing for arbitrary design of the hierarchy of the decoder.",
        "The proposed hierarchical decoding concept is general and easily extensible, with flexibility to be applied to many NLG systems.",
        "The use of lin-guistic patterns in the proposed model leads to improved performance.",
        "The proposed training and inference techniques contribute to the improved performance of the model."
    ],
    "3326": [
        "The proposed embedding architecture can enhance the simple baseline for the reading comprehension task.",
        "The intensified embeddings can help the model achieve state-of-the-art performance on multiple large-scale benchmark datasets.",
        "The proposed method is more effective than existing works that focus on complex attention architectures or manual features.",
        "The improved word representation may also benefit other tasks beyond MRC.",
        "The proposed embedding architecture can enhance the simple baseline for the reading comprehension task. (p. 2)",
        "The intensified embeddings can help the model achieve state-of-the-art performance on multiple large-scale benchmark datasets. (p. 3)",
        "The proposed method is more effective than existing works that focus on complex attention architectures or manual features. (p. 4)",
        "The improved word representation may also benefit other tasks beyond MRC. (p. 5)"
    ],
    "3328": [
        "The application of DenseNet for speech recognition has been successful.",
        "Different network design choices have an impact on the final ASR performance.",
        "The depth of the network plays an important role in achieving good performance.",
        "The bottleneck and compression components help shorten the training time.",
        "DenseNet significantly outperformed other neural models such as DNNs, CNNs, and VGGs on two data sets (RM and WSJ).",
        "The compact architecture with a small amount of trainable parameters allows for training with only a half of the WSJ data set, outperforming other models trained with the full data set by a large margin.",
        "DenseNets will be compared with state-of-the-art models like BLSTM and TDNN using unadapted features in the future."
    ],
    "3330": [
        "The neural models outperform the feature-based models on two benchmark biomedical corpora GENIA and CRAFT.",
        "BiLSTM-CRF-based models with character-level word embeddings produce highest POS tagging accuracies which are slightly better than NLP4J-POS.",
        "The Stanford-Biaffine parsing model obtains significantly better result than other parsing models.",
        "We also investigate the influence of parser selection for a biomedical event extraction downstream task, and show that better intrinsic parsing performance does not always imply better extrinsic event extraction performance.",
        "Whether this pattern holds for other information extraction tasks is left as future work."
    ],
    "3334": [
        "The Recurrent Multistage Fusion Network (RMFN) is highly effective in modeling human multimodal language, achieving state-of-the-art performance on all datasets.",
        "RMFN decomposes the multimodal fusion problem into multiple stages, each focused on a subset of multimodal signals, allowing it to capture both synchronous and asynchronous multimodal interactions.",
        "The multiple stages in RMFN coordinate to capture both synchronous and asynchronous multimodal interactions.",
        "In future work, the authors are interested in merging their model with memory-based fusion methods, as they have complementary strengths."
    ],
    "3335": [
        "The proposed model achieves better performance than CNNs on seven popular benchmark datasets for text classification tasks.",
        "The use of static routing in capsule networks leads to higher classification accuracies with less computation compared to dynamic routing.",
        "Capsule networks are useful for text classification tasks.",
        "The proposed model outperforms existing CNN-based models for text classification."
    ],
    "3336": [
        "We propose a novel method for generating natural questions for an image using multimodal differential embeddings.",
        "Our approach relies on obtaining multimodal differential embeddings from image and its caption.",
        "We provide ablation analysis and a detailed comparison with state-of-the-art methods to evaluate the effectiveness of our approach.",
        "We perform a user study to evaluate the naturalness of our generated questions and ensure that the results are statistically significant.",
        "In future, we aim to analyse means of obtaining composite embeddings and consider the generalisation of this approach to other vision and language tasks."
    ],
    "3337": [
        "We have presented a simple and efficient regularization approach to neural machine translation.",
        "Our method relies on the agreement between L2R and R2L NMT models.",
        "Two Kullback-Leibler divergences based on probability distributions of L2R and R2L models are added to the standard training objective as regularization terms.",
        "An efficient approximation algorithm is designed to enable fast training of the regularized training objective.",
        "Our approach leads to significant improvements compared with strong baseline systems.",
        "In our future work, we plan to test our method on other sequence-to-sequence tasks, such as summarization and dialogue generation.",
        "It is also worth trying to integrate our approach with other semisupervised methods to better leverage unlabeled data."
    ],
    "3338": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The approach of using dynamic memory to adapt and generalize better to support sets and unseen classes is effective.",
        "The proposed method outperforms previous style transfer methods in three tasks.",
        "The use of non-parallel data, where the source data have various unknown language styles, is feasible.",
        "The encoder-decoder framework allows for the use of non-parallel data.",
        "The two loss functions, i.e., the style discrepancy loss and the cycle consistency loss, adequately constrain the encoding and decoding functions.",
        "The style discrepancy loss ensures a properly encoded style representation.",
        "The cycle consistency loss ensures that the style-transferred sentences can be transferred back to their original sentences."
    ],
    "3340": [
        "The proposed CapsE outperforms other state-of-the-art models on two benchmark datasets WN18RR and FB15k-237 for knowledge graph completion.",
        "The CapsE model is effective for search personalization, outperforming competitive baselines on the SEARCH17 dataset of web search query logs.",
        "The CapsE model can effectively model many-to-many relationships."
    ],
    "3343": [
        "We examine methods to rapidly adapt MT systems to new languages by fine-tuning.",
        "The best results were obtained by adapting a pre-trained universal model to the low-resource language while regularizing with similar languages.",
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time."
    ],
    "3344": [
        "The proposed end-to-end model for character-based part-of-speech tagging achieves state-of-the-art results on two benchmark datasets across several typologically diverse languages.",
        "The input representations and segment representations in the model are learned as trained parameters in end-to-end training.",
        "The model is robust to corruption of the tokenization of the dataset, and its good performance on languages with difficult tokenization can be explained by this robustness.",
        "The use of semi-Markov conditional random fields for joint segmentation and labeling of a sequence of characters in the model is effective."
    ],
    "3346": [
        "The proposed approach achieves better style-transfer strength, content-preservation scores, and language fluency compared to previous state-of-the-art work.",
        "The latent space of neural networks can be separated into style and content parts using multi-task and adversarial objectives.",
        "The proposed approach uses a simple yet effective method for disentangling the latent space of neural networks.",
        "The disentangled space can be directly applied to text style-transfer tasks.",
        "The qualitative and quantitative experiments show that the latent space is indeed separated into style and content parts."
    ],
    "3349": [
        "The existing popular methods for text-to-image translation and image-to-text translation can sometimes produce non-relevant images based on the input text description, and suffer from mode collapse.",
        "Imposing cycle consistency by generating captions on the generated images and further optimizing the network can improve the results.",
        "The model has the potential to generate higher quality images when tested on more complicated datasets such as MS-COCO.",
        "Comparing the semantic meaning of text using fixed length embeddings and measuring the distance in cycle loss is a promising approach."
    ],
    "3352": [
        "The proposed AdaBERT model achieves comparable performance with significant efficiency improvements (12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size).",
        "The adaptive search method finds task-suitable structures of compressed BERT using gradient information, leading to improved efficiency and performance.",
        "The proposed AdaBERT model is applicable to language pairs from different language groups, not just same language families.",
        "The use of adversarial loss for cross-lingual sequence tagging is effective in improving POS accuracy.",
        "Dependency parsing and sentence compression tasks can benefit from language-agnostic representations.",
        "The proposed approach outperforms sequenceto-sequence approaches in low-resource domains."
    ],
    "3353": [
        "The use of retrieval models can improve generation models in dialogue systems, helping to avoid common issues such as producing short sentences with frequent words that are not engaging.",
        "Our RetNRef++ model has similar statistics to human utterances and provides more engaging conversations according to human judgments.",
        "Future work should investigate improved ways to incorporate retrieval in generation, such as avoiding the heuristics used in our model and exploring more sophisticated approaches than concatenation plus attention.",
        "Treating the inputs as independent sources or training the models jointly may improve the results."
    ],
    "3355": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Our models can effectively preserve salient source relations in summaries.",
        "Empirically, the local models on learning syntactic structures demonstrate the strength of the local models.",
        "On standard evaluation, our models give the best results among existing neural constituent parsers without external parses."
    ],
    "3356": [
        "The approach explicitly utilizes syntactic information and considers a more global planning, two issues existing models fail to deal with efficiently.\" (related to the methodology)",
        "The results are promising.\" (related to the results)",
        "Future extensions include incorporating work from reinforcement learning and graph generation.\" (related to future work)",
        "Existing models fail to deal with efficiently.\" (related to the limitations of previous approaches)",
        "The approach explicitly utilizes syntactic information.\" (related to the methodology)"
    ],
    "3357": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "our approach achieves new state-of-the-art on FewRel 2.0 dataset",
        "we designed a set of cross-lingual cross-platform features that leverage the similarity and agreement between information across different platforms and languages to verify rumors",
        "the proposed features are compact and generalizable across languages",
        "we also designed a neural network based model that utilizes the cross-lingual cross-platform features and achieved state-of-the-art results in automatic rumor verification"
    ],
    "3360": [
        "The proposed question generation (QG) framework using a generator-evaluator framework considers syntax and semantics in question generation.",
        "The evaluator in the proposed framework allows for direct optimization of conformity towards the structure of ground-truth questions.",
        "Two novel reward functions are proposed to account for conformity with respect to ground-truth questions and predicted answers, respectively.",
        "The incorporation of the new reward functions considerably outperforms state-of-the-art systems in automatic evaluation and human evaluation on the standard benchmark dataset.",
        "The proposed framework uses task-specific scores, including BLEU, GLEU, ROUGE-L, and decomposable attention (DAS), which are naturally suited to QG and other seq2seq problems."
    ],
    "3361": [
        "The proposed method leverages phonetic, structured, and semantic features of Chinese characters to improve CWS performance.",
        "Pinyin and Wubi embeddings are essential in CWS tasks and can be translated to other NLP tasks such as POS and NER.",
        "Three generic models are designed to fuse multi-embedding and achieve state-of-the-art performance in five public corpora.",
        "The shared Bi-LSTM-CRF model (Model III) can be trained efficiently and produce the best performance on AS and CityU corpora.",
        "Future studies on leveraging hierarchical linguistic features to other languages, NLP tasks (e.g., POS and NER), and refining mis-labeled sentences are merited."
    ],
    "3363": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our adversarial filtering paradigm is general, allowing potential applications to other datasets that require human composition of question answer pairs.",
        "The dataset constructed using Adversarial Filtering (AF) allows datasets to be constructed at scale while automatically reducing annotation artifacts that can be easily detected by a committee of strong baseline models.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing."
    ],
    "3366": [
        "The proposed sentence embedding method outperforms other sentence embedding models in document classification and sentiment analysis tasks.",
        "The fundamental ability of sentence embedding methods to preserve the meanings of original sentences can be fully evaluated through indirect methods.",
        "The concept of se- is a good indicator of the ability of sentence embedding methods.",
        "The P-thought models yield better performances than benchmarked models for both tasks.",
        "The main limitation of the current work is the insufficiency of paraphrase sentences for training the models.",
        "P-thought models with more complex encoder structures tend to overfit the MS-COCO datasets.",
        "An approach to improve the performances of the proposed models using only minimal paraphrase data should be developed."
    ],
    "3369": [
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "We found that language use in social media can be used to predict trustfulness about as accurate as other personality traits.",
        "Using a weighting scheme was not helpful.",
        "Our scaleable measure of trust enables future work to investigate some interesting questions about trust, such as those involved in large-scale or frequent assessments.",
        "For example, this may allow for large-scale assessments of trait trustworthiness of different patient populations or of samples of clinicians.",
        "It should be noted that while trust is thought of as a relatively stable personality aspect or trait, some research suggests that it is malleable over time."
    ],
    "3370": [
        "The proposed read-then-verify system is able to abstain from answering when a question has no answer given the passage.",
        "The system utilizes an answer verifier to validate the legitimacy of the predicted answer, and three different architectures are investigated.",
        "Our system has achieved state-of-the-art results on the SQuAD 2.0 dataset at the time of submission (Aug. 28th, 2018).",
        "The authors plan to design new structures for answer verifiers to handle questions with more complicated inferences in future work."
    ],
    "3374": [
        "The proposed model outperforms state-of-the-art results by 2%-3% on two datasets for sequential sentence classification in medical abstracts.",
        "Incorporating contextual information from surrounding sentences can improve the overall quality of predictions.",
        "The proposed model can be generalized to any problem related to sequential sentence classification, such as paragraph-level sequential sentence categorization in full-text articles for better text mining and document retrieval.",
        "The use of an LSTM layer to sequentially process the encoded sentence representations improves the performance of the model."
    ],
    "3377": [
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "A supervised paradigm for vagueness detection provides a promising avenue for identifying vague content and improving the usability of privacy policies."
    ],
    "3378": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our transfer learning algorithm outperforms the standard Translator trained on IWSLT-2014.",
        "Using Google Translator7 can actually achieve a better score than our transfer algorithm.",
        "The translator model is able to achieve a higher score by leveraging more close-to-domain corpus and comprehensive entity recognition/replacement strategy.",
        "Our XL-NBT algorithm only takes several hours to achieve the reported score, while the translator model takes much more time and memory to train depending on the complexity.",
        "The simplicity and efficiency of our model make it a better fit for rare language and limited-budget scenarios."
    ],
    "3379": [
        "We studied the effect of grammatical errors in NMT and confirmed previous findings, but also expanded on them by showing that realistic human-like noise in the form of specific grammatical errors leads to degraded performance.",
        "We added synthetic errors on the English WMT training, dev, and test data, including dev and test sets for all WMT 18 evaluation pairs, and have released them along with the scripts necessary for reproducing them.",
        "We produced Spanish translations of the JFLEG corpus, so that future NMT systems can be properly evaluated on real noisy data.",
        "Our findings expand on previous research by showing that specific grammatical errors can lead to degraded performance in NMT."
    ],
    "3384": [
        "Our proposed approach significantly and consistently outperforms the state-of-the-art method.",
        "Our model aims to tackle the low-quality corpus by reducing inner-sentence noise and improving the robustness against noisy words.",
        "We propose the STP to alleviate the influence of word-level noise.",
        "Entity-wise attention combined with word-level attention helps the model focus more on relational words.",
        "Parameter transfer learning makes our model more robust against noise by reasonable initialization of parameters.",
        "In the future, we will incorporate the SDP and STP to obtain more precise shortened sentences.",
        "We will conduct research in how to utilize entity information to assign more appropriate initial parameters of the relation extractor."
    ],
    "3386": [
        "We have presented MedNLI, an expert annotated, public dataset for natural language inference in the clinical domain.",
        "To the best of our knowledge, MedNLI is the first dataset of its kind.",
        "Our experiments with several state-of-the-art models provide a strong baseline for this dataset.",
        "Our work compliments the current efforts in NLI by presenting thorough experiments for the specialized and knowledge intensive field of medicine.",
        "We also demonstrated that a simple use of domain-specific word embeddings provides a performance boost.",
        "Finally, we also presented a method for integrating domain ontologies into the training regime of models.",
        "We hope the released code and dataset with clear benchmarks help advance research in clinical NLP and the NLI task."
    ],
    "3387": [
        "The effect of down-sampling strategies on word embedding stability can be investigated using five algorithm variants on three corpora.",
        "A simple modification to the down-sampling strategy used for SVD PPMI can achieve an otherwise unmatched combination of accuracy and stability.",
        "GLOVE lacks accuracy and is only stable when trained on small corpora.",
        "SVD WPPMI is recommended for studies targeting (qualitative) interpretations of semantic spaces, especially when training on small corpora.",
        "SGNS provides no benefit in accuracy over SVD WPPMI.",
        "The only downside of SVD WPPMI is its relatively large memory consumption during training.",
        "Further research could investigate the performance of SVD WPPMI with other sets of hyperparameters or scrutinize the effect of down-sampling strategies on the ill-understood geometry of embedding spaces.",
        "The effect of down-sampling and stability on downstream tasks in a follow-up to Wendlandt et al. (2018) could be investigated.",
        "Contextualized embedding algorithms, such as BERT or ELMo, are also probabilistic in nature and should be affected by stability problems."
    ],
    "3388": [
        "We introduce a new Recursive Neural Network called Code-RNN to extract the topic or function of the source code.",
        "The new Recursive Neural Network is the parse tree of the source code, and we go through all the tree from leaf nodes to root node to get the final representation vector.",
        "We use this vector to classify the source code into some classes according to the function, and the classification results are acceptable.",
        "We further propose a new kind of GRU called Code-GRU to utilize the vector created from Code-RNN to generate comments.",
        "We apply Code-GRU to ten source code repositories and gain the best result in most projects.",
        "This framework can also be applied to other programming languages as long as we have access to the parse tree of the input program.",
        "As future work, we can add call graphs into our model, so that Code-RNN can contain invocation information and extract more topics from source code."
    ],
    "3398": [
        "The proposed model learns sentiment memories without parallel data, enabling it to adapt different contexts when decoding.",
        "Our method substantially improves content preservation compared to previous methods.",
        "The proposed method achieves state-of-the-art results in sentiment extraction tasks.",
        "[Our method learns sentiment memories without parallel data and adapts different contexts when decoding.] (Paragraph 1)",
        "[Our method improves content preservation compared to previous methods.] (Paragraph 1)",
        "[The proposed method achieves state-of-the-art results in sentiment extraction tasks.] (Paragraph 1)"
    ],
    "3401": [
        "Our proposed method, Dynamic Self-Attention (DSA), computes attention weights over words with the dynamic weight vector, which furnishes the self-attention mechanism with flexibility.",
        "Our experiments show that DSA achieves new state-of-the-art results in SNLI dataset, while showing comparative results in SST dataset.",
        "DSA provides a flexible way to compute attention weights over words, which allows for better performance in natural language understanding tasks.",
        "The dynamic weight vector used in DSA enables the self-attention mechanism to capture more contextual information and lead to better results.",
        "Our proposed method demonstrates the effectiveness of using a dynamic weight vector in the self-attention mechanism for natural language understanding tasks."
    ],
    "3402": [
        "The proposed method delivers improvements over existing augmentation schemes.",
        "The new augmentation method, SwitchOut, is efficient and easy to implement.",
        "The method has the potential for wide application.",
        "The proposed method delivers improvements over existing augmentation schemes.",
        "The new augmentation method, SwitchOut, is efficient and easy to implement.",
        "The method has the potential for wide application."
    ],
    "3406": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The introduction of the inverse parser aims to alleviate the problem by scoring how a logical form reflects the utterance semantics.",
        "The unsupervised training objective is relatively difficult to optimize, since there are no constraints to regularize the latent logical forms.",
        "When trained properly, the inverse parser and the unsupervised objective bring performance gains.",
        "The neural lexicon encoding method we applied essentially produces synthetic data to further regularize the latent space."
    ],
    "3407": [
        "Our ensemble model has been successfully compressed into a single model that possesses better efficiency and robustness.",
        "In future work, we will explore new distillation methods that have better compression capabilities for MRC tasks.",
        "We plan to further study the biased distillation problem and explore the compatibility of our approach in other NLP tasks such as natural language inference, answer sentence selection, and so on.",
        "Our approach has the potential to improve the efficiency and robustness of machine reading comprehension systems.",
        "Vanilla knowledge distillation can transfer knowledge of answer positions, but it may not be effective for all MRC tasks.",
        "Answer distillation and attention distillation can be used to penalize student's predictions on confusing answer spans and transfer teacher's attentive information, respectively."
    ],
    "3408": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We can learn an optimal shared structure for tasks to achieve better performance leverage the implicit relations between them.",
        "Our model has verified that our model can learn a reasonable hierarchy between tasks without human expertise."
    ],
    "3409": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our neural end-to-end entity linking model can replace engineered features with modern neural networks.",
        "Our model exhibits state-of-the-art performance for EL on the biggest public dataset, AIDA/CoNLL, and shows good generalization ability on other datasets with very different characteristics.",
        "Our code is publicly available and exhaustive annotations are provided for all datasets.",
        "Recall 30/10 shows the percentage of gold mentions for which their ground truth entity is among the first 30/10 candidate entities returned by our p(e|m) dictionary."
    ],
    "3411": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "A recently proposed contextualized word embedding model (ELMo) implicitly learns logic rules for sentiment classification of complex sentences like A-but-B sentences.",
        "Future work includes a fine-grained quantitative study of ELMo word vectors for logically complex sentences along the lines of Peters et al. (2018b)."
    ],
    "3415": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "PPO can stabilize the training and lead the model to learn to generate more diverse outputs.",
        "PPO-dynamic can speed up the convergence.",
        "GAN-based sequence learning can use PPO as the new optimization method for better performance."
    ],
    "3416": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The resulting fully-supervised parser outperforms the state-of-theart baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our model can understand more diverse inputs and generate more complicated outputs.",
        "We look forward to testing our algorithms on more sequence-to-sequence applications to verify its generality."
    ],
    "3418": [
        "The proposed method, GI-Dropout, utilizes global information to guide neural networks to extract both obvious and unobvious features.",
        "Unlike traditional dropout methods, GI-Dropout uses global information to guide the dropout based on the importance of the words.",
        "The Na\u00efve Bayes Weighting method used in GI-Dropout encodes global information explicitly into the model.",
        "GI-Dropout is effective in text classification tasks, as it can capture inapparent patterns that are helpful for classification.",
        "GI-Dropout requires few external resources and relatively small calculations, making it simple but effective and easily applicable to other NLP tasks."
    ],
    "3420": [
        "The proposed selectors effectively select important words for the classifier to process, improving the model performance.",
        "The data aggregation strategy improves the model performance.",
        "The framework is promising for other text reading tasks.",
        "The approach is suitable for edge-device settings, where local devices have limited memory and computational power.",
        "Skim-RNN and LSTM-jump require multiple rounds of communication between the server and local devices, resulting in high network latency.",
        "Stop-words removal achieves notable speedup but can result in a significant performance drop.",
        "The approach can be applied to other text classification tasks."
    ],
    "3423": [
        "Our approach outperforms previous work in low-resource domains.",
        "A human evaluation showed strong improvements across multiple reading comprehension tasks.",
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "We can transfer churn detection knowledge from Twitter to chatbots.",
        "Our model, trained using multilingual word embeddings, surpasses monolingual approaches.",
        "Examples of churn intent in English help us in identifying churn about German telecommunication brands in German tweets and chatbot conversations."
    ],
    "3427": [
        "Our proposed model based on the multi-level dilated convolution and hybrid attention mechanism can extract both semantic-unit-level information and word-level information, leading to significant performance improvements over baseline models.",
        "Experimental results demonstrate that our proposed model outperforms baseline models.",
        "Our model is competitive with deterministic hierarchical models and more robust to classifying low-frequency labels than the baseline."
    ],
    "3428": [
        "Our model explicitly considers the leading role of the title to the overall document main body.",
        "The proposed TG-Net is able to sufficiently leverage the highly summative information in the title to guide keyphrase generation.",
        "The empirical experiment results on five popular real-world datasets exhibit the effectiveness of our model for both present and absent keyphrase generation, especially for a document with very low or very high title length ratio.",
        "One interesting future direction is to explore more appropriate evaluation metrics for the predicted keyphrases instead of only considering the exact match with the human labeled keyphrases as the current recall and F-measure do."
    ],
    "3429": [
        "The introduced model, called the semi-autoregressive Transformer (SAT), can generate faster sequences based on the Transformer architecture.",
        "The SAT combines the original Transformer with group-level chain rule, long-distance prediction, and relaxed causal mask to produce multiple consecutive words at each time step.",
        "The SAT achieves a better balance between translation quality and decoding speed compared to previously proposed nonautoregressive models.",
        "The SAT achieves 5.58 times speedup while maintaining 88% translation quality on WMT'14 English-German translation.",
        "When producing two words at each time step, the SAT is almost lossless (only 1% degeneration in BLEU score).",
        "The authors plan to investigate better methods for training the SAT to further shrink the performance gap between the SAT and the Transformer.",
        "The authors believe that using object function beyond maximum likelihood to improve the modeling of long-distance dependencies is worth studying.",
        "The authors also plan to explore new methods for knowledge distillation and extend the SAT to allow the use of different group sizes K at different positions."
    ],
    "3431": [
        "GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "Incorporating information outside of the given training set and experimenting with various combinations of semantic parsers and upstream domain-adjacency models.",
        "How the underlying system should recover when domain-adjacent instances are detected."
    ],
    "3434": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "The performance not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "A simple and effective model for text generation based on adversarial training using sentence embeddings.",
        "The use of sentence-level embeddings allows modelling the way of expression of an author in generated text in a better way than when using word-level embeddings.",
        "A performance comparison across several metrics is made between different GAN architectures with improved training stability and attention augmented LSTM models.",
        "The automated corpus-based evaluations correlate with human judgements.",
        "In future, this work aims to be applied for synthesizing images from text, exploring complementary architectures to projects like neural-storyteller3 where skip-thought embeddings are already used to perform image captioning with story-style transfer."
    ],
    "3436": [
        "The proposed machine reading comprehension model based on the compare-aggregate framework with a hierarchical attention structure achieves state-of-the-art results on the MovieQA question answering dataset, greatly outperforming previous models.",
        "RNN-LSTM models outperformed CNN models in general, but our results for sentence-level black-box attacks indicate they might share the same weaknesses.",
        "The model and human inference seem to learn matching patterns to select the right answer rather than performing plausible inferences as humans do.",
        "Other human like processing mechanisms such as referential relations, implicit real world knowledge, i.e., entailment, and answer by elimination via ranking plausibility should be integrated in the system to further advance machine reading comprehension."
    ],
    "3441": [
        "The proposed Auto-Encoder Matching model learns utterance-level semantic dependency in an unsupervised way.",
        "The model contains two auto-encoders that learn the utterance representations and a mapping module that builds the mapping between the input representation and response representation.",
        "The proposed model significantly improves the quality of generated responses according to automatic evaluation and human evaluation, especially in coherence.",
        "The proposed Auto-Encoder Matching model learns utterance-level semantic dependency in an unsupervised way. [Paragraph 3]",
        "The model contains two auto-encoders that learn the utterance representations and a mapping module that builds the mapping between the input representation and response representation. [Paragraph 3]",
        "The proposed model significantly improves the quality of generated responses according to automatic evaluation and human evaluation, especially in coherence. [Paragraph 4]"
    ],
    "3445": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The effect of dependency parsing on POS tagging can be investigated by removing the tag representations from parsing in Eq. (4) and Eq. (6), and by removing the syntactic information from tagging in Eq. (2).",
        "Providing lexical information to parsing improves the tagging accuracy itself.",
        "The connection between tagging and parsing is still present even when the explicit connections are disabled.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing."
    ],
    "3448": [
        "Our findings showed that words with high prediction losses in the target language benefit most from additional back-translated data.",
        "As an alternative to random sampling, we proposed targeted sampling and specifically targeted words that are difficult to predict.",
        "We discovered that using the prediction loss to identify weaknesses of the translation model and providing additional synthetic data targeting these shortcomings improved the translation quality in German-English by up to 1.7 BLEU points."
    ],
    "3449": [
        "The Pyramidal Recurrent Unit (PRU) improves perplexity across several settings, including recent state-of-the-art systems.",
        "The PRU better models contextual information by admitting higher dimensional representations with good generalizability.",
        "The PRU improves the flow of gradient and expands the word embedding subspace, resulting in more confident decisions.",
        "The PRU improves language modeling performance.",
        "In future work, the authors plan to study the performance of PRUs on different tasks, including machine translation and question answering.",
        "The authors will also study the performance of the PRU on language modeling with more recent inference techniques, such as dynamic evaluation and mixture of softmax."
    ],
    "3451": [
        "Our model does not require a parallel training corpus and works directly on hidden representations of sentences.",
        "It does not treat the form as a binary variable, enabling a fine-grained change of the form of sentences or specific aspects of meaning.",
        "We evaluate ADNet on two tasks: the shift of language register and diachronic language change. Our solution achieves superior results.",
        "The proposed motivational loss leads to significantly better separation of the form embeddings."
    ],
    "3454": [
        "The proposed model significantly outperforms existing HRED models and its attention variants on the task of named entity recognition.",
        "The use of hand-crafted features in the proposed model provides valuable insights into the data, leading to improved performance.",
        "The method presented in this work can be easily applied to other tasks and models where hand-engineered features are used.",
        "The experimental results show that incorporating an auto-encoder loss to take manual features as input and reconstruct them leads to significant improvements in the task of named entity recognition.",
        "The use of an auto-encoder loss to validate the utility of hand-crafted features in the proposed model is effective and can be applied to other tasks and models."
    ],
    "3456": [
        "The proposed model uses an auto-correlational kernel to detect \"rough copy\" dependencies in speech disfluencies, which improves over a CNN baseline on the disfluency detection task.",
        "The addition of the auto-correlational kernel allows for a fairly conventional architecture to achieve near state-of-the-art results without complex handcrafted features or external information sources.",
        "The performance of the ACNN model can be further improved in future by using more complex similarity functions and incorporating external information (e.g., prosody) used in other disfluency models.",
        "The auto-correlational layer is a generic neural network layer that can be used as a component of other architectures, such as RNNs, and might also be useful in very different applications such as image processing."
    ],
    "3457": [
        "The proposed graph-state LSTM model for crosssentence n-ary relation extraction has several advantages over a bidirectional DAG LSTM baseline, including not changing the input graph structure and being more parallelizable.",
        "The model can easily incorporate sibling information when calculating the contextual vector of a node.",
        "Experimental results show significant improvements over previously reported numbers, including those of the bidirectional graph LSTM model.",
        "Adding coreference information as an entity mention can help with information collection.",
        "Including word sense information can help address confusing caused by word senses, which can be a severe problem not only for content words but also for propositions."
    ],
    "3458": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed approach of leveraging continuous word representations for unsupervised learning of syntactic structure is effective, as demonstrated by experiments on POS induction and unsupervised dependency parsing tasks.",
        "Future work might explore more sophisticated invertible projections or recurrent projections that jointly transform the entire input sequence.",
        "The use of a multi-label classifier trained to predict visual entities from action features can provide a more expressive signal for captioning compared to the raw features themselves.",
        "The proposed approach of using a sequence-to-sequence framework with a recurrent and transformer architecture can be effective in caption generation based on a sequence of action features."
    ],
    "3460": [
        "The proposed neural discourse segmenter can segment text fast and accurately, without relying on hand-crafted features or syntactic parse trees.",
        "The proposed method leverages word representations learned from large corpora to achieve state-of-the-art performance with significant speedup.",
        "The restricted self-attention mechanism proposed in the paper can improve the performance of the discourse segmenter.",
        "The proposed method does not rely on any hand-crafted features, unlike previous methods.",
        "The experimental results on RST-DT show that the system achieves state-of-the-art performance with significant speedup."
    ],
    "3462": [
        "Character-level models are effective because they can represent OOV words and orthographic regularities of words that are consistent with morphology.",
        "However, character-level models depend on context to disambiguate words, and for some words, this context is insufficient.",
        "Case syncretism is a specific example that our analysis identified, but the main results in Table 2 hint at the possibility that different phenomena are at play in different languages.",
        "Prior knowledge of morphology is important, but it can be used in a targeted way: our character-level models improved markedly when we augmented them only with case.",
        "Neural models can still benefit from linguistic analyses that target specific phenomena where annotation is likely to be useful."
    ],
    "3464": [
        "The proposed entailment model incorporating structured knowledge base lookup achieves a 5% improvement on SciTail over the baseline neural model used here.",
        "The methodology can be easily applied to more complex entailment models (e.g., DGEM) as the base neural entailment model.",
        "Accurately identifying the subfacts from a hypothesis is a challenging task, especially when dealing with negation.",
        "Improvements to the fact decomposition should further help improve the model."
    ],
    "3465": [
        "The model combines the prior over bipartite matchings inspired by Haghighi et al. (2008) and the discriminative, rather than generative, approach inspired by Irvine and Callison-Burch (2013).",
        "Our model shows empirical gains on six language pairs.",
        "The bipartite matching prior solves the hubness problem."
    ],
    "3466": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Rational recurrences are in frequent use by several recently proposed recurrent neural architectures, providing new understanding of them.",
        "Our empirical results demonstrate the potential of deriving novel neural architectures from WFSAs."
    ],
    "3467": [
        "The proposed approach utilizes the connection between human rationales and machine attention to improve the performance of low-resource tasks.",
        "The learned mapping from rationales to high-quality attention on resource-rich tasks serves as a better form of supervision for the target task.",
        "The R2A-generated attention produces high-quality attention for low-resource tasks.",
        "The proposed approach improves the performance of low-resource tasks using transfer learning.",
        "The learned mapping is used to provide an additional supervision for the target task."
    ],
    "3468": [
        "'Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.'",
        "'Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.'",
        "'TrDec outperforms CCG when used with synthetic balanced binary trees.'",
        "'Syntax-free trees outperform their syntax-driven counterparts, eliciting a natural question for future work: how do we better model syntactic structure in these models?'",
        "'Using source-side syntax together with target-side syntax supported by TrDec may be interesting to study.'"
    ],
    "3471": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements."
    ],
    "3472": [
        "The proposed approach improves the tradeoff between utility and privacy of neural representations.",
        "The choice of a strategy for model selection is an important direction for future work.",
        "The effects of defense methods can be more drastic than desired in some cases.",
        "The multidetasking approach leads to more stable improvements and should be preferred in most cases.",
        "The adversarial generation method does not require the specification of private variables and is a more general approach.",
        "It is possible for an attacker to recover private variables with higher than chance accuracy using only hidden representations.",
        "The proposed defense methods lead to models with better privacy."
    ],
    "3473": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "There is almost no work on semantic parsing for interlanguages.",
        "The robustness of language comprehension for interlanguage.",
        "The weakness of applying L1-sentence-trained systems to process learner texts.",
        "The significance of syntactic parsing and L2-L1 parallel data in building more generalizable SRL models that transfer better to L2.",
        "We have successfully provided a better SRL-oriented syntactic parser as well as a semantic classifier for processing the L2 data by exploring L2-L1 parallel data."
    ],
    "3474": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Question well-formedness information can be a helpful signal in improving state-of-the-art question generation systems."
    ],
    "3475": [
        "The proposed recurrent chunking mechanisms outperform benchmark models across different datasets.",
        "The approach of using a chunking policy network for machine reading comprehension enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "The addition of a recurrent mechanism allows the information to flow across segments, enabling the model to have knowledge beyond the current segment when selecting answers.",
        "The WikiAtomicEdits corpus contains 43M examples of atomic insertions and deletions in 8 languages, providing a valuable resource for research in semantics, discourse, and representation learning.",
        "Models trained on the WikiAtomicEdits corpus encode different aspects of semantics and discourse than models trained on raw text."
    ],
    "3479": [
        "The beam problem in open-domain question answering can be largely explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The model achieves better performance by leveraging external working memory with dynamic routing to track previous learning experience and adapt to unseen classes.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning, and it is worth investigating this type of models in other learning problems.",
        "The proposed ET-RR model outperforms existing QA models on open-domain multiple-choice datasets.",
        "The model has limitations, and current work needs to be improved through in-depth error analysis.",
        "Future work includes exploring the directions of constructing multihop query and developing end-to-end retriever-reader model via reinforcement learning."
    ],
    "3481": [
        "HAQAE is one of the first models to model the hierarchy that is inherent in this type of real-world knowledge.",
        "Previous work has focused on modeling event sequences with language models, while ignoring the problem of contradictory events and different tracks being jumbled together.",
        "The hierarchical latent space of HAQAE instead attends to the choice points in event sequences, and is able to provide some discrimination between tracks of events.",
        "HAQAE has a substantially lower perplexity on our test set than previous RNN models despite HAQAE's decoder having fewer parameters.",
        "Using a latent space to capture script differences helps identify relevant sequences.",
        "All previous work on script induction has focused on learning single event sequences or bags of events.",
        "We view our proposed model as a new step toward learning different details about scripts, such as tracks and hierarchies.",
        "Understanding exactly what is learned in the latent variables is non-trivial and is a possible direction for future work."
    ],
    "3482": [
        "The beam problem in neural machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to improvements in BLEU scores.",
        "The locally-normalized structure of the model contributes to the beam problem.",
        "The brevity problem can be explained by the locally-normalized structure of the model.",
        "Our method for learning the parameters of globally-normalized models is helpful and easy.",
        "The solution to the brevity problem is a limited form of globally-normalized models.",
        "More research is needed to understand how much, if any, improvement remains to be gained by solving label bias in general."
    ],
    "3483": [
        "With our length prediction model, Bounded word-reward method gains consistent improvement.",
        "Results from length normalization method show that optimal stopping technique gains significant improvement by around +0.9 BLEU.",
        "While with both, our proposed methods beat all previous methods, and gain improvement over hyperparameter-free baseline (i.e. length normalization) by +2.0 BLEU.",
        "Among our proposed methods, Bounded wordreward has the reward r as an hyper-parameter, while the other two methods get rid of that.",
        "We recommend the BP-Norm method, because it is the simplest method, and yet works equally well with others."
    ],
    "3484": [
        "Our multi-task setup effectively improves performance across all tasks.",
        "We show that our multi-task model is better at predicting span boundaries and outperforms previous state-of-the-art scientific IE systems on entity and relation extraction, without using any handengineered features or pipeline processing.",
        "Using our model, we are able to automatically organize the extracted information from a large collection of scientific articles into a knowledge graph.",
        "Our analysis shows the importance of coreference links in making a dense, useful graph.",
        "We still observe a large gap between the performance of our model and human performance, confirming the challenges of scientific IE.",
        "Future work includes improving the performance using semisupervised techniques and providing in-domain features.",
        "We also plan to extend our multi-task framework to information extraction tasks in other domains."
    ],
    "3486": [
        "The proposed CDVAE framework successfully utilizes cross-domain features to improve the capability of VAE for VC, and outperforms the baseline VAE-based VC system in subjective tests.",
        "The latent code of the VAE framework should solely contain phonetic information of the frame, with no information about the speaker, but this decomposition is not explicitly guaranteed.",
        "Hand-crafted features like SP or MCCs possess their own natures, and the required information to reconstruct the inputs from different feature domains may differ.",
        "Involving more speakers during training may not necessarily lead to better decomposition.",
        "Our proposed framework forces the encoder to act more like a speaker-independent phone recognizer, thus filters out unnecessary, speaker-dependent information of the input feature.",
        "The proposed framework not only achieves cross-domain feature property satisfaction but learns more disentangled latent representation of speech.",
        "In the future, we plan to investigate in detail the assumption that the latent code should solely contain phonetic information of the frame.",
        "Introducing WGAN to the proposed framework may further improve the naturalness of converted speech."
    ],
    "3492": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "Our method can still translate correct words even when the word pairs are not aligned.",
        "We propose two methods to tackle the cross-lingual NER problem under the unsupervised transfer setting.",
        "We find translations of words in a shared embedding space built from a seed lexicon.",
        "To alleviate word order divergence across languages, we add a self-attention mechanism to our neural architecture.",
        "With these methods combined, we are able to achieve state-of-the-art or competitive results on commonly tested languages under a cross-lingual setting, with lower resource requirements than past approaches."
    ],
    "3495": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Our base model on CNN with word-, character-and topic-based representations outperforms the state of the art.",
        "Domain relationships were learned to better transfer knowledge across domains."
    ],
    "3496": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Our proposed method uses a self-attentive BiLSTM-CRF-based solution for causality extraction.",
        "We introduce the multihead self-attention mechanism to learn the dependencies between cause and effect.",
        "Our approach outperforms benchmark models across different datasets in machine reading comprehension tasks.",
        "We use pre-trained (contextual) embeddings to make the model fast.",
        "Our graph neural network operates over a compact graph representation of a set of documents, where nodes are mentions to entities and edges signal relations such as within and cross-document coreference.",
        "The model learns to answer questions by gathering evidence from different documents via a differentiable message passing algorithm that updates node representations based on their neighborhood.",
        "Our model outperforms published results, and ablation studies show substantial evidence in favor of multistep reasoning."
    ],
    "3498": [
        "The model can learn complex composition operators from end task supervision.",
        "The linguistically motivated inductive bias imposed by the structure of the model allows it to generalize well beyond its training data.",
        "Scaling the model to other question answering tasks is a future work.",
        "Using more general composition modules is a future work.",
        "Introducing additional module types is a future work."
    ],
    "3499": [
        "We have demonstrated the translation quality of standard NMT architectures operating at the character level.",
        "Character NMT can substantially outperform BPE tokenization for all but the largest training corpus sizes.",
        "To address this cost, we have explored a number of methods for source-sequence compression.",
        "We intend this paper as a call to action. Character-level translation is well worth doing, but we do not yet have the necessary techniques to benefit from this quality boost without suffering a disproportionate reduction in speed."
    ],
    "3500": [
        "The presence of multiple valid next utterances in a dataset can cause issues with making wrong updates.",
        "Using masking can help handle the issue of multiple valid next utterances.",
        "A SL phase where the mask uses the answer, and an RL phase, where the system learns to generate the mask solely from its dialog state vector.",
        "The proposed method is a step in the right direction for bridging the gap between neural methods and human-human dialogs.",
        "Our experiments show a significant drop in performance of present neural methods in the permuted-bAbI dialog task compared to the original-bAbI dialog task."
    ],
    "3501": [
        "The PRPN model presented in Shen et al. (2018) is robust and represents a viable method for grammar induction, despite some experimental design problems that make the results difficult to interpret.",
        "The original paper's results are difficult to interpret due to experimental design problems, but the PRPN model still shows promise as a viable method for grammar induction.",
        "The PRPN model is the first clear success for latent tree learning with neural networks, and it heralds further work on language modeling as a tool for grammar induction research.",
        "The results of Shen et al. (2018) are difficult to interpret due to experimental design problems, but the PRPN model still demonstrates promise as a viable method for grammar induction."
    ],
    "3502": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Two corrections to the model can be made to alleviate or eliminate the beam problem.",
        "Learning the parameters of these corrections is helpful and easy.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to significant BLEU gains, but it remains to be seen how much improvement remains to be gained by solving label bias in general.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset, and it is possible that more general globally-normalized models can be trained in a similarly inexpensive way."
    ],
    "3503": [
        "The paper presents an unsupervised method for discovering root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow for validating the pattern discovery method as well as the root extraction method (JZR).",
        "The performance of the method is not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The approach of formulating an opportunistic active learning problem as a reinforcement learning problem and learning a policy that can effectively trade-off opportunistic active learning queries against task completion is effective.",
        "The policy retrieves the correct object in a larger fraction of dialogs than a previously proposed static baseline, while also lowering average dialog length."
    ],
    "3504": [
        "We exhibit an efficient dynamic program for the exact marginalization of all non-monotonic alignments in a neural sequence-to-sequence model.",
        "We show empirically that exact marginalization helps over approximate inference by REINFORCE and that models with hard, non-monotonic alignment outperform those with soft attention.",
        "The set of all alignments between strings x and y was denoted A(x, y) at times throughout the paper. For simplicity, this set is now consistently denoted A.",
        "In \u00a75.2, eq. (16) was missing a 'a' to the right of the bar due to a macro failure.",
        "Several equations were reformatted to occupy multiple lines for aesthetic reasons.",
        "Infelicities in the bibliography were fixed."
    ],
    "3505": [
        "Our proposed Zero-Shot Adaptive Transfer method for slot tagging achieves significant improvements over state-of-the-art systems, with an improvement of 7.24% in absolute F1-score when training with 2000 instances per domain, and an even higher improvement of 14.57% when only 500 training instances are used.",
        "Our model utilizes the slot description for transferring reusable concepts across domains to avoid drawbacks of prior approaches such as increased training time and suboptimal concept alignments.",
        "We plan to extend our model to consider more context and utilize exogenous resources like parsing information.",
        "Our approach achieves an improvement of 14.57% when only 500 training instances are used, indicating the effectiveness of our method in low-resource settings.",
        "The results show that our model performs significantly better than state-of-the-art systems by a large margin of 7.24% in absolute F1-score when training with 2000 instances per domain."
    ],
    "3508": [
        "Direct Output Connection (DOC) raises the expressive power of RNN language models and improves their quality.",
        "DOC outperformed MoS and achieved the best perplexities on standard benchmark datasets of language modeling: PTB and WikiText-2.",
        "DOC also improved the performance of EncDec and using a middle layer positively affected application tasks such as machine translation and headline generation.",
        "The expressive power of RNN language models can be increased by raising the output connection.",
        "Improving the quality of RNN language models can lead to better performance on various tasks, including language modeling, machine translation, and headline generation."
    ],
    "3509": [
        "The existing metrics for AQG do not accurately account for the answerability of generated questions.",
        "The proposed modification involving learnable weights can improve the correlation between existing metrics and human judgments.",
        "The proposed metric correlates better with human judgments, but there is still scope for improvement, especially for document QA and visual QA.",
        "There is potential for improvement in designing better metrics for answerability and testing a non-linear combination of different elements in the Q-Metric."
    ],
    "3510": [
        "Our study touches on questions of both MT evaluation and MT performance.",
        "It also reveals some information about the pronoun uses that are most problematic for MT.",
        "By re-evaluating a corpus of MT systems, we found that the manual evaluation was not consistent with the automatic evaluation.",
        "The manual evaluation had overestimated the performance of the MT systems, and the automatic evaluation revealed some issues with the MT systems.",
        "Our findings suggest that the current MT evaluation metrics may not be effective in capturing the true performance of MT systems."
    ],
    "3512": [
        "We proposed a novel annotation methodology capturing empathic states actually felt by the author of a statement, instead of relying on third-party assessments.",
        "To ensure high reliability in this single-rating setting, we employ multi-item scales in line with best practices in psychology.",
        "Our analysis shows that the data set excels with high rating reliability and an authentic and diverse language, rich of challenging phenomena such as sarcasm.",
        "We provide experimental results for three different predictive models, our CNN turning out superior.",
        "Our survey being set-up and supervised by an expert psychologist."
    ],
    "3513": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The syntactic scaffold improves performance across the board for frequent frame elements, particularly for Theme and Goal.",
        "Our method improves the performance of competitive baselines for semantic role labeling on both FrameNet and PropBank, and for coreference resolution.",
        "Syntactic scaffolds could be applied in other settings, such as dependency and graph representations.",
        "Semantic scaffolds could be used to improve NLP applications with limited annotated data.",
        "The relative merits of different kinds of scaffolds and multitask learners, and how they can be most productively combined, remain an open empirical question."
    ],
    "3514": [
        "The proposed iterative recursive attention model (IRAM) has the capacity to construct representations of the input sequence in a recursive fashion, making inference more interpretable.",
        "The model can learn to focus on various task-relevant parts of the input and can propagate the information in a meaningful way to handle the more difficult cases.",
        "On the sentiment analysis task, the model performs comparable to the state of the art.",
        "The iterative attention mechanism can be used to extract tree-like sentence structures akin to constituency parse trees.",
        "The model can be extended to support an adaptive number of iterative steps."
    ],
    "3520": [
        "The joint model of part-of-speech tagging and dependency parsing can effectively preserve salient source relations in summaries, as shown by results on benchmark datasets.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The agents' representations are not capturing general conceptual properties of different objects, but they are rather specifically tuned to successfully distinguish images based on inscrutable low-level relational properties.",
        "The agents develop a form of \"conceptual pact\" to achieve communication, where their internal representations align while drifting away from human-level properties of the input.",
        "Enforcing the agent representations to stay more faithful to the perceptual input they receive can help develop more natural word meanings.",
        "It is fundamental to design setups where agents would have stronger reasons to develop human-like communication strategies."
    ],
    "3522": [
        "The proposed content selection model for summarization can identify phrases within a document that are likely included in its summary, leading to improvements in ROUGE scores of over two points on both the CNN-DM and NYT corpora.",
        "The combined bottom-up attention system leads to improvements in ROUGE scores, indicating the effectiveness of the content selector in restricting the ability of abstractive summarizers to copy words from the source.",
        "A comparison to end-to-end trained methods showed that this particular problem cannot be easily solved with a single model, but instead requires finetuned inference restrictions.",
        "The technique, due to its data-efficiency, can be used to adjust a trained model with few data points, making it easy to transfer to a new domain.",
        "Preliminary work that investigates similar bottom-up approaches in other domains that require a content selection, such as grammar correction, or data-to-text generation, has shown some promise and will be investigated in future work."
    ],
    "3524": [
        "The proposed von Mises-Fisher VAE can resolve optimization issues in variational autoencoders for text.",
        "The choice of distribution allows for explicit control of the balance between decoder capacity and latent representation utilization.",
        "Experimental results demonstrate that the proposed model has better performance than a Gaussian VAE across a range of settings.",
        "The vMF VAE is more sensitive to word order information and makes more effective use of the latent code space."
    ],
    "3526": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The dynamic online selection still underperforms the fine-tuning on domain parallel data.",
        "Online denoising (e.g., P3) can significantly outperform the simple finetuning (e.g., P2).",
        "Our method could potentially work with other data filtering methods.",
        "If the underlying noisy data has already been filtered, applying online denoising with trusted data could potentially bring even further improvement than no pre-filtering."
    ],
    "3529": [
        "The influence of error propagation is overstated in literature and cannot explain accuracy drop in neural machine translation models.",
        "Language branching well correlates to the accuracy drop problem, with evidence from n-gram statistics and dependency statistics supporting this correlation.",
        "Left-to-right NMT models fit better for right-branching languages (e.g., English), while right-to-left NMT models fit better for left-branching languages (e.g., Japanese).",
        "Future works will study more left/right-branching languages and other languages with no obvious branching characteristics to further investigate the impact of language branching on neural machine translation.",
        "Investigating how language branching influences other natural language tasks, especially for neural networks based models."
    ],
    "3530": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our method leads to faster training convergence and more fluent translations than a baseline system without LM."
    ],
    "3536": [
        "The popular approach of full parameter sharing may perform well only when the target languages belong to the same family, while a partial parameter sharing approach consisting of shared embedding, encoder, decoder's key and query weights is generally applicable to all kinds of language pairs and achieves the best BLEU scores when the languages are from distant families.",
        "For future work, we plan to extend our parameter sharing approach in two directions: (1) increasing the number of target languages to more than two, and (2) experimenting with additional parameter sharing strategies such as sharing the weights of some specific layers.",
        "We show that the popular approach of full parameter sharing may perform well only when the target languages belong to the same family, while a partial parameter sharing approach consisting of shared embedding, encoder, decoder's key and query weights is generally applicable to all kinds of language pairs and achieves the best BLEU scores when the languages are from distant families.",
        "Our proposed parameter sharing strategies achieve the best BLEU scores when the languages are from distant families.",
        "We plan to extend our parameter sharing approach in two directions: (1) increasing the number of target languages to more than two, and (2) experimenting with additional parameter sharing strategies such as sharing the weights of some specific layers."
    ],
    "3538": [
        "We proposed PReFIL, a new CQA system that improves the state-of-the-art and surpasses human accuracy on two datasets.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments.",
        "For the first time, this work makes an attempt to introduce additional context in neural pinyin-to-character converter for pinyin-based Chinese IME as to our best knowledge.",
        "We propose a gatedattention enhanced model for digging significant context information to improve conversion quality."
    ],
    "3540": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The history captured by the model contains multiple languages, and the model captures 'global' history as opposed to 'local' history captured in most previous works.",
        "Leveraging the bilingual conversation history is significant in such scenarios.",
        "Using wide-range context, the model generates appropriate pronouns and discourse connectives in some cases.",
        "The proposed AdaBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks."
    ],
    "3542": [
        "Our method for transfer learning in neural machine translation outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We demonstrate that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks, as demonstrated by a human evaluation.",
        "The ConMask model has some limitations and room for improvement, such as the nature of the relationship-dependent content masking, which can lead to incorrect predictions.",
        "Starting training in the opposite direction and swapping to the main one afterwards can be an effective trick for low-resource languages.",
        "The size of the parent corpus is a key factor in the gains achieved by our method, rather than e.g. vocabulary overlaps."
    ],
    "3546": [
        "The proposed system achieved a 0.4277 evidence F1-score, a 0.5136 label accuracy, and a 0.3833 FEVER score, indicating its effectiveness in verifying claims.",
        "The system uses NER and TFIDF vector comparison to retrieve candidate Wikipedia sentences for the verification process.",
        "An RTE module and a Random Forest classifier are used to determine the veracity of the claim based on the information present in the retrieved sentences.",
        "The system has the potential to be improved by using triple extraction-based methods to improve the sentence retrieval component and to improve the identification of evidential sentences.",
        "Exploring more sophisticated methods to combine the information obtained from the RTE module and employ entity linking methods to perform named entity disambiguation can further enhance the system's performance."
    ],
    "3547": [
        "DAS (novel framework) can better leverage unlabeled data and achieve substantial improvements over baseline methods.",
        "Feature adaptation is an essential component of the proposed framework, without which semi-supervised learning is not able to function properly.",
        "The proposed framework has the potential to be adapted to other domain adaptation tasks, as future studies will focus on this aspect."
    ],
    "3553": [
        "The model based on gaze features and part-of-speech information achieves accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features are the most discriminative ones for disambiguating categories.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The upgrade to bi-LSTM improves the results of P-Emb and P-Sent.",
        "Both SGU for finetuning and the concatenation method enhance the performance of the P-LM approach.",
        "Adding the P-LM predictions to the ensemble contributes the most to the performance improvement.",
        "The proposed approach based on an ensemble of Transfer Learning techniques yields a higher performance.",
        "The use of refined, high-level features of text, as the ones encoded in language models, yields a higher performance.",
        "Exploring other transfer learning approaches may lead to further improvements.",
        "The source code of the models is released to facilitate further experimentation and reproducibility."
    ],
    "3556": [
        "The experimental results show that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The proposed ReCoSa model utilizes the self-attention mechanism to effectively capture the long distant dependency relations.",
        "The toolkit is modularized to enable easy replacement of components, and extensible to allow seamless integration of any external or customized modules.",
        "We are excited to further enrich the toolkit to support a broader set of natural language processing and machine learning applications."
    ],
    "3557": [
        "RecipeQA is the first machine comprehension dataset that deals with understanding procedural knowledge in a multimodal setting.",
        "The four question styles in the RecipeQA dataset are specifically tailored to evaluate particular skills and require connecting the dots between different modalities.",
        "Results of the baseline models demonstrate that RecipeQA is a challenging dataset.",
        "The authors intend to extend the dataset by collecting natural language questions-answer pairs via crowdsourcing in the future.",
        "RecipeQA will serve other purposes for related research problems on cooking recipes as well."
    ],
    "3558": [
        "'We have introduced recursive declarations and recursive execution mechanisms for running recursive neural networks on top of existing embedded control flow frameworks.'",
        "'With recursively defined computation graphs, recursive neural networks can be implemented in a fashion that better portrays the recursion aspect, and be executed efficiently by letting the framework exploit parallel execution of computations.'",
        "'To achieve this goal, we designed and implemented a programming model and a runtime execution model, including automatic differentiation support for deep learning jobs.'",
        "'We have demonstrated the expressive power and performance of recursive graphs by implementing various recursive neural network models using our programming model and comparing them with iterative and unrolling implementations, showing that recursive graphs outperform other approaches significantly.'"
    ],
    "3559": [
        "The proposed method outperforms conventional approaches that depend on manually annotated resources or word segmenters.",
        "The method is especially useful for languages with unobvious word boundaries, such as unsegmented languages.",
        "The use of an extended dictionary improves word coverage in Japanese language.",
        "The proposed method does not rely on any manually annotated resources or word segmenters.",
        "The method can be used to acquire general-purpose vector representations of words, phrases, and sentences seamlessly."
    ],
    "3565": [
        "We developed a pipeline for causal explanation analysis over social media text.",
        "We found an SVM best for causality prediction and a hierarchy of BiLSTMs for causal explanation identification.",
        "The later task (causal explanation identification) relies more heavily on sequential information.",
        "Replacing either layer of the hierarchical LSTM architecture with an equivalent 'bag of features' approach resulted in reduced accuracy.",
        "Our whole pipeline of causal explanation analysis achieved an F 1 = 0.868 at identifying discourse arguments that are causal explanations.",
        "We demonstrated use of our models in applications, finding associations between demographics and rate of mentioning causal explanations, as well as showing differences in the top words predictive of negative ratings in Yelp reviews.",
        "Utilization of discourse structure in social media analysis has been a largely untapped area of exploration.",
        "We hope the strong results of causal explanation identification here leads to the integration of more syntax and deeper semantics into social media analyses and ultimately enables new applications beyond the current state of the art."
    ],
    "3569": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The proposed learning framework for semantic parsing can be applied to more tasks and can be used to address the spurious program challenge.",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Enabling \"tag \u2192 parse\" only also improves the tagging accuracy itself.",
        "The approach of joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages."
    ],
    "3570": [
        "'There is compelling evidence that the charCNN improves translation of morphology in ways that are complementary to BPE.'",
        "'The architecture is not slow like an RNN over characters would be, and there is no reason the charCNN cannot be combined with other encoder-decoder architectures.'",
        "'Thus, we recommend the use of charCNN for stronger source-side morphological baselines in low-resource settings.'",
        "'Additionally, it seems there is some morphology that BPE helps to capture in non-Semitic languages.'",
        "'Therefore, future work should investigate alternatives to BPE for Semitic languages.'",
        "'All of this work focuses on the encoder, where the char-CNN has seen gains. Additional future work should look into similar approaches for the decoder.'"
    ],
    "3574": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "We have taken the first steps towards constructing a dataset suitable for investigating the kind of semantic information captured by word embedding vectors.",
        "Classifiers, in particular neural networks, can identify which entities have a specific property in cases where this does not follow from general similarity or the overall semantic class the entity belongs to.",
        "The results on the extended datasets partly confirm that visual properties are not well represented by embeddings, while properties relating to function (e.g. cooking, having wheels) and interactions with other entities (e.g. being dangerous or killing) tend to be represented well.",
        "Our current dataset may have bias, but the results on the smaller crowd-only sets for properties with enough available data (is dangerous and does kill) confirm our hypotheses."
    ],
    "3577": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; more general globally-normalized models can be trained in a similarly inexpensive way.",
        "The Capsnet layer achieved the best results among the experimented alternatives for tackling the task of predicting the emotion of a tweet without explicit mentions of emotion terms.",
        "Investigating the possibility of using Capsule networks for other tasks in Natural Language Processing, especially where CNNs are involved, is a potential future work.",
        "Applying transfer learning on the model trained using this semi-automatically annotated dataset to test on human annotated datasets such as Mohammad et al. (2018) is another line of future work."
    ],
    "3579": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We propose the creation of a new dataset with human-generated question-answer pairs.",
        "Document-level CQA requires document question answering, page segmentation, and more.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications."
    ],
    "3584": [
        "The proposed hierarchical multi-head HAN NMT model significantly outperforms two competitive baselines.",
        "Integrating context from source and target sides by directly connecting representations from previous sentence translations into the current sentence translation improves translation quality.",
        "The model improves lexical cohesion and coherence, and the translation of nouns and pronouns.",
        "Target and source context is complementary.",
        "The model is able to identify important previous sentences and words for correct prediction.",
        "Future work will explicitly model discourse connections with the help of annotated data, which may further improve translation quality."
    ],
    "3585": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Our method is as accurate as, is faster than, and uses less GPU memory than the standard full softmax counterpart, on sentence generation tasks of different modalities.",
        "It is interesting to use our method in generative adversarial networks to further improve the sentence generation models."
    ],
    "3586": [
        "We presented the TVQA dataset, a large-scale, localized, compositional video question answering dataset.",
        "Our experiments show both visual and textual understanding are necessary for TVQA.",
        "There is still a significant gap between the proposed baselines and human performance on the QA accuracy.",
        "We hope this novel multimodal dataset and the baselines will encourage the community to develop stronger models in future work.",
        "To narrow the gap, one possible direction is to enhance the interactions between videos and subtitles to improve multimodal reasoning ability.",
        "Another direction is to exploit human-object relations in the video and subtitle, as we observe that a large number of questions involve such relations.",
        "Temporal reasoning is crucial for answering the TVQA questions.",
        "Thus, future work also includes integrating better temporal cues."
    ],
    "3590": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "D-RNNLMs and same-source pretraining provide significant perplexity reductions for codeswitched LMs.",
        "Leveraging generative models to train LMs is potentially applicable beyond codeswitching; D-RNNLMs could be generalized beyond LMs, e.g. speaker diarization."
    ],
    "3591": [
        "We show that training models to align answers both with the persona of their author and the context improves the predicting performance.",
        "The trained models show promising coverage as exhibited by the state-of-the-art transfer results on the PERSONA-CHAT dataset.",
        "As pretraining leads to a considerable improvement in performance, future work could be done fine-tuning this model for various dialog systems.",
        "Future work may also entail building more advanced strategies to select a limited number of personas for each user while maximizing the prediction performance."
    ],
    "3593": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Training on adversarial inputs not only makes the models more robust to perturbations but also helps them achieve new state-of-the-art performance on the original data.",
        "A BPE-enhanced VHRED model that trains faster with comparable performance and is robust to Grammar Errors even without adversarial training can be used as an alternative to traditional model-robustness architectural changes."
    ],
    "3594": [
        "Our proposed solution for digitising Romanised Sanskrit using an OCR-based approach outperforms the commercially available Google OCR on the Saha\u015bran\u0101ma texts.",
        "We find that the use of a copying mechanism in the encoder-decoder performs significantly better than other seq2seq models for the task.",
        "Our model is devoid of any OCR-specific feature engineering and instead uses a post-OCR text correction approach.",
        "Our approach can be easily extended to provide a post-processed OCR for languages that use the Roman alphabet but do not have an OCR system available.",
        "We have found that CopyNet performs stably even for OCR outputs with a CRR as low as 36%.",
        "Our immediate research direction will be to rectify insertion errors which currently are not properly handled."
    ],
    "3598": [
        "Our proposed mixture-of-experts (MoE) approach for unsupervised domain adaptation consistently outperforms various baselines and can robustly handle negative transfer, as demonstrated by experimental results on sentiment classification and part-of-speech tagging.",
        "The effectiveness of our approach suggests its potential application to a broader range of domain adaptation tasks in NLP and other areas.",
        "We model the domain relations through a point-to-set distance metric, and introduce a meta-training mechanism to learn this metric.",
        "Our approach can handle negative transfer, as demonstrated by experimental results on sentiment classification and part-of-speech tagging.",
        "The proposed MoE approach has the potential to be applied to a broader range of domain adaptation tasks in NLP and other areas."
    ],
    "3599": [
        "The proposed method, CAS-LSTM, outperforms plain LSTM-based models in all experiments and is competitive with other state-of-the-art models.",
        "The CAS-LSTM architecture can replace any stacked LSTM only under one weak restriction: the size of states should be identical across all layers.",
        "The proposed method is applicable to various sentence modeling tasks and has the potential to benefit other problems such as sequence labeling and language modeling.",
        "Aggregating diverse contexts from sequential data, e.g., those from forward and backward reading of text, could be an intriguing research direction.",
        "The use of sophisticated modulation on context integration in the CAS-LSTM architecture has the potential to improve the performance of various natural language processing tasks."
    ],
    "3600": [
        "Our approach could bring immediate benefits to SLU researchers and the industry by reducing the cost of building new SLU datasets and improve performances of existing SLU models.",
        "We formulated the generic framework for generative data augmentation (GDA) and derived analytically the most effective sampling approach for generating performance-boosting instances from our proposed generative model, Joint Language Understanding Variational Autoencoder (JLUVA).",
        "Our work has primarily been motivated by the data issues in SLU datasets, but we invite researchers to explore the potential of applying GDA in other NLP tasks.",
        "Similar to the work done by Dao et al. on the analysis of classpreserving transformative DAs using the kernel theory (Dao et al. 2018), our work also calls for deeper theoretical analysis on the mechanism of data-centric regularization techniques.",
        "We wish to address these issues in our future work."
    ],
    "3601": [
        "Our unsupervised method for learning cross-lingual embeddings significantly outperforms existing unsupervised word translation models in all language pairs under a low resource situation.",
        "Our unsupervised method achieves better results than a supervised method in the German-English word alignment task, even though the supervised method was trained with 350 pairs of words from a bilingual dictionary.",
        "Our model learns cross-lingual embeddings across four languages, which we call quadrilingual embeddings, and these embeddings enable us to align equivalent words among four languages in an unsupervised way.",
        "The visualization of the quadrilingual embeddings shows that words with similar meanings have close representations across different languages.",
        "Our approach has the potential to be extended to a semi-supervised method that utilizes a bilingual dictionary, which could improve the performance of the model by leveraging the knowledge from the bilingual dictionary."
    ],
    "3602": [
        "Existing NQG models suffer from a serious problem: a significant proportion of generated questions include words in the question target, resulting in the generation of unintended questions.",
        "To overcome this problem, we introduce a novel NQG architecture that treats the passage and the target answer separately to better utilize the information from both sides.",
        "Experimental results show that our model has a strong ability to generate the right question for the target answer in the passage.",
        "Our model yields a substantial improvement over previous state-of-the-art models."
    ],
    "3603": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Existing datasets and our causality tagging scheme can be used to develop annotated datasets for future work.",
        "Our proposed method can achieve better performance without having to build a high-quality annotated corpus for causality extraction.",
        "The approach of combining our method with distant supervision and reinforcement learning can improve the domain adaption performance.",
        "The representation extractor with the Clustering Promotion Mechanism achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The use of pseudo-labeled target-domain data and labeled source-domain data can improve the domain adaption performance.",
        "Sparse and multimodal embeddings achieve a more faithful representation of human semantics than dense models constructed from a single information source."
    ],
    "3604": [
        "The proposed logographic subword model achieves comparable performance with reduced dictionary size and training/inference time.",
        "The approach reduces the complexity of other computationally expensive NLP problems, potentially impacting large-dictionary real-time offline applications like translation or dialog systems on offline mobile platforms.",
        "The use of abstract subwords in other logographic languages like Chorti and Demotic is promising for understanding different logographic languages from the subword perspective.",
        "The proposed metric, degree of distinctness, effectively quantifies the effect of distinctness and reversibility.",
        "The encoder and decoder can be further optimized to build a faster and more accurate model."
    ],
    "3608": [
        "We propose a new task, textual analogy parsing (TAP), which given a sentence about a set of analogous facts, outputs a frame representation that expresses the points of similarity and difference in their meanings.",
        "Our best model employs a globally optimal decoder to enforce the structural constraints of analogy; its outputs can be mapped to well-formed charts of quantitative information extracted from text.",
        "We view this work as an exciting step in the direction of deeper discourse modeling.",
        "Future work might further extend the recovery of analogy as part of information extraction.",
        "This might include TAP outside of the quantitative domain, or TAP at the paragraph level."
    ],
    "3609": [
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Our experiments with both neural and IR methodologies show that QA models still struggle with synthesizing clues, handling distracting information, and adapting to unfamiliar data.",
        "Our adversarially-authored dataset is only the first of many iterations.",
        "As models improve, future adversarially-authored datasets can elucidate the limitations of next-generation QA systems.",
        "Our procedure is applicable to other NLP settings where there is (1) a pool of talented authors who (2) write text with specific goals."
    ],
    "3614": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "OpenBookQA is a new dataset of about 6000 questions for open book question answering that requires simple common knowledge beyond the provided core facts, as well as multihop reasoning combining the two.",
        "Simple neural methods are able to achieve an accuracy of about 50%, but there is still a significant gap between human performance and current performance on this task.",
        "Better retrieval and reasoning can potentially improve the performance on this task."
    ],
    "3615": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our proposed method is simple yet powerful, achieving a significant improvement over strong baseline models.",
        "Explicit semantics in text comprehension and inference are effective and can be easily integrated into neural models.",
        "The adopted robustness of the semantic role labeler can be analyzed through detailed case studies.",
        "Fusing accurate semantic signals for deeper comprehension and inference is a feasible and effective approach."
    ],
    "3616": [
        "Our approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The proposed Cosine Annealing Strategy combines two methods to improve domain adaptation.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The use of pseudo-labeled target-domain data and labeled source-domain data improves the domain adaption performance.",
        "The approach achieves competitive results on the CoNLL-2009 benchmarks for both English and Chinese, without any kind of syntactic information.",
        "The auxiliary tags and multi-hop self-attention improve the SRL performance."
    ],
    "3622": [
        "depthbounding does empirically have the effect of limiting the search space of an unsupervised PCFG inducer.",
        "a significant correlation between parsing accuracy and data likelihood",
        "interesting linguistic properties such as implicit bounding for unbounded grammars",
        "it is possible to accurately induce a PCFG with no strong universal linguistic constraints",
        "the proposed model is able to achieve state-of-the-art or competitive results on datasets in multiple languages."
    ],
    "3623": [
        "The proposed sequence-to-set model based on deep reinforcement learning outperforms competitive baselines by a large margin for the multi-label text classification task.",
        "The approach is able to capture the correlations between labels and is free from the strict restriction of the label order, leading to better robustness and universality.",
        "The proposed method reduces the dependence on the label order, resulting in better performance.",
        "The approach utilizes human prior knowledge rationally, leading to improved performance.",
        "The experimental results demonstrate the effectiveness of the proposed method in terms of robustness and universality."
    ],
    "3624": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "Our method for discovering root-and-pattern morphology in Semitic languages is unsupervised and validated through intrinsic and extrinsic evaluations.",
        "Our root extraction method (JZR) performs well compared to a rule-based language-specific root extractor.",
        "Word-level loss cannot evaluate the translation properly and suffers from exposure bias, while sequence-level objectives are usually indifferentiable and require gradient estimation.",
        "Probabilistic sequence-level objectives based on ngram matching can relieve the dependence on gradient estimation and directly train the NMT model, successfully alleviating the exposure bias through performing greedy search."
    ],
    "3625": [
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.\" (related to the performance of the proposed approach)",
        "Using less than 1% of the training data, our approach can achieve competitive performance compared to the previous systems.\" (related to the efficiency of the proposed approach)",
        "The SUG metric shows the consistency of the model on the copy mechanism.\" (related to the quality of the model's output)",
        "Models with lower SUG are not necessarily worse, and models with high SUG can potentially have very low recall on rare-word translation by systematically copying bad suggestions and failing to translate rare-words where the annotator is incorrect.\" (related to the limitations of the SUG metric)",
        "A high SUG system can be used reliably with a high quality expert.\" (related to the potential benefits of using a high SUG system)",
        "In censorship management or name translation which is strictly sensitive, this quality can help reducing output inconsistency.\" (related to the application of the proposed approach in specific domains)",
        "The SAC metrics show improvement on rare-word translation, but only on the intersection of the phrase table and the reference.\" (related to the performance of the proposed approach on a specific task)",
        "General rare-word translation quality requires additional effort to find the reference aligned to the rare words in the source sentences, which we consider for future work.\" (related to future work and limitations of the current approach)"
    ],
    "3627": [
        "Our proposed RC system significantly outperforms a back-translation method with a state-of-the-art MT system on the Japanese and French SQuAD task.",
        "The approach using soft-alignments from an attentive NMT model can recover answers in the target language, improving the performance of the RC system.",
        "Future work will involve investigating how to improve system robustness toward paraphrasing and alleviate the problem of missing key phrases that stem from NMT."
    ],
    "3629": [
        "Our experiment on measuring relational similarity demonstrated that NLRA outperforms LRA and can successfully solve the data sparseness problem of word co-occurrences, which is a major bottleneck in pattern-based approaches.",
        "Combining the vector offset model and NLRA yielded competitive performance to the state-of-the-art method, though our method relied only on unsupervised learning.",
        "Our future work will apply word-pair embeddings from NLRA to various downstream tasks related to lexical relational information.",
        "NLRA jointly learns the mapping from the word embedding space into the word-pair embedding space to generalize co-occurrences of word pairs and patterns.",
        "NLRA outperforms LRA and can successfully solve the data sparseness problem of word co-occurrences, which is a major bottleneck in pattern-based approaches."
    ],
    "3630": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We show that incorporating cues from the title, the link structure, and the content significantly beats state of the art.",
        "Using the predicted probabilities of our model, we draw on methods for probability calibration to rank news sources by their ideological proportions which moderately correlates with existing surveys on the ideological placement of news sources.",
        "Our proposed framework effectively leverages cues from multiple views to yield state of the art interpret-able performance and sets the stage for future work which can easily incorporate other modalities like audio, video, and images."
    ],
    "3632": [
        "The proposed method for cross-lingual transformation of monolingual embeddings enjoys both good prediction power and robustness, as evidenced by its impressive performance on multiple evaluation benchmarks.",
        "Simultaneously optimizing the bi-directional mappings with respect to Sinkhorn distances and back-translation losses on both ends leads to a more effective cross-lingual transformation.",
        "The novel method for cross-lingual transformation can be applied in an unsupervised manner, without relying on bilingual dictionaries.",
        "The model shows robustness in the presence of insufficient bilingual dictionaries, making it a promising approach for real-world applications where such dictionaries may not be available.",
        "Extending the work to the semi-supervised setting, where insufficient bilingual dictionaries are available, is a potential direction for future work."
    ],
    "3633": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Tagging ambiguity is mostly local while dependency parsing heavily depends on POS tags to predict syntactic structures.",
        "Enabling \"tag \u2192 parse\" only also improves the tagging accuracy itself.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages."
    ],
    "3636": [
        "We demonstrated that the alignment-assisted systems can achieve competitive performance compared to strong transformer baselines.",
        "The alignment-assisted systems outperformed standard transformer models when used for dictionary-guided translation on two tasks.",
        "We achieved a speedup factor of 1.8 by pruning alignment hypotheses in alignment-based decoding while maintaining translation quality.",
        "In future work, we plan to investigate alternative pruning methods like histogram pruning.",
        "We also plan to investigate the performance of alignment-assisted transformer models in constrained decoding settings, where the user demands specific translation of certain words."
    ],
    "3637": [
        "The new asymmetric word embeddings significantly outperform their base models and achieve state-of-the-art performance on SciTail.",
        "Our approach is effective even when trained on other datasets.",
        "AWE-Decomp-Att and AWE-DeIsTe achieve significant improvements over their base models.",
        "The learned asymmetric word embeddings are helpful for textual entailment tasks.",
        "Future research includes extending this work to other advanced word-word interaction based models and other asymmetric text matching tasks such as questionanswering."
    ],
    "3639": [
        "The proposed subword-level word embedding model and generalization method can capture morphological knowledge and generate good estimates of word vectors for out-of-vocabulary words.",
        "The intrinsic evaluation on word similarity tasks and extrinsic evaluation on POS tagging task demonstrate the effectiveness of the proposed model in capturing morphological knowledge and generating good estimates of word vectors for OOV words.",
        "The proposed method can extend pre-trained word embeddings with fixed size vocabularies to estimate word embeddings for out-of-vocabulary words."
    ],
    "3640": [
        "Incorporating external knowledge into question answering and generative models improves total accuracy.",
        "Our framework for machine reading comprehension includes a question answering model and a question generation model, and incorporates four open KBs as external knowledge.",
        "Incorporating additional evidence from open KBs improves the performance of our approaches.",
        "Our extensive model analysis and error analysis show the advantages and limitations of our approaches.",
        "Our framework has the potential to improve machine reading comprehension tasks by leveraging external knowledge."
    ],
    "3641": [
        "Integrating various types of knowledge, including unlabeled question utterances, typing information from external resources, and an improved grammar, improves the accuracy of a conversational table-based semantic parser.",
        "Our approach has the ability to discovery and utilize previously generated logical forms to understand the meaning of the current utterance.",
        "Incorporating knowledge improves the accuracy of our model, which achieves state-of-the-art accuracy on the SequentialQA dataset.",
        "Each module in the entire pipeline can be conventionally improved.",
        "Our conversational table-based semantic parser, CAMP, integrates various types of knowledge and has the ability to understand the meaning of the current utterance."
    ],
    "3647": [
        "GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone.",
        "For the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Our metric can be used one component at a time: MA for evaluating model predictions only, S and SES for analyzing the quantitative and semantic reliability of a dataset, respectively.",
        "Overall, MASSES provides a single accuracy score that makes it comparable to other metrics such as VQA3+ or WUPS.",
        "Further investigation is needed to explore the functioning of our metric with other VQA models, as well as the impact of using various word embeddings techniques and similarity thresholds on the overall score."
    ],
    "3652": [
        "The authors present a new game-chat based videocontext and many-speaker dialogue task and dataset.",
        "The authors compare their baselines and state-of-the-art discriminative and generative models on this task.",
        "The authors hope that this testbed will be a good starting point to encourage future work on the challenging video-context dialogue paradigm.",
        "In future work, the authors plan to investigate the effects of multiple users (i.e., the multi-party aspect of this dataset).",
        "The authors plan to explore advanced video features such as activity recognition, person identification, etc.",
        "As a simple non-trained retrieval baseline, the authors just return (or rank) the responses based on their frequency in the training set."
    ],
    "3653": [
        "Our proposed sequence-to-sequence architecture with a closed-book decoder significantly outperforms pointer-generator baselines in terms of ROUGE and METEOR scores in both cross-entropy and reinforcement learning setups.",
        "Our 2-decoder model has a stronger encoder with better memory capabilities, which allows it to generate summaries with more salient information from the source text.",
        "The representation power of the encoders' final state is studied in this work, and our simple, insightful 2-decoder architecture can be useful for other tasks that require long-term memory from the encoder.",
        "Our model achieves improvements in a test-only transfer setup on the DUC-2002 dataset in both cross-entropy and reinforcement learning cases."
    ],
    "3655": [
        "The proposed approach of reusing the encoder from a multilingual NMT system as a pre-trained component achieves significant improvements on three downstream tasks.",
        "The approach enables surprisingly competitive zero-shot classification on an unseen language and outperforms cross-lingual embedding base methods.",
        "The results showcase the efficacy of multilingual NMT to learn transferable contextualized representations for many downstream tasks.",
        "The approach allows for performing zero-shot classification on an unseen language, which is a challenging task in natural language processing.",
        "The use of a multilingual NMT system as a pre-trained component leads to better performance on downstream tasks compared to cross-lingual embedding base methods."
    ],
    "3656": [
        "The proposed method uses neural sequence-to-sequence learning to generate radiology impressions from findings.",
        "The customized neural model uses encoded background information to guide the decoding process.",
        "The proposed method outperforms non-neural and neural baselines.",
        "The generated summaries have significant clinical validity.",
        "The proposed method demonstrates cross-organization transferability."
    ],
    "3660": [
        "We explore a novel way to train a machine commenting model in an unsupervised manner.",
        "According to the properties of the task, we propose using the topics to bridge the semantic gap between articles and comments.",
        "Our topic-based approach significantly outperforms previous lexicon-based models.",
        "The model can also profit from paired corpora and achieves state-of-the-art performance under semi-supervised scenarios."
    ],
    "3661": [
        "The lack of supervised data for languages other than English, and particularly for low-resource languages, is a typical problem in industrial applications.",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We extend the development and test sets of the Multi-Genre Natural Language Inference Corpus to 15 languages, including low-resource languages such as Swahili and Urdu.",
        "Our dataset, dubbed XNLI, is designed to address the lack of standardized evaluation protocols in crosslingual understanding.",
        "We present several approaches based on cross-lingual sentence encoders and machine translation systems.",
        "While machine translation baselines obtained the best results in our experiments, these approaches rely on computationally-intensive translation models either at training or at test time.",
        "Cross-lingual encoder baselines provide an encouraging and efficient alternative, and that further work is required to match the performance of translation based methods."
    ],
    "3662": [
        "Our results demonstrate the power of CLMs for recognizing named entity tokens in a diverse range of languages, and that in many cases they can improve off-the-shelf NER system performance even when integrated in a simplistic way.",
        "However, the results from Section 4.2 show that this is not true for all languages, especially when only considering unseen entities in Test: Tagalog and Farsi do not follow the trend for the other languages we assessed even though CLM performs well for Named Entity Identification.",
        "The CLM approach captures a very large fraction of the entity/non-entity distinction capacity of full NER systems, and can be rapidly trained using only entity and non-entity token lists -i.e., it is corpus-agnostic.",
        "For some languages, the CLM can be used directly to improve NER performance; for others (such as Tagalog), the strong NEI performance indicates that while it does not immediately boost performance, it can ultimately be used to improve NER there too."
    ],
    "3665": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "The data annotation method has a significant impact on the model being able to learn how to use a specific style and sentence structures, without an unreasonable impact on the error rate.",
        "Our future work plans to utilize transfer learning in the style-subset method to improve the model's ability to apply various different styles at the same time.",
        "These methods are a convenient way for achieving the goal of stylistic control when training a neural model with an arbitrary existing large corpus."
    ],
    "3666": [
        "Our proposed method enhances generative models with information retrieval technologies for dialogue response generation.",
        "Given a dialogue context, our methods generate a skeleton based on historical responses that respond to a similar context.",
        "The skeleton serves as an additional knowledge source that helps specify the response direction and complement the response content.",
        "Our method is effective in generating more informative and appropriate responses."
    ],
    "3669": [
        "The proposed model achieves superior performance on both bilingual and monolingual evaluation datasets.",
        "The model is the first purely sense-level crosslingual representation learning model with efficient sense induction, where several monolingual and bilingual modules are jointly optimized.",
        "A newly collected dataset for evaluating bilingual contextual word similarity is presented, which provides potential research directions for future work.",
        "The proposed model has efficient sense induction, meaning that it can learn to represent words in a way that is sensitive to their meaning and not just their linguistic form.",
        "The model is able to achieve superior performance on both bilingual and monolingual evaluation datasets, indicating that it is effective at learning to represent words in a way that is appropriate for the task at hand."
    ],
    "3671": [
        "The proposed method outperforms widely-known baselines and only marginally worse than a more sophisticated GRU-based sequence-to-sequence baseline.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The risk of overfitting to a (relatively small) development set in the multiple-choice setting can be demonstrated by a loss in performance from 36.37% to 33.20%.",
        "Incorporating additional co-attention between questions, candidate answers, and the retrieved evidence can improve the performance of the system.",
        "Concatenating the top-50 results of a single (joint) query and feeding the result into a neural reader optimized by several lightly-supervised 'reading strategies' can achieve an accuracy of 37.4% on the test set."
    ],
    "3675": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Using manually annotated scores from word similarity datasets in comparison to single word embeddings and unsupervised word meta-embedding approaches leads to significant performance increases.",
        "Angular-based loss functions are well suited for word meta-learning for both self-supervised learning and the proposed multi-task semi-supervised learning method.",
        "The proposed multi-task learning approach to learning word meta-embeddings as an auxiliary reconstruction task leads to consistent improvements against baselines.",
        "Using meta-embeddings as an auxiliary task for downstream tasks such as Named Entity Recognition, Sentiment Classification and Universal Dependency PoS tagging leads to consistent improvements."
    ],
    "3676": [
        "We presented a curriculum learning sampling strategy to mitigate compounding errors in sequence-to-sequence models.",
        "We find performance improvements particularly for sampling functions that generate monotonically increasing sampling rates that are inversely proportional to the slope of decreasing validation performance.",
        "This is empirically demonstrated when comparing a standard 2-hidden layer LSTM (with identical hyperparameter settings) with the following settings: (1) no sampling strategies, (2) a baseline transition probability replacement sampling, (3) the proposed nearest neighbor replacement sampling technique, (4) scheduled sampling and (5) a combination of (3) and (4).",
        "We find the best sampling probability settings for each dataset along with the corresponding cumulative probability functions and conclude that in general, an exponential increase and static sampling probability during training performs better over sigmoid and linear functions.",
        "In other words, a schedule that samples with relatively high probability early in training leads to a performance degradation whereas sampling with high probability towards the end of training can improve performance.",
        "We also find that test set perplexity scores increase for both datasets when using nearest neighbor replacement sampling in conjunction with scheduled sampling."
    ],
    "3677": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "AIM can be viewed as a more principled version of the classical MMI method in that AIM is able to directly optimize the (lower bounder of) the MMI objective in model training while the MMI method only uses it to rerank response candidates during decoding.",
        "Our methods lead to more informative and diverse responses in comparison to existing methods."
    ],
    "3678": [
        "The creation of a new corpus, CompSent-19, which contains 7,199 sentences from diverse domains, including 27% comparative sentences with annotated preference directions.",
        "The importance of words between compared items in detecting comparisons and categorizing the preference direction.",
        "The development of a feature-based supervised approach for identifying comparative sentences and categorizing their preference directions.",
        "The integration of the best classifier into a system that can efficiently mine comparative sentences from web-scale sources and identify the direction of comparisons.",
        "The potential for future work in exploiting neural classification approaches, integrating contextualized word representations, and better handling direction shifters like negations and complex implicit syntactic comparative constructions."
    ],
    "3681": [
        "The proposed extensions to the back-translation model can significantly boost the style transfer accuracy for all three tasks, while preserving the meaning.",
        "The first extension focuses on creating a latent representation that is better grounded in meaning.",
        "The second extension targets providing a feedback to the generator that guides it to produce sentences similar to the original sentence.",
        "The extensions allow for a considerable increase in style transfer accuracy, while maintaining the meaning of the generated sentences."
    ],
    "3682": [
        "Our approach (feature mover GAN) delivers good performance compared to existing text generation approaches.",
        "FM-GAN has the potential to be applied on other tasks such as image captioning, joint distribution matching, unsupervised sequence classification, and unsupervised machine translation.",
        "Our model demonstrates good performance on several tasks.",
        "Existing text generation approaches can be improved by applying our novel approach.",
        "There is potential for FM-GAN to be applied on a wide range of tasks in natural language processing and computer vision."
    ],
    "3684": [
        "'We propose a completely automated end-to-end neural network model, DeClarE, for evidence-aware credibility assessment of natural language claims without requiring hand-crafted features or lexicons.'",
        "'DeClarE captures signals from external evidence articles and models joint interactions between various factors like the context of a claim, the language of reporting articles, and trustworthiness of their sources.'",
        "'Extensive experiments on real world datasets demonstrate our effectiveness over state-of-the-art baselines.'"
    ],
    "3685": [
        "Commercial parsers trained using data-driven approaches often have poor performance after deployment when encountering a variety of complex paraphrases and out-of-vocabulary words.",
        "Our proposed paraphrase-driven parsing approach can improve the performance of standalone base parsers by converting complex paraphrases to more familiar utterances using a paraphrase generator.",
        "We propose two new paraphrase generation techniques suitable for use in our application, and our experimental results validate the effectiveness of our combined models irrespective of the base parser being used or the test data distribution observed."
    ],
    "3686": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Using triads plays an important role in coreference resolution tasks, but it is often ignored.",
        "Our triad-based model addresses this gap.",
        "This model can be additionally constrained to improve performance, and if necessary, easily extended from triads to polyads with higher order."
    ],
    "3687": [
        "Leveraging contextual information and introducing distributional priors via pre-trained word embeddings in our proposed topic models result in learning better word/document representation for short and long documents, and improve generalization, interpretability of topics and their applicability in text retrieval and classification.",
        "The CD-ROM and manuals for the March beta - there is no X windows server there. Will there be? Of course. (Even) if Microsoft supplies one with NT , other vendors will no doubt port their's to NT."
    ],
    "3688": [
        "The current phenomena of word embeddings being biased towards frequency information rather than semantic information is problematic.",
        "More than half of rare words in both tasks are nouns, such as company names and city names, which may share similar topics with popular entities.",
        "Rare words and popular words should lie in the same region of the embedding space, but they are not in the current embeddings.",
        "The embeddings learned in several tasks are biased towards word frequency, and this makes learned word embeddings ineffective, especially for rare words.",
        "A simple yet effective adversarial training method can improve the model performance, which is verified in a wide range of tasks.",
        "The theoretical aspects of word embedding learning and the proposed adversarial training method will be investigated in the future.",
        "The problem of biased word embeddings is not limited to NLP, and it will be studied in other applications as well."
    ],
    "3690": [
        "Our method can significantly speed up the moderation process and make it more reliable.",
        "Our method highlights the words that made the model think so, which is significant help for moderators.",
        "We believe that our method can significantly improve the moderation process.",
        "We obtained good results on inappropriate comments detection task and nearly 90% precision when highlighting inappropriate parts of these comments.",
        "In the future, we plan to improve the evaluation of highlighting and analyze how well it performs with various types of inappropriateness.",
        "We are also looking into the possibility of incorporating additional data into our algorithm, such as other comments from the same thread, article for which the comments are created, or even user profiles."
    ],
    "3691": [
        "The proposed Latent Topic Conversational Model (LTCM) can generate more interesting and diverse responses compared to other latent variable models.",
        "The LTCM combines a seq2seq model and a neural topic model to learn global semantic representations and local word transitions jointly.",
        "The LTCM has shown its capability to generate more interesting and diverse responses, as confirmed by both corpus-based evaluation and human assessment.",
        "Future work could be to study the learned representations and use them to control the meaning of the generated responses."
    ],
    "3692": [
        "DTLM surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The results of our experiments on four varied transduction tasks show large gains over alternative approaches."
    ],
    "3693": [
        "Our method helps improve a video feature encoder by leveraging the diversity of captions and a multitask framework to solve a multitask loss function through a convex optimization.",
        "Our method shows promising results and in a human evaluation was ranked the highest and most useful among other methods for helping the visually impaired.",
        "Our method leverages the diversity of captions and a multitask framework to solve a multitask loss function through a convex optimization.",
        "We have presented a novel multitask encoder-decoder framework for semantic video and movie description."
    ],
    "3694": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "Human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The dichotomy between the two levels of embeddings hints at applications in style-transfer."
    ],
    "3695": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets."
    ],
    "3696": [
        "The paragraph-level neural network model outperforms previous systems for SE type classification and approaches human-level performance.",
        "The model can incorporate SE type information in various downstream applications, such as many information extraction applications that require distinguishing specific fact descriptions from generic statements.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning, and the model can be investigated in other learning problems.",
        "The paragraph-level model builds context-aware clause representations by modeling inter-dependencies of clauses in a paragraph."
    ],
    "3697": [
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The ensemble especially outperforms when there is high variance within the data and on classes with few examples.",
        "Some combinations such as shallow learners with deep neural networks are especially effective.",
        "Our error analysis on results of the ensemble identified difficult subtasks of toxic comment classification.",
        "A large source of errors is the lack of consistent quality of labels.",
        "Most of the unsolved challenges occur due to missing training data with highly idiosyncratic or rare vocabulary.",
        "We suggest further research in representing world knowledge with embeddings to improve distinction between paradigmatic contexts."
    ],
    "3699": [
        "'XELMS is the first approach that can train a single model for multiple languages, making more efficient use of available supervision than previous approaches.'",
        "'Our analysis sheds light on the poor performance of zero-shot XEL in realistic scenarios where the prior probabilities for candidates are unlikely to exist.'",
        "'We also show how in low-resource settings, XELMS makes it possible to achieve competitive performance even when only a fraction of the available supervision in the target language is provided.'",
        "'The task of candidate generation is currently limited by existence of a target language Wikipedia and remains a key challenge.'",
        "'A joint inference framework which enforces coherent predictions could also lead to further improvements for XEL.'",
        "'Similar techniques can be applied to other information extraction tasks like relation extraction to extend them to multilingual settings.'"
    ],
    "3700": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism.",
        "Two methods are combined by the proposed Cosine Annealing Strategy.",
        "The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.",
        "We observe that disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our overall contribution is threefold: first, we have demonstrated that a principled prediction of both binary and gradient argumenthood judgments is possible with informed selection of lexical features; second, we justified the utility of our binary PP argumenthood classification as a standalone task by reporting performance gains on multiple end-tasks through encoder pretraining.",
        "We have conducted a proof-of-concept study with a novel gradient argumenthood prediction task, paired with a new public dataset."
    ],
    "3705": [
        "Our proposed method, Cross-View Training (CVT), enables semi-supervised learning by leveraging model predictions on unlabeled data to produce effective representations that yield accurate predictions.",
        "CVT allows models to effectively leverage their own predictions on unlabeled data, training them to produce effective representations that yield accurate predictions even when some of the input is not available.",
        "We achieve excellent results across seven NLP tasks, especially when CVT is combined with multi-task learning."
    ],
    "3706": [
        "The proposed Dynamic Memory Induction Networks (DMIN) achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The model can adapt and generalize better to support sets and unseen classes through the use of external working memory with dynamic routing.",
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The initial experiments on the byte-level NER models across datasets motivate these models as a useful end-to-end alternative for entities that naturally exist at the subword level.",
        "Further investigations into byte-level models could help facilitate more precise byte-level annotation schemes for the biomedical domain."
    ],
    "3708": [
        "The proposed pipeline can classify Hinglish text into three categories with better performance than other systems.",
        "The pipeline includes an LSTM-based model that is effective in hate speech detection tasks for code-switched languages.",
        "The authors release the code, dictionary, and embeddings trained in the process.",
        "The model achieves better performance compared to other systems.",
        "The proposed pipeline has potential applications in hate speech detection tasks for code-switched languages."
    ],
    "3709": [
        "Our proposed deformable stacked structure allows for dynamic connections between adjacent layers, leading to improved performance on NLP tasks.",
        "We propose an approximate strategy to make the whole neural network differentiable and end-to-end trainable.",
        "Our model achieves state-of-the-art performances on the OntoNotes dataset.",
        "There are potential directions for future work, including extending the model to build more flexible neural architectures and applying it to other NLP tasks such as parsing.",
        "The model does not require task-specific knowledge, making it easy to apply to a variety of NLP tasks."
    ],
    "3710": [
        "The effectiveness of SLOR as a metric for fluency evaluation of NLG tasks:",
        "The adaptation of SLOR to WordPieces (WP-SLOR):",
        "The correlation between proposed referenceless metrics and fluency ratings:",
        "The applicability of referenceless metrics in various settings:",
        "The improvement of fluency evaluation using ROUGE-LM:"
    ],
    "3711": [
        "Our proposed method outperforms several competitive baselines.",
        "Our model includes textual, visual, and acoustic modalities, and incorporates several grammatical and acoustic constraints.",
        "Empirical experiments on a movie dataset demonstrated the effectiveness of our proposed method.",
        "An SC-MemN2N model that leverages our speaker naming model can achieve state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.",
        "The dataset annotated with character names introduced in this paper is publicly available from http://lit.eecs.umich.edu/downloads.html."
    ],
    "3713": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Our approach for attenuating biases works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The new challenging and realistic semantic parsing task directly benefits both NLP and DB communities.",
        "Experimental results on several state-of-the-art models on this task suggest plenty of space for improvement."
    ],
    "3715": [
        "Our approach outperforms sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The DNN model is able to generate compact embedded representations for the subtasks, while the CRF can perform global inference over arbitrary graph structures.",
        "Combining the two approaches (DNNs and CRF) yields state-of-the-art results.",
        "The framework has the potential to be extended to a cross-domain or cross-language setting.",
        "Trying an ensemble of neural networks with different initial seeds is another possible research direction."
    ],
    "3716": [
        "I evaluated the efficiency of different feature groups and found that readability and complexity scores as well as topic models to be effective predictors.",
        "Gradient Boosted Trees outperform the rest of the models in this problem.",
        "The system performed excellent achieving 98.2% in the test data.",
        "One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used.",
        "I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.",
        "The claims highlight the effectiveness of certain feature groups and the performance of the system in achieving high accuracy. Additionally, the claims raise questions about potential biases in the process and the need for further analysis and validation to answer these questions."
    ],
    "3717": [
        "The authors provide a manually crafted, tentative reference for Open Information Extraction (OIE) tasks, including 343 manually extracted facts and over 57 sentences.",
        "The reference includes n-ary relations and coreference information, and the authors believe it is valuable because it offers a common frame of reference for testing and comparing OIE systems.",
        "The authors assess seven OIE systems using a fine-grained token-level scorer and find that the MinIE system performs best.",
        "The authors provide guidelines for annotating data with a reasonable inter-annotator agreement, but note that the task of OIE is difficult due to the fine line between useful reformulation of information and ill-advised inference.",
        "The authors believe that their reference is not too overwrought and invite further contributions from other researchers."
    ],
    "3719": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We have the following recommendations: \u2022 Charts in the wild: The charts in FigureQA and DVQA were methodologically generated, but human-generated charts in real-world business and scientific documents can contain variations that these datasets omit.",
        "Human-generated questions: The questions in both FigureQA and DVQA were created with templates, which do not capture all the nuances of natural language.",
        "Document-level CQA: FigureQA and DVQA have well-defined image regions and all information needed to answer a question is contained in that image."
    ],
    "3725": [
        "We proposed a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "We have shown that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globallynormalized models be trained in a similarly inexpensive way?",
        "We have demonstrated the success of a neural architecture based on a graph convolutional network for relation extraction.",
        "Our model has complementary strengths to sequence models, and that the proposed pruning technique can be effectively applied to other dependency-based models."
    ],
    "3728": [
        "ITS is an iteration-based extractive summarization model inspired by the observation that humans often need to read an article multiple times to fully understand and summarize it.",
        "Experimental results on CNN/DailyMail and DUC corpora demonstrate the effectiveness of our model.",
        "Our model is based on the observation that humans often need to read an article multiple times to fully understand and summarize it.",
        "The effectiveness of our model is demonstrated through experimental results on CNN/DailyMail and DUC corpora.",
        "Our model is an iteration-based extractive summarization model."
    ],
    "3730": [
        "The model performs remarkably well on standard hate speech datasets despite minimal tuning of hyperparameters.",
        "Our clustering analysis adds interpretability to the results, enabling inspection of the results.",
        "The majority of recent deep learning models in hate speech rely on word embeddings for the bulk of predictive power.",
        "Sequence-based approaches are typically important when phenomena such as negation, co-reference, and context-dependent phrases are salient in the text.",
        "We plan to investigate character-based representations using character CNNs and highway layers along with word embeddings to allow robust representations for sparse words such as hashtags."
    ],
    "3732": [
        "The proposed attention-based CNN model achieves a precision of 79% on the full-text dataset and 66% on the symptoms set.",
        "On a confidence threshold of 0.6, precision increases to 85% and 75%, respectively.",
        "The learned attention weights allow for computing the symptom relevance and extracting warning symptoms more precisely.",
        "The attention score is used to make the recommendation rationale transparent."
    ],
    "3735": [
        "Our proposed AMR aligner is tuned by a novel transition-based AMR oracle parser, which enhances its performance.",
        "Our aligner achieves higher alignment F1 scores and consistently improves two open-sourced AMR parsers.",
        "The intrinsic evaluations show the effectiveness of our aligner.",
        "The extrinsic evaluations demonstrate the superiority of our aligner over other AMR parsers.",
        "Our aligner is enhanced by rich semantic resources, which improve its performance.",
        "Our transition-based AMR parser achieves a performance of 68.4 Smatch F1 score via ensemble with only words and POS tags as input."
    ],
    "3736": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Seq2Seq models can learn to verbalize structured inputs in a decent way, and their success depends on the extent of the domain and available (clean) training data.",
        "Neural NLG models can even surpass human performance in terms of automatic evaluation measures.",
        "The character-based model performed better on the E2E dataset, whereas the word-based model generated more correct outputs on the WebNLG dataset.",
        "Character-based models were found to have a significantly higher output diversity.",
        "Trained on two templates, models could generalize beyond their training data and come up with novel texts.",
        "In future work, we would like to extend this line of research and train more model variants on a higher number of templates."
    ],
    "3738": [
        "The proposed dialogue state tracker has state-of-the-art accuracy.",
        "The model does not require manually-tagged user utterances.",
        "The model is scalable for an increasing number of slots, and the number of model parameters does not increase with the number of slots.",
        "The model is independent of the number of slot values.",
        "The model can share parameters among different slots.",
        "The model can make predictions on new values for a given slot as long as we have the corresponding word vector.",
        "The use of a fixed-size candidate set can reduce computational complexity.",
        "The proposed method has not been evaluated in scenarios with new slots or more unobserved slot values.",
        "The domain-transferring ability of the models is yet to be evaluated."
    ],
    "3739": [
        "our model significantly outperforms existing HRED models and its attention variants.",
        "the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "all three metrics are needed to make meaningful comparisons among models.",
        "our components can be applicable to a broad range of generation tasks.",
        "the correlation between Met and Sim is very large."
    ],
    "3741": [
        "The combined model (using both hand-built and ELMo features, and both LSTM and CRF components) shows superior performance in all tasks, as shown in Table 2.",
        "The use of both kinds of features (hand-built and ELMo) and both major model components (LSTM and CRF) is making meaningful use of the data.",
        "The structural models are on-par with or surpass state-of-the-art published systems, as shown in the results on benchmark datasets."
    ],
    "3744": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our model uses zero-shot transfer from closely-related high-resource languages and improves accuracy by 17% on average over baseline systems.",
        "We also show its ability to transfer across orthographies through phonological representations.",
        "An immediate future focus for our work could be training a model that predicts the 'best' pivot language for a given named entity mention, which can replace the language-specific phylogenetic-distance-based weights used in this work.",
        "Universal multilingual encoders have had success in tasks like translation and can potentially ease the scaling up of our model to a large number of languages."
    ],
    "3747": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We propose a relation extraction approach based on capsule networks with attention mechanism.",
        "The model improves the precision of the predicted relations.",
        "In the future, we tend to resolve the situation of how to assign predicted relationship to multi entity pairs when two entities have multi-relations by utilizing prior knowledge such as entity type and joint training with named entity recognition.",
        "We will also try to optimize the model in terms of speed and focus on other problems by leveraging class ties between labels, specially on multilabel learning problems.",
        "Dynamic routing could also be useful to improve other natural language processing tasks such as the sequenceto-sequence task and so on."
    ],
    "3748": [
        "The proposed variational self-attention model (VSAM) builds a self-attention vector as random variables by imposing a probabilistic distribution, allowing for multi-modal attention distributions.",
        "VSAM is more robust against overfitting compared to conventional deterministic counterparts, especially for small datasets.",
        "The stochastic units incorporated by VSAM allow for more flexible modeling of attention distributions, leading to superior performance on the stance detection task.",
        "Marginalizing over the latent variables in VSAM helps to reduce overfitting and improve the method's robustness."
    ],
    "3750": [
        "We have studied the new task of text infilling, which aims to fill missing portions of a given sentence/paragraph.",
        "The task generalizes previous settings and permits an arbitrary number of missing portions each of which can originally have an arbitrary unknown number of tokens.",
        "On a variety of supervised datasets, the self-attention model improved over the seq2seq and GAN-based models.",
        "Text infilling is of wide practical use in real life.",
        "We look forward to investigating more sophisticated solutions.",
        "A More Details of Text Infilling Self-Attention Model Here we provide detailed description of the self-attention model adapted from (Vaswani et al., 2017) for our task."
    ],
    "3751": [
        "The proposed novel constituent hierarchy predictor based on recurrent neural networks outperforms the state-of-the-art baseline parser by achieving higher F1 scores on standard WSJ and CTB evaluations.",
        "The resulting fully-supervised parser maintains the same model size and speed as the baseline parser, but with improved performance.",
        "The proposed method addresses the limitation of shift-reduce parsers in not leveraging right-hand side syntax for local decisions.",
        "The systematic probe for measuring biases in word representations using natural language inference reveals that GloVe, ELMo, and BERT embeddings all encode gender, religion, and nationality biases.",
        "A projection-based method can effectively attenuate biases in contextualized embeddings (ELMo, BERT) by debiasing the first non-contextual layer alone, without loss of entailment accuracy.",
        "The approach to attenuating biases in contextualized embeddings is effective for the well-characterized gender direction."
    ],
    "3754": [
        "The CLEVR-Ref+ dataset is a new dataset for referring expressions that minimizes dataset bias and provides ground truth visual reasoning processes.",
        "Existing state-of-the-art referring object detection and referring image segmentation models are evaluated on CLEVR-Ref+, with the IEP-Ref model outperforming them by a large margin.",
        "The IEP-Ref model uses a module network approach and reveals an easy and natural way of revealing the reasoning process.",
        "The quantitative evaluation shows high IoU at intermediate steps, proving that the neural modules have learned the job they are supposed to do.",
        "The IEP-Ref model can correctly handle false-premise referring expressions.",
        "Going forward, it is interesting to see whether these findings will transfer and inspire better models on real data expression categories.",
        "The random sampling during training may be responsible for the improved performance, rather than active learning."
    ],
    "3756": [
        "a caption generator benefits from having its embedding layer and RNN transferred from a language model, but mostly when transferring from an in-domain corpus.",
        "pre-training the embedding layer and RNN on the text of the same captions dataset that it will eventually be trained on after transferring",
        "partial training of the language model does not seem like a reliable way to prevent overspecialization from happening",
        "training a caption generator on MSCOCO after having pre-trained the prefix encoding parameters on the text of MSCOCO itself",
        "limiting the amount of images they are trained on can make CNNs produce more generic image features that are more useful for caption generators",
        "the observation that better models do not necessarily transfer better holds in general"
    ],
    "3758": [
        "Reducing vocabulary using word-label mapping is a simple and effective way of smoothing phrase translation models, improving translation quality by up to +0.7% BLEU and -0.8% TER compared to a standard phrase-based SMT baseline.",
        "Different word-label mappings are almost equally effective for smoothing phrase translation models, allowing for the use of any type of word-level label, including randomized vocabularies, for the smoothing.",
        "The same level of performance can be achieved using different types of word-level labels for smoothing phrase translation models, emphasizing the fundamental sparsity of the standard phrase translation model.",
        "Vocabulary reduction is more suitable for largescale translation setups, and OOV handling is more crucial than smoothing phrase translation models for low-resource translation tasks.",
        "The effect of changing a single parameter in estimating the label space on scoring hypotheses is not significant, as many other models than the smoothed translation model, such as language models, are involved with large weights.",
        "An exact linguistic explanation is still to be discovered for the improvement in translation quality achieved by smoothing phrase translation models."
    ],
    "3759": [
        "Our proposed pipeline greatly improves sentence translation based on cross-lingual word embedding.",
        "We achieved context-aware lexical choices using beam search with LM.",
        "Our novel insertion noise shows a promising performance even combined with other noise types.",
        "Our methods do not need back-translation steps but still outperform costly unsupervised neural MT systems.",
        "We proved that for general translation purpose, an effective cross-lingual mapping can be learned using only a small set of frequent words, not on subword units."
    ],
    "3762": [
        "The described techniques have a significant impact on the final result, as shown by ablation studies.",
        "Removing one of the features (LSTM-800) results in a decrease in performance.",
        "The best model (ep 2) uses two layers with a hidden size of 800.",
        "The LSTM-400 layer has a smaller size than the LSTM-800 layer."
    ],
    "3763": [
        "The MIMN model achieves new state-of-the-art results on the SNLI, MPE, and SCITAL datasets.",
        "The MIMN model employs a multi-turns inference mechanism to process multi-perspective matching features.",
        "The MIMN model uses a memory mechanism to carry proceeding inference information.",
        "The MIMN model is good at handling the relationships of entailment and contradiction.",
        "The MIMN model can extract important information from multiple premises for the final judgment."
    ],
    "3764": [
        "The Multi-Perspective Fusion Network (MPFN) outperforms previous models on the Commonsense Reading Comprehension (CMC) task, achieving an accuracy of 83.52% on MCScript.",
        "Union fusion based on choice-aware passage, question, and choice can surpass TriAN and HMA models.",
        "Difference fusion performs stably and is comparable with union fusion.",
        "Word-level union fusion significantly influences context-level fusion.",
        "Choice-aware passage word embedding can activate similarity fusion.",
        "Combinating similar parts and difference parts together can obtain the best performance among two-perspective models.",
        "Taking three types of fusion methods into consideration, MPFN model achieves a state-of-the-art result."
    ],
    "3765": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Our proposed model, Masque, is based on multi-source abstractive summarization and learns multi-style answers together.",
        "The key to its success is transferring the style-independent NLG capability to the target style by use of the question-passages reader and the conditional pointer-generator decoder.",
        "Our future work will involve exploring the potential of our multi-style learning towards natural language understanding."
    ],
    "3767": [
        "The approach used in this study relies mainly on the strength of the Transformer Network based entailment classification and involves a minimum of heuristics.",
        "The main performance gains come from adding retrievals that resolve named entities rather than matching the claim text only, filtering fewer of the retrievals, and making the entailment classifier somewhat aware of the topic of what it is reading by including the title.",
        "If higher quality and more plentiful multi-evidence claims would be constructed, it would be nice to incorporate dynamic retrievals into the system, allowing the classifier to decide that it needs more information about keywords it encountered during reading."
    ],
    "3768": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "A sequential matching model based only on chain sequence can outperform all previous models, including hierarchy-based methods.",
        "The proposed model achieved top one results on both datasets under the noetic end-to-end response selection challenge in DSTC7, and yielded new state-of-the-art performances on two largescale public multi-turn response selection benchmarks."
    ],
    "3772": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Using a new grammatically annotated analysis set, we identify several syntactic phenomena that are predictive of good or bad performance of current state-of-the-art sentence encoders on CoLA.",
        "Our findings can guide future work on sentence representation.",
        "Transformer models appear to have an advantage over sequence models with long-distance dependencies, but still struggle with these constructions relative to more local phenomena.",
        "This analysis set can be used by engineers interested in evaluating the syntactic knowledge of their encoders.",
        "Our results are purely correlational, and do not mark whether a particular construction is crucial for the acceptability of the sentence.",
        "Future experiments following Ettinger et al. (2018) and Kann et al. (2019) can semi-automatically generate datasets by manipulating, for example, length of long-distance dependencies, inflectional violations, or the presence of interrogatives, while controlling for factors like sentence length and word choice, in order to determine the extent to which these features impact the quality of sentence representations."
    ],
    "3773": [
        "The proposed framework consists of a Generator and a Reward Manager to solve the SEG problem.",
        "The Generator is capable of handling OOV and repetitive words using a pointer-generator network with coverage mechanism.",
        "A mixed loss method is introduced to encourage the Generator to produce story endings of high semantic relevance with story plots.",
        "The Reward Manager can fine tune the Generator through policy-gradient reinforcement learning, promoting the effectiveness of the Generator.",
        "Experimental results on ROCStories Corpus demonstrate good performance in both automatic evaluation and human evaluation."
    ],
    "3777": [
        "The system achieves 8.1% WER on a test set of broadcast conversations.",
        "The system also performs punctuation recovery and speaker identification.",
        "The speaker identification models are created using weakly supervised training and achieve 66% time-weighted speaker recognition recall at 93% precision on a broadcast news test set.",
        "The current state of the TT \u00dc Estonian speech transcription system is described.",
        "The system achieves 12.9% WER on conference speeches and 22.7% WER on various user-made recordings 'from the wild'."
    ],
    "3784": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets using Dynamic Memory Induction Networks (DMIN) for few-shot text classification.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning, and we will investigate this type of models in other learning problems.",
        "It is possible to modify models to be incremental while achieving similar performance as standard question answering methods."
    ],
    "3785": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Document-level CQA: FigureQA and DVQA have well-defined image regions and all information needed to answer a question is contained in that image.",
        "To understand charts in documents, information in the rest of the document may be necessary to answer questions about the chart.",
        "Human-generated questions: The questions in both FigureQA and DVQA were created with templates, which do not capture all the nuances of natural language."
    ],
    "3788": [
        "The current NDCG evaluation method for Visual Dialog is not reliable, as it relies on human annotations and can be influenced by biases in the dataset.",
        "The task of Visual Dialog remains unchanged, despite the introduction of explicit classification tasks on candidate answers.",
        "The CCA baseline proposed by Massiceti et al. (7) is not close to state-of-the-art and ignores other important metrics such as MRR and R@k.",
        "The VisDial dataset and evaluation are not perfect and may have biases and trivial correlations that can be picked up by models.",
        "Automatic evaluation of dialog is an open research problem, and human evaluation paired with dialog models would be a more robust form of evaluation.",
        "There is scope for improvement across all axes - task/dataset, evaluation, as well as methods."
    ],
    "3789": [
        "The proposed method (SCITE) achieves better performance by leveraging transfer learning and multihead self-attention. However, the performance is still limited by the insufficiency of high-quality annotated data.",
        "The existing HRED models and attention variants are not effective in capturing the long distant dependency relations, leading to poor performance in multiturn dialogue generation.",
        "The proposed ReCoSa model significantly outperforms existing HRED models and its attention variants, with relevant contexts that are coherent with human judgments.",
        "The use of self-attention mechanism can effectively capture the long distant dependency relations, improving the quality of multiturn dialogue generation.",
        "The model is robust when dealing with out-of-vocabulary issues, showing better performance in practical applications."
    ],
    "3790": [
        "The model PReFIL surpasses prior state-of-the-art methods for both DVQA and FigureQA.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The use of better OCR methods can improve PReFIL's performance.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "The model has the potential to improve retrieval of information from charts, which has numerous applications.",
        "The analysis of the graphs captures dependencies over sub-word units, and the graphs are not discrete.",
        "The use of LSTM encoders in the model can lead to more useful and potentially long-distance dependencies.",
        "The graphs induced by the model are sparse, with average entropy much lower than expected."
    ],
    "3796": [
        "The proposed error-correcting neural language model maintains performance compared to using the full conditional and related approximate methods, given a sufficient codeword size to account for correlations among classes.",
        "The use of Latent Variable Mixture Sampling improves performance when rank ordering the codebook via embedding similarity, where the query is the embedding of the most frequent word.",
        "Latent Variable Mixture Sampling can mitigate exposure bias and outperform scheduled sampling methods with a full softmax, hierarchical softmax, and adaptive softmax on an image captioning task, with less decoder parameters than the full softmax with only 200 bits, 2% of the original number of output dimensions."
    ],
    "3797": [
        "Our framework achieves an accuracy of 54.04% on our dataset, which is far better than previous rule-based systems (9 times improvement in accuracy).",
        "Our framework is trained end-to-end, fully data-driven, and independent of external chemical knowledge.",
        "The advantage of our framework is that it can convert non-systematic names to systematic names, enabling the related chemical information extraction into more practical use stages.",
        "This work starts a brand new research line for the related chemical information extraction, as far as our best knowledge is concerned.",
        "Our framework is able to automatically convert non-systematic names to systematic names, which was previously thought to be impossible."
    ],
    "3799": [
        "The proposed training objective for neural dialogue generation can control the sentiment of the generated response explicitly.",
        "The training objective is implemented through a conditional adversarial training paradigm, which involves using a discriminator to assist in generating sentiment-controlled responses.",
        "The generator in the system can be either the standard SEQ2SEQ or CVAEs-based SEQ2SEQ models.",
        "The system uses a policy gradient algorithm to deal with the optimization challenge posed by discrete generator outputs.",
        "Experiments demonstrate the effectiveness of the proposed training objective in successfully controlling the sentiment of the response and improving content quality.",
        "Future directions include validating the approach on more fine-grained sentiment-based data and combining it with other advanced techniques in reinforcement learning and adversarial learning, such as reward shaping."
    ],
    "3800": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The use of multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "The novel end-to-end model for joint slot label alignment and recognition outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The use of Causal Language Modeling (CLM) and Masked Language Modeling (MLM) objectives for pretraining provides strong cross-lingual features that can be used for pretraining models.",
        "MLM pretraining is extremely effective for unsupervised machine translation, and reaches a new state of the art of 34.3 BLEU on WMT'16 German-English, outperforming the previous best approach by more than 9 BLEU.",
        "The cross-lingual language model can be used to improve the perplexity of a Nepali language model, and provides unsupervised cross-lingual word embeddings.",
        "The translation language modeling (TLM) objective improves cross-lingual language model pretraining by leveraging parallel data, and naturally extends the BERT MLM approach by using batches of parallel sentences instead of consecutive sentences.",
        "The supervised approach using TLM in addition to MLM beats the previous state of the art on XNLI by 4.9% accuracy on average."
    ],
    "3802": [
        "We proposed a context-sensitive spelling corrector based on word embeddings.",
        "Our spell checker is light-weight, unsupervised and can be easily incorporated into downstream applications.",
        "It achieved a favorable spelling correction performance when compared with general purpose spellchecking tools such as PyEnchant, Ekphrasis and Google spell checkers on both synthetic and real misspellings from different datasets.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-the-art published systems."
    ],
    "3805": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "More research is needed to solve label bias in general and to train more general globally-normalized models for NMT.",
        "The application of Universal Transformer architecture to abstractive headline generation outperforms the abstractive state-of-the-art result on the New York Times Annotated corpus.",
        "The newly released Rossiya Segodnya corpus is presented and results achieved by our model applied to it."
    ],
    "3806": [
        "Our approach achieves better performance at rating prediction and is almost on par with other state-of-the-art approaches in aspects coherence.",
        "We jointly learn interpretable user and item representations using ABAE, metric learning, and autoencoder-enriched learning.",
        "Our results contribute to the research effort of analyzing and interpreting deep neural networks, an important recent trend.",
        "Future work may involve improving prediction quality, integrating methods that can remove \"purely sentimental\" aspects into interpretable models, and developing visualization techniques for user profiles."
    ],
    "3812": [
        "The proposed approach outperformed the best results on TREC-2017 LiveQA medical test questions.",
        "Deep learning models achieved interesting results on open-domain and clinical datasets, but obtained a lower performance on consumer health questions.",
        "The proposed approach can be applied and adapted to open-domain as well as specific-domain QA.",
        "The use of a large collection of consumer health questions for training can improve the performance of deep learning models.",
        "Integration of a Question Focus Recognition module can enhance candidate question retrieval."
    ],
    "3813": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The beam problem can largely be explained by the brevity problem.",
        "We have proposed a convolutional neural network-based model for finding location references present in the tweets.",
        "We achieved our best result with an F 1 -score of 0.96 when we used 3-CNN and 2-Dense layers with dropout.",
        "We can find location information of several granularities, such as streets, buildings, city, district, and country name with very impressive accuracy.",
        "This system can be utilized in other domains, such as road traffic management and sports events, in several location-based services by training it with domain-specific tweets.",
        "The trained models can even be integrated with mobile devices to also find location information of tweets on the fly.",
        "A semi-supervised approach can be used to reduce the task of manual labeling to some extent."
    ],
    "3815": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "We present an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Our root extraction method (JZR) with performance not too far from a rule-based language-specific (in this case Arabic) root extractor."
    ],
    "3825": [
        "Glyce treats Chinese characters as images and uses Tianzige-CNN to extract character semantics.",
        "Glyce provides a general way to model character semantics of logographic languages, making it general and fundamental.",
        "Just like word embeddings, Glyce can be integrated to any existing deep learning system.",
        "Glyce achieves better performance compared to traditional methods, as it uses Tianzige-CNN to extract character semantics.",
        "The proposed Glyce is a general and fundamental way to model character semantics of logographic languages, making it applicable to any existing deep learning system."
    ],
    "3827": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-the-art published systems.",
        "The end-to-end model tends to output words with similar meaning, or garbled words, resulting in the generation of unnatural sentences.",
        "It is necessary to balance the trade-off between fluency and diversity when selecting a model.",
        "Evaluation metric for selecting models should consider the sentence length and the number of content words in the sentence.",
        "The average number of word types in the generated corpus can be used to evaluate the diversity of the model."
    ],
    "3833": [
        "The Twitter Job/Employment Corpus has been presented as a new dataset for extracting discourse on work from public social media.",
        "The locally-normalized structure of the model can explain the beam problem in abstractive summarization.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "More research is needed to determine how much improvement remains to be gained by solving label bias in general, and whether more general globally-normalized models can be trained in an inexpensive way."
    ],
    "3839": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We propose a Seq2Seq paradigm for text attribute transfer applications that overcomes lack of parallel data.",
        "Our framework can employ any Seq2Seq model and outperforms previous methods under all measured criteria (content preservation, fluency, and attribute correctness) in both human and automatic evaluation.",
        "The simplicity and flexibility of our approach can be useful in applications that require intricate edits or complete sentence rewrites."
    ],
    "3841": [
        "The proposed semantic framework allows for modeling linguistic expressions as combinations of simple, real-valued referential properties of predicates and their arguments.",
        "The framework was used to construct a dataset covering the entirety of the Universal Dependencies English Web Treebank.",
        "Hand-engineered and learned type-and token-level features were probed for their ability to predict annotations in the dataset.",
        "The proposed approach improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\")."
    ],
    "3844": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "A hierarchy of stochastic layers is employed in the hierarchically-structured variational autoencoder for long text generation, where the priors of the latent variables are learned from the data.",
        "The generated samples exhibit superior quality relative to those from several baseline methods."
    ],
    "3846": [
        "NeuralDater outperforms existing state-of-the-art approaches for document dating.",
        "NeuralDater exploits syntactic and temporal structures in a principled way, leveraging deep learning techniques for document dating.",
        "This is the first application of deep learning techniques for the problem of document dating.",
        "The representation learning techniques explored in this paper will inspire further development and adoption of such techniques in the temporal information processing research community."
    ],
    "3848": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "Cross-lingual transfer does not work out of the box, especially when using large numbers of source languages, and distantly related target languages.",
        "Our unsupervised method, BEA uns , provides a fast and simple way of annotating data in the target language, which is capable of reasoning under noisy annotations, and outperforms several competitive baselines, including the majority voting ensemble.",
        "Light supervision improves performance further, and that our second approach, RaRe, based on ranking transfer models and then retraining on the target language, results in further and more consistent performance improvements."
    ],
    "3851": [
        "The current evaluation and comparison of cross-lingual word embedding (CLE) methods are not comprehensive and hinder our ability to correctly interpret and generalize the key results.",
        "The quality of CLE models is largely task-dependent, and overfitting the models to the BLI task can result in deteriorated performance in downstream tasks.",
        "There is a need for a comprehensive evaluation protocol for CLE models, as well as for unified and comprehensive evaluation protocols.",
        "The study has shed new light on the ability of current cutting-edge CLE models to support cross-lingual NLP.",
        "The study has exposed the need for reassessing existing baselines and has highlighted the most robust supervised and unsupervised CLE models.",
        "The study hopes to encourage future work on CLE evaluation and analysis, and to assist in guiding the development of new CLE models."
    ],
    "3853": [
        "We have presented Recurrent Dual Attention Network (ReDAN), a new multimodal framework for visual dialog, by incorporating image and dialog history context via a recurrently-updated query vector for multi-step reasoning.",
        "This iterative reasoning process enables the model to achieve a fine-grained understanding of multimodal context, thus boosting question answering performance over state-of-the-art methods.",
        "Experiments on the Vis-Dial dataset validate the effectiveness of the proposed approach.",
        "Q: what color is the carpet? A: it is a tan color",
        "Q: is he wearing a bow tie? A: no",
        "Q: how many people? A: just 1",
        "Q: is he wearing a helmet? A: no, he is not"
    ],
    "3856": [
        "The proposed hypergraph-based summarization model captures groups of semantically related sentences and generates informative summaries by extracting a subset of sentences jointly covering the relevant themes of the corpus.",
        "The approach based on the detection of hypergraph transversals can generate summaries of minimal length and achieve a target coverage, or generate a summary achieving a maximal coverage of relevant themes while not exceeding a target length.",
        "The hypergraph model is shown to produce more accurate summaries than other models based on term or sentence clustering.",
        "The overall system outperforms related graph-or hypergraph-based approaches by at least 10% of ROUGE-SU4 score.",
        "Analyzing the performance of other algorithms for the detection of hypergraph transversals, such as methods based on LP relaxations, may further improve the accuracy of the summaries.",
        "Taking into account the polysemy of terms and adapting the model for solving related problems, such as community question answering, may also improve the performance of the approach."
    ],
    "3857": [
        "The proposed model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic Memory Induction Networks (DMIN) outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed model is able to capture the correct relationship between entities in certain cases, even when the name of the entity does not appear in the description.",
        "There are limitations and room for improvement in the current model, such as the nature of the relationship-dependent content masking, which can lead to incorrect predictions.",
        "Exploring news generation techniques to generate high-quality news followed by these writing guidelines is a potential future direction."
    ],
    "3861": [
        "We propose a new approach for inferring concept hierarchies from large text corpora.",
        "Our approach combines Hearst patterns with hyperbolic embeddings to set appropriate constraints on the distributional contexts and improve the consistency in the embedding space.",
        "By computing a joint embedding of all terms that best explains the extracted Hearst patterns, we can then exploit these properties for improved hypernymy prediction.",
        "The natural hierarchical structure of hyperbolic space allows us to learn very efficient embeddings that reduce the required dimensionality substantially over SVD-based methods.",
        "Our embeddings achieve state-of-the-art performance on a variety of commonly-used hypernymy benchmarks."
    ],
    "3865": [
        "The proposed solution can simultaneously extract multiple relations with one-pass encoding of an input paragraph for MRE tasks, achieving a new state-of-the-art results with high efficiency on the ACE 2005 benchmark.",
        "The proposed structured prediction and entity-aware self-attention layers on top of BERT have potentially broader applications beyond relation extraction, such as entity-centric passage encoding in question answering.",
        "The idea of encoding a passage regarding multiple entities has the potential to be applied in other applications beyond relation extraction.",
        "The proposed method achieves high efficiency and surpasses state-of-the-art published systems on benchmark datasets."
    ],
    "3866": [
        "Our method for separating the generation of actions and entities improves story quality, as shown by both human evaluation and automated metrics.",
        "We propose a novel decomposition that effectively improves the quality of short stories.",
        "The proposed method allows for more efficient and effective writing of short stories.",
        "Our approach enables the generation of high-quality stories with well-defined entities and coherent actions.",
        "The human evaluation results show that our method significantly outperforms other state-of-the-art methods in terms of story quality."
    ],
    "3870": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The difficulty of the translation task is not related to the fluency of the references.",
        "Translation quality is similar across all document IDs.",
        "Documents originating from Nepali are harder to translate than documents originating in English.",
        "The existing parallel corpus is closer to English Wikipedia than Nepali Wikipedia."
    ],
    "3871": [
        "The proposed Dual-Attention model for visually-grounded multitask learning uses Gated and Spatial-Attention to disentangle attributes in feature representations and align them with the answer space, leading to improved performance on both Semantic Goal Navigation and Embodied Question Answering tasks.",
        "The proposed model is able to transfer the knowledge of words across tasks and outperforms the baselines by a considerable margin.",
        "Disentangled and interpretable representations make the model modular and allow for easy addition of new objects or attributes to a trained model.",
        "The model has the potential to be extended to transferring knowledge across different domains by using modular interpretable representations of objects which are domain-invariant."
    ],
    "3872": [
        "We presented a novel semantic framework for modeling fine-grained temporal relations and event durations.",
        "We used this framework to construct the largest temporal relations dataset to date -UDS-T -covering the entirety of the UD-EWT.",
        "We used this dataset to train models for jointly predicting fine-grained temporal relations and event durations, reporting strong results on our data and showing the efficacy of a transfer-learning approach for predicting standard, categorical TimeML relations.",
        "Our results suggest that the community is ready for more difficult CQA datasets.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "Better OCR methods led to better results for DVQA."
    ],
    "3873": [
        "We show how training on synthetic character-level noise, similar in spirit to dropout, can significantly improve a translation model's robustness to natural spelling mistakes.",
        "Deleting and inserting random characters play a key role in preparing the model for test-time orthographic variations.",
        "Our method works well on typos, but it does not appear to generalize to non-standard text in social media.",
        "Spelling mistakes constitute a relatively small part of the deviations from standard text, and that the main challenges in this domain stem from other linguistic phenomena."
    ],
    "3875": [
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We release our MultiATIS++ corpus to facilitate future research on cross-lingual NLU to bridge the gap between cross-lingual transfer and supervised methods.",
        "The use of multilingual BERT encoder, machine translation, and label projection are evaluated in our study.",
        "Incremental reference resolution using an end-to-end differentiable memory network is demonstrated to be viable.",
        "Semisupervised learning from a language modeling objective substantially improves performance.",
        "The performance on longer texts, such as the full-length news articles encountered in OntoNotes, is a key question for future work.",
        "Reducing the amount of training data and incorporating linguistically-motivated constraints based on morphosyntactic features are potential directions for future exploration."
    ],
    "3876": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We propose document-level CQA, which requires document question answering, page segmentation, and more to better match real-world usage.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments."
    ],
    "3877": [
        "The dominance of character-based model units in the LAS speech model is not due to the amount of training data.",
        "The behavior of the model is more likely related to the model itself (e.g., the decoder is conditioned on all predecessor labels).",
        "Word-piece based attention models can achieve a relatively low oracle WER with only 8-best hypotheses and rescoring that N-best hypotheses using graphemic or phonemic models gives good improvements.",
        "Streaming end-to-end approaches (e.g., RNN-T [37, 38] ) may show similar trends as word-piece based attention models."
    ],
    "3879": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The performance of AC-GCN improves with the length of documents, thus indicating that richer context leads to better model prediction.",
        "The performance of OE-GCN improves with the number of event-time mentions in the document, thus further reinforcing our claim that more temporal information improves model performance.",
        "AD3 overcomes this limitation using attentive graph convolution, which successfully filters out noisy time mentions as is evident.",
        "We propose AD3, an ensemble model which exploits both syntactic and temporal information in a document explicitly to predict its creation time (DCT)."
    ],
    "3888": [
        "We show that BERT is a Markov random field language model.",
        "Formulating BERT in this way gives rise to a practical algorithm for generating from BERT based on Gibbs sampling that does not require any additional parameters or training.",
        "The power of this framework is in allowing the principled application of Gibbs sampling, and potentially other MCMC algorithms, for generating from BERT.",
        "Future work might explore these improved sampling methods, especially those that do not need to run the model over the entire sequence each iteration and that more robustly handle variable-length sequences."
    ],
    "3890": [
        "The novel knowledge selection mechanism used in this paper is the first neural model that makes use of both prior and posterior distributions over knowledge to facilitate knowledge selection.",
        "The model effectively approximates the posterior distribution using the prior distribution, allowing it to generate appropriate responses during inference.",
        "Extensive experiments on both automatic and human metrics demonstrate the effectiveness and usefulness of the model.",
        "The model can be extended for selecting knowledge in multi-turn conversations in future work.",
        "(p.?) The novel knowledge selection mechanism used in this paper is the first neural model that makes use of both prior and posterior distributions over knowledge to facilitate knowledge selection.",
        "(p.?) The model effectively approximates the posterior distribution using the prior distribution, allowing it to generate appropriate responses during inference.",
        "(p.?) Extensive experiments on both automatic and human metrics demonstrate the effectiveness and usefulness of the model.",
        "(p.?) The model can be extended for selecting knowledge in multi-turn conversations in future work."
    ],
    "3892": [
        "Our methods can be applied to current state-of-the-art neural models for NER.",
        "We proposed a neural adapter for connecting the target and the source models to mitigate the forgetting of learned knowledge.",
        "The empirical results show the effectiveness of the proposed methods and techniques.",
        "We will make data and models available to support this new line of research.",
        "In future work, we would like to test our approach on several different sequence labeling tasks to fully demonstrate the generality of our approach."
    ],
    "3893": [
        "The proposed approach of incorporating contextual information into self-attention networks is effective and universal across language pairs.",
        "The deep and global approaches to modeling context are complementary.",
        "The context-aware model enhances the original representations in the self-attention model, and the model can flexibly model different types of contextual information.",
        "The proposed approach is effective in other tasks such as reading comprehension, language inference, and stance classification.",
        "Incorporating linguistic knowledge, such as phrases and syntactic categories, could be a promising direction for further improving the performance of Transformer models.",
        "Combining the proposed approach with other techniques, such as those presented in (Shaw et al., Uszkoreit et al., Li et al., Dou et al., Kong et al., Yang et al.), could further boost the performance of Transformer models."
    ],
    "3894": [
        "Our best model, which utilizes EM-based iterative routing to estimate the agreement between inputs and outputs, has achieved significant improvements over the baseline model across language pairs.",
        "By visualizing the routing process, we find that capsule networks are able to extract most active features shared by different inputs.",
        "Our study suggests potential applicability of capsule networks across computer vision and natural language processing tasks for aggregating information of multiple inputs.",
        "Future directions include validating our approach on other NMT architectures such as RNN (Chen et al. 2018 ) and CNN (Gehring et al. 2017) , as well as on other NLP tasks such as dialogue and reading comprehension.",
        "It is also interesting to combine with other techniques (Shaw, Uszkoreit, and Vaswani 2018; Li et al. 2018; Dou et al. 2018; Yang et al. 2018; Yang et al. 2019; Kong et al. 2019) to further boost the performance of Transformer."
    ],
    "3895": [
        "The proposed techniques improve the hierarchical representation-based semantic parsing models using ensembling, contextualized word embeddings, and language model-based re-ranking.",
        "The three approaches improve the model on different types of errors.",
        "The best model uses a combination of the techniques.",
        "The proposed model reduces the error rate by 33% on the TOP dataset."
    ],
    "3896": [
        "DBD can handle unaligned sequences by considering all possible alignments between the input and the target.",
        "The beam search decoder can be used to efficiently approximate a discriminative model which alleviates exposure bias from the mismatch between training and inference.",
        "We avoid the label bias problem by using unnormalized scores and performing a sequence-level normalization.",
        "DBD jointly trains the scoring function and the transition models, doing away with the need for decoder hyper-parameter tuning on a held-out validation set.",
        "DBD can achieve better WERs at a substantially smaller beam size than a well-tuned ASG baseline.",
        "DBD allows us to train much simpler and smaller acoustic models with better error rates.",
        "Including an explicit language model further decreases the burden on the acoustic model, since it does not need to learn an implicit language model.",
        "Models with fewer parameters and half the temporal receptive field can achieve equally good error rates when using DBD."
    ],
    "3898": [
        "The proposed joint topic-equation model (TopicEq) outperforms existing topic models and equation models for scientific texts.",
        "TopicEq generates text and equations simultaneously, leveraging the topical correspondence between text and mathematical equations.",
        "The model demonstrates applications and extensions, such as equation topic inference and topic-aware alignment of mathematical symbols and words.",
        "The use of a topic-dependent RNN in the joint model enhances the generation of equations.",
        "The qualitative analysis of TopicEq shows its potential for improving the readability and accessibility of scientific documents."
    ],
    "3899": [
        "The brevity problem in NMT can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "Existing works using deep learning are currently suffering from various difficult issues when dealing with short and informal messages.",
        "Negation-based data augmentation, transfer learning for word embeddings, and changing the core classification technique from CNNs to GRUs can significantly improve performance.",
        "The attention-based approach of casting NLP tasks into question answering (QA) problems has the potential to achieve better performance on real datasets.",
        "Integrating domain knowledge into such an approach may lead to better performance."
    ],
    "3906": [
        "We showed that AMRs can improve neural machine translation.",
        "The structural semantic information from AMRs can be complementary to the source textual input by introducing a higher level of information abstraction.",
        "A graph recurrent network (GRN) is leveraged to encode AMR graphs without breaking the original graph structure.",
        "Experiments on a standard bench-mark showed that AMRs are helpful regardless of the sentence length, and are more effective than other more popular choices, such as dependency trees and semantic roles."
    ],
    "3908": [
        "We propose a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Mixture models exhibit complex behaviors depending on different combinations of design choices.",
        "Our recommended configurations enable mixture models to offer much better trade-offs between quality and diversity than variational models as well as heuristic diverse decoding approaches.",
        "In the future, we hope to broaden the scope of this work by looking at other generation tasks such as dialogue and image captioning.",
        "We would also like to investigate models with richer and more structured latent representations, and narrow the gap between model and human performance."
    ],
    "3911": [
        "Using both approaches, the neural models obtain significant increase from the CNN-BiLSTM-CRF and the multitask baseline on small training data.",
        "However, one might not need to fine-tune if: (1) a large unlabeled in-domain data is already at hand then one can train language model directly without any fine-tuning, and (2) an adequate amount of labeled in-domain data (in our case it's > 5000 sentences) is present then one can opt for simpler models such as the multitask model.",
        "Furthermore, we also find that the pretrained LM encodes part-of-speech information, which is a strong predictor for named entity recognition.",
        "The neural sequence labeler, on the other hand, seems to encode another information other than part-of-speech to help it perform well on NER task on small training data."
    ],
    "3914": [
        "The proposed approach of learning multilingual sentence embeddings using a bi-directional dual-encoder with additive margin softmax leads to better retrieval performance on the United Nations (UN) parallel corpus and on the BUCC task.",
        "NMT systems trained on mined UN data retrieved using our models perform as well as NMT systems trained on the original UN bitext.",
        "Document-level embeddings obtained by simply averaging the sentence-level embeddings from our model achieve a new state-of-the-art on UN document-level mining.",
        "Our model achieves performance competitive with the current state-of-the-art on BUCC.",
        "When combined with a BERT rescoring model, our performance on BUCC achieves a new state-of-the-art."
    ],
    "3915": [
        "We proposed saliency learning, a novel approach for teaching a model where to pay attention.",
        "The results show that saliency learning enables us to obtain better precision, F1 measure and accuracy on these tasks and datasets.",
        "Saliency learning gives us more reliable predictions while delivering better performance than traditionally trained models.",
        "Our verification experiments illustrate that the saliency-trained models show higher sensitivity to the removal of contributory words in a positive example.",
        "We will extend our study to examine saliency learning on NLP tasks in an active learning setting where real explanations are requested and provided by a human."
    ],
    "3916": [
        "Domain-specific embedding models outperform open-domain embedding models.",
        "All types of embeddings enable consistent gains in concept extraction tasks when pretrained on a clinical domain corpus.",
        "Contextual embeddings outperform traditional embeddings in performance.",
        "Pre-training a deep language model from a large corpus, followed by a task-specific fine-tuning, can achieve large improvements.",
        "Contextual embeddings provide interesting semantic information that is not accounted for in traditional word representations.",
        "Embeddings through unsupervised pretraining on clinical text corpora achieve higher performance than off-the-shelf embedding models and result in new state-of-the-art performance across all tasks."
    ],
    "3917": [
        "The model based on gaze features and part-of-speech information achieves accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerge as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Unlike phonograms, logograms have word and phrase meanings singularly.",
        "The images of Chinese characters contain rich semantic information.",
        "Our model extracts both image features and contextual semantic information.",
        "Our model is an end-to-end model that updates the image's feature parameters in real time during training, achieving better results than the GWE model.",
        "Our research focuses on simplified Chinese word embeddings, and the idea can also be applied to other languages that share a similar writing system, such as traditional Chinese, Japanese, and so on."
    ],
    "3918": [
        "We introduced an augmentation methodology which relies on the use of KGs to improve the performance of NMT systems.",
        "Our devised strategies for incorporating KG embeddings into NMT models work on wordand character-based models.",
        "Our extensive evaluation with a manual analysis showed consistent enhancements provided by KGs in NMT.",
        "The overall methodology can be applied to any NMT model since it does not modify the main NMT model structure and also allows replacing different EL systems."
    ],
    "3924": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Lexicalization is not necessary to achieve very high parsing results in discontinuous constituency parsing.",
        "The unlexicalized system produces shorter derivations and has a better incrementality.",
        "Our parser's errors on discontinuous constituents can be qualitatively analyzed."
    ],
    "3928": [
        "Our hypothesis is that this task (multi-relational question answering expressed through personal narrative) will become increasingly important as users begin to teach personal knowledge about their world to the personal assistants embedded in their devices.",
        "This task naturally synthesizes two main branches of question answering research: QA over KBs and QA over free text.",
        "One of our main contributions is a collection of diverse datasets that feature rich compositional questions over a dynamic knowledge graph expressed through simulated narrative.",
        "Another contribution of our work is a thorough set of experiments and analysis of different types of end-to-end architectures for QA at their ability to answer multi-relational questions of varying degrees of compositionality.",
        "Our long-term goal is that both the data and the simulation code we release will inspire and motivate the community to look towards the vision of letting end-users teach our personal assistants about the world around us."
    ],
    "3932": [
        "Our approach relies on a separation between syntax and semantics, allowing dedicated optimization schemes for each module.",
        "We found it important to have an unbiased estimator of the parser gradients and to allow multiple gradient steps with PPO.",
        "Our learned parser generalises to sequences of any length and distinguishes grammatical from ungrammatical expressions by forming meaningful representations for well-formed expressions.",
        "For natural language tasks, our approach prefers to fall back to trivial strategies, in line with what was previously observed by Shi et al. (2018).",
        "Our approach performs competitively on several real natural language tasks.",
        "In the future, we would like to explore further relaxation-based techniques for learning the parser, such as REBAR (Tucker et al., 2017) or ReLAX (Grathwohl et al., 2017).",
        "We plan to look into applying recursive approaches to language modelling as a pre-training step and measure if it has the same impact on downstream tasks as sequential models."
    ],
    "3933": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our final MuRel network is very competitive and outperforms state-of-the-art results on two of the most widely used datasets.",
        "The use of pairwise combination, and the multi-step iterations in the whole process are key components of our approach that contribute to its success.",
        "Our vectorial representation to model the attention is effective in capturing the relationships between different parts of the image and the question."
    ],
    "3934": [
        "The embeddings of GloVe, ELMo, and BERT contain gender, religion, and nationality biases.",
        "A projection-based method can be used to attenuate these biases.",
        "The method works for static GloVe embeddings.",
        "Debiasing the first non-contextual layer of ELMo and BERT embeddings can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The proposed constituent hierarchy predictor based on recurrent neural networks can capture global sentential information.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser.",
        "The GQA dataset can help drive VQA research in the right directions of deeper semantic understanding, sound reasoning, enhanced robustness, and improved consistency.",
        "Integrating visual knowledge extraction and question answering may lead to more compositional, interpretable, and cogent reasoning models."
    ],
    "3942": [
        "Using multi-task learning with contextualized word representations in FG-NER task can be effective, as our best model achieves an SOTA result of 83.35%.",
        "Different parameter sharing schemes for multi-task sequence labeling can affect the effectiveness of the model.",
        "Learning with a neural language model can improve the performance of the FG-NER model.",
        "Learning at different word representation settings can also improve the performance of the FG-NER model.",
        "Our best model, which does not use any additional manual effort for creating data and designing features, achieves an SOTA result compared to previous FG-NER models."
    ],
    "3943": [
        "Our approach achieves a macro F1 score of 61.67%, improving over the baseline by 12.37%.",
        "Using only discussion posts, previous posts, and the target post to classify the target post's stance can lead to few examples that are not answerable by humans.",
        "We plan to extend our system with a relevance scoring system to score all discussion posts and pick up the most relevant ones to preserve the context of understanding."
    ],
    "3946": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "ERP data provides a rich testbed for comparing models of language processing and probing the representations constructed by NLP systems.",
        "Analyzing the differences among models as a function of processing time scratches the surface of what is possible using our framework, especially for understanding the more complex neural models used in NLP."
    ],
    "3952": [
        "The proposed model, Induction Networks, outperforms existing state-of-the-art few-shot text classification models.",
        "The Induction Module, which combines dynamic routing with a meta-learning framework, makes the model more general and able to recognize unseen classes.",
        "The proposed model achieves better performance than existing models on few-shot learning tasks, as shown by the experiment results.",
        "Both the matrix transformation and routing procedure contribute consistently to the few-shot learning tasks."
    ],
    "3953": [
        "We explore fast and accurate dependency parsing as sequence labeling.",
        "Our results on the PTB and a subset of UD treebanks show that this paradigm can obtain competitive results, despite not using any parsing algorithm nor external structures to parse sentences.",
        "We tested four different encodings, training a standard BiLSTM-based architecture.",
        "Our approach does not rely on any parsing algorithm nor external structures to parse sentences."
    ],
    "3954": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "The authors developed a new CQA system called PReFIL that improves the state-of-the-art and surpasses human accuracy on two datasets.",
        "The authors used an end-to-end trained neural network for handwriting recognition, which improved recognition accuracy by 20% to 40% relative depending on the language.",
        "The authors encoded touch inputs using a B\u00e9zier curve representation, which performed at least as well as raw touch inputs and allowed for faster recognition.",
        "The authors compared their system's performance to the state of the art on publicly available datasets such as IAM-OnDB, IBM-UB-1, and CASIA, and improved over the previous best published result on IAM-OnDB."
    ],
    "3964": [
        "The ConMask model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL exceeded the human baseline for FigureQA, but results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The proposed SPACEFUSION model can jointly optimize diversity and relevance by leveraging novel regularization terms to fuse the latent space of a S2S model with that of an autoencoder model.",
        "The proposed approach brings significant improvement compared to strong baselines in terms of both diversity and relevance."
    ],
    "3966": [
        "We have presented a method to learn the curriculum for presenting training samples to an NMT system.",
        "Using reinforcement learning, our approach learns the curriculum jointly with the NMT system during the course of a single NMT training run.",
        "Empirical analysis on the Paracrawl and WMT English-French corpora shows that this approach beats the uniform sampling and filtering baseline by large margins.",
        "In addition, we were able to match a state-of-the-art hand designed curriculum on Paracrawl and beat it on WMT.",
        "We see this as a first step toward enabling NMT systems to manage their own training data.",
        "In the future, we intend to improve our approach by eliminating the static exploration schedule and binning strategy, and extend it to handle additional data attributes such as domain, style, and grammatical complexity."
    ],
    "3968": [
        "Massively multilingual many-to-many models outperform many-to-one and bilingual models with similar capacity and identical training conditions when averaged over 8 language pairs into English.",
        "Many-to-many models are inferior in performance when going out-of-English in comparison to a one-to-many model.",
        "The improvement over the many-to-one models is attributed to the multiple target language pairs acting as regularizers, especially in this low-resource multi-way-parallel setting that is prone to memorization.",
        "Many-to-many models are limited by the English being over-represented in the English-centric many-to-many setting, where it appears as a target language in 58 out of 116 trained directions.",
        "The diversity in each training batch is very different between the different settings, and tailored hyperparameter choices for each setting may be necessary to improve performance even further."
    ],
    "3970": [
        "The dataset of complex reading comprehension questions, DROP, is substantially more challenging than existing datasets, with the best baseline achieving only 32.7% F1, while humans achieve 96%.",
        "The authors hope that the DROP dataset will spur research into more comprehensive analysis of paragraphs and into methods that combine distributed representations with symbolic reasoning.",
        "The authors have presented initial work in this direction by augmenting QANet with limited numerical reasoning capability, achieving 47% F1 on DROP."
    ],
    "3973": [
        "'We introduced a new problem of predicting the algorithm classes for programming word problems.'",
        "'Our classifiers are falling short only by about 9 percent of the human score.'",
        "'Increasing the size of the train dataset improves the accuracy.'",
        "'These problems are much harder than high school math word problems as they require a good knowledge of various computer science algorithms and an ability to reduce a problem to these known algorithms.'",
        "'Even our human analysis shows that trained computer science graduates only get an F1 of 51.8.'",
        "'Algorithm class prediction is compatible with and can be solved using text classification.'"
    ],
    "3975": [
        "Our proposed method achieves state-of-the-art results for dementia detection in Mandarin Chinese, despite the stark differences between English and Mandarin, and the fact that the parallel corpus is out-of-domain for the task.",
        "Our method does not require any Mandarin data for training, which is important given the difficulty of acquiring sensitive clinical data.",
        "Future work will investigate the use of automatic speech recognition to reduce the need for manual transcripts, which are impractical in a clinical setting.",
        "Our model only uses lexicosyntactic features, and ignores acoustic features (e.g., pause duration) which are significant for dementia detection in English.",
        "It remains to apply this method to other languages, such as French (Fraser et al., 2019), for which datasets have recently been collected."
    ],
    "3976": [
        "The beam problem in NMT can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model of part-of-speech tagging and dependency parsing improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "The manual creation of high-quality datasets in the digital humanities domain provides a valuable resource for evaluating word embedding and relation extraction systems.",
        "The suitability of word embeddings trained on small corpora for the tasks at hand can be evaluated using unigram and n-gram tasks, and analyzing the impact of task difficulty and corpus term frequencies on accuracy.",
        "Leveraging multimodal data (images, videos, etc.) available in abundance in the given domain for the creation of entity representations is an interesting direction of future research."
    ],
    "3979": [
        "The proposed model, WEB-CWS, can effectively improve cross-domain CWS.",
        "The WEB-CWS model only requires a baseline segmenter and a raw corpus in the target domain, and can deploy word embeddings for CWS.",
        "The WEB-CWS model improves the performance of the state-of-the-art baseline segmenter on four datasets in special domains, especially in segmenting domain-specific noun entities."
    ],
    "3986": [
        "Our framework, FAST NAVIGATOR, can be easily plugged into the most advanced agents to immediately improve their efficiency.",
        "Empirical results on the Room-to-Room dataset show that our agent achieves state-of-the-art Success Rates and SPLs.",
        "Our search-based method is easily extendible to more challenging settings, e.g., when an agent is given a goal without any route instruction [6, 12] , or a complicated real visual environment [7] .",
        "Figures A1 through A3 show three examples comparing our approach to the previous state-of-the-art.",
        "The following URL includes a 90 second video (https:// youtu.be/AD9TNohXoPA) showing a first-person view of several agents navigating the environment with corresponding birds-eye-view maps."
    ],
    "3988": [
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We have explored simple and effective ways to alleviate or eliminate the beam problem.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "We utilize a graph convolution layer to incorporate type co-occurrence statistics and word-level type similarities, which implicitly captures the label correlations in the latent vector space.",
        "Our method does not require external knowledge about the label structures, and we believe our method is general enough and has the potential to be applied to other multi-label tasks with plain-text labels."
    ],
    "3993": [
        "Our proposed dataset, CLEVR-Dialog, allows for the study of multi-round reasoning in visual dialog and visual coreference resolution.",
        "We benchmarked several qualitatively different models from prior work on our dataset, which serve as baselines for future work.",
        "Our dataset provides a way to evaluate how well models do on visual coreference resolution without the need for expensive annotations on real datasets.",
        "The model is able to accurately ground references in the question consistently for several question types, as reflected in a higher average NDCG.",
        "The visual grounding in Fig. 11b (average NDCG of 0.7) is significantly superior to a random baseline (NDCG of 0.3)."
    ],
    "3995": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We have the following recommendations: \u2022 Charts in the wild: The charts in FigureQA and DVQA were methodologically generated, but human-generated charts in real-world business and scientific documents can contain variations that these datasets omit. Additional text in the chart or human annotations would likely cause the dynamic encoding method used by PReFIL to fail.",
        "Next generation datasets should contain charts extracted from real-world documents.",
        "Human generated questions: The questions in both FigureQA and DVQA were created with templates, which do not capture all the nuances of natural language.",
        "Document-level CQA: FigureQA and DVQA have well-defined image regions and all information needed to answer a question is contained in that image. To understand charts in documents, information in the rest of the document may be necessary to answer questions about the chart."
    ],
    "4003": [
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Sharing more components between main and auxiliary tasks is usually better, and autoencoding generally provides the most benefit for our task.",
        "Analysis showed that this is mainly because MTL helps the model learn that most characters should stay the same, and that its beneficial effect vanishes as the size of the training set increases.",
        "Our models did not beat the nonneural models of Bollmann (2019), but our work still provides interesting insights into the impact of MTL for low-resource scenarios."
    ],
    "4006": [
        "We present MMKG, a collection of three knowledge graphs that contain multi-modal data, to benchmark link prediction and entity matching approaches.",
        "An interesting property of MMKG is that the three knowledge graphs are very heterogeneous with respect to the number of relation types and the degree of sparsity, for instance.",
        "An extensive set of experiments validate the utility of the data set in the sameAs link prediction task."
    ],
    "4008": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism, which uses Similarity Entropy Minimization to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We hope to continuously improve the data quality and observe its impact on NLU performance.",
        "We are also working on extending the data set with spoken user utterances, rather than typed input."
    ],
    "4009": [
        "Our study contributes to the research on patent landscaping in three aspects.",
        "We introduced a new benchmarking dataset for automated patent landscaping and provided a practical study for automated patent landscaping.",
        "Our model showed a high overall classification performance in patent landscaping, as compared to existing models.",
        "We experimentally analyzed how the technical codes and text data affect models in patent classification.",
        "We believe this research will help to reduce the repetitive patent analysis tasks required of practitioners.",
        "Further research is required on patent classification, including exploring the use of various metadata in patent documents, such as assignees, inventors, and citations."
    ],
    "4010": [
        "The proposed fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed AdaBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Fine-tuning and feature extraction approaches can be empirically analyzed across diverse datasets, and the relative performance depends on the similarity of the pretraining and target tasks.",
        "Practical recommendations can be provided for adapting pretrained representations to NLP practitioners."
    ],
    "4011": [
        "The proposed problem of question answering from streaming data is novel and challenging, as it requires the model to answer questions after reading through unlimited amount of context that cannot fit into the system memory.",
        "The proposed solution, Episodic Memory Reader (EMR), is a memory-augmented network with RL-based memory-scheduler that learns the relative importance among memory entries and replaces the entries with the lowest importance to maximize the QA performance for future tasks.",
        "EMR significantly outperforms rule-based memory scheduling and an RL baseline that does not model relative importances among memory entries, as confirmed by extensive experiments on three QA datasets.",
        "The ability of EMR to retain important instances for future QA tasks is a key factor contributing to its good performance, as confirmed by qualitative analysis of memory contents after learning."
    ],
    "4012": [
        "Our approach can effectively improve the performance of the base model and achieve new state-of-the-art results on formality transfer task.",
        "Our approach can be readily generalized to other unsupervised style transfer tasks and perform consistently well on multiple benchmarks.",
        "We fully exploit formality style classification data through classification feedback and various reconstruction constraints to assist the model learning.",
        "Our approach uses a bidirectional style transformation seq2seq model to train formality transfer models from hybrid textual annotations.",
        "Our method achieves new state-of-art results on formality transfer task and can be applied to other unsupervised style transfer tasks."
    ],
    "4015": [
        "The proposed model for context-aware citation recommendation delivers a significant improvement in MAP, MRR, and Recall@K over the existing model.",
        "The basis for the breakthrough performance improvement is the adaptation of the BERT model, which has performed well in recent NLP tasks, to the context-aware framework.",
        "Through the context encoding via BERT, our framework improves the representation learning of the context side.",
        "We apply VGAE to mitigate over-fitting to local contexts when BERT is applied alone.",
        "The combination of the encoded paper network and the encoded context is regularly, resulting in a performance increase over a BERT-based model.",
        "Existing datasets for context-aware citation recommendation are not up-to-date and lack clear context detection.",
        "To address this problem, we devised and released the FullTextPeerRead dataset, which comprises updated papers to an extent including papers up-to 2017 and provides a method to readily and accurately extract context metadata and has a well-organized perspective."
    ],
    "4016": [
        "performing meaning-preserving adversarial perturbations for NLP models\" is important, especially in the context of machine translation (MT).",
        "There is a need for a consistent evaluation framework for assessing the effectiveness of adversarial perturbations in preserving meaning.",
        "Existing \"naive\" attacks do not preserve meaning in general, and there is a need for alternative approaches to address this issue.",
        "Adversarial training can be useful in this paradigm.",
        "The proposed evaluation framework and alternatives to \"naive\" attacks can help ensure that future work in this area of research consistently evaluates meaning conservation."
    ],
    "4018": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The proposed method is effective in extracting causality in natural language text based on our causality tagging scheme.",
        "The use of multihead self-attention mechanism to learn the dependencies between cause and effect improves the performance of SCITE.",
        "The approach can be combined with distant supervision and reinforcement learning to achieve better performance without requiring a large amount of high-quality annotated data for causality extraction."
    ],
    "4019": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The model is an extension of conditional VAEs in the framework of unsupervised learning, in which the topics are extracted from the data with clustering structure rather than predefined labels.",
        "An effective inference method based on Householder flow is designed to encourage the complexity and the diversity of the learned topics.",
        "Experimental results are encouraging, across multiple NLP tasks."
    ],
    "4023": [
        "Calculating vector features representations and feeding into scikit-learn classifiers proved to under-perform against end-to-end machine learning methods.",
        "Word2Vec, in general, also underperformed compared to naive features (probably due to the size of the training set and tweet-specific language in it), but a combination of two yielded a better result.",
        "Building a convolutional model or using a state-of-art model which we just modified for this task yielded the best results."
    ],
    "4024": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The effect of dependency parsing on POS tagging can be investigated by removing the tag representations from parsing in Eq. (4) and Eq. (6), and by removing the syntactic information from tagging in Eq. (2).",
        "Providing lexical information to parsing improves the tagging accuracy itself.",
        "The connection between tagging and parsing is still present even when the explicit connections are disabled, as evidenced by the improvement in tagging accuracy when \"tag \u2192 parse\" is enabled.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing."
    ],
    "4026": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Cross-sentence pretraining is crucial for many tasks.",
        "Pre-training continues to improve performance up to 18B tokens and would likely continue to improve with more data.",
        "The novel cloze-driven training regime is more effective than predicting left and right tokens separately."
    ],
    "4028": [
        "The proposed tool, compare-mt, allows for holistic analysis of machine translation results and discovery of salient patterns to guide further analysis.",
        "The tool is open-source and evolving, with plans to add more functionality as needed to better understand cutting-edge techniques for MT.",
        "Future plans include improving integration with example-by-example analysis and many more improvements to be made as needed.",
        "The tool enables the discovery of salient patterns that may help guide further analysis, making it possible to compare multiple examples simultaneously."
    ],
    "4033": [
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "The proposed methods in their possible combinations achieve state-of-the-art SLU accuracy.",
        "Detailed analysis of effectiveness of the proposed methods demonstrated that the proposed methods increase the accuracy of SLU individually.",
        "Using a distance vector in the history representation under various conditions is effective.",
        "The use of a distance vector in the history representation under the same condition as in the role-level attention (Table 3 ) is effective.",
        "Intent can be used alone, and this approach is more intuitive than using both intent and distance."
    ],
    "4034": [
        "The proposed approach of hierarchical attention for context-aware NMT based on sparse attention is scalable and efficient.",
        "The approach surpasses context-agnostic and two recent context-aware baselines in offline and online document MT settings.",
        "The sparsity at sentence-level allows the model to identify key sentences in the document context.",
        "The sparsity at word-level allows the model to focus on key words in those sentences, resulting in an efficient compression of memory.",
        "Future work will involve digging deeper into the benefits of sparse attention in terms of better interpretability of context-aware NMT models."
    ],
    "4036": [
        "The features generated by pretrained contextualizers are sufficient for high performance on a broad set of tasks.",
        "Learning task-specific contextual features helps encode the requisite knowledge.",
        "The lowest layer of LSTMs encodes the most transferable features, while transformers' middle layers are most transferable.",
        "Higher layers in LSTMs are more task-specific (and thus less general), while transformer layers do not exhibit this same monotonic increase in task-specificity.",
        "Bidirectional language model pretraining yields representations that are more transferable in general than eleven other candidate pretraining tasks."
    ],
    "4037": [
        "The proposed framework, RAP-Net, is effective in measuring the relations between dialogue contexts and responses for dialogue response selection.",
        "The multi-cast attention network (MCAN) and the proposed knowledge-grounded features are useful for dialogue response selection.",
        "Each attention and pooling mechanism is effective in capturing salient information from dialogues.",
        "RAP-Net is capable of selecting a proper response for two different types of dialogue data.",
        "The proposed model can be evaluated on other retrieval-based tasks to test its capability of generalization."
    ],
    "4039": [
        "We experiment with backtranslation and token perturbation as data augmentation strategies for NLP.",
        "Backtranslation yields significant improvements over ULMFit in low resource environments.",
        "The gains from backtranslation disappear when ULMFit is given access to the full dataset.",
        "We do not observe gains for token perturbation techniques.",
        "Ensembling ULMFit predictions on backtranslated examples with other models' predictions yields small improvements in the full dataset task."
    ],
    "4040": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our model exhibits the strong ability in resistance to the data noise introduced by our pseudo labels.",
        "Our model exhibits even better performance when we address tasks with more category such as 20-newsgroup.",
        "There still exist several aspects to improve the model further due to its flexibility.",
        "More sampling strategies might be explored to construct instances with higher confidence.",
        "We can also change this single pair topology to a pair-wise topology taking double pairs of instances as input."
    ],
    "4041": [
        "The proposed methods for NLP augmentation are meaning-preserving, i.e., they preserve the fundamental meaning of the sentence for most of the tested languages.",
        "Unlike majority of previous NLP augmentation techniques, the proposed methods are meaning-preserving.",
        "The proposed methods can be used for a variety of problems such as semantic role labeling, sentiment analysis, and text classification.",
        "The authors did not use development sets to choose one good augmentation model, but instead focused on observing which augmentation technique would improve which language.",
        "POS tagging performance is a good indicator of the performances of other structured prediction tasks, since POS tags are crucial features for higher-level NLP tasks."
    ],
    "4044": [
        "The proposed variational knowledge-grounded conversation system models the relations between dialogue contexts and external facts in an end-to-end fashion.",
        "The system demonstrates the difficulty of this task, as almost all current models fail to generate reasonable responses on DSTC7.",
        "Further study is required to advance the machine's capacity of producing informative and knowledgable conversations.",
        "The knowledge-grounded dialogue modeling is a potential research direction for improving the machine's ability to capture interactions between external information and dialogues."
    ],
    "4046": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Our parser based model is very close to the overall performance of the Standford parser based model.",
        "Our detailed analysis reveals that our parser based model makes correct prediction for several instances for which Stanford parser based model makes wrong predictions."
    ],
    "4047": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The use of multilingual BERT encoder brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The novel end-to-end model for joint slot label alignment and recognition requires no external label projection.",
        "The simple projection baseline using fast-align is outperformed by our model on most languages.",
        "The content and structure of peer reviews under the argument mining framework can be analyzed and mined for interesting patterns in proposition types and content."
    ],
    "4049": [
        "heterogeneous representations from text and facts in an existing knowledge base",
        "previous work that learn the two disparate representations independently and use simple schemes to integrate predictions from each model",
        "a novel framework using an elegant loss function that allows the proper connection between the heterogeneous representations to be learned seamlessly during training",
        "the proposed framework outperforms previous strategies to combine heterogeneous representations and the state-of-the-art for the RE task",
        "our framework enables both independent models to enhance each other"
    ],
    "4051": [
        "We introduced the problem of embedding space projection with noisy lexicons, and showed that existing projection methods are sensitive in the presence of noise.",
        "We proposed an EM algorithm that jointly learns the projection and identifies the noisy pairs.",
        "The algorithm can be used as a drop-in replacement for the OP algorithm, and was demonstrated to improve results on two NLP tasks."
    ],
    "4052": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed system can be integrated into an open-domain chatting machine with both graph reasoning and knowledge-aware response generation capabilities.",
        "Experimental results demonstrate the effectiveness of our system on two datasets compared to state-of-the-art approaches."
    ],
    "4055": [
        "The proposed approach outperforms prior work on LM-based GEC, especially when combined with a neural LM.",
        "The FST-based approach achieves better results than previous work on hybrid systems for SMT lattices.",
        "FSTs provide a powerful and effective framework for constraining neural GEC systems."
    ],
    "4059": [
        "One source, multiple targets\" is a common text generation task.",
        "CVAE-based methods have great potential for this task.",
        "The KL-vanishing problem occurs when CVAE works with RNNs.",
        "The self-labeling mechanism can improve the generating diversity of CVAE.",
        "One source, multiple targets\" is a common text generation task.",
        "CVAE-based methods have great potential for this task.",
        "The KL-vanishing problem occurs when CVAE works with RNNs.",
        "The self-labeling mechanism can improve the generating diversity of CVAE."
    ],
    "4060": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "There is a 26% difference in F1-scores between the development portion (0.890) and the test set (0.659) of the GDI 2017 data obtained by the HeLI 2.0 method.",
        "The domain difference between the two sets explains why iterative adaptation performs better with the test set than with the development set.",
        "There is only a 1.4% difference between the macro F1-scores obtained from the development and the test sets in the GDI 2018 dataset.",
        "The small difference (7.8%) between the F1-scores attained using the development set and the test set of the ILI 2018 data is partly due to the fact that the parameters of the identification method have been optimized using the development set."
    ],
    "4066": [
        "The proposed approach enables the use of multilingual datasets to improve performance.",
        "Using a new language during training improves image retrieval for other languages.",
        "The method uses a CNN to extract image information and aligned multilingual word embeddings to produce text representations.",
        "The approach provides image and text embeddings that can be trained to produce comparable features.",
        "The method can be used to retrieve an image from a text or text from an image with a multilingual representation.",
        "Using BIVEC embeddings enables the use of another language to improve performance.",
        "MUSE embeddings allow for embedding more languages in the same model.",
        "Adding other languages decreases performance for English but increases recall in a multilingual environment.",
        "The approach achieves a 3.35% increase in performance on the COCO dataset and a 15.15% increase on the Multi30K dataset."
    ],
    "4068": [
        "the beam problem can largely be explained by the brevity problem",
        "solving the brevity problem leads to significant BLEU gains",
        "our solution to the brevity problem requires globally-normalized training on only a small dataset",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "using a slightly modified copy of the target, instead of its full BT, is an efficient solution for domain adaptation",
        "cheaper alternatives to BT, such as copies of the target noised with GANs, can be almost as good as low quality BTs",
        "BT seems preferable to integrating external LM, at least in our data condition",
        "further experiments with larger LMs are needed to confirm this observation and evaluate the complementarity of both strategies",
        "the impact of BT on subparts of the network needs to be better understood",
        "other cheap ways to generate artificial data can be investigated in future work"
    ],
    "4069": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed AdaBERT is effective and efficient, adaptively compressing BERT for various downstream tasks using Neural Architecture Search.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The existing methods could not take into account the word selection according to length constraint, and it was difficult to evaluate methods to controlling output length because headlines of different lengths are written based on different goals.",
        "The length-restricted test set, which was extracted from length-insensitive headlines and corresponding articles, could not adequately evaluate the multiple length system outputs depending on the specified length."
    ],
    "4070": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Extending query reformulation for multiple languages.",
        "Assessing if anaphora resolution using query reformulation is possible for longer dialogues."
    ],
    "4072": [
        "The proposed approach yields state-of-the-art results on a wide range of transfer learning tasks and implicit discourse recognition.",
        "Removing 'simple' examples is detrimental to transfer results, while preventing the model from exploiting the relationship between sentences has a negligible effect.",
        "There is still room for improvement: models that adequately exploit the relationship between sentences would be better at leveraging the supervision of the dataset, and could yield even better sentence representations.",
        "The approach can be further improved by increasing the coverage of discourse markers, such as using more lenient patterns that capture a wider range of markers, including multi-word markers."
    ],
    "4076": [
        "We develop a neural architecture that can learn to classify duplicates and cluster them into meaningful latent topics without additional supervision.",
        "The architecture decomposes the latent semantic space of a word to only distill the topical information into a few designated dimensions, and uses a two-step attention module to focus on different textual parts for the two tasks.",
        "We share the challenges of annotating a user reported bug dataset with non-technical annotators, as opposed to using annotations from engineers.",
        "Experimental results on different types of datasets indicate that the proposed approach is promising compared to existing techniques for both tasks.",
        "Our model's construction is generic and presents new possibilities in various domains for modeling sub-tasks for free, with partial supervision from another task."
    ],
    "4085": [
        "Our models achieved significant and consistent improvements compared with the models using only conventional intra-bag attentions.\" (related to the effectiveness of the proposed neural network architecture)",
        "our models can effectively preserve salient source relations in summaries\" (related to the performance of the structural models in abstractive summarization)",
        "the noisy sentences are expected to have smaller weights\" (related to the strategy for dealing with noisy sentences)",
        "an inter-bag attention module is designed to deal with the noisy bag problem by calculating the bag-level attention weights dynamically during model training\" (related to the proposed solution for the noisy bag problem)",
        "To deal with the multi-label problem of relation extraction and to integrate external knowledge into our model will be the tasks of our future work.\" (related to future research directions)"
    ],
    "4086": [
        "Combining text analysis with PSL rules defined over a knowledge graph can improve predictions about drug-disease treatment relations.",
        "The techniques can be applied even in domains that lack rich, broad coverage ontologies.",
        "Using heuristically derived knowledge graphs can still lead to effective predictions about drug-disease treatment relations.",
        "The proposed method can improve the accuracy of predicting drug-disease treatment relations."
    ],
    "4089": [
        "Increasing the context window size can improve performance on word similarity benchmarks, but only up to a certain point.",
        "The effect of increasing the context window size varies across different benchmarks and may even have a negative impact in some cases.",
        "CBOW is more sensitive to the context window size than SGNS, and increasing the window size typically improves performance for CBOW but has little impact on SGNS.",
        "There are exceptions to the general trend, such as SimLex999 and SimVerb3500, where increasing the window size has no effect for CBOW and negative impact for SGNS.",
        "The investigation focused on the relation between words and their part of speech, but other relations such as syntactic dependency relations, semantic relations like hypernymy and synonymy, and the effect of different vector dimensions are also of interest to the NLP community.",
        "Future work may extend the analysis to other relations and hyperparameters."
    ],
    "4096": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "FAIRSEQ is a fast, extensible toolkit for sequence modeling that is scalable and suitable for many applications.",
        "The development of the toolkit will enable further research advances in the future."
    ],
    "4098": [
        "Our approach outperforms benchmark models across different datasets.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "including PAWS training data for state-of-the-art models dramatically improves their performance on challenging examples and makes them more robust to real-world examples.",
        "We also demonstrate that PAWS effectively measures sensitivity of models on word order and syntactic structure."
    ],
    "4099": [
        "We compare several methods for approximate inference in neural structured prediction and find that inference networks achieve a better speed/accuracy/search error trade-off than gradient descent.",
        "We propose instance-level inference network fine-tuning and using inference networks to initialize gradient descent, finding further reductions in search error and improvements in performance metrics for certain tasks.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4)."
    ],
    "4102": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "UHop works well for lengthy relation extraction and can be applied to small, simple KBs with task-specific relations.",
        "The current framework uses a greedy search for each single hop. We expect in the future that incorporating a beam search may further improve performance."
    ],
    "4103": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.",
        "The ConMask model has some limitations and room for improvement, such as the nature of the relationship-dependent content masking, which can lead to incorrect predictions.",
        "Tense and aspect are two of the most important factors for performing natural language inference.",
        "Distributional embedding models capture a considerable amount of the morphosyntactic information relating to tense and aspect in their embedding spaces.",
        "Neither the embedding models, nor two pre-trained biLSTMs, were able to outperform a simple rule-based baseline on TEA, primarily due to their reliance on contextual similarity for inference."
    ],
    "4104": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed AdaBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "S 0 models from previous work, while strong, still imperfectly capture the behavior that people exhibit when generating text; and an explicit pragmatic modeling procedure can improve results.",
        "Both pragmatic methods evaluated in this paper encourage prediction of outputs that can be used to identify their inputs, either by reconstructing inputs in their entirety or distinguishing true inputs from distractors.",
        "Future work might allow finer-grained modeling of the tradeoff between underand over-informativity within the sequence generation pipeline (e.g., with a learned communication cost model) or explore applications of pragmatics for content selection earlier in the generation pipeline."
    ],
    "4106": [
        "We show that single-sense embeddings such as word2vec do not adequately reflect all meanings of polysemes and homonyms.",
        "Improvements can be obtained by using multi-sense embeddings both for the target words and for the words in the input description.",
        "Our proposed method based on attention automatically selects the correct sense from a set of pre-trained multi-sense vectors depending on the context in an end-to-end fashion.",
        "It outperforms single-sense vectors, multi-sense embeddings trained in a task-specific way as well as pre-trained contextual embeddings.",
        "Our analysis of the sense selection process shows avenues for interesting future work."
    ],
    "4107": [
        "The proposed training data augmentation method combines the efficiency of type-based learning and the expressive power of a context-sensitive lemmatization model, leading to improved lemmatization accuracy on all ten languages.",
        "The use of Wikipedia sentences as contextualized examples for unambiguous inflection-lemma pairs from UniMorph tables improves lemmatization accuracy, despite the examples being noisy and biased.",
        "Context is helpful for lemmatization accuracy, both overall and especially on unseen words, as demonstrated in a very low-resource setting.",
        "The proposed method improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\")."
    ],
    "4112": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Attending to informative and reliable contexts improves representations of rare and medium-frequency words for a diverse set of evaluations.",
        "Investigating whether attention mechanisms on the word level can further improve the model's performance.",
        "Investigating whether the proposed architecture is also beneficial for languages typologically different from English, e.g., morphologically rich languages."
    ],
    "4115": [
        "Our proposed multi-modal generative adversarial network (MAGAN) incorporates additional information from product image and attribute tags to generate short product titles, which improves the effectiveness of the model compared to conventional methods that only consider textual information.",
        "Extensive experiments on a large real-world E-commerce dataset verify the effectiveness of our proposed MAGAN when comparing with several state-of-the-art baselines.",
        "The online deployment of our MAGAN in a real environment of an E-commerce app shows that our method can improve the click through rate and click conversion rate.",
        "Our approach uses less than 1% (1000 sentences) of the training data, achieving competitive performance compared to previous systems trained using the full dataset.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard."
    ],
    "4117": [
        "'Our proposed SegRNN approach performs better than all other systems for ES-WIX.'",
        "'SegRNNs have clear advantages over all baselines if we consider mixed words only.'",
        "'We extended the LID task to the subword level, which is particularly important for code-switched text in morphologically rich languages.'",
        "'Our subword-level LID datasets for ES-WIX and DE-TR are publicly available.'"
    ],
    "4119": [
        "Using the Norma tool when only little training data (<500 tokens) is available\" can be a practical recommendation for projects seeking to employ normalization techniques.",
        "Using cSMTiser otherwise, ideally with additional data for language modelling\" can also be a useful recommendation.",
        "Making use of the naive memorization/lookup technique for in-vocabulary tokens when possible\" is another practical recommendation.",
        "The qualitative analysis in the paper provides deeper insight into the properties of the models and datasets used in the study."
    ],
    "4120": [
        "The beam problem in machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Fine-tuning self-attention networks with strong regularization and task-specific layer attention is beneficial for multilingual learning.",
        "Multilingual learning is most beneficial for low-resource languages, even ones that do not possess a training set.",
        "Self-attention networks are capable of capturing syntactic patterns and can scale to a large number of languages without degrading performance."
    ],
    "4121": [
        "Our method is comparable to or outperforms previous methods in unsupervised parsing, chunking, and phrase representations.",
        "We show that our model learns syntactic structure of language effectively.",
        "Our model obtains higher segment recall than a comparable model and outperforms strong baselines on phrase representations on a chunking dataset.",
        "While the current model seems to focus primarily on syntax, future work can improve the model's ability to capture fine-grained semantics.",
        "We are also eager to apply DIORA to other domains and languages which do not have rich linguistically annotated training sets."
    ],
    "4122": [
        "We have shown that BioELMo and BioBERT representations are highly effective on biomedical NER and NLI.",
        "BioELMo works even without complicated downstream models and outperforms untuned BioBERT in our probing tasks.",
        "The ability of BioELMo as a fixed feature extractor to encode entity types and especially their relations is the reason for its effectiveness.",
        "Learning universal text representations is a long-term goal of NLP, and our probing tasks can be used to test whether learnt representations effectively encode entity-type or relational information.",
        "Comprehensive characterizations of BioELMo and BioBERT as fixed feature extractors would also be an interesting further direction to explore."
    ],
    "4124": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach outperforms benchmark models across different datasets.",
        "Sentences with fewer labels across all tasks have smaller reconstruction loss, and are therefore easier to compress in a low-dimensional vector space.",
        "Depending on the amount of such sentences, either the most dense or the moderately dense will have the highest loss.",
        "Future work should seek to provide a better theoretical distribution of labels across the sentence-task label matrix.",
        "Extending analysis to the infinite-dimensional case may be beneficial."
    ],
    "4125": [
        "The multi-task model outperforms the single-task model on the NAIST Test Corpus for both tasks.",
        "The model achieves a state-of-the-art result for PASA.",
        "This is the first work to employ neural networks for ENASA.",
        "In future work, we plan to consider multiple predicates and event-nouns."
    ],
    "4126": [
        "The performance gains of these models demonstrate that incorporating broader discourse information is a powerful feature for metaphor identification systems.",
        "Incorporating broader discourse information is a powerful feature for metaphor identification systems, aligning with our qualitative analysis and the theoretical and empirical evidence suggesting metaphor comprehension is heavily influenced by wider context.",
        "Given the simplicity of our representations of context in these models, we are interested in future models which use discourse in more sophisticated ways, e.g. by modeling discourse relations or dialog state tracking.",
        "We are interested in future models which leverage more sophisticated neural architectures."
    ],
    "4128": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The performance of our model is encouraging in terms of BLEU scores, and the outputs suggest that it is successfully utilizing the semantic information encoded in the word vectors to produce new, coherent and diverse sentences.",
        "The use of dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning, and we will investigate this type of models in other learning problems.",
        "The model is able to capture the correct relationship between entities even when the name of the entity does not appear in the description.",
        "The ConMask model has some limitations and room for improvement, such as the ability to apply a filter to modify the list of predicted target entities so that entities that are same as the relationship will be rearranged."
    ],
    "4129": [
        "The proposed approach outperforms existing baseline parsers by achieving higher F-1 scores on standard WSJ and CTB evaluations.",
        "The proposed approach improves the efficiency of shift-reduce parsers by leveraging right-hand side syntax for local decisions.",
        "The proposed approach demonstrates stronger correlations with human scores than existing automated metrics on a binary sentiment dataset.",
        "The proposed approach facilitates evaluation when it is infeasible to collect human scores due to prohibitive cost or limited time."
    ],
    "4130": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL using oracle OCR exceeded humans across all question types.",
        "Our proposed method promotes adequate and fluent summaries that can serve as document surrogates to answer important questions, directly addressing users' information needs.",
        "Our proposed method demonstrated efficacy over state-of-the-art baselines, assessed by both automatic metrics and human evaluators."
    ],
    "4131": [
        "The proposed extract-edit approach achieves better performance than the back-translation mechanism.",
        "The extract-edit approach has stricter constraints on the domains of the source and target corpus, and works well when there is information overlap in the two language spaces.",
        "The back-translation mechanism requires less overlap in terms of the language spaces, but may be more widely applicable due to the availability of easily obtainable corpora.",
        "The extract-edit approach advances previous state-of-the-art NMT systems across four language pairs using monolingual corpora only.",
        "The extract-edit learning framework can be generalized to other types of unsupervised machine translation systems and even some other unsupervised learning tasks."
    ],
    "4132": [
        "The proposed approach achieves up to +3 BLEU zero-shot improvement over the Johnson et al. (2016) baseline.",
        "The agreement-based learning approach is competitive with pivoting and does not lose in performance on supervised directions.",
        "The theory and methodology behind agreement-based learning could be useful beyond translation, especially in multi-modal settings.",
        "The approach could be applied to tasks such as cross-lingual natural language inference, style-transfer, or multilingual image or video captioning.",
        "Exploring different hand-engineered or learned data representations could encourage models to agree on during training."
    ],
    "4133": [
        "Incorporating knowledge as graphs improves performance",
        "We introduced GraphWriter, featuring a new attention model for graph encoding",
        "Our new attention model for graph encoding demonstrated utility through human and automatic evaluation compared to strong baselines",
        "We provide a new resource for the generation community, the AGENDA dataset of abstracts and knowledge",
        "Future work could address the problem of repetition and entity coverage in the generated texts"
    ],
    "4134": [
        "We propose a density matching based unsupervised method for learning bilingual word embedding mappings.",
        "Our approach, DeMa-BWE, performs well in the task of bilingual lexicon induction.",
        "In future work, we will integrate Gaussian embeddings (Vilnis and McCallum, 2015) with our approach."
    ],
    "4135": [
        "We propose a multiscale, entity-centric approach for document-level n-ary relation extraction.",
        "We vastly increase maximum recall by scoring document-level candidates.",
        "Meanwhile, we preserve precision with a multiscale approach that combines representations learned across the subrelation hierarchy and text spans of various scales.",
        "Our method substantially outperforms prior crosssentence n-ary relation extraction approaches in the high-value domain of precision oncology.",
        "Our document-level view opens opportunities for multimodal learning by integrating information from tables and figures (Wu et al., 2018a) .",
        "We used the ternary drug-gene-mutation relation as a running example in this paper, but knowledge bases often store additional fields such as effect (sensitive or resistance), cancer type (solid tumor or leukemia), and evidence (human trial or cell line experiment).",
        "It is straightforward to apply our method to such higher-order relations.",
        "Finally, it will be interesting to validate our approach in a real-world assisted-curation setting, where a machine reading system proposes candidate facts to be verified by human curators."
    ],
    "4137": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The use of Cosine Annealing Strategy combined with Clustering Promotion Mechanism and Adversarial Distribution Alignment improves the domain adaption performance.",
        "The ReWE loss has not exhibited a similarly smooth behavior as the NLL loss during training, and it shows major increases at the re-starts of the optimizer.",
        "The MSE loss is less suited for use as a training objective compared to the NLL loss and ReWE loss.",
        "The proposed regularization technique based on joint learning setting has consistently improved over both its baseline and recent state-of-the-art results from the literature."
    ],
    "4138": [
        "The proposed model significantly outperforms earlier models on the SwDA dataset and is close to state-of-the-art on MRDA, demonstrating the effectiveness of the context-aware self-attention mechanism.",
        "Utterance representations learned at lower levels can impact the classification performance at higher levels, highlighting the importance of using appropriate utterance representation learning methods.",
        "Employing self-attention enables the model to learn richer and more effective utterance representations for the task, demonstrating the benefits of using this attention mechanism.",
        "The use of self-attention has not been previously applied to this task, indicating the potential for innovation and improvement in the field.",
        "Experimenting with other attention mechanisms such as multihead attention, directional self-attention, block self-attention, or hierarchical attention could further improve the model's performance, suggesting the possibility of future improvements."
    ],
    "4139": [
        "The proposed chest X-ray radiology report generation system uses a hierarchical structure to generate topics from images, then words from topics, which allows the model to use largely templated sentences while preserving its freedom to generate diverse text.",
        "The final system is optimized with reinforcement learning for both readability and clinical correctness metrics.",
        "The proposed system outperforms a variety of compelling baseline methods across readability and clinical efficacy metrics on both MIMIC-CXR and Open-I datasets.",
        "The use of reinforcement learning for readability and clinical correctness metrics improves the quality of the generated text.",
        "The hierarchical structure of the system allows for the generation of diverse text while preserving its freedom to generate templated sentences."
    ],
    "4140": [
        "The proposed model mimics how humans approach the task of Reading Comprehension with Multiple Choice Questions.",
        "The model uses a combination of elimination and selection to arrive at the correct option.",
        "The elimination module takes a soft decision as to whether an option should be eliminated or not.",
        "The passage representation is modified based on the amount of orthogonality or alignment determined by two gating functions.",
        "The model outperforms current state-of-the-art models on 7 out of 13 question types on the RACE dataset.",
        "An ensemble of the elimination-selection approach with a state-of-the-art selection approach improves performance by 3.1% over the best reported performance on the RACE dataset.",
        "As future work, the use of reinforcement learning techniques to learn a policy for hard elimination is proposed."
    ],
    "4141": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed AdaBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Splitting a Java method and comment dataset by project or by function affects the task of source code summarization.",
        "The dataset provided in this paper includes 2.1 million pairs of Java methods and one sentence method descriptions in a cleaned and tokenized format.",
        "Care must be taken to avoid unrealistic scenarios when using training sets for automatic source code summarization in an IDE."
    ],
    "4145": [
        "Our main contribution is an extractive model for clip localization based on text queries, which works better empirically than ranking-driven approaches used in the past.",
        "Our approach is modular and can be easily inserted with different architectures for encoders and span predictors.",
        "Our method naturally lays the foundation for training a single generalizable model across datasets and possibly related tasks, such as adding temporal attention to handle more complicated temporal references or extending the approach to work for longer videos like movies.",
        "The experimental results show that our model significantly outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgments."
    ],
    "4146": [
        "The proposed Seq2Seq framework for sentence simplification outperforms previous state-of-the-art systems using SARI, the standard metric for simplification.",
        "The model is able to generate shorter and simpler sentences while remaining competitive regarding humanevaluated fluency and adequacy.",
        "The proposed techniques can be extremely helpful in leveled and personalized text simplification, where the goal is to generate different sentences based on who is requesting the simplification.",
        "The use of a complexity-weighted loss function to encourage the model to choose simpler words improves the performance of the model.",
        "The similarity penalty during inference and clustering post-inference helps to generate candidate simplifications with significant differences.",
        "The reranking system to select the simplification that promotes both fluency and adequacy further improves the performance of the model."
    ],
    "4148": [
        "The system presented in the paper, EMOTICONS, can generate responses with controlled emotions.",
        "The flexibility of the presented solution allows it to be used in any kind of neural architecture as long it fits the encoder-decoder framework.",
        "Currently, EMOTICONS does not generate different emotions equally well.",
        "Future work could include incorporating contextual information that would help EMOTICONS to better capture emotional content."
    ],
    "4150": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model",
        "solving the brevity problem leads to significant BLEU gains",
        "our solution to the brevity problem requires globally-normalized training on only a small dataset",
        "BERT works relatively well out-of-the-box, yielding equivalent performance to the best prior unsupervised domain adaptation approach",
        "domain-adaptive fine-tuning on unlabeled target domain data yields significant further improvements",
        "a single contextualized embedding model that works well across a wide range of domains, genres, and writing styles",
        "training a single contextualized embedding model that works well across a wide range of domains, genres, and writing styles does not result in catastrophic forgetting of the source domain"
    ],
    "4152": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model performance and rankings considerably vary depending on the corpus, suggesting that a single-corpus evaluation can be unreliable.",
        "Therefore, cross-corpora evaluation should be applied to GEC models."
    ],
    "4155": [
        "We quantify and reduce gender bias in word level language models by defining a gender subspace and penalizing the projection of the word embeddings onto that gender subspace.",
        "Our proposed methodology can deal with distribution of words in a vocabulary in word level language model and it targets one way to measure bias, but it's highly likely that there is significant bias in the debiased models and data, just not bias that we can detect on this measure.",
        "We observe a perplexity bias tradeoff as a result of the additional bias regularization term. In order to reduce bias, there is a compromise on perplexity.",
        "Intuitively, as we reduce bias the perplexity is bound to increase due to the fact that, in an unbiased model, male and female words will be predicted with an equal probability.",
        "We found mixed results on amplification of bias as stated by Zhao et al. (2017) , but the debiasing method shown by Bolukbasi et al. (2016) was validated with the use of novel and robust bias measure designed in this paper."
    ],
    "4156": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed Transformer model with an additional recurrence encoder achieves significant improvements over the baseline TRANS-FORMER.",
        "The proposed model generates more informative representations, especially in syntactic structure features.",
        "The model's performance can be further improved by validating it in other tasks such as reading comprehension, language inference, and sentence classification.",
        "Directly augmenting the Transformer encoder with a recurrence model without an additional encoder is a promising direction for future work."
    ],
    "4157": [
        "Our approach on various machine translation tasks consistently and significantly outperforms the strong TRANSFORMER baseline.",
        "Experimental results across 10 linguistic probing tasks reveal that our EM routing-based model indeed produces more informative representation, which benefits multi-head attention to capture more syntactic and semantic information.",
        "In addition, our approach on various machine translation tasks consistently and significantly outperforms the strong TRANSFORMER baseline.",
        "Extensive analysis further suggests that only applying EM routing to low-level two layers of the encoder can best balance the translation performance and computational efficiency.",
        "Our EM routing-based model produces more informative representation, which benefits multi-head attention to capture more syntactic and semantic information.",
        "The routing algorithm iteratively updates the proportion of how much a partial representation should be assigned to the final output representation, based on the agreement between parts and wholes."
    ],
    "4158": [
        "modeling locality is beneficial to SANs\" - This claim suggests that intentionally modeling locality in self-attention mechanisms can improve the performance of sequence-to-sequence models.",
        "interacting features across multiple heads at attention time can further improve the performance\" - This claim suggests that allowing features from different heads to interact at attention time can lead to better performance, potentially because it allows the model to capture more contextual relationships between input sequences.",
        "to some extent, the dynamic weights are superior to their fixed counterpart (i.e. CSANs vs. CNNs) on local feature extraction\" - This claim suggests that using dynamic weights in self-attention mechanisms can lead to better performance compared to using fixed weights, at least for certain tasks like local feature extraction.",
        "the proposed approach is not limited to the task of machine translation\" - This claim suggests that the proposed method for enhancing feature extraction through parameter-free convolutional self-attention can be applied to other sequence modeling tasks beyond machine translation, such as reading comprehension, language inference, semantic role labeling, sentiment analysis, and sentence classification."
    ],
    "4160": [
        "Outliers are often the most interesting parts of our data, but outlier detection has received relatively little attention in NLP beyond its application to finding annotation errors.",
        "This paper introduces the first neural outlier detection method for short text and demonstrates its effectiveness across multiple metrics in multiple experiments.",
        "We also propose a way to integrate outlier detection into data collection, developing and evaluating a novel crowdsourcing pipeline.",
        "This pipeline supports the creation of higher quality datasets to yield higher quality models by both reducing the number of errors and increasing the diversity of collected data.",
        "While the experiments discussed herein are concerned with components of dialog systems, we believe that similar data collection strategies could yield benefits to other areas of NLP as well."
    ],
    "4166": [
        "Our model achieves state-of-the-art results on entity recognition and relation extraction tasks across a diverse range of domains.",
        "The key contribution of our model is the dynamic span graph approach, which enhances interaction across tasks and allows the model to learn useful information from broader context.",
        "Unlike many IE frameworks, our model does not require any preprocessing using syntactic tools, and has significant improvements across different IE tasks including entity, relation extraction, and overlapping entity extraction.",
        "The addition of co-reference and relation propagation across sentences adds only a small computation cost to inference; the memory cost is controlled by beam search.",
        "Our model welcomes the community to test our model on different information extraction tasks.",
        "Future directions include extending the framework to encompass more structural IE tasks such as event extraction."
    ],
    "4169": [
        "JESSI performs competitively among participating models, obtaining second place on Subtask A with an F-Score of 77.78%.",
        "It also performs well on Subtask B, with an F-Score of 79.59%, even without using any additional external data.",
        "BERT alone performs bad and unstably when tested on out-of-domain samples.",
        "We mitigate the problem by appending an RNN-based sentence encoder above BERT, and jointly combining a CNN-based encoder.",
        "JESSI builds upon jointly combined encoders, borrowing pre-trained knowledge from a language model BERT and a translation model CoVe."
    ],
    "4170": [
        "Existing automated metrics for evaluating dialogue systems show poor correlation with human annotations, making it essential to develop a new paradigm for evaluation.",
        "The proposed approach based on state-of-the-art entailment techniques can provide an unbiased estimate of response quality and correlates reasonably well with human judgment.",
        "There is still room for improvement in evaluating dialogue systems, such as measuring the engagingness of the conversation to improve the evaluation of different dialogue strategies.",
        "The proposed approach does not require human annotation, which can lead to a scalable evaluation approach."
    ],
    "4171": [
        "The proposed NLG system with an explicit symbolic planning component outperforms an end-to-end neural system regarding faithfulness to the input.",
        "The plan-based system performs on par with a strong end-to-end neural system regarding automatic evaluation metrics and human fluency evaluation.",
        "The explicit user-control in the planning stage allows for generating diverse sentences.",
        "The proposed NLG system has the potential to be more faithful to the input compared to end-to-end neural systems.",
        "The planning stage of the proposed NLG system enables explicit user-control and generates diverse sentences, which can be pursued in future work."
    ],
    "4172": [
        "Our parallelization scheme for StackLSTM fully exploits GPU parallelism and scales linearly with increasing batch size.",
        "Our method yields comparable performance to previous work while incorporating minibatching.",
        "We leave the parallelization of certain architectures (Arc-Standard transition system and token-level composition function) for future work.",
        "Our parallelization scheme makes it feasible to run large-data experiments for various tasks that require large training data to perform well, such as RNNG-based syntax-aware neural machine translation."
    ],
    "4174": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our system is capable of detecting offensive language robustly, and it has a good chance of identifying the target.",
        "There is room for improvement in capturing subtle meaning and overcoming data sparsity."
    ],
    "4177": [
        "Our system achieved a 72.9% and 60.8% accuracy on the test data that is labeled by-Article and by-Publisher, respectively.\" (related to the performance of the logistic regression model)",
        "We also evaluated additional features that represent different aspects of the article's text such as its vocabulary richness, the kind of language it uses according to different lexicons, and its level of complexity.\" (related to the feature set used in the model)",
        "Initial experiments showed that these features hurt the model.\" (related to the effect of certain features on the model's performance)",
        "However, with proper preprocessing and scaling we were able to achieve significant performance improvements of up to 2% in absolute accuracy.\" (related to the impact of preprocessing and scaling on the model's performance)",
        "These results were obtained after the competition's deadline, hence were not considered as part of our submission.\" (related to the timing of the results and their non-inclusion in the submitted work)"
    ],
    "4178": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The use of a neural CRF approach for entity tracking achieves state-of-the-art results on the PROPARA dataset.",
        "The model's performance can be improved by applying a filter to modify the list of predicted target entities, especially for entities that are similar to the given relationships.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "The model's ability to track entities and locations recurrently is an important contribution to the field of entity tracking."
    ],
    "4180": [
        "We introduced a method to improve fine-tuning using 3 main ideas...",
        "adding random units and jointly learn them with pre-trained ones",
        "normalising the activations of both to balance their different behaviours",
        "applying learnable weights on both predictors to let the network learn which of random or pre-trained one is better for every class",
        "We have demonstrated its effectiveness on domain adaptation from newswire domain to three commonly used Tweets-datasets for POS tagging."
    ],
    "4182": [
        "Incorporating syntax into neural models improves performance, as shown in our work and previous studies by Strubell et al. (2018) and Shi et al. (2018).",
        "Explicitly modeling syntax provides useful inductive biases for language modeling.",
        "The interaction between a constrained parser and a more general grammar learner can lead to the emergence of apparently grammatical constraints, which is an intriguing but underexplored hypothesis for explaining human linguistic biases.",
        "Our work suggests that incorporating syntax into neural models can improve performance in language modeling tasks.",
        "The use of syntactic heads in supervising intermediate attention layers improves semantic role labeling, as shown by Strubell et al. (2018)."
    ],
    "4186": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our experimental results showed that SAMIE worked especially well when given a very small amount of labeled data, and that it outperformed the baseline methods.",
        "In the future, we would consider further reducing the need for labeled data and trying to achieve unsupervised learning with the help of pre-training techniques."
    ],
    "4188": [
        "This work introduces a new benchmark of political discussions from online fora, annotated with issue frames following the Policy Frames Cookbook.",
        "Online fora are influential platforms that can have an impact on public opinion, but the language used in such fora is very different from newswire and other social media.",
        "Multi-task and adversarial learning can facilitate transfer learning from such domains, leveraging previously annotated resources to improve predictions on informal, multi-party discussions.",
        "Our best model obtained a micro-averaged F1-score of 0.548 on our new benchmark."
    ],
    "4190": [
        "Our adaptation framework for speech act recognition in asynchronous conversation achieves state-of-the-art results for in-domain training.",
        "The conversational word embeddings are crucial to the performance of our base model.",
        "Adversarial training effectively leverages out-of-domain meeting data and improves the results further.",
        "Our approach demonstrates effectiveness compared to existing methods and baselines in different training scenarios."
    ],
    "4191": [
        "The proposed evaluation framework for Greek word embeddings provides an intrinsic evaluation of the models.",
        "The newly introduced corpus 2 was used for training the Greek models, and specific linguistic aspects of the Greek language were added to the word analogy questions 1 test set.",
        "The models were able to create meaningful word representations, as shown by the results.",
        "The models will be evaluated in other extrinsic tasks like POS tagging, language modeling, and text classification in future work.",
        "(Intrinsic evaluation framework for Greek word embeddings) [Paragraph 1]",
        "(Use of corpus 2 for training Greek models) [Paragraph 2]",
        "(Ability to create meaningful word representations) [Paragraph 3]",
        "(Plans for extrinsic task evaluation in future work) [Paragraph 4]"
    ],
    "4195": [
        "We introduced a differentiable sampling algorithm which exposes a sequence-to-sequence model to its own predictions during training and compares them to reference sequences flexibly to backpropagate reliable error signals.",
        "Our approach consistently improves BLEU over maximum likelihood and scheduled sampling baselines on three IWSLT tasks, with larger improvements for greedy search and smaller beam sizes.",
        "Our approach is also simple to train, as it does not require any sampling schedule."
    ],
    "4197": [
        "We introduced the AUTOSEM framework, a two-stage multi-task learning pipeline that automatically selects relevant auxiliary tasks and learns their optimal mixing ratio.",
        "Our experimental results show that AUTOSEM outperforms strong baselines on several GLUE tasks.",
        "The first stage of our framework automatically selects the relevant auxiliary tasks, and the second stage learns their optimal mixing ratio.",
        "We ablated the importance of each stage of our framework and discussed the intuition of selected auxiliary tasks.",
        "Our approach improves the performance of GLUE tasks by learning the optimal mixing ratio of auxiliary tasks."
    ],
    "4199": [
        "The agent is supervised with a mixture of imitation learning and reinforcement learning.",
        "The agent is fine-tuned with semi-supervised learning, using speaker-generated instructions.",
        "The limited variety of environments is the bottleneck of back translation, and we overcome it via 'environmental dropout' to generate new unseen environments.",
        "Our model achieves rank-1 in the Vision and Language Navigation (VLN) challenge leaderboard under all experimental setups."
    ],
    "4203": [
        "The proposed text generation model using exemplar-informed adaptive decoding achieves strong performance and outperforms comparable baselines on both text summarization and data-to-text generation tasks.",
        "The proposed model is applicable in other conditioned text generation tasks.",
        "The use of retrieved exemplars in the decoder reparameterization improves the performance of the text generation model.",
        "The implementation of the proposed model is available at <https://homes.cs.washington.edu/\u02dchapeng>."
    ],
    "4205": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "LCTM allows us to formulate queries when concepts do not perfectly match with a word or word expression in the corpus.",
        "Our method can also successfully be used with queries formulated as coordinates on the word embedding space.",
        "The LCTM model inferred from a court decision corpus using the method described by Hu and Tsujii (2016) improves performance on single-word queries and multiword expressions."
    ],
    "4208": [
        "Our cross-modal self-attention module captures long-range dependencies between visual and linguistic modalities, which results in a better feature representation to focus on important information for referred entities.",
        "The proposed gated multi-level fusion module adaptively integrates features from different levels via learnable gates for each individual level.",
        "Our proposed network achieves state-of-the-art results on all four benchmark datasets."
    ],
    "4215": [
        "We propose a Bi-directional Attention entity Graph convolutional network (BAG) for multihop reasoning QA tasks.",
        "Regarding task characteristics, graph convolutional networks (GCNs) are efficient to handle relationships among entities in documents.",
        "We demonstrate that both bidirectional attention between nodes and queries and multi-level features are necessary for such tasks.",
        "The former one aims to obtain query-aware node representation for answering, while the latter one provides contextual comprehension of isolated nodes in graphs.",
        "Our experimental results not only demonstrate the effectiveness of two proposed modules, but also show BAG achieves state-of-the-art performance on the WIKIHOP dataset.",
        "Our future work will be making use of more complex relations between entities and building graphs in more general way without candidates."
    ],
    "4217": [
        "The authors introduce a novel neural model that utilizes meta-embeddings learned from domain-specific word embeddings and task-specific features to capture contextual information.",
        "The authors present a unique dataset of cyber security related noisy short text collected from Twitter.",
        "The authors use word2vec, GloVe, and fastText to learn domain-specific word embeddings on the unlabeled tweet corpus.",
        "The authors reveal contextual information using a contextual embedding encoder.",
        "The authors combine a CNN and an RNN to detect cyber security related events.",
        "The authors favor simple models over complex ones, but for their task, detecting cyber security related events requires tedious effort as well as domain knowledge.",
        "The authors design handcrafted features with domain experts to address some of the challenges of their problem.",
        "The authors learn to extract features using deep neural networks.",
        "The authors provide ablations to show which part of the proposed method adds how much value to the overall success."
    ],
    "4218": [
        "Our approach achieves new state-of-the-art performance on FewRel 2.0 dataset using inductive unsupervised domain adaption.",
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We propose a variational approach to weakly supervised DMSC that extracts many target-opinion word pairs from dependency parsers using simple rules.",
        "Our objective function is to predict an opinion word given a target word, and we can outperform weakly supervised baselines by a large margin and achieve comparable results to the supervised method with hundreds of labels per aspect.",
        "In the future, we plan to explore better target-opinion word extraction approaches to find better \"supervision\" signals."
    ],
    "4219": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Varying the input (textual, visual, or multimodal) affects the accuracy of the verb prediction.",
        "When visual information is added to textual features, models in both German and Spanish predict the correct label."
    ],
    "4222": [
        "The method eliminates the need to specify which biases are to be mitigated, and allows simultaneous mitigation of multiple biases.",
        "Our method leverages the societal biases encoded in word embeddings of names.",
        "It discourages an occupation classifier from learning a correlation between the predicted probability of an individual's occupation and a word embedding of their name.",
        "Both variations simultaneously reduce race and gender biases, with almost no reduction in the classifier's overall true positive rate.",
        "Our method is conceptually simple and empirically powerful, and can be used with any classifier, including deep neural networks.",
        "Although we focus on English, we expect our method will work well for other languages, but leave this direction for future work."
    ],
    "4224": [
        "The proposed POS tag grounding strategy based on decipherment does not require human-labeled data and can be used in real-world situations.",
        "The decipherment model considers state or word cluster IDs of a CL as a cipher text to be deciphered back to a POS sequence.",
        "The choice of PL to decipher POS tags from is crucial for performance, and both criteria (model confidence and typological similarities) are not correlated with tagging accuracy scores.",
        "A cipher model combination strategy can be used to leverage the word-order patterns in several PLs, resulting in a completely language-agnostic grounder.",
        "The combined grounder provides a non-trivial signal for improvement of downstream tasks, and obtains state-of-the-art results for name tagging in Kinyarwanda and Sinhalese."
    ],
    "4227": [
        "The beam problem in neural machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Solving label bias in general may lead to further improvements in machine translation systems.",
        "Our approach to unified visual-semantic embedding learns a joint representation space of vision and language in a factorized manner.",
        "The proposed approach for semantic components is contrastive learning, and the enforcement of semantic coverage is introduced for efficient learning.",
        "Unified VSE shows superiority on multiple cross-modal retrieval tasks and can effectively defend text-domain adversarial attacks.",
        "The proposed approach can empower machines that learn vision and language jointly, efficiently and robustly."
    ],
    "4230": [
        "Our proposed model (PReFIL) surpasses prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL exceeds human baseline for FigureQA, but results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our method is able to find and utilize underlying dialog structures for dialog inference in both tasks, demonstrating its generality and effectiveness.",
        "The use of a GNN backbone and EM-style inference algorithm enables the estimation of latent relations between nodes and missing values of unobserved nodes."
    ],
    "4233": [
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension that enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow information to flow across segments, enabling the model to have knowledge beyond the current segment when selecting answers.",
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "We present an unsupervised approach to mapping multiple topic-based DSMs to a unified vector space, capturing different contextual semantics of words.",
        "Our model yields state-of-the-art results on contextual similarity compared to previously proposed unsupervised approaches for multiple word embeddings creation.",
        "The projected word embeddings outperform single vector representations in downstream NLP tasks.",
        "We provide insightful visualizations and examples that demonstrate the capability of our model to capture variations in topic semantics of words.",
        "The area a word covers in the mapped space reveals its semantic range."
    ],
    "4236": [
        "The proposed attention mechanism outperformed existing baselines by 1.1% on MRR.",
        "The general factor graph based attention mechanism can operate on any number of utilities.",
        "The proposed attention mechanism was applied to the recently introduced visual dialog dataset."
    ],
    "4245": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our method consistently outperforms state-of-the-art models in distantly supervised fine-grained entity typing.",
        "Our method is more robust than the former state-of-the-art approach as the portion of noisy data rises.",
        "The proposed method is general for other tasks with imperfect annotation."
    ],
    "4247": [
        "We proposed an approach to discover cross-lingual symbol mapping for helping model better transferred with knowledge learned previously from abundant source data.",
        "Experiment results show that our method enables the model to produce far more natural-sounding speech than the model trained only on target data.",
        "Our method achieves promising results compared with the method using strong linguistic background expertise.",
        "The approach works for low-resource languages.",
        "The model can be trained with a small amount of target data."
    ],
    "4248": [
        "\"UR-FUNNY is the first multimodal dataset for humor detection in the NLP community.",
        "\"Humor can be better modeled if all three modalities (text, vision, and acoustic) are used together.",
        "\"Both context and punchline are important in understanding humor.",
        "\"The dataset and accompanying experiments will be made publicly available."
    ],
    "4250": [
        "We analyze the performance of three discourse segmenters on News and Medical.",
        "All segmenters suffer a drop in performance on Medical, but this drop is smaller on the best News segmenter.",
        "An error analysis reveals difficulty in both domains for cases requiring a fine-grained syntactic analysis, as dictated by the RST-DT annotation guidelines.",
        "In the Medical domain, we find that differences in syntactic construction and formatting, including use of punctuation, account for most of the segmentation errors.",
        "We hypothesize these errors can be partly traced back to tokenizers and word embeddings also trained on News.",
        "Both suffer in sections with more complex discourse.",
        "We have proposed a set of next steps to expand the corpus and improve the segmenter."
    ],
    "4251": [
        "We presented LDMI, a model to estimate distributed representations of the multi-sense words.",
        "LDMI is able to efficiently identify the meaningful senses of words and estimate the vector embeddings for each sense of these identified words.",
        "The vector embeddings produced by LDMI achieves state-of-the-art results on the contextual similarity task by outperforming the other related work."
    ],
    "4253": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our proposed method alleviates the brevity problem using a self-attentive BiLSTM-CRF-based solution.",
        "We have explored simple and effective ways to alleviate or eliminate the beam problem.",
        "The local-global surprisal principle is a useful conceptual tool for detecting puns, but it is not yet formalized in a robust enough way to be used both as a principle for evaluating sentences and directly optimized to generate puns.",
        "Developing methods that can recognize the difference between creativity and nonsense is key to future progress in humor and creative text generation."
    ],
    "4254": [
        "We find that in the context of inexact search over large output spaces, globally normalized models are more effective than locally normalized models.",
        "Our extension to the continuous relaxation to beam search proposed by Goyal et al. (2017b) to train search-aware globally normalized models and comparable locally normalized models is effective in reducing label bias.",
        "Empirical analysis shows that search-aware optimization and global normalization are important factors in reducing label bias.",
        "Inexact search over large output spaces can be challenging, but globally normalized models are more effective than locally normalized models in this context.",
        "The proposed extension to the continuous relaxation to beam search is effective in improving the performance of search-aware globally normalized models."
    ],
    "4257": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks."
    ],
    "4258": [
        "Direct models outperform cascaded models when there is enough data available, but suffer from poor performance with auxiliary data.",
        "Two-stage models are more data-efficient than direct models and can overcome the shortcomings of direct models, but may suffer from error propagation issues.",
        "A novel attention-passing model alleviates error propagation issues and outperforms all other tested models, while also being more data-efficient than the direct model.",
        "There is a trade-off between data efficiency and error propagation in speech translation systems.",
        "Future work includes testing better ASR attention models, adding other types of external data such as ASR data, unlabeled speech, or monolingual texts, and exploring further model variants."
    ],
    "4261": [
        "Simply fine-tuning our BERT BASE model greatly improved F 0.5 scores for grammatical error detection task.",
        "Our MHMLA model outperformed previous models for grammatical error detection, establishing new state-of-the-art F 0.5 scores.",
        "Our analysis demonstrated that we succeeded at learning appropriate representations for a given task using information from different layers.",
        "Future work includes applying MHMLA to other language representation models like Open AI GPT model [2] .",
        "We will explore whether our layers learned the same syntactic and semantic roles as a previous work [12] , also what exactly self-attention learns at a token-level for grammatical error detection."
    ],
    "4262": [
        "early discussion features are predictive of eventual controversiality in several reddit communities.",
        "considering an expressive feature set of early discussions hadnt been thoroughly explored in prior early prediction work.",
        "One promising avenue for future work is to examine higher-quality textual representations for conversation trees.",
        "our mean-pooling method did produce high performance, but the resulting classifiers do not transfer between domains effectively.",
        "Developing a more expressive algorithm (e.g., one that incorporates reply-structure relationships) could boost predictive performance, and enable textual features to be less brittle."
    ],
    "4265": [
        "The proposed method can control the output sequence length in Transformer.",
        "The proposed method significantly improved the quality of headlines on the Japanese headline generation task while preserving the given length constraint.",
        "The proposed method also generated headlines with the desired length precisely and achieved the top ROUGE scores on the DUC-2004 test set."
    ],
    "4268": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our proposed method is effective, but the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "In future work, we will attempt to solve this problem by developing annotated datasets from multiple sources and combining our method with distant supervision and reinforcement learning."
    ],
    "4273": [
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR), with performance not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "The large space of presented results has allowed us to analyze the main properties of subword-informed representation learning.",
        "Different components of the framework such as segmentation and composition methods, or the use of position embeddings, have to be carefully tuned to yield improved performance across different tasks and languages.",
        "This study will guide the development of new subword-informed word representation architectures."
    ],
    "4279": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Using MC-CNN helped overcome the overfitting caused by the embedding.",
        "In order to decrease the variance of the system, we used duplications of this model and averaged the results.",
        "This system reached 4th place at the HateEval task with an F 1 score of 0.535, and 2nd place at sub-task B in the OffensEval task, with an F 1 score of 0.739."
    ],
    "4280": [
        "Our main contributions in this work are two-fold: Firstly, we introduced the concept of automatic AMR accuracy prediction.",
        "Given only an automatic parse and the sentence, from whence it was derived, the goal is to predict evaluation metrics cheaply and possibly at runtime.",
        "We framed the task as a multiple-output regression task and developed a hierarchical neural model to predict a rich suite of AMR evaluation metrics.",
        "Our model was able to reproduce rankings similar to the true rankings in two use cases.",
        "The feasibility of automatic AMR accuracy prediction in general (significant correlation with gold scores on unseen indomain and out-of-domain data).",
        "Our method outperformed the random selection baseline by 5.7 pp. average Smatch F1 (in-domain) and 5.2 pp. (out-of-domain)."
    ],
    "4281": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Using a pre-trained CTC model with the NN architecture of interest can guide spike timings of CTC models in various scenarios, including posterior fusion of multiple CTC models and knowledge distillation between CTC models.",
        "The proposed guided CTC training achieves state-of-the-art WERs in the CTC-based direct acoustic-to-word setting without using any data augmentation or language model.",
        "The spike timings were aligned between the guiding and the guided models, and between the multiple guided models."
    ],
    "4283": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "While we demonstrate that NLP techniques can be used to support literary analyses and obtain new insights, they also have clear limitations (e.g., in understanding abstract themes).",
        "As text representation methods become more powerful, we hope that (1) computational tools will become useful for analyzing novels with more conventional structures, and (2) literary criticism will be used as a testbed for evaluating representations."
    ],
    "4285": [
        "Our hypotheses suggest that homonyms seem to repel each other, like particles with the same electric charge.",
        "Distinct homonyms are rarely observed in connection with a single translation, discourse, collocation, or sense cluster.",
        "Contraventions of the empirical predictions made by our theory more often than not identify errors in existing resources.",
        "We plan to develop an operational method for identifying Type-B homonyms on the basis of translation sets involving multiple languages.",
        "We anticipate that translations extracted from parallel corpora will facilitate the creation of high-quality coarse-grained sense inventories via sense clustering.",
        "As a step towards this goal, we will investigate the problem of automated mapping between senses and translations."
    ],
    "4287": [
        "We investigated analytical methods for obtaining interpretable word spaces.",
        "Relevant methods were examined with the tasks of lexicon induction, word analogy and debiasing.",
        "We gratefully acknowledge funding through a Zentrum Digitalisierung.Bayern fellowship awarded to the first author.",
        "This work was supported by the European Research Council (# 740516).",
        "We thank the anonymous reviewers for valuable comments."
    ],
    "4289": [
        "The proposed method, Time Warping, improves performance but is not the major factor contributing to the improvement.",
        "The effect of time warping, while small, is still existent even when other augmentations (time masking and frequency masking) are turned off.",
        "Time warping is the most expensive and least influential of the augmentations discussed in this work, and should be the first augmentation to be dropped given any budgetary limitations.",
        "Augmentation converts an over-fitting problem into an under-fitting problem.",
        "The current reported performance was obtained by using a harsh augmentation policy and making wider, deeper networks and training them with longer schedules to address the under-fitting.",
        "Standard approaches to alleviate under-fitting, such as making larger networks and training longer, yield significant improvements in performance.",
        "The proposed method of frequency masking has been studied in the context of CNN acoustic models in [49].",
        "There are other ideas for structurally omitting frequency data of spectrograms that have been discussed in [50]."
    ],
    "4290": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Existing datasets do not support a systematic exploration of the research efforts towards the goal of answering questions about everyday images that involve reading and reasoning about text in these images.",
        "The TextVQA dataset contains questions which can only be answered by reading and reasoning about text in images.",
        "LoRRA significantly outperforms the current state-of-the-art VQA models on TextVQA.",
        "Our OCR model, while mature, still fails at detecting text that is rotated, a bit unstructured (e.g., a scribble) or partially occluded."
    ],
    "4293": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks."
    ],
    "4297": [
        "The proposed recurrent chunking mechanisms outperform benchmark models across different datasets.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "mBERT performs well in a cross-lingual zeroshot transfer setting on five different tasks covering a large number of languages.",
        "mBERT outperforms crosslingual embeddings, which typically have more cross-lingual supervision.",
        "Fixing the bottom layers of mBERT during fine-tuning observes further performance gains.",
        "Language-specific information is preserved in all layers.",
        "Sharing subwords helps cross-lingual transfer; a strong correlation is observed between the percentage of overlapping subwords and transfer performance.",
        "mBERT effectively learns a good multilingual representation with strong cross-lingual zero-shot transfer performance in various tasks."
    ],
    "4299": [
        "Our method allows the model to learn to translate source-side target phrases by 'copying' them to the output, achieving consistent improvements over previous lexical constraint methods on large NMT test sets.",
        "We are the first to leverage code switching for NMT with pre-specified translations.",
        "Our method utilizes code-switched source sentences and their translations as augmented training data, allowing the model to learn to translate source-side target phrases by 'copying' them to the output.",
        "The use of code switching for NMT with pre-specified translations has not been explored before, and our method is the first to do so.",
        "Our approach achieves consistent improvements over previous lexical constraint methods on large NMT test sets."
    ],
    "4300": [
        "Our approach uses multilingual word embeddings that are aligned into a single vector space to allow for cross-lingual transfer of models.",
        "Using English as a source language in a zeroshot setting, our approach was able to reach an F 1score of 0.50 for Spanish and 0.46 for Dutch.",
        "By using multiple source languages, we increased the zero-shot performance to F 1 -scores of 0.58 and 0.53, respectively, which correspond to 85% and 87% in relative terms.",
        "We investigated the benefit of augmenting the zeroshot approach with additional data points from the target language. Here, we observed that we can save several hundreds of annotated data points by employing a cross-lingual approach.",
        "Turkish seemed to benefit the least from cross-lingual learning in all experiments. The reason for this might be that Turkish is the only agglutinative language in the dataset.",
        "We compared two approaches for aligning multilingual word embeddings in a single vector space and found their results to vary for individual language pairs but to be comparable overall.",
        "Our multilingual model achieves competitive performances for some languages and even presents the best system for Russian and Turkish."
    ],
    "4301": [
        "Integrating knowledge into pre-training language models can improve performance on various natural language processing tasks.",
        "Pre-training on heterogeneous data can enable the model to obtain better language representation.",
        "Using syntactic parsing or weak supervised signals from other tasks can improve language representation.",
        "The proposed method can be applied to other languages."
    ],
    "4302": [
        "The approach of jointly training POS tagging and dependency parsing improves over previous work on these tasks.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The use of a span-extractive approach for POS tagging and dependency parsing can improve performance compared to traditional task-specific architectures.",
        "The span-extractive approach is robust in the presence of limited training data.",
        "Less inductive bias in the design of task-specific architectures may be required when approaching NLP tasks.",
        "A span-extractive approach can be extended to text classification and regression problems as well.",
        "The use of a unified architecture for a broader set of tasks can improve performance and reduce the need for architectural modifications across datasets or tasks."
    ],
    "4303": [
        "Our approach can substantially outperform previous parallel decoding methods and approach the performance of sequential autoregressive models while decoding much faster.",
        "Our results provide a significant step forward in nonautoregressive and parallel decoding approaches to machine translation.",
        "Masked language models are useful not only for representing text, but also for generating text efficiently.",
        "Our approach can approach the performance of sequential autoregressive models while decoding much faster.",
        "There is still a need to condition on the target's length and the dependence on knowledge distillation."
    ],
    "4305": [
        "The existing Transformer architectures, such as BERT and GPT, are not able to capture the strong word-level context required in language modeling.",
        "Fine-tuning a subset of parameters can improve the coarse-grain representations obtained from pre-trained Transformer models.",
        "Adding LSTM layers can capture fine-grained sequence information and improve language modeling performance.",
        "The proposed coordinate architecture search (CAS) algorithm can select an effective architecture based on finetuning results, outperforming state-of-the-art language models on three benchmark datasets.",
        "CAS can be applied to both other neural network architectures and fine-tuning other NLP tasks that require strong word-level context."
    ],
    "4306": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our novel contextual utterance encoder learns better distance and communities than state-of-the-art competitors.",
        "Energy-based Self-attentive Learning of Abstractive Communities for Spoken Language Understanding.",
        "Using the siamese and triplet meta-architectures, we showed that our novel contextual utterance encoder learns better distance and communities than state-of-the-art competitors.",
        "Our approach is one of the first applications of energy-based learning to ACD."
    ],
    "4307": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our approach is simple and easy to implement, while achieving strong performance on various domains.",
        "The deduction of manual data curation efforts for such tasks is of great potential and importance for many real-world applications."
    ],
    "4311": [
        "We propose to recognize the translated PAST and untranslated FUTURE contents via parts-to-wholes assignment in neural machine translation.",
        "Our approach explicitly separates source words into PAST and FUTURE guided by PRESENT target decoding status at each decoding step.",
        "We empirically demonstrate that such explicit separation of source contents benefit neural machine translation with considerable and consistent improvements on three language pairs.",
        "Our approach learns to model the PAST and FUTURE as expected, and alleviates the inadequate translation problem.",
        "It is interesting to apply our approach to other sequence-to-sequence tasks, e.g., text summarization (as listed in Appendix)."
    ],
    "4313": [
        "'ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.'",
        "'Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.'",
        "'There is still a significant performance gap between the best-performing model (68.5%) and human readers (96.0%).'",
        "'AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.'",
        "'The proposed AdaBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.'",
        "'The impacts of distractor plausibility and data augmentation on the performance of state-of-the-art neural models.'"
    ],
    "4315": [
        "The Syntactic Attention model incorporates ideas from cognitive and computational neuroscience into the neural machine translation framework, leading to systematic generalization.",
        "The model separates sequential information used for alignment (syntax) from information used for mapping individual inputs to outputs (semantics), allowing it to generalize the usage of a word with known syntax to many of its valid grammatical constructions.",
        "This principle may be a useful heuristic in other natural language processing tasks and in other systematic or compositional generalization tasks.",
        "The success of the approach suggests a conceptual link between dynamic selective-attention mechanisms in the prefrontal cortex and the systematicity of human cognition, and points to the untapped potential of incorporating ideas from cognitive science and neuroscience into modern approaches in deep learning and artificial intelligence."
    ],
    "4316": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Despite human performance of close to 90%, computational approaches based on large pretrained language models only achieve accuracies up to 65%, suggesting that these social inferences are still a challenge for AI systems.",
        "Transfer learning from SOCIAL IQA to other commonsense challenges can yield significant improvements, achieving new state-of-the-art performance on both COPA and Winograd Schema Challenge datasets."
    ],
    "4319": [
        "The Wasserstein-Fisher-Rao metric is applied as one unsupervised document distance (WFR document distance) which is demonstrated to be theoretically solid, easy to interpret, and proved to be much more robust than WMD.",
        "WFR and its derivatives can be calculated efficiently by WFR Sinkhorn iterations with GPU acceleration.",
        "The new proposed metric benefits from the semantic similarity of word embedding space while employing automatically re-weighted transport plan to overcome the overestimation issue appearing in varying-length situations.",
        "Numerical experiments confirm the effectiveness and efficiency of the new proposed metric."
    ],
    "4322": [
        "This work demonstrates the potential of computational linguistics to aid neuropsychiatric practice in the clinic.",
        "We believe it is critically important to tie computational methods to established clinical practice in order to bridge the gap between the latest developments in NLP and clinical practice.",
        "The sentence embedding and coherence metrics computed in this study are by no means an exhaustive list of potential methods.",
        "We are interested in finding a more concise group of clinically relevant language features with which we can perform this analysis.",
        "Additionally, we can look at more language metrics within each subject group to further subtype and cluster individuals within each group based on language metrics.",
        "These methods can also be applied to clinical assessments beyond the SSPA tasks and for a wider variety of psychiatric conditions.",
        "Lastly, we would like to examine how classification and modeling of clinical test scores changes when computed features are used in conjunction with other clinical tests to model task performance and classification of groups."
    ],
    "4323": [
        "Experiments show that neural networks are effective for identifying blame ties from news articles.",
        "Our approach can enable researchers to quantify the importance of a frame and to understand how and why it became prevalent.",
        "It can also enable researchers to study actors' position alignments over time.",
        "We release our code and model for automatic blame tie extraction to facilitate such research."
    ],
    "4325": [
        "The community first made significant deep learning advances with on ImageNet (Russakovsky et al., 2015) by predicting single label classes (Krizhevsky et al., 2012).",
        "The community progressively moved to sequence outputs such as image captioning (Xu et al., 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015).",
        "End-to-end models began to produce entire paragraphs of text describing a single image (Krause et al., 2017).",
        "Our method is able to learn acoustic and language features while being able to generalize to unseen speakers using a newly collected dataset of multiple speakers in multiple languages.",
        "We hope the speech and signal processing community will build on our work, moving to larger and more complex models for even longer sentences and full paragraphs."
    ],
    "4327": [
        "Visual grounding is indeed helpful even in the presence of textual supervision.",
        "Joint training with both visual and textual supervision results in consistently improved retrieval.",
        "The current set of queries and human judgments is small, limiting the generalization of the results.",
        "A natural next step is to collect more human evaluation data and to consider a wider variety of queries, including multi-word queries.",
        "There is room for more exploration of different types of multi-view representation losses and more structured speech models that can localize the relevant words/phrases for a given query."
    ],
    "4328": [
        "NMT behavior can be erratic due to contextual patterns not observed during training.",
        "Localized and minor ASR errors can cause long distance errors in translation.",
        "NMT duplicates content words when minor ASR errors cause the modification of function words.",
        "The observable errors are caused by minor substitution errors caused by noisy ASR.",
        "Evaluating NMT architectures that model coverage as well as the representation of inputs with subword units can help to expand this analysis further."
    ],
    "4331": [
        "We have shown that neural sequence-to-sequence models can be used to generate high quality natural language text from Minimal Recursion Semantics representations.",
        "Furthermore, we have demonstrated that a large hand-crafted grammar can be leveraged to produce large training sets, which improves performance of neural generators substantially.",
        "Therefore we argue that the ability to generate high quality text from MRS makes it a good choice of representation for text generation applications that require semantic structure.",
        "For future work, we are interested in applying graph-to-sequence neural networks (Beck et al., 2018; Song et al., 2018) to MRS-to-text generation."
    ],
    "4332": [
        "The proposed spatio-temporal video QA task requires systems to jointly localize relevant moments, detect referred objects/people, and answer questions.",
        "The STAGE framework is an end-to-end trainable framework that jointly performs all three tasks (localization, detection, and question answering).",
        "Temporal and spatial predictions help improve QA performance, as well as providing explainable results.",
        "The STAGE achieves state-of-the-art performance, but there is still a large gap compared with human performance, leaving space for further improvement."
    ],
    "4335": [
        "We have extended previous analyses based on the WEAT test in multiple dimensions: across seven languages, four embedding models, and three different types of text.",
        "Different models may produce embeddings with very different biases, which stresses the importance of embedding model selection when fair text representations are to be created.",
        "Surprisingly, we find that the user-generated texts, such as tweets, may be less biased than redacted content.",
        "We have investigated the bias effects in cross-lingual embedding spaces and have shown that they may be predicted from the biases of corresponding monolingual embeddings.",
        "We make the XWEAT dataset and the testing code publicly available, hoping to fuel further research on biases encoded in word representations."
    ],
    "4336": [
        "We established strong baselines for two story narrative understanding datasets: CaTeRS and RED.",
        "Neural network-based models can outperform feature-based models with wide margins.",
        "Contextualized representation learning can boost performance of NN models.",
        "Further research can focus on more systematic study or build stronger NN models over the same datasets used in this work.",
        "Exploring possibilities to directly apply temporal relation extraction to enhance performance of story generation systems is another promising research direction."
    ],
    "4343": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The SAOKE format is designed to express different types of facts in a unified manner, and it publicly releases the largest manually labeled data set for OIE tasks in SAOKE form.",
        "Logician, an end-to-end neural sequenceto-sequence model, is trained to transform sentences in natural language into facts, and it shows superiority in various open-domain information extraction tasks compared to state-of-the-art algorithms.",
        "There are at least three promising directions for future work, including investigating knowledge expression methods to extend SAOKE to express more complex knowledge, developing novel learning strategies to improve the performance of Logician, and extending SAOKE format and Logician algorithm in other languages."
    ],
    "4345": [
        "The proposed method for sentiment analysis on tweets using distributed representation of words and sentences is memory efficient, theoretically.",
        "LSTMs are better at Sentiment Analysis compared to other models.",
        "The proposed method shows promising results in accurately classifying tweets as positive, negative, or neutral.",
        "The use of distributed representation of words and sentences improves the performance of sentiment analysis.",
        "The method is theoretically efficient, indicating that it can be applied to large-scale datasets without compromising performance."
    ],
    "4348": [
        "Using context-aware ZSL significantly improves predictions.",
        "Removing the dependence on the detection of object boxes makes the approach fully applicable to real-world images.",
        "Designing grounded word embeddings that include more visual context information would benefit such models.",
        "Using contextual information can sometimes degrade predictions.",
        "The model M(V) retrieves the correct label, given only the region of interest.",
        "Integrating contextual information in the final model M(C S L \u222aS H \u222aT L , V) leads to worse performances over M(V)."
    ],
    "4349": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "UDA combines well with representation learning, e.g., BERT, for text data.",
        "UDA outperforms prior works by a clear margin and nearly matches the performance of fully supervised models trained on the full labeled sets for vision data.",
        "Data augmentation and semi-supervised learning are well connected, and better data augmentation can lead to significantly better semi-supervised learning.",
        "The proposed method, UDA, employs state-of-the-art data augmentation found in supervised learning to generate diverse and realistic noise and enforces the model to be consistent with respect to these noise."
    ],
    "4350": [
        "Human annotations for conversations show significant variance.",
        "It is still possible to train models which can extract meaningful signal from the human assessment of the conversations.",
        "We show that these models can provide useful turn-level guidance to response generation models.",
        "Our feedback is interpretable on 2 major axes of conversational quality: engagement and coherence.",
        "We also plan to provide similar evaluators to the university teams participating in the Alexa Prize competition.",
        "Combining both techniques results in the best performance.",
        "We view this work as complementary to other recent work in improving dialog systems such as Li et al. (2015) and Shao et al. (2017).",
        "While such open-domain systems are still in their infancy, we view the framework presented in this paper to be an important step towards building end-to-end coherent and engaging chatbots."
    ],
    "4353": [
        "Our approach achieves competitive performance to standard methods in both online and offline settings.",
        "We present a new approach to similarity measurement that requires a set of clear choices - model, likelihood, and information criterion.",
        "Our method is suitable for a variety of modeling scenarios due to the freedom in specifying the generative process.",
        "The graphical model we employ is adaptable to encode structural dependencies beyond the i.i.d. data-generating process.",
        "Relaxing the assumption of unimodality is an interesting area for future research.",
        "Word embedding magnitude carries information relevant for sentence level tasks, which agrees with prior intuition built from (Schakel & Wilson, 2015)."
    ],
    "4354": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The Transformer can be effective for ASR with the key being to set up very deep stochastic models.",
        "State-of-the-art results among end-to-end models on two standard benchmarks are achieved, and our networks are among the deepest configurations for ASR.",
        "Future works will involve developing the framework under more realistic and challenging conditions such as real-time recognition, in which latency and streaming are crucial."
    ],
    "4358": [
        "We present SuperGLUE, a new benchmark for evaluating general-purpose language understanding systems.",
        "SuperGLUE updates the GLUE benchmark by identifying a new set of challenging NLU tasks, as measured by the difference between human and machine baselines.",
        "The set of eight tasks in our benchmark emphasizes diverse task formats and low-data training data tasks, with nearly half the tasks having fewer than 1k examples and all but one of the tasks having fewer than 10k examples.",
        "We evaluate BERT-based baselines and find that they still lag behind humans by nearly 20 points.",
        "Given the difficulty of SuperGLUE for BERT, we expect that further progress in multi-task, transfer, and unsupervised/self-supervised learning techniques will be necessary to approach human-level performance on the benchmark.",
        "Overall, we argue that SuperGLUE offers a rich and challenging testbed for work developing new general-purpose machine learning methods for language understanding."
    ],
    "4362": [
        "The proposed model effectively exploits both global and local patterns, allowing locales to selectively share knowledge with each other.",
        "The universal model's classification performance is notable on immature locales/domains with insufficient data and locales-specific domains.",
        "The model architecture is limited to supporting multiple locales using the same language only, but there is potential for expanding it to multi-lingual scenarios in future work.",
        "There are challenges in capturing and sharing knowledge of common patterns of utterances belonging to the same domain but written in different languages across different locales.",
        "There is a need to prevent a locale from interfering with other locales using different language for learning the linguistic context of utterances."
    ],
    "4366": [
        "The proposed method for contextual inflection using a hybrid architecture shows consistent improvements over state-of-the-art results across several diverse languages.",
        "The inclusion of morphological features prediction is an important element in the system, and the contextual inflection can be a highly challenging task.",
        "There are two types of morphological categories in the system: contextual and inherent, with the former relying on agreement and the latter coming from a speaker's intention."
    ],
    "4367": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets."
    ],
    "4368": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our approach separates the task into two independent steps: verb clustering and role labelling, using combination of these embeddings enhanced with syntactical features.",
        "Our approach showed the best performance in Subtask B.1 and also finished as the runner-up in Subtask A of this shared task.",
        "Our approach can be easily extended to process other datasets and languages."
    ],
    "4370": [
        "The proposed augmentation strategy helped achieve higher scores in the Gendered Pronoun Resolution challenge without fine-tuning.",
        "The technique of anonymizing idiosyncrasy in individual names and handling gender and other biases contributed to the improved performance.",
        "The system could be altered slightly to achieve a better score or become more gender-unbiased by using different ensemble weights or BERT embedding layers.",
        "The feature-based approach used in the solution could also work well with fine-tune BERT approaches, potentially further improving the score."
    ],
    "4374": [
        "There is no overlapped relations between the training text corpus and the KB.",
        "Our method improves the results over all baseline models without harming the scalability.",
        "We believe this framework is as flexible as other constraint models to be applied to many applications when we think the semantics of entities and relations provided by the KB is useful."
    ],
    "4375": [
        "The BERT model is adaptable to different tasks, as demonstrated by our successful results.",
        "With a relatively small training set of articles, we were able to train models with high accuracy on both the validation set and the test set.",
        "Our models classified different parts of a given article identically, demonstrating that the overall hyperpartisan aspects were similar across an article.",
        "The model had significantly lower accuracy when word pieces were shuffled around, but that accuracy was almost entirely restored when shuffling around chunks of four or more word pieces.",
        "In future work, we would like to make use of the entire article and explore cheaper computations on other chunks.",
        "Our system is named after Clint Buchanan4, a fictional journalist on the soap opera One Life to Live."
    ],
    "4377": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We reported all the steps for creating this dataset and a user study to evaluate the quality of the dataset.",
        "Our models, DOTBILSTM and SOFTSVM, outperform the state-of-the-art model on AskUbuntu dataset.",
        "The dataset and models are available online."
    ],
    "4379": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Poly-encoders are more accurate than Bi-encoders, while being far faster than Cross-encoders.",
        "Pre-training strategies more closely related to the downstream task bring strong improvements.",
        "The methods introduced in this work are not specific to dialogue, and can be used for any task where one is scoring a set of candidates."
    ],
    "4382": [
        "The authors have developed a grammar for a Minecraft assistant.",
        "The authors have created a dataset of natural language utterances with associated logical forms from this grammar.",
        "The authors have trained several neural models for parsing natural language instructions using this dataset.",
        "The models trained by the authors were able to fit the templated data nearly perfectly and the rephrased data with some accuracy, but struggled to adapt to human-generated data.",
        "The use of a small number of annotated (grammar-free) human data with the infinite generations of the grammar is an exciting area of research.",
        "The authors support several actions in their dataset, including Build, Copy, Noop, Spawn, Resume, Fill, Destroy, Move, Undo, Stop, Dig, Tag, FreeBuild and Answer.",
        "The authors have presented the detailed action tree for each action in the following subsections.",
        "Figure 4 shows an example for a BUILD action."
    ],
    "4384": [
        "\"Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "\"We explore the use of a projection-based method for attenuating biases. Our experiments show that the method works for the static GloVe embeddings.",
        "\"We propose AdaBERT, an effective and efficient model that adaptively compresses BERT for various downstream tasks.",
        "\"Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "\"MASS achieved significant improvements over the baseline without pre-training or with other pre-training methods.",
        "\"MASS achieved the state-of-the-art BLEU scores for unsupervised NMT on three language pairs, outperforming the previous state-of-the-art by more than 4 BLEU points on English-French."
    ],
    "4386": [
        "Our study shows the effectiveness of multiple asynchronous microphones for meeting transcription in real-world scenarios.",
        "Both front-end and back-end algorithms improve word error, speaker-attributed word error, and diarization error metrics.",
        "System combination was generalized such that it benefits both word and speaker hypotheses.",
        "On non-overlapped speech, the error rate is only 3.0% absolute worse than with close-talking microphones.",
        "A major remaining challenge is recognition of overlapped speech."
    ],
    "4388": [
        "Our proposed method, SAWR, can bring significantly better performances for both Chinese-English and English-Vietnamese translation tasks.",
        "Our method does not require discrete syntax trees as inputs, instead encoding dependency syntax implicitly.",
        "We compared our method with two approaches based on Tree-RNN and Tree-Linearization, finding that our method is more effective and meanwhile very efficient.",
        "Our proposed method is more effective and efficient than previous approaches for syntax integration in NMT.",
        "Experiments showed that the method can achieve better performances for both Chinese-English and English-Vietnamese translation tasks."
    ],
    "4389": [
        "The authors explore models of natural language grounded in the shape of common objects, using a corpus of highly descriptive referring expressions for shapes in context.",
        "The best variants of the models exhibit strong performance, drawing on both 2D and 3D object representations and appearing to reflect human-like part decomposition.",
        "The learned models are surprisingly robust, transferring to real images and to new classes of objects.",
        "Future work will be required to understand the transfer abilities of these models and how this depends on the compositional structure they have learned."
    ],
    "4394": [
        "Evaluation is a critical task when developing and researching dialogue systems.",
        "Many methods and concepts have been proposed for evaluating dialogue systems over the past decades.",
        "The current trend is moving towards building end-to-end trainable dialogue systems based on large amounts of data.",
        "These systems have different requirements for evaluation than a finite state, machine-based system.",
        "The problem of evaluation is evolving in tandem to the progress of the dialogue system technology itself."
    ],
    "4395": [
        "The use of deep Transformer language models for speech recognition leads to better performance compared to shallow stacks of LSTM-RNNs.",
        "The application of crucial components of deep Transformers, such as layer normalization, to deeper LSTM models can improve their performance.",
        "The use of regularization on models for the LibriSpeech task does not lead to overfitting, and therefore, larger models may be beneficial.",
        "Scaling up the size of the model and using regularization may still improve the models' performance."
    ],
    "4405": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The results for DVQA are more nuanced due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results suggest that the community is ready for more difficult CQA datasets.",
        "Charts in the wild contain variations that are not captured by current datasets.",
        "Human-generated questions should be included in future CQA datasets.",
        "Document-level CQA requires document question answering, page segmentation, and more.",
        "The proposed PReFIL system has the potential to improve retrieval of information from charts."
    ],
    "4406": [
        "Our approach significantly improves typical log-linear classifiers for neural language modeling tasks, regardless of the input encoding architecture.",
        "Deeper representations of the output structure lead to better transfer across the output labels, especially the low-resource ones.",
        "The proposed output layer parameterization can match or improve state-of-the-art context encoding architectures and outperform previous output layer parameterizations based on a joint input-output space.",
        "Our findings should apply to other conditional neural language modeling tasks, such as image captioning and summarization.",
        "Learning a deep residual output label encoding can improve the transfer of the output labels, especially the low-resource ones.",
        "The proposed output layer parameterization preserves the basic principles and generality of previous output layer parameterizations based on a joint input-output space.",
        "Investigating the use of more elaborate descriptions or contextualized representations of the output labels could lead to further improvements in different tasks."
    ],
    "4407": [
        "Word segmentation is not necessary for deep learning of Chinese representations.",
        "Char-based models consistently outperform word-based models in NLP tasks.",
        "The sparseness of word distributions leads to more out-of-vocabulary words, overfitting, and lack of domain generalization ability in word-based models.",
        "Word-based models have inferior performance due to the long-existing task of CWS."
    ],
    "4408": [
        "The proposed approach achieves new state-of-the-art results on three benchmark datasets for entity-relation extraction.",
        "The approach uses a multi-turn question answering paradigm to improve the performance of entity-relation extraction.",
        "The proposed model outperforms benchmark models across different datasets, demonstrating its effectiveness in entity-relation extraction.",
        "The approach requires hierarchical relation reasoning, which is not present in previous works.",
        "The proposed model achieves the best performance on a new entity-relation extraction dataset that requires hierarchical relation reasoning."
    ],
    "4409": [
        "The proposed approach improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages. (Claiming improvement in performance)",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\"). (Claiming the effectiveness of a specific approach)",
        "The joint model improves the tagging accuracy itself. (Claiming the benefit of the joint model)",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality. (Claiming the importance of interactions between tagging and parsing)",
        "Our investigation reveals specific characteristics that can make CONCEPTNET relation classification difficult, such as heterogeneous arguments and inner-relation diversity. (Claiming the challenges posed by CONCEPTNET)",
        "The proposed model is able to tackle specific properties of CONCEPTNET, such as relation ambiguity and heterogeneity of relation arguments. (Claiming the ability of the proposed model to handle specific properties of CONCEPTNET)",
        "Future work on using CONCEPTNET should focus on addressing relation ambiguity and heterogeneity of relation arguments. (Claiming a future direction for research)"
    ],
    "4411": [
        "The proposed cross-lingual embedding alignment procedure based on a probabilistic latent variable model increases performance across various tasks compared to previous methods.",
        "The resulting embeddings in the aligned space preserve their quality, as shown by results on tasks that assess word and sentence-level monolingual similarity correlation with human scores.",
        "The resulting embeddings significantly increase the precision of sentence retrieval in multilingual settings.",
        "The preliminary results on aligning more than two languages at the same time provide an exciting path for future research."
    ],
    "4413": [
        "The top layer of BERT is more useful for text classification.",
        "With an appropriate layer-wise decreasing learning rate, BERT can overcome the catastrophic forgetting problem.",
        "Within-task and in-domain further pre-training can significantly boost its performance.",
        "A preceding multi-task fine-tuning is also helpful to the single-task fine-tuning, but its benefit is smaller than further pre-training.",
        "BERT can improve the task with small-size data."
    ],
    "4417": [
        "Using pre-trained models, we can successfully craft small datasets to perform fine-grained analysis on machine learning models.",
        "Our new dataset is able to isolate a few competence issues regarding structural inference and allows us to bring to the surface some interesting comparisons between recurrent neural networks and pre-trained Transform-based models.",
        "BERT presents a considerable advantage in learning structural inference compared to recurrent models, and this result appears even when fine-tuning one version of the model that was not pre-trained on the target language.",
        "By the stratified nature of our dataset, we can pinpoint BERT's inference difficulties and identify space for improving the model's counting understanding.",
        "One possible area for future research is to check if the same results can be attainable using simple structural inferences that occur within complex sentences."
    ],
    "4419": [
        "The proposed deep multitask framework leverages the interdependence of two related tasks (multi-modal sentiment and emotion analysis) to learn a joint representation for both tasks, resulting in improved performance.",
        "Experimental results suggest that sentiment and emotion assist each other when learned in a multitask framework, as evidenced by the higher performance achieved by the proposed approach compared to various existing systems.",
        "The proposed approach achieves higher performance for all cases, indicating the effectiveness of the joint representation learning approach.",
        "There is potential for further exploration of other dimensions to the multitask framework, such as sentiment classification and intensity prediction, emotion classification and intensity prediction, and all four tasks together."
    ],
    "4425": [
        "The proposed framework generates natural language-based descriptions of the decisions made by deep learning models for time-series analysis, improving the intelligibility and reliability of the provided explanations.",
        "The influence tracer identifies the most salient regions of the input, and statistical features from the sequence are simultaneously extracted from the statistical feature extractor module.",
        "The system uses statistical features due to their strong theoretical foundations and transparency, which significantly improves the intelligibility and reliability of the provided explanations.",
        "A confidence estimate is provided by the system using the sanity check module, based on assessment whether the estimated influential point is indeed causal for the prediction.",
        "The generated explanations are directly intelligible for both expert and novice users alike, avoiding the need for domain expertise to understand the encapsulated information.",
        "The proposed framework demonstrates improved performance on synthetic and real anomaly detection datasets, and the explanations can help users increase their confidence in the performance of the deep model."
    ],
    "4426": [
        "We take a close look at the selection bias of NLSM datasets and focus on the selection bias embodied in the comparing relationships of sentences.",
        "To mitigate the bias, we propose an easy-adopting method for leakage-neutral learning and evaluations.",
        "However, there is still much to do to form a clearer scope of this problem.",
        "We suggest for future NLSM datasets, the providers should pay more attention to this problem.",
        "Furthermore, they could reveal the more detailed strategy of sample selection, and might publish some official weights to eliminate the bias.",
        "A Detailed Settings for the Experiments in Section 2.1"
    ],
    "4428": [
        "The proposed aspect extraction model with two kinds of control modules outperforms state-of-the-art methods.",
        "Asynchronous update improves the performance of the model.",
        "The locally-normalized structure of the model can largely explain the beam problem.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The solution to the brevity problem is a very limited form of globally-normalized models for NMT."
    ],
    "4431": [
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our joint sourcetarget model with local attention achieve state of the art results on standard WMT benchmarks, and significantly improves the best published result on the IWSLT'14 de-en benchmark."
    ],
    "4435": [
        "The proposed sentence scoring method using a biLM for rescoring in ASR outperforms the conventional uniLM for rescoring the N-best list.",
        "The biLM is more robust than the uniLM, especially when a recognized sentence is short or the earlier part of the sentence is misrecognized.",
        "The proposed method captures the interactions between the past and the future words in a sentence.",
        "The use of a biLM for rescoring in ASR leads to significant and consistent improvements in performance."
    ],
    "4436": [
        "Our proposed method for text-to-speech and automatic speech recognition leverages only a few paired speech and text data and unpaired data, achieving high intelligibility rates and low error rates.",
        "The method consists of several key components, including denoising autoencoder, dual transformation, bidirectional sequence modeling, and a unified model structure to incorporate the above components.",
        "The further analyses verify the importance of each component of our method in achieving high intelligibility rates and low error rates.",
        "Future work will push toward the limit of unsupervised learning by purely leveraging unpaired speech and text data, with the help of other pre-training methods.",
        "The use of an advanced vocoder model, such as WaveNet, can enhance the quality of the generated audio."
    ],
    "4438": [
        "The proposed QA model achieves consistent improvements over previous methods on the We-bQSP benchmark with incomplete KBs.",
        "The graph attention technique efficiently accumulates question-related knowledge for each KB entity in one-pass of the KB sub-graph.",
        "The designed gating mechanisms successfully incorporate the encoded entity knowledge while processing the text documents.",
        "The proposed idea can be extended to other QA tasks with evidence of multimodality, such as combining with symbolic approaches for visual QA."
    ],
    "4439": [
        "The proposed model, ERNIE, incorporates knowledge information into language representation models, leading to better abilities of denoising distantly supervised data and fine-tuning on limited data compared to BERT.",
        "The experimental results demonstrate the effectiveness of ERNIE in improving language understanding.",
        "There are three important directions for future research: injecting knowledge into feature-based pre-training models, introducing diverse structured knowledge into language representation models, and annotating more real-world corpora heuristically for building larger pre-training data.",
        "The proposed knowledgeable aggregator and pre-training task dEA contribute to better fusion of heterogeneous information from both text and KGs.",
        "The use of pre-training tasks dEA and knowledgeable aggregator leads to more effective language understanding."
    ],
    "4442": [
        "We propose a new GNN-based method for multihop RC across multiple documents.",
        "Our end-to-end trained single neural model delivers competitive results while our ensemble model achieves the state-of-the-art performance.",
        "We introduce the HDE graph, a heterogeneous graph for multiple-hop reasoning over nodes representing different granularity levels of information.",
        "We use co-attention and self-attention to encode candidates, documents, entities of mentions of candidates and query subjects into query-aware representations, which are then employed to initialize graph node representations.",
        "In the future, we would like to investigate explainable GNN for this task, such as explicit reasoning path in (Kundu et al., 2018) , and work on other data sets such as HotpotQA."
    ],
    "4443": [
        "The proposed neural architecture achieves statistically significant improvements over the state-of-the-art in end-to-end relation extraction tasks, across two datasets from different domains.",
        "The method simultaneously recognizes entity boundaries, the type of each entity, and the relationships among them, by learning intermediate table representations through repeated application of 2D convolutions.",
        "The approach operates at substantially reduced training and testing times, with testing times that are seven to ten times faster than previous state-of-the-art methods.",
        "The proposed architecture can be visually analyzed by observing the hidden pooling activity leading to preliminary or intermediate decisions.",
        "The current implementation is designed for extracting relations involving two entities and occurring within sentence bounds, but the architecture can be extended to handle n-ary relations and explore document-level extraction involving cross-sentence relations in future work."
    ],
    "4444": [
        "Our ensemble-based system benefited mostly from improved recall, aligning with our initial expectation.",
        "Including NLM-180 as training data resulted in significant performance gains during 11-fold cross validation, but the same improvements were not as dramatic on either test sets.",
        "There may be a semantic or annotation drift between the datasets due to evolution of annotation guidelines over time and annotators becoming more experienced.",
        "Having few but higher quality examples may be more advantageous than having many but lower quality examples for this particular task.",
        "The top performing system exhibits superior performance on Test Set 1 compared to Test Set 2, which may indicate that our system struggles with data that is more \"sparse\".",
        "We will experiment with Graph Convolution Networks over dependency trees as a \"drop-in\" replace for Bi-LSTMs to assess its suitability for this task in future efforts."
    ],
    "4445": [
        "The three-stage training framework TransBERT can transfer not only general language knowledge but also specific kinds of knowledge from various semantically associated supervised tasks for a target task, such as SCT.",
        "The three-stage training framework TransBERT can enable a better and task-specific initialization for different target tasks, which is superior to the widely used two-stage pre-training and fine-tuning framework.",
        "The MNLI-enhanced BERT model pushes the SCT v1.0 task to 91.8% accuracy, which is much closer to human performance.",
        "The MNLI-enhanced BERT model gets the SOTA performance of 90.3% on SCT v1.5."
    ],
    "4446": [
        "Our proposed model significantly outperforms existing state-of-the-art models for hashtag generation.",
        "The proposed neural seq2seq model with bi-attention over a dual encoder can effectively capture indicative representations from target posts and conversation contexts.",
        "Our model can generate rare and even unseen hashtags."
    ],
    "4447": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Developing annotated datasets from multiple sources based on existing datasets and the causality tagging scheme can improve the performance of SCITE.",
        "Combining SCITE with distant supervision and reinforcement learning can achieve better performance without requiring a large amount of high-quality annotated data.",
        "The proposed novel constituent hierarchy predictor based on recurrent neural networks can capture global sentential information and outperform state-of-the-art baseline parsers.",
        "The fully-supervised parser achieved higher F1 scores than the state-of-the-art baseline parser on standard WSJ and CTB evaluations."
    ],
    "4457": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We propose Target Conditioned Sampling (TCS), an efficient data selection framework for multilingual data by constructing a data sampling distribution that facilitates the NMT training of LRLs.",
        "TCS brings up to 2 BLEU improvements over strong baselines with only slight increase in training time."
    ],
    "4463": [
        "The QFE model, based on a summarization model, achieves state-of-the-art performance in evidence extraction tasks.",
        "The usage of the summarization model improves the dependency among the evidence and the coverage of the question.",
        "The architecture with QFE achieves better performance compared to competing models in RTE tasks.",
        "The adaptive termination contributes to the exact matching and precision score of the evidence extraction.",
        "The difficulty of questions for QFE depends on the number of required evidence sentences.",
        "This study is the first to base its experimental discussion on HotpotQA and show a joint approach for RC and FEVER."
    ],
    "4464": [
        "The out-of-the-box fairseq CNN architecture reaches dramatically better performance on the SCAN compositional generalization tasks compared to RNNs previously tested in the literature.",
        "The CNN is not learning rule-like compositional generalizations, as its mistakes are nonsystematic and evenly spread across different commands.",
        "The CNN achieved a considerable degree of generalization, even on an explicitly compositional benchmark, without something akin to rule-based reasoning.",
        "Fully understanding generalization of deep seq2seq models might require a less clear-cut view of the divide between statistical pattern matching and symbolic composition.",
        "The best LSTM architecture of Lake and Baroni has two 200-dimensional layers, and it is consequently more parsimonious than our best CNN (1/4 of parameters).",
        "In informal experiments, shallow CNNs were incapable to handle even the simplest random split, while it is hard to train very deep LSTMs, and it is not clear that the latter models need the same depth as CNNs require to \"view\" long sequences."
    ],
    "4467": [
        "Our method can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Our method uses a KL-divergence criterion to approximate onto a more compact k-gram topology.",
        "Our method provably finds the best KL-minimized solution for a given topology, whereas previous methods are optimal only when the size of the k-gram model tends to infinity.",
        "Our approach performs better compared to that of Deoras et al. [22] .",
        "Our method is applicable to k-gram models specifically, not the larger class of target WFA to which our methods apply."
    ],
    "4468": [
        "Our proposed attention-based model achieves the best performance on two AMR corpora.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "We are interested in extending our model to other semantic parsing tasks (Oepen et al., 2014; Abend and Rappoport, 2013) .",
        "We are also interested in semantic parsing in cross-lingual settings (Zhang et al., 2018; Damonte and Cohen, 2018) ."
    ],
    "4469": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.'",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "TRADE shares all of its parameters across multiple domains and achieves state-of-the-art joint goal accuracy and slot accuracy on the MultiWOZ dataset for five different domains.",
        "Domain sharing enables TRADE to perform zero-shot DST for unseen domains and to quickly adapt to few-shot domains without forgetting the learned ones."
    ],
    "4472": [
        "The proposed acoustic-to-word model can utilize conversational context to better process long conversations.",
        "The model is trained with conversational context information in an end-to-end framework, which improves its performance compared to previous end-to-end speech recognition models.",
        "The incorporation of preceding conversational context representations leads to improved performance.",
        "The model outperforms previous end-to-end speech recognition models trained on isolated utterances."
    ],
    "4473": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Using a single pre-trained Transformer LM for sequence-to-sequence tasks simplifies the model, reduces the number of parameters, and removes the non-pre-trained encoder-decoder attention weights.",
        "Experiments fine-tuning the model on only 1% training data have shown that our approach achieves impressive sample efficiency gains.",
        "It would be interesting to further test whether this approach leads to similar sample efficiency gains on tasks beyond summarization, such as dialogue."
    ],
    "4474": [
        "We present a deep learning based fine-grained POS tagger for German Twitter data using both domain adaptation and regularization techniques.",
        "On top of an efficient POS tagger, we implemented domain adaptation by using a L2-norm regularization mechanism, which improved the model's performance by 5 percentage points.",
        "Since this performance is significant, we conclude that finetuning and domain adaptation techniques can successfully be used to improve the performance when training on a small target-domain corpus.",
        "Our experiments show that the combination of different regularization techniques is recommendable and can further optimize already efficient systems.",
        "The advantage of our approach is that we do not need a large annotated target-domain corpus, but only pretrained weights.",
        "Using a pretrained model as a prior for training on a small amount of data is done within minutes and therefore very practicable in real world scenarios."
    ],
    "4482": [
        "The proposed method of learning query-aware document traversal is effective and achieves high EM scores on the WikiHop dataset.",
        "The proposed approach reduces the size of the input to the RC model, increasing both training speed and accuracy.",
        "The use of a multi-hop setting in the proposed method improves the performance of the system.",
        "The proposed method is able to significantly reduce the size of the input to the RC model while maintaining high accuracy, which is a key contribution of the paper."
    ],
    "4484": [
        "Adapting domain for BERT is not effective for binary classification tasks.",
        "Domain adaptation can perform just as well or better than the original model for other tasks.",
        "ELMo and ULMFiT can perform well even when trained on a small subset of the language model.",
        "Fine-tuning both ELMo and BCN layers on a target task can improve performance."
    ],
    "4488": [
        "Existing language GANs use maximum likelihood pretraining to minimize adversarial training challenges, but this approach has shown little to no performance improvements over traditional language models.",
        "The authors have removed the need for maximum likelihood pre-training in language GANs using large batch sizes, dense rewards, and discriminator regularization.",
        "The authors are the first to use Generative Adversarial Networks to train word-level language models successfully from scratch, without the need for maximum likelihood pretraining.",
        "Removing the need for maximum likelihood pretraining in language GANs opens up a new avenue of language modeling research, with future work exploring GANs with one-shot feedforward generators and specialized discriminators that distinguish different features of language.",
        "The authors have measured the quality and diversity of ScratchGAN samples using BLEU metrics, Fr\u00e8chet distance, and language model scores, but these metrics are not sufficient to evaluate language generation alone."
    ],
    "4489": [
        "We studied the effects of grammatical errors in NMT.",
        "We expanded on findings from previous work, performing analysis on real data with grammatical errors using a SOTA system.",
        "We were able to identify classes of grammatical errors that are recoverable or irrecoverable.",
        "We presented ways to evaluate a MT system's robustness to noise without access to gold references.",
        "We discussed the limitations of our study and outlined avenues for further investigations towards building more robust NMT systems."
    ],
    "4490": [
        "obtaining such persona description requires human effort\" - This claim highlights the challenge of obtaining persona descriptions for personalizing dialogue agents, and the need for a more efficient approach.",
        "a dialogue agent trained with meta-learning achieves a more consistent dialogue\" - This claim suggests that the proposed meta-learning setting can improve the consistency of dialogues generated by the agent.",
        "we plan to apply meta-learning to comment generation and task-oriented dialogue systems\" - This claim indicates the potential for applying the proposed approach to other types of dialogue systems, such as comment generation and task-oriented dialogue systems.",
        "a more consistent dialogue by both automatic measures and human evaluation\" - This claim highlights the effectiveness of the proposed approach in improving the consistency of dialogues, as evaluated both by automatic measures and human evaluation."
    ],
    "4502": [
        "MHA does not always leverage its theoretically superior expressiveness over vanilla attention to the fullest extent.",
        "Several heads can be removed from trained transformer models without statistically significant degradation in test performance.",
        "Some layers can be reduced to only one head.",
        "The encoder-decoder attention layers are much more reliant on multi-headedness than the self-attention layers.",
        "The relative importance of each head is determined in the early stages of training.",
        "We hope that these observations will advance our understanding of MHA and inspire models that invest their parameters and attention more efficiently."
    ],
    "4503": [
        "The proposed model overrides the expressiveness restrictions of most embedding models for KG embedding approaches.",
        "The model allows for the modeling of all three relation patterns, which is not possible with most existing KG embedding approaches.",
        "Using independent vectors and multiple views to translation embeddings can improve the performance of the model in the link prediction task.",
        "The experimental results confirm the competitive performances of MDE in MR and Hit@10 on the benchmark datasets.",
        "MDE outperforms all current state-of-the-art models for the benchmark of composition relation patterns."
    ],
    "4504": [
        "The proposed answer selection method, hashing based answer selection (HAS), can significantly reduce memory cost for storing the matrix outputs of encoders in answer selection.",
        "HAS is fast and has low memory cost when deployed for prediction, making it particularly meaningful for deployment at embedded or mobile systems.",
        "Experimental results on three popular datasets show that HAS outperforms existing methods to achieve state-of-the-art performance.",
        "HAS is flexible to integrate other encoders and question-answer interaction mechanisms, and the authors plan to pursue this in their future work."
    ],
    "4506": [
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "While one might expect a trade-off between speed and correlation with human judgments, SEMBLEU appears to outperform SMATCH in both dimensions.",
        "The improvement in correlation with human judgments comes from the fact that SEM-BLEU considers larger fragments of the input graphs.",
        "The improvement in speed comes from avoiding the search over mappings between the two graphs.",
        "SEMBLEU can be potentially used to compare other types of graphs, including cyclic graphs."
    ],
    "4508": [
        "Our approach, Attentive Path Ranking (APR), significantly improves over state-of-the-art path-ranking based methods and knowledge graph embedding methods on two benchmark datasets WN18RR and FB15k-237.",
        "APR leverages type hierarchies of entities to discover new path patterns from data, which provides insights into how APR achieves a balance between generalization and discrimination.",
        "Our attention-based RNN model is effective in discovering the new path patterns from data.",
        "The discovered path patterns provide insights into how APR achieves a balance between generalization and discrimination."
    ],
    "4509": [
        "The proposed Hy-perIM can explicitly learn the word-label similarities by embedding the words and labels jointly and preserving the label hierarchy simultaneously.",
        "HyperIM acquires label-aware document representations to extract fine-grained text content along each label, which significantly improves the hierarchical multi-label text classification performance.",
        "The hierarchical parent-child relations between labels can be well modeled in the hyperbolic space.",
        "The proposed Hy-perIM can improve the performance of hierarchical multi-label text classification.",
        "There is usually no such hierarchically organized labels in practice, especially for extreme multi-label classification (XMLC).",
        "The labels in XMLC usually follow a power-law distribution due to the amount of tail labels.",
        "It will be interesting to extend HyperIM for XMLC in the future."
    ],
    "4511": [
        "Proposed curriculum learning based Pointergenerator networks for reading long narratives achieve state-of-the-art performance on the challenging Narra-tiveQA benchmark.",
        "Sub-sampling diverse views of a story and training them with a curriculum scheme is potentially more effective than techniques designed for open-domain question answering.",
        "The proposed IAL-CPG model achieves state-of-the-art performance on the Narra-tiveQA benchmark.",
        "Extensive ablation studies and qualitative analysis show that the task at hand is challenging, and sub-sampling diverse views of a story and training them with a curriculum scheme is effective.",
        "The proposed curriculum learning based Pointergenerator networks for reading long narratives are potentially more effective than techniques designed for open-domain question answering."
    ],
    "4512": [
        "a structured representation of the complete thread as the context is better than a bag-of-words, featurerich representation",
        "attention-based models to infer and select a context -defined as a contiguous subsequence of student posts -to improve over a model that always takes the complete thread as a context to prediction intervention",
        "Our Any Post Attention (APA) model enables instructors to tune the model to predict intervention early or late",
        "we posit our APA model will enable MOOC instructors employing varying pedagogical styles to use the model equally well",
        "the recall of the predictive models for longer threads (that is, threads of length greater 2) can still be improved",
        "an ensemble model or a multi-objective loss function is thus planned in our future work to better prediction on such longer threads"
    ],
    "4515": [
        "The family of dependency parsers construct a dependency tree by generating a sequence of edge sets.",
        "The learning method does not presuppose a generation order.",
        "A 'coaching' method, which weights actions in the loss according to the model, improves parsing accuracy compared to a uniform weighting.",
        "The model's sequential aspect, along with the coaching method and training on a state distribution which resembles the model's own behavior, yielded improvements in unlabeled dependency parsing over strong one-step baselines."
    ],
    "4516": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "QuesNet can effectively gain an understanding of test questions, with impressive performance among all three typical educational applications.",
        "There are still some directions for future studies, such as working on domain-specific model architectures to model logic among questions in a more fine-grained way, and exploring the possibilities of our work on other heterogeneous data and tasks."
    ],
    "4517": [
        "We explored action prediction from written stories.",
        "Our approach outperformed standard NLP approaches, including logistic regression and LSTMs.",
        "Vanilla models achieved a higher performance for actions that occurred a few times in the training set.",
        "There were difficulties to discriminate among semantically related actions.",
        "The challenge here proposed corresponded to a fictional domain.",
        "A future line of work we are interested in is to test whether the knowledge learned with this dataset could be transferred to real-word actions (i.e. real-domain setups), or if such transfer is not possible and a model needs to be trained from scratch."
    ],
    "4521": [
        "As character and word-piece inputs become commonplace in modern NLP pipelines, it is worth highlighting the vulnerability they add.",
        "We show that minimally-doctored attacks can bring down accuracy of classifiers to random guessing.",
        "We recommend word recognition as a safeguard against this and build upon RNN-based semi-character word recognizers.",
        "We discover that when used as a defense mechanism, the most accurate word recognition models are not always the most robust against adversarial attacks.",
        "Additionally, we highlight the need to control the sensitivity of these models to achieve high robustness."
    ],
    "4527": [
        "The proposed DSReg model utilizes distant supervision as a regularizer to improve the performance of NLP tasks.",
        "The DSReg model transforms the original task into a multi-task learning problem, which leads to significant performance boost in text classification, sequence labeling, and machine reading comprehension tasks.",
        "The use of distant supervision as a regularizer can retrieve hard-negative examples that are useful for improving the model's performance.",
        "The proposed strategy of utilizing distant supervision as a regularizer leads to improved performance in NLP tasks.",
        "The DSReg model achieves state-of-the-art results on multiple NLP tasks, including text classification, sequence labeling, and machine reading comprehension."
    ],
    "4528": [
        "The authors present two systems made out of the best performing taggers for adverse drug reactions and related factors.",
        "The systems achieved F1-scores of 76% and 75.58% on the testing data, indicating room for improvement.",
        "Additional informative features of the text could help improve the CRF machine learning taggers.",
        "More representative word embeddings could be helpful for the BLSTM-based taggers.",
        "Alternative ensemble methods, such as utilizing an additional meta-classifier to combine the CRF and BLSTM results, may improve performance.",
        "The BLSTM model's performance depends on the word embeddings used, and alternative word representation models could be explored.",
        "There is still a challenge of labelling classes with a low number of examples, which can make it difficult to create good performing machine learning models.",
        "Rule-based approaches can be further improved with additional samples and looking at additional data.",
        "Machine learning performance can be improved by using additional annotated data and external data sets."
    ],
    "4531": [
        "The proposed method achieves controlled text generation without supervision.",
        "The constrained posterior can never match the isotropic Gaussian prior, which may lead to a potential future direction to resolve this mismatch.",
        "Introducing a Lagrange multiplier \u03bb helps to define the Lagrange function as L(p 1 , p 2 , . . . , p K , \u03bb) = K i=1 p 2 i -\u03bb( K i=1 p i -1).",
        "The optimal point is found by setting \u2202 \u2202p i K i=1 p 2 i -\u03bb( K i=1 p i -1) = 2p i -\u03bb = 0, i = 1, 2, . . . , K, which shows that all p i are equal.",
        "By using the constraint i p i = 1, we find p i = 1 K , i = 1, 2, . . . , K."
    ],
    "4535": [
        "The CLS metric and R4R provide a better toolkit for measuring the impact of better language understanding in VLN.",
        "Our findings suggest ways that future datasets and metrics for judging agents should be constructed and set up for evaluation.",
        "The R4R data still has considerable headroom for improvement, with our reimplementation of the RCM model achieving only 34.6 CLS on paths in R4R's Validation Unseen houses.",
        "Future agents will need to make effective use of language and its connection to the environment to both drive CLS up and bring NE down in R4R.",
        "Path fidelity is crucial for many VLN-based problems, such as games where the instructions being given take the agent around a trap or help it avoid opponents.",
        "Future extensions of VLN will likely involve games where the instructions being given take the agent around a trap or help it avoid opponents.",
        "In search-and-rescue human-robot teams, going straight to the goal could be literally deadly to the robot or agent."
    ],
    "4539": [
        "The proposed method achieves better performance compared to previous systems by using less than 1% of the training data.",
        "The method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The learned ranking models recommend much better transfer languages than those suggested by considering only single language or dataset features.",
        "The analysis of the learned ranking models provides insights on the types of features that are most influential in selecting transfer languages for each of the NLP tasks.",
        "The method can be used to inform future ad hoc selection even without using the proposed method."
    ],
    "4540": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The use of a projection-based method for attenuating biases can effectively reduce bias in contextualized embeddings without loss of entailment accuracy.",
        "Monolingual models can outperform bilingual ones for paraphrase identification and data-augmentation through paraphrasing.",
        "Access to parallel data is still advantageous for paraphrase generation, and our monolingual method can be a helpful resource for languages where such data is not available."
    ],
    "4541": [
        "We proposed a neural information retrieval system consisting of a convolutional neural encoder and a fast approximate nearest neighbor search index to solve the large-scale question paraphrase retrieval task.",
        "We developed a new loss function inspired by label smoothing to deal with the issue of overfitting in the learning-to-rank setting.",
        "Our batch-wise smoothed loss formulation is applicable to a variety of metric learning and information retrieval problems for which triplet loss is currently popular.",
        "We believe that our loss function framework is flexible enough to experiment with different priors, such as allocating probability masses based on the distances between the points."
    ],
    "4542": [
        "undirected neural sequence models achieve performance comparable to conventional, state-of-the-art autoregressive models",
        "constant-time translation in these models performs similar to linear-time translation",
        "most sequences are generated either monotonically or outside-in",
        "our generalized framework opens new avenues in developing and understanding generation algorithms for a variety of settings",
        "our work could also be applied to other structured data such as grids (for e.g. images) and arbitrary graphs"
    ],
    "4544": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our method uses questions and answers simply as the input of BERT to model the interaction between the paragraph and each dialogue history independently and outperformed published models on both QuAC and CoQA.",
        "The gold answer history, which may not be given in real conversational situations, contributed to the model performance most on both datasets.",
        "We also found that the model performance on QuAC decreased significantly when we used predicted answers instead of gold answers.",
        "On the other hand, we can substitute the question history for the gold answer history on CoQA.",
        "For future work, we will investigate a more realistic and more difficult CMC setting, where the history of questions posed by the asker that does not see the evidence paragraph is given and the gold answer is not given for input.",
        "We will also investigate how to obtain related and effective context for the current question in the previous question and answer history."
    ],
    "4546": [
        "The proposed graph representation works well under cases where the set of domain slotvalue pairs have significant overlaps, like Restaurant and Hotel.",
        "The current compression scheme is unable to distinguish between dialog acts that are merged with different information, such as \"hotel-informprice\" or \"restaurant-inform-location\".",
        "The use of a projection-based method can effectively attenuate biases in contextualized embeddings without loss of entailment accuracy.",
        "The proposed method can generalize to the broader domain by using group similar concepts together as hypernym and using one switch to control the hypernym.",
        "The trade-off between compression and expressiveness is a pending expressiveness problem that needs to be addressed in future research."
    ],
    "4552": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our representation language provides an extra layer of supervision that can be used to reduce the influence of statistical bias in datasets like AQuA.",
        "Generated operation programs like the examples in figure 5 demonstrate the effectiveness of these operation formalisms for representing math word problems in a human interpretable form.",
        "The gap between the performance of our models and human performance indicates that our MathQA still maintains the challenging nature of AQuA problems."
    ],
    "4556": [
        "The discriminator model is capable of differentiating high-quality examples from low-quality ones in machine-generated augmentation to VLN datasets.",
        "The discriminator when trained with alignment-based similarity score on cheaply mined negative paths learns to align similar concepts in the two modalities.",
        "The navigation agent when initialized with the discriminator generalizes to instruction-path pairs from previously unseen environments and outperforms the benchmark.",
        "The multi-modal alignment learned by the discriminator can be used to segment the instruction-path pair into several shorter instruction-path pairs which can then be used for creating a curriculum of easy to hard tasks for the navigation agent to learn on.",
        "The trained discriminator model is general enough to be useful for any downstream task which can benefit from such multi-modal alignment measure and not limited to VLN task that we use in this work."
    ],
    "4558": [
        "The proposed binary classification loss function improves the model's performance in open IE.",
        "Iteratively optimizing the loss function enables the model to learn from trial and error, leading to substantial improvement.",
        "The error analysis provides insights into possible future directions for improving the model.",
        "(Paragraph 2) The proposed binary classification loss function improves the model's performance in open IE.",
        "(Paragraph 3) Iteratively optimizing the loss function enables the model to learn from trial and error, leading to substantial improvement.",
        "(Paragraph 4) The error analysis provides insights into possible future directions for improving the model."
    ],
    "4560": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our approach now generates more content-related captions with higher diversity, as validated by human evaluation results.",
        "Despite the effectiveness of our proposed GRU encoder-decoder model with a sentence-level loss, there is still a significant gap between model predictions and human annotations."
    ],
    "4563": [
        "We achieve a F1-score of 0.585 for the task of extracting orthodontic problems from findings, and a correlation coefficient of 0.584 with human ranking for treatment prioritization.",
        "Our future work includes fine-tuning on in-domain data of sentence encoder [8], robust sentence encoding for incomplete sentences, consideration of the findings for treatment prioritization, and text simplification from treatment protocol summaries to consent form documents.",
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves."
    ],
    "4564": [
        "'ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.'",
        "'Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.'",
        "'Perceived emotions can be different from expressed emotions in such event-focused corpus, which also affects classification performance.'",
        "'Emotions vary substantially in their properties, both linguistic and extra-linguistic, which affects both annotation and modeling.'",
        "'There is high consistency across the language pair English-German.'",
        "'The straightforward application of machine translation for model transfer to another language does not lead to a drop in prediction performance.'"
    ],
    "4565": [
        "The authors have collected and annotated two new datasets that will be useful for the cybersecurity community.",
        "The authors have explored several group-level behavioral traits to model inter-personal collusive dynamics in a group.",
        "The authors have proposed a novel method called DeFrauder to detect and rank fraud reviewer groups.",
        "The authors have shown the superiority of DeFrauder compared to five baselines through exhaustive experiments performed on four datasets."
    ],
    "4566": [
        "The proposed method, Differentiable Average Lagging (DAL), has internally consistent assumptions about timing and is differentiable.",
        "The method can be used to measure the time it takes to write each target token.",
        "The method is a modified version of Average Lagging.",
        "The method is differentiable, which allows for more accurate measurements of timing."
    ],
    "4567": [
        "The performance of state-of-the-art VQA models significantly drops on OK-VQA.",
        "Answering questions on OK-VQA requires reasoning on external knowledge resources.",
        "There is a large room for improvement on OK-VQA.",
        "Background knowledge can improve results on OK-VQA."
    ],
    "4568": [
        "We present an investigation into the feasibility of scoring singletons and pairs according to their likelihoods of producing summary sentences.",
        "Our framework is founded on the human process of selecting one or two sentences to merge together and it has the potential to bridge the gap between compression and fusion studies.",
        "Our method provides a promising avenue for domain-specific summarization where content selection and summary generation are only loosely connected to reduce the costs of obtaining massive annotated data."
    ],
    "4570": [
        "'We introduce two approaches for effectively adapting pretrained language model representations to abstractive summarization: domain-adaptive training, and source embeddings.'",
        "'We evaluate the effect of both approaches across three abstractive summarization testbeds: CNN/DailyMail, XSum, and Newsroom, and achieve state of the art ROUGE-L results on two of them, while showing superior human evaluation performance on the third.'",
        "'The ROUGE-L metric often used for abstractive summarization evaluation is quite sensitive to summary length, allowing it to be exploitable by approaches that use heuristics to control summary length.'"
    ],
    "4573": [
        "The authors propose a new dataset, COS960, which contains 960 word pairs with two component words and is designed to evaluate word similarity in Chinese.",
        "The authors hope that this dataset will contribute to the development of distributional semantics in Chinese.",
        "The authors provide detailed information about the construction process of the dataset.",
        "Existing word embedding models are evaluated on the dataset to demonstrate their performance.",
        "The authors suggest that the dataset will be useful for researchers working on distributional semantics in Chinese."
    ],
    "4577": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our method only adds a fully-connected layer during training, which can be removed during inference, and we show thorough quantitative and qualitative results demonstrating around 20% or 30% relative improvements in visual grounding accuracy over existing methods for image and video captioning tasks."
    ],
    "4578": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "We propose Multimodal Transformer (MulT) for analyzing human multimodal language, which serves as a strong baseline capable of capturing long-range contingencies regardless of the alignment assumption.",
        "The results of MulT on unaligned human multimodal language sequences suggest many exciting possibilities for its future applications, such as Visual Question Answering tasks.",
        "We hope the emergence of MulT could encourage further explorations on tasks where alignment used to be considered necessary, but where crossmodal attention might be an equally (if not more) competitive alternative."
    ],
    "4579": [
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improves the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "APES is a new automatic summarization evaluation metric for news articles datasets based on the ability of a summary to answer questions regarding salient information from the text.",
        "APES is useful in domains with source documents of about 1k words that focus on named entities -such as news articles, where named entities are effectively aligned with Pyramid SCUs.",
        "In other non-news domains, and longer documents, other methods for generating questions should be designed.",
        "A new abstractive model that optimizes APES scores on the CNN/Daily Mail dataset by attending salient entities from the input document, which also provides competitive ROUGE scores."
    ],
    "4580": [
        "G-BERT outperforms all baselines in prediction accuracy on medication recommendation task.",
        "It adapted BERT to the EHR data and integrated medical ontology information using graph neural networks.",
        "Our pre-training model G-BERT for medical code representation and medication recommendation is, to our best knowledge, the first that utilizes language model pre-training techniques in healthcare domain.",
        "Adding more auxiliary and structured data can further improve the performance of G-BERT."
    ],
    "4581": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our proposed method is effective in fine-tuning a pre-trained NMT model to correctly translate unknown words when switching to new domains.",
        "Our method can be applied to improve the performance of NMT models in domain adaptation tasks."
    ],
    "4582": [
        "We propose a new vision-based approach to induce bilingual lexicon with images and their associated sentences.",
        "The two types of features, linguistic features and localized visual features, are complementary for word translation.",
        "Experimental results on multiple language pairs demonstrate the effectiveness of our proposed method, which leads to significant performance improvement over the state-of-the-art vision-based approaches for all types of part-of-speech.",
        "In the future, we will further expand the vision-pivot approaches for zero-resource machine translation without parallel sentences."
    ],
    "4585": [
        "Our proposed two-stage method for unknown intent detection yields consistent improvements compared to baseline methods.",
        "We replace softmax loss with margin loss to learn discriminative deep features and improve the detection of unknown intents.",
        "Our method can be applied to broader families of anomaly detection algorithms, including end-to-end solutions.",
        "We plan to design a solution that can identify unknown intents from known intents and cluster them in an end-to-end fashion."
    ],
    "4586": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our single model joint system outperforms the current state-of-the-art single model for VQA."
    ],
    "4589": [
        "We proposed a novel setting for controlled text generation that does not require prior knowledge of all the values the control variable might take on.",
        "Our proposed approaches do not rely on a test-time parser or tagger and outperform our baselines.",
        "Further analysis shows the model has learned both interpretable and disentangled representations.",
        "The proposed variational model accompanied with a neural component and multiple multi-task training objectives for addressing this task does not require prior knowledge of all the values the control variable might take on.",
        "Our baselines are outperformed by the proposed approaches."
    ],
    "4590": [
        "'Our proposed method of jointly training the semantic parser and NL generator by exploiting the structural connections between them is effective.'",
        "'The method of DIM can be used to exploit the duality and provide a principled way to optimize the dual information.'",
        "'Our extension of supervised DIM to semi-supervised scenario (SEMIDIM) is effective.'",
        "'The use of automatically mined datasets for semantic parsing can be effective, but these datasets are noisy and it is hard to train robust models out of them.'",
        "'In the future, we will further apply DIM to learn semantic parser and NL generator from the noisy datasets.'"
    ],
    "4591": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "In some cases learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.",
        "RNN and DiSAN exactly perform better than SAN on extracting word order information in the case they are trained individually for our task.",
        "There is no evidence that SAN learns less word order information under the machine translation context.",
        "Modeling recurrence is universally-effective to learn word order information for SAN.",
        "RNN is more sensitive on erroneous word order noises in machine translation system."
    ],
    "4592": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our approach to solving the brevity problem leads to significant BLEU gains.",
        "Solving label bias in general may lead to further improvements in machine translation systems.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset, which is inexpensive.",
        "More general globally-normalized models can be trained in a similarly inexpensive way.",
        "Experimental results show that our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Federated Hierarchical Hybrid Networks can be considered as Hierarchical Hybrid Networks trained by Clickbait Federated Learning.",
        "Clickbait Federated Learning can effectively utilize non-shared data in the Data Island setting and train a federated model which is comparable to the model with the same architecture trained using the traditional training method in the ideal situation.",
        "Federated Hierarchical Hybrid Networks performed well on clickbait detection tasks."
    ],
    "4594": [
        "The proposed method can accurately debias pre-trained word embeddings, outperforming previously proposed debiasing methods, while preserving useful semantic information.",
        "The proposed method can be extended to debias other types of demographic biases such as ethnic, age, or religious biases in future work."
    ],
    "4595": [
        "We introduced a framework for decomposing complex tasks into steps of planning and execution, connected with a natural language interface.",
        "We experimented with this approach on a new strategy game which is simple to learn but features challenging strategic decision making.",
        "We collected a large dataset of human instruction generations and executions, and trained models to imitate each role.",
        "Results show that exploiting the compositional structure of natural language improves generalization for both the instructor and executor model, significantly outperforming agents without latent language.",
        "Future work should use reinforcement learning to further improve the planning and execution models, and explore generating novel instructions."
    ],
    "4596": [
        "The proposed approach achieves new state-of-the-art results on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The Cosine Annealing Strategy combines the two methods to improve domain adaptation performance.",
        "The approach utilizes pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "The model achieves better performance than ProBERT on the miniRCV1 and ODIC datasets.",
        "GREP corrects some errors made by ProBERT, reflected in F1, and is more confident in its predictions, reflected in log loss.",
        "GREP is able to build evidence for or against the given candidates, indicating that the model is successfully able to build evidence for or against the given candidates.",
        "The evidence pooling mechanism introduced here is able to leverage upon the strengths of off-the-shelf coreference solvers without being hindered by their weaknesses (gender bias)."
    ],
    "4597": [
        "We address the zero-resource machine translation problem by exploiting visual images as pivots.",
        "Due to the nature that a picture tells a thousand words, description sentences for an image may be semantically nonequivalent, which leads to noisy supervisions to train the NMT model in previous works.",
        "We propose a progressive learning approach which consists of progressive easy-to-advanced steps towards learning effective NMT models under zero resource settings.",
        "The learning starts with word-level translation with image pivots and then progresses to sentence-level translation assisted by the word translation and image pivots.",
        "Experiments on IAPR-TC12 and Multi30k datasets prove the effectiveness of the proposed approach, which significantly outperforms previous image-pivot methods."
    ],
    "4599": [
        "The addition of morphological supervision to character language models via multitask learning improves BPC on 24 languages.",
        "The gain in BPC is observed even when the morphological annotations and language modeling data are disjoint, providing a simple way to improve language models without requiring additional annotation efforts.",
        "The addition of morphology benefits inflected forms more than uninflected forms.",
        "Training CLMs on additional language modeling data does not diminish the gains in BPC.",
        "The gains in BPC can also be projected across closely related languages by sharing morphological annotations.",
        "The multitasking approach helps CLMs capture morphology better than the LM objective alone."
    ],
    "4600": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model of part-of-speech tagging and dependency parsing improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves the accuracy of tagging and parsing.",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The proposed structural sentence splitting approach achieves high scores on all three simplification corpora with regard to SAMSA, and comes close second in terms of SAMSA abl.",
        "The extrinsic evaluation that was carried out based on the task of Open IE verified that downstream semantic applications profit from making use of our proposed structural TS approach as a preprocessing step."
    ],
    "4601": [
        "Our work suggests that for a significant fraction of natural speech, human perception can be altered by using subtle, learnable perturbations.",
        "This is an initial step towards exploring the density of illusionable phenomenon for humans, and examining the extent to which human perception may be vulnerable to security risks like those that adversarial examples present for ML systems.",
        "We hope our work inspires future investigations into the discovery, generation, and quantification of multimodal and unimodal audiovisual and auditory illusions for humans.",
        "There exist many open research questions on when and why humans are susceptible to various types of illusions, how to model the illusionability of natural language, and how natural language can be made more robust to illusory perturbations.",
        "Additionally, we hope such investigations inform our interpretations of the strengths and weaknesses of current ML systems.",
        "Finally, there is the possibility that some vulnerability to carefully crafted adversarial examples may be inherent to all complex learning systems that interact with high-dimensional inputs in an environment with limited data; any thorough investigation of this question must also probe the human cognitive system."
    ],
    "4607": [
        "We close the gap of the performance of between zero-shot translation and pivot-based zero-resource translation.",
        "Our proposed methods significantly improve the vanilla zero-shot NMT.",
        "Our method consistently outperforms the pivot-based methods.",
        "We propose two simple and effective strategies for zero-shot translation.",
        "Experiments on the Europarl, IWSLT and MultiUN corpora show that our proposed methods significantly improve the vanilla zero-shot NMT and consistently outperform the pivot-based methods."
    ],
    "4610": [],
    "4611": [
        "We propose a new kNN-margin loss function that yields strong and robust performance across different model architectures and datasets.",
        "Our proposed loss function is a trade-off between naive nearest neighbor search and more polished inference strategies.",
        "Using Inverted Softmax (IS) and Cross-modal Local Scaling (CSLS) can significantly improve scores of all metrics.",
        "We analyze the limitations of this work and indicate the next step for improving both the loss function and the inference method."
    ],
    "4613": [
        "Our model can better understand the structure of the article and capture the main point of the article.",
        "Experiment results show that our model can generate more coherent and informative comments.",
        "There are still some comments conflicting with the world knowledge.",
        "Introducing external knowledge into the graph could make the generated comments more logical."
    ],
    "4614": [
        "Our proposed iterative predicate selection (IPS) parsing model achieves a new state of the art for three SDP formalisms, as demonstrated by our experiments.",
        "We apply multi-task learning to learn general representations of parser configurations, and use reinforcement learning for task-specific fine-tuning.",
        "Our approach learns an easy-first strategy and some syntactic features through fine-tuning with reinforcement learning.",
        "The number of graphs and the percentages of graphs with circles when not using heuristics to avoid circles are shown in Table 6, and we note that circles are mostly small and local, so they do not affect other arc structures."
    ],
    "4615": [
        "The nature of phenomenon-explanation mapping is one-to-many, with different people offering drastically different explanations for the same phenomenon.",
        "Requiring machines to generate plausible explanations is a more useful goal for models to achieve, as it can provide better responses to why questions.",
        "Models trained on traditional chatbot corpora are unable to answer why questions due to data sparsity.",
        "The generated results may not be similar to the original explanations but are often acceptable by human assessment.",
        "Providing plausible explanations as judged by humans is an important goal for neural sequenceto-sequence models.",
        "A large dataset of phenomenon-explanation pairs can be curated to learn how to provide plausible explanations, and formulate responses to general why-questions."
    ],
    "4617": [
        "The proposed Chinese cloze dataset (ChID) contains 581K passages and 729K queries, covering 3,848 Chinese idioms, providing a benchmark for evaluating the ability of Chinese cloze test with idiom.",
        "The difficulty level of Chinese cloze test with idiom correlates positively with the method of choosing candidate choices, as found through the analysis of embedding similarity and synonymity.",
        "Idiom representation is a key factor to the success of reading comprehension models in the task, due to the common non-compositionality and metaphorical meaning of Chinese idiom.",
        "Existing cloze test models perform much worse than human performance on the corpus, indicating the need for further research to improve model performance."
    ],
    "4619": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Exploiting sentential context improves performances over the state-of-the-art TRANSFORMER model.",
        "The proposed approach captures more linguistic information as expected."
    ],
    "4623": [
        "The proposed cross-sentence latent variable model (CS-LVM) outperforms other models by a large margin.",
        "Fine-tuning with semantic constraints gives additional increase in performance.",
        "The current model tends to prefer relatively short and safe sentences, limiting its ability to generate more natural and diverse text.",
        "Applying recent advancements on deep generative models could improve the model's ability to perform more natural generation.",
        "The model could be used for data augmentation and addressing adversarial attacks in future work."
    ],
    "4624": [
        "We propose a novel automatic method to generate training data for scientific papers summarization, based on conference talks given by authors.\" (related to the methodology)",
        "We show that the a model trained on our dataset achieves competitive results compared to models trained on human generated summaries, and that the dataset quality satisfies human experts.\" (related to the results)",
        "In the future, we plan to study the effect of other video modalities on the alignment algorithm.\" (related to future work)",
        "We hope our method and dataset will unlock new opportunities for scientific paper summarization.\" (related to the potential impact of the research)"
    ],
    "4628": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments.",
        "We introduce a novel PU learning algorithm to perform the NER task using only unlabeled data and named entity dictionaries.",
        "Our approach can greatly reduce the requirement on sizes of the dictionaries.",
        "Extensive experimental studies on four NER datasets validate its effectiveness."
    ],
    "4630": [
        "The proposed multilingual language model achieves significant improvements in very low resource scenarios compared to a state-of-the-art monolingual LM.",
        "The results highlight the benefits of cross-lingual transfer learning for a more effective generalization of LMs on extreme data scarcity scenarios.",
        "The proposed multilingual LM is effective in handling low-resource domains and languages.",
        "The model achieves significant improvements consistently across four languages."
    ],
    "4632": [
        "The proposed novel constituent hierarchy predictor based on recurrent neural networks outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The joint model of part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The ability of M-BERT to generalize crosslingually is underpinned by a multilingual representation, without being explicitly trained for it.",
        "Effective transfer to typologically divergent and transliterated targets will likely require the model to incorporate an explicit multilingual training objective.",
        "Having word pieces used in all languages (numbers, URLs, etc) which have to be mapped to a shared space forces the co-occurring pieces to also be mapped to a shared space, thus spreading the effect to other word pieces, until different languages are close to a shared space."
    ],
    "4633": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The use of a multi-label classifier trained to predict visual entities from action features can provide a more expressive signal for captioning, compared to the raw features themselves.",
        "HALF significantly outperforms state-of-the-art models in rating prediction in recommendation.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The use of a multi-label classifier trained to predict visual entities from action features can provide a more expressive signal for captioning, compared to the raw features themselves.",
        "HALF significantly outperforms state-of-the-art models in rating prediction in recommendation."
    ],
    "4634": [
        "The proposed LeafNATS toolkit can be used to build, train, test, and deploy NATS models.",
        "The live news blogging system demonstrates how NATS models can make writing headlines and summaries for news articles more efficient.",
        "The proposed model achieves competitive results with fewer number of parameters.",
        "The extensive set of experiments on different benchmark datasets has demonstrated the effectiveness of the implementations."
    ],
    "4635": [
        "The authors have presented the first large-scale dataset of goal-oriented, visually grounded dialogues for investigating shared linguistic history.",
        "The dataset exhibits a significant shortening of utterances throughout a game, with final referring expressions starkly differing from both standard image captions and initial descriptions.",
        "Information accumulated over a reference chain helps to resolve later descriptions, suggesting that more sophisticated models are needed to fully exploit shared linguistic history.",
        "The current paper showcases only some of the aspects of the PhotoBook dataset, which can be used to further investigate common ground and conceptual pacts, or exploit the combination of vision and language to develop computational models for referring expression generation.",
        "The PhotoBook task can be used in the ParlAI framework for Turing-Test-like evaluation of dialogue agents."
    ],
    "4641": [
        "KERMIT can model the joint data distribution and its decompositions (i.e., marginals and conditionals).",
        "KERMIT can generate text in an arbitrary order, including bidirectional machine translation and cloze-style infilling.",
        "KERMIT uses a simple neural architecture that can additionally produce contextualized vector representations of words and sentences.",
        "KERMIT is capable of matching or exceeding state-of-the-art performance on three diverse tasks: machine translation, representation learning, and zero shot cloze question answering."
    ],
    "4642": [
        "\"ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "\"The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "\"Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "\"Our NP-SG model learns better representations even with misspellings and reaches competitive results with skip-gram on similarity tasks, even outperforming with 2.5x-10x fewer parameters."
    ],
    "4643": [
        "We have introduced asymptotic acceptance as a new way to characterize neural networks.",
        "It provides a useful and generalizable tool for building intuition about how a network works, as well as for comparing the formal properties of different architectures.",
        "Further, by combining asymptotic characterizations with existing results in mathematical linguistics, we can better assess the suitability of different architectures for the representation of natural language grammar.",
        "Empirically, however, we observe that this discrete analysis fails to fully characterize the range of behaviors expressible by neural networks.",
        "Introducing a small amount of noise into a network's activations seems to prevent it from implementing non-asymptotic strategies.",
        "Asymptotic characterizations might be a good model for the types of generalizable strategies that noise-regularized neural networks trained on natural language data can learn."
    ],
    "4644": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "The ConMask model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our method is much faster than RNN-based alternatives in both training and inference settings.",
        "The brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "There is room for improvement in the ConMask model, as it still has some limitations and can be improved further."
    ],
    "4645": [
        "The proposed method improves word translation accuracy of different mapping-based CLWE algorithms across languages.",
        "The proposed method has the potential to improve other downstream tasks in the future.",
        "The proposed method is effective in reducing the error of word translation across languages.",
        "The proposed method uses less training data than previous systems."
    ],
    "4646": [
        "Our approach achieves an accuracy of 0.875 with a false accusation rate of 0.141 and a catch rate of 0.896.",
        "We show that false accusation rate can be improved at the cost of catch rate and accuracy.",
        "The results are good enough for practical use, even with a slightly lower catch rate.",
        "The system is expected to have a preventive effect, despite a slightly lower catch rate.",
        "The data set is not 50/50 balanced, which affects the results.",
        "Making a split imitating the real world is hard due to the difficulty in approximating the actual fraction of ghostwriters.",
        "Finding a clean data set or establishing ground truth would alleviate the problems and be an interesting prospect for future work.",
        "Analyzing writing style changes over time more in-depth, motivated by the chosen combination strategy and preliminary experiments, could be an interesting direction for future work."
    ],
    "4648": [
        "The LSTM model learns a stronger representation of time than the feedforward baseline, as evidenced by its better performance in predicting the year of composition of novel sentences.",
        "The LSTM learns a representation of time that encompasses both syntactic and lexical change.",
        "The continuous notion of change represented by the model can be reconciled with the discreteness of natural language grammar through theories such as multiple grammars or gradual changes in syntactic parameters.",
        "Further work could use similar methods to examine how neural networks represent patterns of change in specific grammatical constructions, and evaluate the degree to which individual syntactic changes are continuous."
    ],
    "4650": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "We showed that the brevity problem can be solved using globally-normalized training on only a small dataset.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "Subtask A (question classification) was easier and all submitted systems managed to improve over the majority class baseline.",
        "Subtask B (answer classification) proved to be much more challenging, and no team managed to improve over the majority class baseline, even though several teams came very close.",
        "Using external resources and preprocessing proved to be crucial for Subtask B."
    ],
    "4652": [
        "The authors introduce a new multi-document news summarization dataset called Multi-News, which they hope will promote work in this area similar to the progress seen in single-document summarization.",
        "The authors propose an end-to-end model that incorporates Maximum Marginal Relevance (MMR) into a pointer-generator network, which performs competitively compared to previous multi-document summarization models.",
        "The authors benchmark their model on their new dataset and show that it performs well.",
        "In the future, the authors plan to explore interactions among documents beyond concatenation and experiment with summarizing longer input documents.",
        "We introduce Multi-News, the first large-scale multi-document news summarization dataset.\" (Paragraph 1)",
        "We propose an end-to-end model that incorporates MMR into a pointer-generator network, which performs competitively compared to previous multi-document summarization models.\" (Paragraph 2)",
        "We benchmark our model on our dataset and show that it performs well.\" (Paragraph 3)",
        "In the future, we plan to explore interactions among documents beyond concatenation and experiment with summarizing longer input documents.\" (Paragraph 4)"
    ],
    "4653": [
        "Our model achieves state-of-the-art results, outperforming previous models by 10.5 CoNLL F 1 points on events.",
        "We represent a mention using its text, context, and inspired by the joint model of Lee et al. (2012) we make an event mention representation aware of coreference clusters of entity mentions to which it is related via predicate-argument structures, and vice versa.",
        "Our model outperforms previous models by 10.5 CoNLL F 1 points on events.",
        "Providing the first cross-document entity coreference results on ECB+.",
        "Future directions include investigating ways to minimize the pipeline errors from the extraction of predicate-argument structures, and incorporating a mention prediction component, rather than relying on gold mentions."
    ],
    "4656": [
        "The proposed model, which incorporates a multimapping mechanism and an auxiliary matching loss, can learn the one-to-many relationship for diverse response generation.",
        "The model is able to capture distinct responding regularities by selecting the corresponding mapping module according to the target response.",
        "The model works as expected and generates responses of diversity and high quality.",
        "The proposed model, which incorporates a multimapping mechanism and an auxiliary matching loss, can learn the one-to-many relationship for diverse response generation. (This claim is related to the model's ability to generate multiple responses for a given input.)",
        "The model is able to capture distinct responding regularities by selecting the corresponding mapping module according to the target response. (This claim is related to the model's ability to capture different patterns in the data.)",
        "The model works as expected and generates responses of diversity and high quality. (This claim is related to the model's performance and the quality of the generated responses.)"
    ],
    "4658": [
        "We proposed a novel framework called DOER for aspect-based sentiment analysis, which involves a joint sequence labeling approach and two auxiliary tasks to enhance the representation of sentiment and alleviate the difficulty of long aspect terms.",
        "Experimental results on three benchmark datasets verified the effectiveness of DOER and showed that it significantly outperforms the baselines on aspect term-polarity co-extraction.",
        "Our framework focuses on the interaction between two separate routes for aspect term extraction and aspect sentiment classification, which improves the representation of sentiment and alleviates the difficulty of long aspect terms.",
        "To enhance the representation of sentiment and alleviate the difficulty of long aspect terms, two auxiliary tasks were also introduced in our framework.",
        "The proposed DOER framework significantly outperforms the baselines on aspect term-polarity co-extraction."
    ],
    "4662": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The proposed domain adaptation method for CCG parsing based on automatic generation of new CCG treebanks from dependency resources shows significant improvements in parsing experiments on speech conversation and math problems.",
        "The achievement of more than 5 points in the unlabeled metric is observed when applying the proposed method to domain adaptation."
    ],
    "4663": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The attention mechanism employed by GANE enjoys the benefits of being naturally sparse and self-normalized, a global sequence matching scheme, and able to capture long-term interactions between two sentences.",
        "The attention mechanism can be applied to tasks such as relational networks, natural language inference, and QA systems."
    ],
    "4667": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed method for training bilingual sentence embeddings shows consistently strong performance in both sentence alignment recovery and the WMT 2018 parallel corpus filtering tasks with only a single model.",
        "Cosine similarity is preferred for a mining task, and our MLP similarity is very effective in a filtering task.",
        "A moderate level of negativity is appropriate for training the MLP similarity.",
        "Regularizing the MLP training to obtain a smoother distribution of the similarity scores could supplement the weakness of the MLP similarity.",
        "The proposed method should be tested on many other language pairs which do not have parallel data involving a pivot language."
    ],
    "4669": [
        "The proposed hierarchical abstractive system can disentangle interleaved texts and generate appropriate abstractive summaries.",
        "The system's phrase-level attention mechanism (\u03b2) provides explainability of the output.",
        "The system achieves performance gains of 20-40% on both real-world and synthetic datasets.",
        "The architecture addresses error propagation and fluency issues that occur in two-step architectures.",
        "The system can generate abstractive summaries covering the text threads.",
        "The use of novel hierarchical attention allows for end-to-end trainability and addresses issues such as error propagation and fluency."
    ],
    "4670": [
        "The refinement method using hyperbolic Poincar\u00e9 embeddings consistently yields improvements over strong baselines and is a better representation of distributional vectors compared to word2vec.",
        "Poincar\u00e9 embeddings can be efficiently created for a specific domain from crawled text without the need for an existing database such as WordNet, confirming the theoretical capability of learning hierarchical relations.",
        "The use of hyperbolic embeddings as the sole signal for taxonomy extraction is a promising direction for future work.",
        "Combining distributional and hyperbolic embeddings may be interesting to cover different relations between terms and improve the accuracy of taxonomy extraction."
    ],
    "4671": [
        "We propose an imitation learning framework for non-autoregressive neural machine translation to bridge the performance gap between NAT and AT.",
        "Employing a knowledgeable AT demonstrator to supervise every decoding state of NAT across different time steps and lay-ers leads to remarkable improvements and largely closes the performance gap between NAT and AT on several benchmark datasets.",
        "Introducing more powerful demonstrators with different structures (e.g., right to left) can improve the performance of NMT.",
        "Applying the proposed imitation learning framework to similar scenarios such as simultaneous interpretation is a future work."
    ],
    "4672": [
        "The proposed dataset of Facebook comments on posts from mainstream media in Slovenia and Great Britain covering the topics of migrants and LGBT has been manually annotated with the type and target of socially unacceptable discourse.",
        "More SUD (Socially Unacceptable Discourse) is produced in Slovene than in English media.",
        "More SUD is produced on the topic of migrants than LGBT.",
        "The inter-annotator agreement is similar for both type and target, with a medium agreement below the expected quality in social science.",
        "Initial experiments show that comparing professional annotations with the modes (most frequent annotations) of non-professional annotations can significantly improve agreement, moving it to the area of useful annotations for social sciences.",
        "The dataset has been collected with around eight annotations per comment, which will be exploited further in future work on extracting annotations of highest quality possible.",
        "There is a trend of a similar number of users who produce large, medium and low quantities of SUD in Slovene, while the English users tend to produce mostly an average amount of SUD, with just small numbers of users on the extremes of the (non)production of SUD.",
        "Future work on this dataset will focus primarily on the two envisaged usages of the dataset: (1) further analysis of the phenomenon of SUD and (2) (semi-)automation of SUD identification."
    ],
    "4673": [
        "The annotation task for terminology extraction is high complexity.",
        "Combining all available statistics significantly improves multi-word term prediction, with a relative improvement of AUC by 25% over the single best-performing statistic.",
        "Adding morphosyntactic pattern and character length to the single-word term predictor does not improve the results on either of the problems.",
        "The context information does not improve the results."
    ],
    "4675": [
        "The authors present an unsupervised method for discovering root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of the rules allow for validating the pattern discovery method and root extraction method (JZR).",
        "The performance of the method is not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "The method presents an unsupervised way to discover root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "The method is validated through intrinsic and extrinsic evaluations.",
        "The performance of the method is comparable to a rule-based language-specific root extractor."
    ],
    "4676": [
        "Margin-loss is a more plausible training objective for COPA-style plausibility tasks than log-loss.",
        "Adversarial construction can drive a logloss approach to encode plausible statements as either extremely likely or unlikely.",
        "The intuition of using margin-based loss leads to a new state-of-the-art in the original COPA task.",
        "Using margin-based loss can preserve salient source relations in summaries.",
        "Structural models are on-par with or surpass state-of-the-art published systems."
    ],
    "4677": [
        "SP-10K has much larger coverage and can better represent ground truth SP compared to other evaluation methods.",
        "Two novel two-hop SP relations, 'dobj amod' and 'nsubj amod', are introduced.",
        "Using SP to represent commonsense knowledge can be beneficial for the acquisition and application of commonsense knowledge.",
        "The two-hop relations demonstrate the importance of representing commonsense knowledge."
    ],
    "4681": [
        "We present SIVAE, a novel syntax-infused variation autoencoder architecture for text generation, leveraging constituency parse tree structure as the linguistic prior to generate more fluent and grammatical sentences.",
        "The new lower bound objective accommodates two latent spaces, for jointly encoding and decoding sentences and their syntactic trees.",
        "The first version of SIVAE exploits the dependencies between two latent spaces, while the second version enables syntactically controlled sentence generation by assuming the two priors are independent.",
        "Experimental results demonstrate the incorporation of syntactic trees is helpful for reconstruction and grammar of generated sentences.",
        "In addition, SIVAE can perform unsupervised paraphrasing with different syntactic tree templates."
    ],
    "4685": [
        "'We proposed a novel imitation learning approach to unsupervised parsing.'",
        "'We achieve a new state-of-the-art result of unsupervised parsing on the NLI dataset.'",
        "'In future work, we would like to combine more potential parsers-including chartstyle parsing and shift-reduce parsing-and transfer knowledge from one to another in a co-training setting.'",
        "'Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.'",
        "'A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.'"
    ],
    "4686": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline",
        "in some cases learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves",
        "disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality",
        "our approach allows lexicality and syntax to interact with each other in the joint search process, it improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages",
        "the dataset features wide semantic coverage and a diverse set of contextual dependencies between questions",
        "the model accuracy is far from satisfactory and stratifying the performance by question position shows that both models degenerate in later turns of interaction, suggesting the importance of better context modeling"
    ],
    "4693": [
        "Our approach improves both the translation performance and the robustness of NMT models.",
        "We have introduced a white-box method to generate adversarial examples for NMT.",
        "Experimental results demonstrate the capability of our approach in improving the translation performance and the robustness.",
        "In future work, we plan to explore the direction to generate more natural adversarial examples dispensing with word replacements and more advanced defense approaches such as curriculum learning."
    ],
    "4694": [
        "The end-to-end NMT model generates a translation word by word with the ground truth words as context at training time, unlike previous models that use previous words generated by the model as context at inference.",
        "To mitigate the discrepancy between training and inference, the authors use a sampling scheme to feed either the ground truth word or the previous predicted word as context.",
        "The predicted words, referred to as oracle words, can be generated with word-level or sentence-level optimization.",
        "Compared to word-level oracle, sentence-level oracle can further equip the model with the ability of overcorrection recovery.",
        "The authors verify the effectiveness of their method with two strong baseline models and related works on real translation tasks, achieving significant improvement on all datasets.",
        "The sentence-level oracle shows superiority over the word-level oracle."
    ],
    "4697": [
        "Our approach using word-level word embeddings captures information about semantic classes of derivational relations between words, despite not having any information about the orthography or morphological makeup of the words.",
        "The word2vec embeddings generally result in a better clustering than embeddings from NMT models with attention.",
        "Embeddings from the decoder of a plain RNN model perform better than those from NMT models with attention.",
        "All these methods outperform a random-assignment clustering baseline and POS clustering baseline."
    ],
    "4698": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We propose the creation of a new dataset with human-generated question-answer pairs.",
        "Document-level CQA requires document question answering, page segmentation, and more.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications."
    ],
    "4699": [
        "We have shown work in progress on determining the compositionality of compounds over time.",
        "Information theory based measures seem to capture compositionality better for our current setup.",
        "Adding temporal information increases the predictive power of these features to prognosticate synchronic compositionality.",
        "Our best performing models trace the compositionality of compounds over time, delineating the behavior of compounds of varying levels of compositionality."
    ],
    "4700": [
        "Our results show that even mediocre suggestion models have a positive effect in terms of agreement between annotators and annotation speed, while annotation biases are negligible.",
        "Based on our experiments on training suggestion models, we propose for future annotation studies that annotation suggestions can be given after having annotated only a small amount of data (in our case 70 texts), which ensures a sufficient model performance (0.5 macro-F1).",
        "We expect our work to have a large impact on future work requiring expert annotations, in particular regarding new tasks with no or little available data.",
        "Our method can achieve competitive performance compared to the previous systems by using less than 1% of the training data.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Using the hard task of epistemic activity identification as an example, we present the first study of annotation suggestions for discourse-level sequence labelling requiring expert annotators."
    ],
    "4702": [
        "Our method proves effective at transferring to distant languages.",
        "Through learning a new latent embedding space as well as languagespecific knowledge with unlabeled target data, our method is effective at transferring to distant languages.",
        "The proposed method learns a structured flow model in a cross-lingual setting, which proves effective at transferring to distant languages.",
        "Our method achieves high POS tagging accuracy (%) and dependency parsing UAS (%) results when using mBERT as the aligned embeddings.",
        "The proposed method is able to transfer to distant languages with high accuracy, as shown by the POS tagging accuracy (%) and dependency parsing UAS (%) results."
    ],
    "4703": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Adapting word embeddings from a partly useful source corpus to a target topic using our method can improve the performance of the model.",
        "The use of regularization and sources election methods for adapting word embeddings can lead to better results than using only the target corpus.",
        "The performance of the model can be improved by down-weighting irrelevant snippets without help from the IR-based selection.",
        "Including 5% random snippets from DS in addition to those in SrcSel and weighing them all by their Q(w, C) score can improve the performance of the model."
    ],
    "4708": [
        "The system designed to help ESL learners differentiate confusing words is effective, as evidenced by students' substantial progress and production of more structural sentences.",
        "Learning to use appropriate words is a demanding task that requires higher language proficiency.",
        "The learner's first language may lead to confusion in different areas, and this is taken into account with a novel approach.",
        "The example sentences in the refined list were considered more useful for learning by Amazon mechanical turkers and the expert English editor.",
        "ESL learners such as students and some of the turkers tend to prefer example sentences with similar patterns to mitigate cognitive overhead.",
        "Future work will focus on providing example sentences with similar patterns but diverse contexts."
    ],
    "4709": [
        "We have extended existing capsule networks into a new framework with advantages concerning scalability, reliability, and generalizability.",
        "Our experimental results have demonstrated its effectiveness on two NLP tasks: multi-label text classification and question answering.",
        "Through our modifications and enhancements, we hope to have made capsule networks more suitable to large-scale problems and, hence, more mature for real-world applications.",
        "We plan to apply capsule networks to even more challenging NLP problems such as language modeling and text generation in the future.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages."
    ],
    "4713": [
        "We proposed DECOMPRC, a system for multihop RC that decomposes a multi-hop question into simpler, single-hop sub-questions.",
        "We recasted sub-question generation as a span prediction problem, allowing the model to be trained on 400 labeled examples to generate high quality sub-questions.",
        "DECOMPRC achieved further gains from the decomposition scoring step.",
        "DECOMPRC achieved the state-of-the-art on HOTPOTQA distractor setting and full wiki setting, while providing explainable evidence for its decision making in the form of sub-questions and being more robust to adversarial settings than strong baselines."
    ],
    "4715": [
        "The proposed method for estimating degrees of compositionality in noun phrases outperforms previous state-of-the-art models.",
        "The use of Poincar\u00e9 embeddings, which are novel to this task, improves the performance of compositionality prediction.",
        "Mixing hypernymy and distributional information via Euclidean and hyperbolic embeddings significantly improves the performance of compositionality prediction.",
        "The pretrained embeddings and source codes are publicly available for future work and exploration of other languages and compositionality functions.",
        "The method can be extended to other languages using multilingual resources or translation data."
    ],
    "4716": [
        "The proposed method (SCITE) is effective in extracting causality in natural language text, but its performance is limited by the insufficiency of high-quality annotated data.",
        "The method uses a self-attentive BiLSTM-CRF-based solution and introduces the multihead self-attention mechanism to learn the dependencies between cause and effect.",
        "The proposed method achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The three-stage architecture in neural QA is effective, and answer re-ranking is responsible for bolstering the overall performance considerably.",
        "The RankQA system provides a new, strong baseline for future research on neural QA."
    ],
    "4718": [
        "Our proposed method, TRE, outperforms state-of-the-art methods on two popular relation extraction datasets (TACRED and SemEval 2010 Task 8).",
        "Pre-trained language representations improve the sample efficiency of our approach.",
        "Language representations capture features very informative to the relation extraction task.",
        "Our generic architecture enables integration with additional contextual information and background knowledge about entities, which could further improve performance.",
        "The extent of syntactic structure captured in language representations is worth investigating, compared to information provided by dependency parsing."
    ],
    "4719": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The proposed method outperforms the strong Transformer baselines while using fewer model parameters.",
        "The number of shared features is different for each word pair, depending on the degree of relevance between the parallel word pairs.",
        "The private features enable the words to better capture the monolingual characteristics, resulting in an improvement of the overall translation quality.",
        "The proposed method improves the learning of word embeddings for NMT by using shared and private features."
    ],
    "4722": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Traditional offline evaluation metrics significantly overestimate model performance, indicating the importance of using evaluation metrics more relevant to a production setting.",
        "The proposed model performs well, both on offline metrics and on a human evaluation.",
        "The proposed model is suitable for use in a production conversational system.",
        "A deeper analysis of the whitelist selection process is an important direction for future work.",
        "There are underlying trade-offs between different characteristics of the whitelists, such as recall and coverage, that may lead to improved whitelist generation methods and further improve the performance of retrieval-based models."
    ],
    "4724": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The resulting fully-supervised parser outperforms the state-of-theart baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our embeddings outperform all other baselines in the target domain, especially the embeddings directly from the target domain.",
        "Our proposed models still perform robustly even when in-domain data is extremely limited."
    ],
    "4728": [
        "Integrating existing clinical information extraction tools with deep learning models is an important direction for bridging the gap between rule-based and learning-based methods.",
        "The widely-used clinical concept annotator cTAKES does not improve performance over raw text alone on the clinical coding task in two settings.",
        "The amount of word variation captured and the differentiation between the named-entity recognition and ontology-mapping tasks may affect cTAKES' effectiveness.",
        "Automated coding is one application area, but the models presented here could easily be extended to other downstream prediction tasks such as patient diagnosis and treatment outcome prediction.",
        "Evaluating newly-developed clinical NER tools with similar functionalities to cTAKES in our framework can potentially serve as a means to evaluate the effectiveness of newer systems vis-\u00e0-vis cTAKES."
    ],
    "4730": [
        "asynchronous training damages the final BLEU of the NMT model",
        "the damage with the Transformer is significantly more severe",
        "asynchronous training also requires a smaller learning rate to perform well",
        "with the same number of processors, asynchronous SGD has a smaller effective batch size",
        "stale gradients play a bigger role in the training performance of asynchronous Transformer",
        "applying a modification in asynchronous training by accumulating a few gradients in the server before applying an update can increase the batch size while also reducing the average staleness"
    ],
    "4737": [
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We will work on how to reduce the noise of pseudo labels to improve the domain adaption performance."
    ],
    "4742": [
        "Attention does not necessarily correspond to importance.",
        "The highest attention weights do not always have a large impact on the model's final decision.",
        "Attention weights often fail to identify the sets of representations most important to the model's final decision.",
        "The number of zeroed attended items is often too large to be helpful as an explanation.",
        "The contextualization scope preceding the attention layer affects the number of attended items affecting the model's decision.",
        "Attention may yet be interpretable in other ways, but as an importance ranking, it fails to explain model decisions."
    ],
    "4744": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements."
    ],
    "4745": [
        "We propose an extract-then-classify framework with a span-based labeling scheme instead of sequence tagging methods for open-domain targeted sentiment analysis.",
        "Our approach firmly outperforms the sequence tagging baseline as well as previous state-of-the-art methods on three benchmark datasets.",
        "The main performance improvement comes from the span-level polarity classifier, and the multi-target extractor is more suitable for long sentences.",
        "The pipeline model consistently surpasses both the joint model and the collapsed model."
    ],
    "4749": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The ConMask model can successfully rank the correct entities within the top-3 results in most cases, but it still has some limitations and room for improvement.",
        "The ConMask model is able to capture the correct relationship between entities and the words that describe them, even when the name of the entity does not appear in the description.",
        "The use of a projection-based method can effectively attenuate biases in contextualized embeddings without loss of entailment accuracy.",
        "The CCG-based pipeline system can perform visual-textual inference with semantically complex sentences without requiring any supervised data."
    ],
    "4751": [
        "Pre-trained word embedding features can be used for text classification tasks, and different feature sets have varying levels of performance.",
        "Hierarchical architectures in end-to-end models can significantly improve the accuracy of text classification tasks.",
        "Combining pre-trained features with end-to-end models can further improve the accuracy of text classification tasks.",
        "The choice of hyperparameter search method can impact the performance of the model, and a Gaussian Process (GP) can be used for faster hyperparameter search."
    ],
    "4753": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The AGRR-2019 corpus is a valuable source of information for researchers interested in gapping and can bring the community closer to resolving ellipsis.",
        "The size and diversity of the AGRR-2019 corpus will provide researchers with a large subset of Russian language gapping.",
        "The AGRR-2019 corpus is not an artificial creation of Compreno parser, but rather covers a large subset of Russian language gapping."
    ],
    "4757": [
        "Our multi-task neural network is capable of extracting relevant features from noisy user-generated text.",
        "A CRF classifier can boost the neural network results by using the whole sentence to predict the most likely set of labels.",
        "POS tags in conjunction with gazetteers are important for NER tasks.",
        "Twitter word embeddings and orthographic character embeddings are relevant for the task.",
        "Our approach emphasizes the importance of using a combination of features, including POS tags and gazetteers, to improve the results.",
        "Ongoing work aims at improving these results by gaining a better understanding of the strengths and weaknesses of our model.",
        "Evaluating the current system in related tasks where noise and emerging NEs are prevalent is an important area for future work."
    ],
    "4758": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "In future work, we plan to improve FAKTA's underlying components (e.g., stance detection), extend FAKTA to cross-lingual settings, and incorporate temporal information for fact checking."
    ],
    "4759": [
        "dramatic forgetting is at play in VQA",
        "Rehearsal works better than EWC",
        "the order in which models learn tasks is important",
        "WH\u2192Y/N facilitates continual learning more than the opposite order",
        "taking the kind of mistakes made by the models into account is important",
        "differences in the inherent difficulty of the tasks at hand can have a strong impact on continual learning",
        "regularization-based methods like EWC appear to work less well when applied to tasks with different levels of difficulty"
    ],
    "4765": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "A substantial amount of linguistic knowledge can be found not only in the hidden states, but also in the attention maps.",
        "Probing attention maps complements other model analysis techniques and should be part of the toolkit used by researchers to understand what neural networks learn about language."
    ],
    "4767": [
        "Our results show that ensembles from different sources can improve model performance much more greatly than ensembles from a single source.",
        "Our methods are proved effective in the MEDIQA2019 shared task.",
        "We present new methods for multi-source transfer learning for the medical domain."
    ],
    "4771": [
        "The proposed model, SDGCN, employs GCN to model the sentiment dependencies between different aspects in one sentence.",
        "Experiments on SemEval 2014 verify the effectiveness of the proposed mode, and SDGCN-BERT obtains new state-of-the-art results.",
        "The case study shows that SDGCN can pay attention to those words which are important for predicting the sentiment polarities of aspects, but also pay attention to the words which are helpful for judging the sentiment dependencies between different aspects.",
        "The two kinds of undirected sentiment graphs in this work are coarse, and we conjecture that making use of textual information to define a graph may create a better graph structure.",
        "Employing GCN to model the sentiment dependencies between different aspects in one sentence benefits from such dependencies which are always ignored in previous studies."
    ],
    "4772": [
        "The approach introduced a new way of converting between masculine-inflected and feminine-inflected noun phrases in morphologically rich languages.",
        "The approach uses a Markov random field with an optional neural parameterization to infer the manner in which a sentence must change to preserve morpho-syntactic agreement when altering the grammatical gender of particular nouns.",
        "The task of converting between masculine-inflected and feminine-inflected noun phrases has not been studied previously, and there is no existing annotated corpus of paired sentences that can be used as \"ground truth.",
        "Despite the lack of an existing annotated corpus, the approach was evaluated both intrinsically and extrinsically, achieving promising results.",
        "The approach reduces gender stereotyping in neural language models.",
        "The approach identifies avenues for future work, such as the inclusion of co-reference information."
    ],
    "4773": [
        "The proposed PP-GCN model achieves state-of-the-art performances in fine-grained social event categorization.",
        "The PP-GCN model is able to overcome the problems of large category size and sparse small number of samples per class, preventing overfitting in the tasks.",
        "Experimental results show that the PP-GCN and KIES similarity measure significantly outperform state-of-the-art baseline methods on two real-world social datasets.",
        "The proposed framework is able to learn both meta-paths weights and discriminant event instance representation.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The use of the proposed PP-GCN model and KIES similarity measure leads to significant improvements in the tasks, demonstrating the effectiveness of the approach.",
        "The framework is flexible and can be applied to other complex parameter leaning and applications.",
        "The interpretability of the different importance of meta-paths will be studied in the future."
    ],
    "4774": [
        "RE 3 QA outperforms the pipelined baseline with faster inference speed.",
        "RE 3 QA achieves state-of-the-art results on four challenging reading comprehension datasets.",
        "An end-to-end training strategy can bring in additional benefits.",
        "Future work will concentrate on designing a fast neural pruner to replace the IR-based pruning component.",
        "Developing better end-to-end training strategies.",
        "Adapting our approach to other datasets such as Natural Questions (Kwiatkowski et al., 2019)."
    ],
    "4776": [
        "Our method achieves comparable performance to state-of-the-art neural models on two biochemistry datasets for inter-sentence RE.",
        "All edge types are effective for inter-sentence RE.",
        "The proposed method is applicable to other relation extraction tasks.",
        "Incorporating joint named entity recognition training can further improve the performance of the proposed model.",
        "Incorporating sub-word embeddings can further improve the performance of the proposed model."
    ],
    "4783": [
        "The proposed incremental learning framework (IDS) has the potential to handle new situations and simulate new user actions after deployment.",
        "The IDS model is robust to new user actions and can update itself online by selecting the most valuable data.",
        "The usage of IDS will cumulate more and more knowledge over time as it is used more frequently.",
        "The IDS model requires no data for initialization and can learn from humans in the loop.",
        "The proposed dataset consisting of five different subsets is effective in simulating new user actions and evaluating the performance of IDS."
    ],
    "4784": [
        "The proposed Bi-directional Selective Encoding with Template (BiSET) model for abstractive sentence summarization improves the quality of summary generations.",
        "The use of high-quality existing summaries as templates contributes to better article representations and subsequent summarization.",
        "The bidirectional selective layer plays a crucial role in the performance of the model, as validated by an ablation study.",
        "The model can quickly retrieve high-quality templates from the training corpus, laying the foundation for effective article representations and summary generations.",
        "The results show that the proposed model outperforms all baseline models and sets a new state of the art.",
        "Human evaluation confirms that the generated summaries are informative, concise, and readable."
    ],
    "4785": [
        "Our approach achieves competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our cross-lingual results display some very interesting characteristics that we enumerate and attempt to explain in this section."
    ],
    "4788": [
        "The proposed 'EPAr' system outperforms all published models on the dev set on WikiHop, and achieves results competitive with the current state-of-the-art on the test set.",
        "The EPAr system outperforms all previously published models on the leaderboard test set on MedHop.",
        "The system proposes an answer candidate for every root-to-leaf chain and merges key information from all reasoning chains to make the final prediction.",
        "The system constructs a 'reasoning tree' to facilitate the explanation of its reasoning capabilities.",
        "The system achieves results competitive with the current state-of-the-art on the test set on WikiHop and outperforms all previously published models on the leaderboard test set on MedHop."
    ],
    "4789": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We have proposed PReFIL, a new CQA system that improves the state-of-the-art and surpasses human accuracy on two datasets.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments."
    ],
    "4790": [
        "We contrast the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.",
        "We propose Dynamic Memory Induction Networks (DMIN) for few-shot text classification, which builds on external working memory with dynamic routing, leveraging the latter to track previous learning experience and the former to adapt and generalize better to support sets and hence to unseen classes.",
        "We also explore multi-task cell learning for generalizability."
    ],
    "4791": [
        "The Scratchpad Mechanism effectively guides future generation by letting the decoder 'keep notes' on the encoder, resulting in state-of-the-art performance in Machine Translation, Question Generation, and Summarization on standard metrics and human evaluation across multiple datasets.",
        "Our approach decreases training time and model complexity compared to other leading approaches.",
        "Our success on a diverse set of tasks, input data, and volumes of training data underscores the generalizability of our approach and its conceptual simplicity makes it easy to add to any sequence-to-sequence model with attention."
    ],
    "4792": [
        "GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We introduce COMmonsense Transformers (COMET) for automatic construction of commonsense knowledge bases.",
        "COMET is a framework for adapting the weights of language models to learn to produce novel and diverse commonsense knowledge tuples.",
        "Empirical results on two commonsense knowledge bases, ATOMIC and ConceptNet, show that COMET frequently produces novel commonsense knowledge that human evaluators deem to be correct.",
        "These positive results point to future work in extending the approach to a variety of other types of knowledge bases, as well as investigating whether COMET can learn to produce OpenIE-style knowledge tuples for arbitrary knowledge seeds."
    ],
    "4793": [
        "The proposed Entailment-driven Extract and Edit network (E 3) achieves a new state-of-the-art result on the ShARC CMR dataset, outperforming existing systems and a new extractive QA baseline based on BERT.",
        "E 3 provides a more explainable alternative to prior work that does not model document structure.",
        "The proposed model inquires about rules that are not entailed by the conversation history.",
        "The model achieves strong performance and is able to extract implicit decision rules from text."
    ],
    "4794": [
        "Our approach for open domain Arabic QA achieves a F1 score of 61.3 and a 90.0% sentence match on ARCD, and a 27.6 F1 score on an open domain version of ARCD.",
        "We demonstrated the effectiveness of using translated data as a training resource for QA.",
        "Future work will aim to expand the size of ARCD and improve the end-to-end system by focusing on paragraph selection."
    ],
    "4795": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our novel Intra-Inter Transformer architecture for document-level LMs achieved significant reductions in perplexity and minor improvements in BLEU over very strong baselines.",
        "A combination of checkpoint averaging and EWC proved to be an effective way to regularize fine-tuning.",
        "Our systems are competitive on both English-German and German-English, especially considering the immense speed with which our field has been advancing in recent years."
    ],
    "4797": [
        "We introduce BLUE, a collection of resources for evaluating and analyzing biomedical natural language representation models.",
        "The BERT models pre-trained on PubMed abstracts and clinical notes see better performance than do most state-of-the-art models.",
        "Our benchmarking can be used to evaluate the capacity of the models to understand the biomedicine text.",
        "Moreover, our analysis sheds light on the future directions for developing biomedicine language representations."
    ],
    "4798": [
        "The introduction of sentiment-based, psycholinguistic, and demographic features improved the performance of the model, demonstrating that implicitly-learned features (although impressive) still cannot encode conversational characteristics of dementia to the extent that other, more targeted features can.",
        "A bi-directional LSTM and attention mechanism can improve the performance of a dementia detection model.",
        "Modifying the loss function to take into consideration the class imbalance in the DementiaBank dataset can improve the performance of the model.",
        "The new approach presented in this work represents the new state of the art for AD detection on the DementiaBank dataset.",
        "In the future, the authors plan to explore additional psycholinguistic, sentiment-based, and stylistic features for this task, as well as to experiment with features from other modalities.",
        "The authors plan to work towards interpreting the neural features implicitly learned by the model, in order to understand some of the latent characteristics it captures in AD patients' conversational transcripts."
    ],
    "4799": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our framework achieves more powerful reasoning ability than previous path-based methods.",
        "We find that our method suffers from the non-connectivity of KGs.",
        "In future work, we intend to improve the System 1 by allowing expanding unconnected nodes."
    ],
    "4801": [
        "We incorporated character information with RNN language models.",
        "Our proposed charn-MS-vec improved the performance of state-of-the-art RNN language models and achieved the best perplexities on Penn Treebank, WikiText-2, and WikiText-103.",
        "Moreover, we investigated the effect of charn-MS-vec on application tasks, specifically, machine translation and headline generation.",
        "Our experiments show that charn-MS-vec also improved the performance of a neural encoder-decoder on both tasks."
    ],
    "4802": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL exceeded the human baseline for FigureQA, but results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The proposed AdaBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The lattice transformer can generalize to other common lattice-based inputs, and it applies to the lattice input without probability scores.",
        "The lattice model can be trained on a regular sequential input, even if probability scores are unavailable.",
        "The encoder-decoder attention in the lattice transformer takes the key and value representations from the lattice input and aggregates the marginal scores, despite the sequential target forbidding the use of lattice self-attention in the decoder.",
        "The training or inference time for the lattice transformer is acceptable, and preprocessing the lattice input to obtain the position matrix for the whole dataset reduces almost no overhead to training and inference."
    ],
    "4803": [
        "We build a human-like conversational agent by endowing it with the ability of proactively leading the conversation.",
        "We create a new dataset named DuConv, which is used to establish baseline results for several state-of-the-art models.",
        "Experimental results show that dialogue models that plan over knowledge graph can make more full use of related knowledge to generate more diverse conversations.",
        "Our dataset and proposed models are publicly available, which can be used as benchmarks for future research on constructing knowledge-driven proactive dialogue systems."
    ],
    "4805": [
        "The proposed approach can detect and correct discrepancies between long-term generations of language models and the true distributions they estimate sequentially.",
        "The approach is based on a calibration-based approach to detect and provably correct the discrepancies.",
        "For state-of-the-art neural language models, large degradations of the entropy rate have been observed under iterative generation.",
        "A proposed first-order correction is both computationally tractable and effective.",
        "The same calibration approach has been used to derive estimators for the amount of information extracted by these models from the deep past.",
        "The work inspires a more principled line of discourse on the quality of long-term generations in language models.",
        "It remains an interesting open problem to study other \"future-aware\" generation-improving heuristics (beam search, reverse language models, GANs) in this framework of calibration."
    ],
    "4807": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Our efficient Knowledge Constraint Fine-grained Entity Typing Annotation Tool further improves entity typing process through entity linking together with some practical functions."
    ],
    "4808": [
        "The proposed unsupervised end-to-end model for generating abstractive summaries of single product reviews is competitive with or outperforms other unsupervised approaches.",
        "The model achieves competitive or better performance compared to supervised models for relatively long reviews.",
        "The induced latent discourse tree shows that the child sentences present additional information about their parent, and the generated summary abstracts the entire review.",
        "The model can be applied to other applications such as argument mining, as arguments typically have the same discourse structure as reviews.",
        "The model not only generates the summary but also identifies the argumentative structures.",
        "The induced trees can be used to identify argumentative structures, but direct comparison with a discourse parser is not possible due to the lack of human-annotated datasets.",
        "Future work includes comparing the induced trees with those of a human-annotated dataset."
    ],
    "4809": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Starting from sentiment-labeled text in resource-rich source languages, we propose an effective method to synthesize labeled code-mixed text without designing switching grammars.",
        "Augmenting scarce natural text with synthetic text improves sentiment detection accuracy."
    ],
    "4811": [
        "Our model can read words 6,000 times faster under a controlled environment and 43 times faster in a real setup than DrQA while achieving 6.4% higher EM.",
        "We believe that even further speedup and larger coverage of documents can be done with a dedicated similarity search package for dense+sparse vectors.",
        "The gap due to query-agnostic constraint still exists and is at least 6.1% EM.",
        "More effort on designing a better phrase representation model is needed to close the gap."
    ],
    "4812": [
        "The Object Relation Transformer outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Incorporating spatial relationship information into the image captioning task leads to better performance, as demonstrated by the improvement in the SPICE captioning metric.",
        "The proposed Transformer encodes 2D position and size relationships between detected objects in images, building upon the bottom-up and top-down image captioning approach.",
        "Incorporating geometric attention in the decoder cross-attention layers between objects and words is a promising next step for improving performance and interpretability of the model.",
        "The current model only takes into account geometric information in the encoder phase, suggesting that incorporating geometric attention in the decoder phase could lead to additional performance gains."
    ],
    "4813": [
        "The debiasing conceptor can successfully debias word embeddings, outperforming previous state-of-the-art 'hard' debiasing methods.",
        "The best results are obtained when lists are broken up into subsets of 'similar' words (pronouns, professions, names, etc.) , and separate conceptors are learned for each subset and then OR'd.",
        "Conceptors for different protected subclasses such as gender and race can be similarly OR'd to jointly debias.",
        "Contextual embeddings such as ELMo and BERT, which give a different vector for each word token, work particularly well with conceptors, since they produce a large number of embeddings.",
        "Embedding debiasing may leave bias which is undetected by measures such as WEAT Gonen and Goldberg (2019) ; thus, all debiasing methods should be tested on end-tasks such as emotion classification and co-reference resolution."
    ],
    "4815": [
        "We present DocRED, a large-scale document-level RE dataset.",
        "DocRED features data size, reading and reasoning over multiple sentences, and distantly supervised data.",
        "Experiments show that human performance is significantly higher than RE baseline models.",
        "There is ample opportunity for future improvement."
    ],
    "4817": [
        "The combination of i-vectors and environment-dependent unsupervised second-pass training of affine transformations significantly improves speech recognition performance.",
        "The choice of features and normalization for i-vector estimation has a large influence on adaptation performance.",
        "Environment adaptation and speaker adaptation perform best at different locations within the network.",
        "Our best single system achieves a word error rate of 10.2% on the Hub5'00 evaluation corpus when trained only on 283 hours of training data, which is state-of-the-art for a recognition system not based on system combination."
    ],
    "4818": [
        "The proposed VTQA model significantly improves visual question answering accuracy by combining visual and paragraph-captioning features through early, late, and later fusion.",
        "The model's performance is improved by using a sentence in the paragraph caption as an obvious clue for answering questions.",
        "The sentence \"two cows are grazing in a field\" gives the correct answer \"2\" directly.",
        "The model can infer the correct answer by integrating information from different sentences such as \"the man is holding a tennis racket\" and \"a man is standing on a tennis court\".",
        "The proposed method outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model has some limitations and room for improvement, such as the need to apply a filter to modify the list of predicted target entities so that entities that are same as the relationship will be rearranged."
    ],
    "4819": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets."
    ],
    "4820": [
        "Our approach improves over a strong baseline and yields a new state of the art.",
        "Our model encodes hierarchical information to a large extent despite its sequential architecture.",
        "Our approach is a general one that can be applied to other student model architectures, such as Transformers.",
        "The findings suggest that the question of structural biases continues to be relevant for improving syntactic competence, even in scalable architectures that can benefit from evergrowing amounts of training data."
    ],
    "4821": [
        "TaggedBT improves over the current state-of-the-art method of Noised Back-Translation, while also being simpler and more robust.",
        "TaggedBT performs well on all tasks.",
        "Our best BLEU score of 33.4 BLEU, obtained using Iterative TaggedBT, shows a gain of +3.5 BLEU over the highest previously published result on this test-set that we are aware of.",
        "We furthermore match or out-perform the highest published results we are aware of on WMT EnDe that use only back-translation, with higher or equal BLEU on five of seven test sets.",
        "Noising in the context of back-translation acts merely as an indicator to the model that the source is back-translated, allowing the model to treat it as a different domain and separate the helpful signal from the harmful signal.",
        "Heuristic noising techniques like those discussed here, although they produce text that may seem like a nigh unintelligible mangling to humans, have a relatively small impact on the cross-lingual signal."
    ],
    "4828": [
        "The authors introduce a family of models for text generation using VAEs with exponential family mixture as priors.",
        "They theoretically analyze the cause of mode-collapse problem in the training.",
        "They propose a method to fix the problem and effectively train the models.",
        "Their method achieves good performance in interpretable text generation.",
        "The authors introduce a family of models for text generation using VAEs with exponential family mixture as priors.",
        "They theoretically analyze the cause of mode-collapse problem in the training.",
        "They propose a method to fix the problem and effectively train the models.",
        "Their method achieves good performance in interpretable text generation."
    ],
    "4830": [
        "We have proposed a dialogue response selection model, Source-aware Recurrent Entity Network (SEntNet), that is built on top of a memory network architecture and is able to select responses aware of source-specific history for end-to-end TDSs.",
        "Experimental results suggest that SEntNet consistently outperforms the baselines for end-to-end TDS.",
        "Optimizing embeddings while training SEntNet is found to be useful for end-to-end task performance.",
        "SEntNet is more tolerant of sparse data than baselines and has the potential to handle different degrees of lexical diversity.",
        "One limitation of SEntNet is the increase of learnable parameters with introducing extra memory modules.",
        "However, the parallel update mechanism design inherited from EntNet can offset the use of the computational resources.",
        "This mechanism makes SEntNet scalable to real-world systems that have to deal with even more sources of information."
    ],
    "4831": [
        "We present a multi-task learning architecture to learn NMT for search query translation.",
        "The ranking effectiveness of our proposed architecture was evaluated using sentences from the target side of the parallel corpus as queries to retrieve relevant documents.",
        "One big challenge in this landscape is to sample meaningful queries from sentences as sentences do not directly convey information need.",
        "In the future, we hope to learn models that are able to sample search queries or information needs from sentences and use the output of that model to get relevant documents."
    ],
    "4835": [
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The proposed IMN introduces a novel message passing mechanism that allows informative interactions between tasks, enabling the correlation to be better exploited.",
        "IMN is able to learn from multiple training data sources, allowing fine-grained token-level tasks to benefit from document-level labeled corpora.",
        "The proposed architecture can potentially be applied to similar tasks such as relation extraction, semantic role labeling, etc."
    ],
    "4837": [
        "We presented the task of open domain event extraction, extracting unconstrained types of events from news clusters.",
        "A novel latent variable neural model was investigated, which explores latent event type vectors and entity mention redundancy.",
        "In addition, GNBusiness dataset, a largescale dataset annotated with diverse event types and explainable event schemas, is released along with this paper.",
        "To our knowledge, we are the first to use neural latent variable model for inducing event schemas and extracting events."
    ],
    "4838": [
        "The proposed approach of imposing a separate utterance rewriter improves multi-turn dialogue modelling.",
        "The rewriter is trained to recover coreferred and omitted information of user utterances.",
        "The high-quality manually annotated dataset and Transformer-pointer based architecture improve the performance of the utterance rewriter.",
        "The integrated utterance rewriter significantly improves intention detection and user engagement in two online chatbot applications.",
        "The collected dataset and proposed model can benefit future related research."
    ],
    "4840": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We proposed a way to train embeddings that directly represent a graph-based similarity measure structure.",
        "Our model, path2vec, relies on both global and local information from the graph and is simple, effective, and computationally efficient.",
        "Our approach generalizes well across graphs (WordNet, Freebase, and DBpedia).",
        "We integrated it into a graph-based WSD algorithm, showing that its vectorized counterpart yields comparable F1 scores on three datasets.",
        "Path2vec enables a speed-up of up to four orders of magnitude for the computation of graph distances as compared to 'direct' graph measures."
    ],
    "4842": [
        "The model can locate the answer without multi-hop reasoning in certain cases, exploiting reasoning shortcuts.",
        "Adversarial examples can significantly drop the performance of a state-of-the-art model.",
        "Training on adversarial data can improve the baseline's performance on adversarial evaluation.",
        "Using a control unit that dynamically attends to the question can guide the bi-attention in multi-hop reasoning, improving robustness against adversarial examples.",
        "This 2-hop model outperforms the baseline after being trained with adversarial data, and achieves further improvements on the adversarial evaluation.",
        "The proposed approach of combining explicit compositional reasoning with adversarial training can lead to improved performance and robustness."
    ],
    "4851": [
        "The proposed task of zero-shot entity linking has the potential to be a valuable benchmark for evaluating entity linking models, especially those focused on specialized domains where labeled mentions are not available.",
        "The multi-world dataset constructed for this task can be used as a shared benchmark for entity linking research, and can help evaluate the performance of different models in various domains.",
        "Combining powerful neural reading comprehension with domain-adaptive pre-training provides a strong baseline for the proposed task.",
        "Future variations of the task could incorporate NIL recognition and mention detection, which could further improve the performance of entity linking models.",
        "The candidate generation phase has significant room for improvement, and models that jointly resolve mentions in a document would perform better than resolving them in isolation."
    ],
    "4855": [
        "The proposed multi-graph decoding and rescoring scheme for speech recognition with code-switching (CS) improves the monolingual ASR performance on high-resourced languages.",
        "Using multiple graphs in parallel provides considerable improvement in the monolingual ASR performance, reducing the WERs by 3.7% to 16.3% compared to the baseline single-graph system.",
        "The proposed technique computes well-calibrated scores for hypotheses stored in separately trained graphs using a shared acoustic model.",
        "The ASR results show that the proposed technique improves the performance of the FAME! corpus, with WERs reduced from 23.1% to 20.4% on the development and test sets."
    ],
    "4858": [
        "The use of language model pre-training can achieve new state-of-the-art results for named entity recognition in Historic German.",
        "Language model pre-training can be a strong competitor to CRFonly methods for low-resource domains like named entity recognition for Historic German.",
        "Using synthetic masked language model pre-training (SMLM) can achieve comparable results for Historic named entity recognition, even when trained only on contemporary texts.",
        "The effectiveness of language model pre-training is better than using transfer learning with labeled datasets for Historic named entity recognition."
    ],
    "4859": [
        "We propose a curriculum-based transfer learning approach that allows us to train a very competitive SLU end-to-end system from speech that gets state-of-the-art results.",
        "This approach can also be applied in order to train a model dedicated to a new slot filling task from an already pre-trained model (here ASR \u2022 N ER), in the same spirit as the BERT model for textual language understanding.",
        "We think we will outperform soon the current state-of-art approach by injecting external information.",
        "Our current investigations on speaker adaptation and language transfer for the MEDIA task, not presented in this paper by lack of space, also provide very competitive and complementary results."
    ],
    "4862": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Developing annotated datasets from multiple sources based on existing datasets and the causality tagging scheme can improve the performance of SCITE.",
        "Combining SCITE with distant supervision and reinforcement learning can achieve better performance without requiring a large, high-quality annotated corpus for causality extraction.",
        "The Image Editing Request dataset contains image pairs and human-annotated editing instructions, which can be used to improve the ability to capture visual relationships.",
        "Merging three datasets by learning a joint image representation or transferring domain-specific knowledge can further explore the possibility of enlarging the Image Editing Request dataset with newly-released posts on Reddit and Zhopped."
    ],
    "4864": [
        "The field of speaker diarization has seen significant changes in the past two years, with a renewed interest in the field.",
        "The DIHARD I challenge demonstrated that there is interest in robust diarization, and this interest is evident in the recent announcement of the Fearless Steps challenge.",
        "The number of registered teams for DIHARD II has more than doubled compared to DIHARD I, indicating a significant increase in interest and effort put into speaker diarization.",
        "Existing evaluation metrics for speaker diarization are forgiving and can lead to miscomparison of systems, making it difficult to compare and improve diarization systems.",
        "The goal of truly robust diarization is still a challenge, and marked progress towards this goal is needed."
    ],
    "4865": [
        "Training corpora is insufficient due to the domain nature of clinical text.",
        "Pretrained language models, transfer learning methods, and data augmentation can be used to boost the train instances.",
        "The BioBERT pretrained on bio-medical corpus shows better performance than BERT on the general domain corpus.",
        "The CompAggr with bio-ELMO and the BioBERT behave differently in classifying the MedNLI dataset due to the difference in their own architecture.",
        "Transfer learning with NLI tasks in the general domain does not hurt the ability of the BioBERT capturing language representations of the clinical domain.",
        "The BioBERT transfers positive knowledge from general NLI tasks to the MedNLI task.",
        "A abbreviation expansion method needs particular care when adopting, as it may hurt the model to predict the conditional probability distribution of the task."
    ],
    "4868": [
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The proposed CS detection system provides the lowest EERs on development and test set of the FAME! corpus.",
        "Using language posterior for CS detection yields more robust detection with a considerably reduced number of false alarms due to the improved alignment of the same-language phones."
    ],
    "4870": [
        "We presented transfer learning and active learning frameworks for entity resolution with deep learning and demonstrated that our models can achieve competitive, if not better, performance as compared to state-of-the-art learning-based methods while only using an order of magnitude less labeled data.",
        "Although our transfer learning alone did not suffice to construct a reliable and stable entity resolution system, it contributed to faster convergence and stable performance when used together with active learning.",
        "Our frameworks of transfer and active learning for deep learning models are potentially applicable to low-resource settings beyond entity resolution.",
        "The use of transfer learning and active learning can provide a unified data integration method for downstream NLP tasks."
    ],
    "4871": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but the results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "BERT and XLNet perform partial prediction, but XLNet captures more dependency pairs than BERT.",
        "PReFIL has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments."
    ],
    "4873": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Developing annotated datasets from multiple sources based on existing datasets and our causality tagging scheme can improve the performance of SCITE.",
        "Combining our method with distant supervision and reinforcement learning can achieve better performance without having to build a high-quality annotated corpus for causality extraction."
    ],
    "4875": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed framework for learning binary and general-purpose sentence representations allows for efficient storage and fast retrieval over massive corpora.",
        "A regularized autoencoder augmented with semantic-preserving loss exhibits the best empirical results, degrading performance by only around 2% while saving over 98% memory footprint.",
        "Two other model variants with a random projection or PCA transformation require no training and demonstrate competitive embedding quality even with relatively small dimensions."
    ],
    "4876": [
        "Our approach outperforms other knowledge graph completion models on metrics such as Mean Rank and MRR.",
        "The learned transformations in our approach yield semantically meaningful results.",
        "Our approach is robust to scarce text descriptions.",
        "Combining our model with approaches like ConMask can be an interesting direction for future work.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR."
    ],
    "4878": [
        "Our approach achieves a new state-of-the-art performance on the UN parallel document mining task (en-fr, en-es).",
        "Document embeddings computed by simply averaging sentence embeddings provide a very strong baseline for clean datasets.",
        "Hierarchical embedding models perform best on noisier data.",
        "Document embeddings based on aggregations of sentence embeddings are surprisingly robust to variations in sentence embedding quality, particularly for our hierarchical models."
    ],
    "4880": [
        "look-ahead of user experience as the key to generating empathetic responses",
        "our approach is an effective way to generate more empathetic responses compared to models that condition on current user emotional state, or maximize the sentiment of the generated response itself.",
        "utilizing a reinforcement learning framework to encourage more empathetic responses.",
        "the predicted score is utilized as the reward under a reinforcement learning framework",
        "the approach of looking ahead of user experience to generate more empathetic responses."
    ],
    "4882": [
        "We propose DISTRE, a Transformer with an attentive selection mechanism for multi-instance learning scenario.",
        "DISTRE achieves a lower precision for the 300 top ranked predictions, but observes a state-of-the-art AUC and balanced performance, especially for higher recall values.",
        "Our approach predicts a larger set of distinct relation types with high confidence among the top predictions.",
        "In contrast to RESIDE, our approach only utilizes features implicitly captured in pre-trained language representations, allowing for increased domain and language independence.",
        "Our approach does not require explicitly provided side information or linguistic features, which could lead to an increased error reduction.",
        "We plan to further investigate the extent of syntactic structure captured in deep language language representations.",
        "DISTRE's generic architecture allows for integration of additional contextual information, such as background knowledge about entities and relations, which could prove useful to further improve performance."
    ],
    "4884": [
        "The LASER method outperforms the LASER local model by 0.4 BLEU in certain scenarios, indicating that it can function well in zero-shot scenarios.",
        "The pretrained LASER model performs better than the Bicleaner method for Nepali-English.",
        "Additional supervision can improve the performance of the LASER method for languages it is being tested on.",
        "The LASER local model provides much better results for Nepali-English compared to the pretrained LASER model.",
        "The LASER method can be improved by incorporating additional data and supervision."
    ],
    "4885": [
        "We propose a multi-task architecture that jointly trains a model to perform the relation identification task with cross-entropy loss and the relation classification task with ranking loss.",
        "To mitigate the problem of having too many negative instances, we introduce the embeddings of characterwise/word-wise BIO tag from the named entity recognition task to enrich the input representation.",
        "Experiment results on ACE 2005 Chinese and English corpus show that our proposed approach can successfully address the data imbalance problem and significantly improve the performance, outperforming the state-of-the-art models in terms of F1-score.",
        "We find BIO tag embeddings very effective, which we believe could be used as a general part of character/word representation."
    ],
    "4887": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "The ConMask model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "BilLex outperforms state-of-the-art methods on word and sentence translation tasks."
    ],
    "4888": [
        "Our approach achieves new state-of-the-art results on the ProPara dataset for procedural text comprehension.",
        "We have developed a task-and model-general learning framework called LaCE, which leverages the expectation of consistency between multiple independent descriptions to improve the system's performance.",
        "Our approach can benefit from unlabeled paragraphs through semi-supervised learning, something that prior systems for this task were unable to do.",
        "We have identified several avenues for further improvement, including the potential for gaining additional improvements."
    ],
    "4890": [
        "The proposed approach significantly outperforms a state-of-the-art grapheme and wordpiece model by 16% and 8%, respectively, in terms of relative WER reductions.",
        "Biasing at the phoneme level enables the avoidance of the OOV problem in the wordpiece model.",
        "Wordpiece biasing is complimentary to phoneme biasing and adds an additional 2% reduction.",
        "Exploring longer phonemic units such as phoneme pieces for biasing may further improve performance."
    ],
    "4896": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The Con-Mask model ranked the correct screenwriter David Duncan as the 2nd candidate, but the name 'David Duncan' does not actually appear in the film's description.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "RNNs with a gating mechanism (LSTM, GRU, and QRNNs) are the first computational models of natural language that have the capacity to reproduce the long memory in natural language text.",
        "The LSTM models were the best among the neural language models, as their long memory behavior was closer to that of the original text as compared to the GRU and QRNN models.",
        "Our analysis demonstrated that RNNs with a gating mechanism (LSTM, GRU, and QRNNs) are the first computational models of natural language that have the capacity to reproduce the long memory in natural language text."
    ],
    "4901": [
        "We introduced variational sequential labelers for semi-supervised sequence labeling.",
        "Our best models use multiple latent variables arranged in a hierarchical structure.",
        "We demonstrate systematic improvements in NER and POS tagging accuracy across 8 datasets over a strong baseline.",
        "We also find small, but consistent, improvements by using unlabeled data."
    ],
    "4905": [
        "The proposed method of Dynamic Memory Induction Networks (DMIN) achieves new state-of-the-art results on few-shot text classification tasks.",
        "The use of external working memory with dynamic routing in DMIN allows for better adaptation and generalization to support sets and unseen classes.",
        "The model is able to track previous learning experience and adapt to new data, leading to improved performance.",
        "The use of dynamic memory as a learning mechanism has the potential to be more general than what has been used in this study.",
        "The proposed method outperforms state-of-the-art published systems on benchmark datasets."
    ],
    "4906": [
        "The model achieves accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our method mainly considers the compression of multi-head attention but has not changed other layers in Transformer.",
        "The output of the original attention can be represented by summing over the 3-order tensor.",
        "Reducing parameters alleviates overfitting, and relatively large dimensions of the word embedding can lead to overfitting, resulting in performance degradation.",
        "Our model requires a relatively small dimension of the embedding compared with the original Transformer."
    ],
    "4907": [
        "Our proposed method uses a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow information to flow across segments, so that the model could have knowledge beyond the current segment when selecting answers.",
        "Our approach outperforms benchmark models across different datasets.",
        "We propose two self-supervised tasks to tackle the training data bottleneck.",
        "Our method can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our method has two problems in selecting appropriate event causality relations: over-generalization and lack of response naturalness.",
        "We propose a selection of response candidates generated from a neural conversational model (NCM) utilizing event causality relations.",
        "The proposed method can be applied to any languages that have a semantic parser, because it uses predicate-argument structure based event expressions."
    ],
    "4913": [
        "The use of attention mechanisms and contextualized word embeddings can improve the performance of sequence processing tasks, as shown by recent improvements in NLP.",
        "The addition of an attention layer as an additional encoding of the input does not improve upon the current state-of-the-art approach of a Bi-LSTM.",
        "The attention mechanism fails for a low-dimensional vector space.",
        "Contextualized word embeddings can slightly improve the performance of the baseline architecture, but there is no advantage over non-contextualized embeddings.",
        "The current state-of-the-art approach for the task of AM is based on a Bi-LSTM architecture."
    ],
    "4914": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "We propose RUBi to reduce unimodal biases learned by Visual Question Answering (VQA) models.",
        "RUBi is a simple learning strategy designed to be model agnostic.",
        "It is based on a question-only branch that captures unwanted statistical regularities from the question modality.",
        "We demonstrate a significant gain of +5.94 percentage point in accuracy over the state-of-the-art result on VQA-CP v2, a dataset specifically designed to account for question biases.",
        "We also show that RUBi is effective with different kinds of common VQA models."
    ],
    "4918": [
        "Our proposed method uses word saliency and SmoothGrad to interpret word alignments from NMT predictions, and is model-agnostic, applicable either offline or online, and does not require any parameter updates or architectural change.",
        "Our method is capable of generating word alignment interpretations of much higher quality compared to its attention-based counterpart.",
        "Even without any special architecture or training algorithm, some NMT models have already implicitly learned interpretable word alignments of comparable quality to fast-align."
    ],
    "4922": [
        "The proposed approach of grounding the input text to reduce the cognitive load of the annotator is effective in improving the coherence and inter-annotator agreement in coreference annotation.",
        "The use of a pre-populated entity list for selecting antecedents results in faster and more coherent annotations compared to manual span annotation.",
        "The grounded task leads to cleaner annotation patterns than the span annotation task, as observed throughout the dataset.",
        "The proposed approach of grounding the input text can reduce the cognitive load of the annotator and improve the quality of coreference annotations.",
        "The use of a pre-populated entity list for selecting antecedents can increase the inter-annotator agreement in coreference annotation tasks."
    ],
    "4923": [
        "We proposed a hierarchical method for comparing natural language documents that leverages optimal transport, topic modeling, and word embeddings.",
        "Empirically, these combine to give superior performance on various metric-based tasks.",
        "Modeling documents by their representative topics is better for highlighting differences despite the loss in resolution.",
        "HOTT appears to capture differences in the same way a person asked to compare two documents would: by breaking down each document into easy to understand concepts, and then comparing the concepts.",
        "Our use of a nested Wasserstein metric suggests further analysis of this hierarchical transport space.",
        "End-to-end training that efficiently optimizes these three components jointly would likely improve performance and facilitate analysis of our algorithm as a unified approach to document comparison.",
        "The performance improvements we observe stem directly from a reduction in the size of the transport problem.",
        "Investigation of larger corpora with longer documents, and applications requiring the full set of pairwise distances are now feasible.",
        "We also can consider applications to modeling of images or 3D data."
    ],
    "4927": [
        "We introduce a neural pedagogical agent for real-time user modeling of response correctness.",
        "We demonstrate improved and more efficient performance over existing methods.",
        "We integrate our method into a smart review system which addresses characteristic problems of mobile education platform users.",
        "For future work, we plan to experiment with additional network architectures such as Transformers [39].",
        "We plan to apply the principles from this paper to related downstream tasks."
    ],
    "4932": [
        "Our new dataset of manually curated argument trees opens up interesting avenues for research in argumentation.",
        "It is easier to predict the stance of claims with a parent-child relationship, while relative specificity is more difficult to predict.",
        "Future work may be interested in understanding which other models would be effective in claim specificity and stance detection tasks.",
        "Developing techniques to incorporate claim stance and specificity detection models in argument generation for more coherent and consistent arguments is a promising research direction."
    ],
    "4933": [
        "We explored automated CTA transcript parsing, which is a challenging task due to the lack of direct supervision data and the requirement of document level understanding.",
        "We proposed a weakly supervised framework to utilize the full information in data.",
        "We noticed the importance of context in the CTA parsing task and exploited model variants to make use of context information.",
        "Our evaluation on manually labeled test set shows the effectiveness of our framework."
    ],
    "4938": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "Our method for discovering root-and-pattern morphology in Semitic languages is unsupervised and validated through intrinsic and extrinsic evaluations.",
        "Our root extraction method (JZR) performs well compared to a rule-based language-specific root extractor.",
        "Our contextual emotion classifier, consisting of the transferable language model and dynamic max pooling, successfully alleviates the three inherent problems in the EmotionX shared task and outperforms the previous state-of-the-art model."
    ],
    "4941": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We have demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks."
    ],
    "4946": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "Our modified models achieve up to 1.4 BLEU (0.9 BLEU on average) improvement on 5 standard WMT datasets, at a small cost in computing time and model size.",
        "Lexical connections are useful to both encoder and decoder, and remain effective when included in smaller models.",
        "The addition of shortcuts noticeably reduces the similarity of hidden states to the initial embeddings, indicating that dynamic lexical access aids the network in learning novel, diverse information.",
        "Ablation studies comparing different shortcut variants demonstrated that one effect of lexical shortcuts is an improved WSD capability.",
        "The presented findings offer new insights into the nature of information encoded by the transformer layers, supporting the iterative refinement view of feature learning."
    ],
    "4947": [
        "Our low-resource system outperforms prior work on FST-based GEC for grammatical error correction.",
        "Our restricted track submission is a purely neural system based on standard NMT and LM architectures.",
        "Techniques used in machine translation, such as oversampling, back-translation, and fine-tuning, are also useful for grammatical error correction.",
        "Our models have been used in a joint submission with the Cambridge University Computer Lab."
    ],
    "4948": [
        "We presented a range of sentiment grammers for using neural networks to model sentiment composition explicitly.",
        "Empirically showed that explicit modeling of sentiment composition with fine-grained sentiment subtypes gives better performance compared to state-of-the-art neural network models in sentiment analysis.",
        "By using EMLo embeddings, our final model improves fine-grained accuracies by 1.3 points compare to the current best result."
    ],
    "4949": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Our models outperform several baselines by a large margin.",
        "Extensive analysis shows that our model can decrease the label confusion compared to previous work, especially for reflections and rare labels.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrval, table reconstruction, and enabling better understanding of charts by people with visual impairments.",
        "We proposed PReFIL, a new CQA system that improves the state-of-the-art and surpasses human accuracy on two datasets."
    ],
    "4951": [
        "We introduce a self-supervised task, inconsistent order detection, to explicitly capture the order signal of the dialogue.",
        "Previous methods suffer from forgetfulness problem when modeling dialogue history.",
        "Our SSN approximately encodes the dialogue history and highlights the order signal.",
        "We show how our SSN can contribute to real dialogue learning.",
        "Our method advances the previous state-of-the-art dialogue systems in both open-domain and task-oriented scenarios.",
        "Theoretically, we believe this self-supervision can be generalized to other types of temporal order in different NLP tasks."
    ],
    "4952": [
        "We introduced BERTPHONE, a self-attentive, phonetically-aware, acoustic contextual representations which can be used with small task-specific models to jointly improve performance on multiple speech tasks.",
        "Future work can explore the additional gains from unfreezing BERTPHONE as done in the original BERT work, although this removes the multi-functional property of our representations.",
        "In addition to tuning \u03bb, one could also try using intermediate layers to improve performance.",
        "One can also evaluate the use of BERTPHONE for speech recognition pretraining by adding further layers and implementing CTC decoding.",
        "The L 1 loss can be used by itself on unlabeled audio, suggesting the possibility of training on larger, unlabeled audio corpora."
    ],
    "4953": [
        "The proposed neural network architecture achieves a new state-of-the-art F1 score on nested NER tasks, with an improvement of nearly 8 F1 points over the previous best result.",
        "The architecture is also competitive at flat NER tasks, despite being trained only for nested NER.",
        "The proposed approach provides intuitive embeddings for a variety of multi-word entities, which could be useful for downstream tasks such as entity linking and coreference resolution.",
        "The architecture performs well on both nested and flat NER tasks, demonstrating its versatility and adaptability to different types of natural language processing tasks."
    ],
    "4956": [
        "We studied the problem of learning accurate embedding for Out-Of-Vocabulary words.",
        "We formulated the problem as a K-shot regression problem and proposed a hierarchical context encoder (HiCE) architecture that learns to predict the oracle OOV embedding by aggregating only K contexts and morphological features.",
        "We further adopt MAML for fast and robust adaptation to mitigate semantic gap between corpus.",
        "Experiments on both benchmark corpus and downstream tasks demonstrate the superiority of HiCE over existing approaches."
    ],
    "4957": [
        "We propose a novel method for question generation called Weak Supervision Enhanced Generative Network.",
        "Our approach first leverages easily reachable labels to train a discriminator for matching passage-answer pairs, capturing semantic relations between passages and answers.",
        "We design our question generator with the Multi-Interaction method to transfer the knowledge of this discriminator dynamically, considering obtaining more fine-grained information.",
        "The experimental results show the effectiveness of our approach in automatic evaluations and human evaluations."
    ],
    "4958": [
        "The model EQuANt extends QANet to cope with unanswerable questions.",
        "The performance of EQuANt was evaluated and found to be effective in predicting answerability.",
        "The lightweight QANet implementation was laid out in detail, and the 3 EQuANt architectures were trained.",
        "The context-query attention maps within the lightweight QANet were investigated, allowing for insight into why the initial architecture did not predict answerability effectively.",
        "Multi-task learning is a valuable approach in the context of MRC, as suggested by the observed performance of EQuANt 3 on SQuAD 1.1."
    ],
    "4965": [
        "Our proposed approach for learning playlist embeddings is effective in capturing the semantic properties of playlists.",
        "Our approach can be used for tasks such as playlist discovery and recommendation.",
        "Integrating content-based song-embedding models can further improve the performance of our approach.",
        "Using variational sequence models can generate new playlists with desired properties.",
        "Our proposed model outperforms two BoW-based models on various tasks in natural language processing and music."
    ],
    "4967": [
        "This work presents an unsupervised method for discovering root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR).",
        "The performance of the method is not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "The introduced WHAM! dataset is used to benchmark several speech enhancement and speech separation approaches.",
        "Initial results show that T-F based separation approaches still perform effectively in the presence of noise.",
        "Future work includes evaluating stereo approaches for noisy speech separation, evaluating robustness to reverberation plus noise, and further exploration of the convolutional models discussed in Section 4.3."
    ],
    "4971": [
        "We have described a framework to leverage the complementary nature of constituency and dependency parsing.",
        "It combines multi-task learning, auxiliary tasks, and sequence labeling parsing, so that constituency and dependency parsing can benefit each other through learning across their representations.",
        "We have shown that MTL models with auxiliary losses outperform single-task models.",
        "MTL models that treat both constituency and dependency parsing as main tasks obtain strong results, coming almost at no cost in terms of speed.",
        "Source code will be released upon acceptance."
    ],
    "4972": [
        "Nonverbal features from speech signals are most effective as a class for predicting group performance.",
        "Verbal features are also effective for predicting group performance and deserve an increased focus in the computing literature.",
        "The GAP corpus is a new dataset that is significantly larger than previous datasets used in this area (e.g., [8] ).",
        "The GAP corpus is being made available for research purposes, addressing the limited availability of small groups corpora.",
        "Future work will attempt to further improve performance on the task through data augmentation and domain adaptation.",
        "Finer-grained verbal features that capture information about how language changes over the course of a conversation can be extracted in future work."
    ],
    "4973": [
        "The proposed attention layer presents a unified mechanism to aggregate general and contextual information, extending the self-attention layer of a transformer with persistent vectors that can store complementary information.",
        "The persistent vectors in the attention layer can replace the feedforward layers in a transformer network with no loss of performance, simplifying the model architecture and helping to better understand how information is processed and stored.",
        "The proposed mechanism can help improve the performance of transformer-like sequence models, enabling better understanding of how information is processed and stored in these models."
    ],
    "4976": [
        "Our approach significantly outperforms the state-of-the-art transformer baseline.",
        "Our proposed strategy is a general approach that can be universally applicable to other model architectures, including LSTM and CNN.",
        "We will further explore efficient strategies that can jointly train all modules of the deep model with minimal increase in training complexity.",
        "Our approach constructs deeper models with better performance.",
        "The proposed strategy is effective in constructing and training deep NMT models."
    ],
    "4977": [
        "Our approach outperforms recent alternative approaches to goal location prediction, and achieves credible results on the full VLN task without using RL or data augmentation.",
        "Our approach achieves unprecedented interpretability and less reliance on the simulator's navigation constraints.",
        "Our method can handle the complex problem of Vision-and-Language Navigation (VLN) without using RL or data augmentation.",
        "Our approach maintains a semantic spatial map of the environment, and an explicit probability distribution over alternative possible trajectories in that map.",
        "We show that instruction following can be formulated as Bayesian state tracking in our model."
    ],
    "4980": [
        "The proposed hierarchical multi-task learning framework for discourse coherence generalizes well to different domains and prediction tasks, achieving a new state of the art in terms of coherence assessment.",
        "The use of contextualized embeddings (e.g., BERT, Devlin et al. (2018) ) for coherence assessment may allow multi-task learning frameworks to learn complementary aspects of language.",
        "The framework demonstrated effectiveness against a number of baselines not only on standard binary evaluation coherence tasks, but also on tasks involving the prediction of varying degrees of coherence.",
        "The use of contextualized embeddings has been shown to carry syntactic information of words (Tenney et al., 2019)."
    ],
    "4985": [
        "We address the dialog utterance generation problem by jointly modeling previous dialog context and discrete dialog attributes.",
        "Composed dialog attributes help generate interesting responses.",
        "We formulate the dialog attribute prediction problem as a reinforcement learning problem.",
        "We fine tune the attribute selection policy network trained with supervised learning using REINFORCE and demonstrate improvements in diversity scores compared to the Seq2Seq model.",
        "In the future, we plan to extend the model for additional dialog attributes like emotion, speaker persona etc. and evaluate the controllability aspect of the responses based on the dialog attributes."
    ],
    "4986": [
        "The proposed 1D convolutional DNN is capable of learning spatio-temporal feature representations from raw audio data, and its performance is comparable with models trained on current audio feature extraction toolkits.",
        "The end-to-end method emphasizes generalizability and transferability to other domains, e.g., in computational paralinguistics, contrary to problem-specific feature engineering.",
        "Our proposed architecture is especially suitable for context-aware multimedia recommendation systems, where the system could recommend e.g., radio stations or songs dependent on the fatigue level, detected in the user's voice.",
        "The system could be used to recommend media content based on the user's fatigue level, which is detected using the proposed 1D convolutional DNN.",
        "Future work could deepen the extent of architecture search and parameter tuning as other automatic ML approaches suggest, to ultimately further democratize AI research."
    ],
    "4989": [
        "the limited success of well-established solutions when restricted to in-domain data\" - This claim highlights the challenge of tackling low-resource machine translation tasks using established methods, and suggests that there is room for improvement.",
        "a multilingual multistage fine-tuning approach substantially improves Ja\u2194Ru translation by over 3.7 BLEU points compared to a strong baseline\" - This claim demonstrates the effectiveness of the proposed approach in improving machine translation performance, specifically in the Ja\u2194Ru direction.",
        "we hope that our paper can act as a guideline to researchers attempt-ing to tackle extremely low-resource translation\" - This claim suggests that the authors are providing a useful resource for researchers working on low-resource machine translation tasks.",
        "In the future, we plan to confirm further finetuning for each of specific translation directions\" - This claim indicates that the authors intend to continue improving the proposed approach and exploring its applicability in different translation directions.",
        "we will also explore the way to exploit out-of-domain pseudo-parallel data, better domain adaptation approaches, and additional challenging language pairs\" - This claim suggests that the authors are considering a range of potential improvements for their proposed approach, including exploring alternative sources of data and adapting to different language pairs."
    ],
    "4990": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The approach proposed in this work can generate section titles that lead to strong improvements in reading comprehension tasks.",
        "The proposed method outperforms sequence-to-sequence approaches in low-resource domains.",
        "The use of a novel approach to section title generation can improve the readability of text.",
        "The open-sourcing of code used in this contest can further enable research on this task in the future.",
        "The dataset collected for this study is 16 times larger than the competition dataset, allowing for more effective pre-training of a language model."
    ],
    "4992": [
        "The proposed system does not require task-specific labeled data.",
        "Our system outperforms existing state-of-the-art zero-shot systems by a significant margin.",
        "We show that without reliance on task-specific supervision, one can achieve relatively robust transfer across datasets."
    ],
    "4993": [
        "The proposed model achieves state-of-the-art performance for RE on both the general setting and cross-domain setting.",
        "Using dependency trees indirectly in a multi-task learning framework improves the performance of the model.",
        "Jointly predicting dependency relations between words and relations between entity mentions of interest is effective.",
        "A new control mechanism that regulates the information flow in the model based on given entity mentions for RE and gating techniques is proposed.",
        "The model outperforms state-of-the-art baselines for RE on both the general setting and cross-domain setting."
    ],
    "4994": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "NIESR achieves significant boosts in performance on ASR."
    ],
    "4997": [
        "Proposed CAMIT model improves the interactive efficiency of NMT models in both decoding and learning aspects.",
        "The proposed sequential bi-directional decoder updates the whole sentence after each revision, leading to improved decoding efficiency.",
        "The proposed approach exploits interaction history for sentence-level and word-level learning, resulting in improved learning efficiency.",
        "Experimental results show that the CAMIT model significantly improves the interactive efficiency of NMT models."
    ],
    "4998": [
        "Our models can effectively preserve salient source relations in summaries.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "Our detailed observations can provide more hints for the follow-up researchers to design more powerful learning frameworks."
    ],
    "5000": [
        "We proposed a knowledge-aware pronoun coreference resolution model that can leverage different external knowledge for this task.",
        "The proposed model is an attempt of the general solution of incorporating knowledge (in the form of KG) into the deep learning based pronoun coreference model, rather than using knowledge as features or rules in a dedicated manner.",
        "Experimental results on two different corpora from two domains demonstrate the superiority of the proposed model to all baselines.",
        "Our model learns to use knowledge rather than just fitting the training data, our model achieves much better and more robust performance than state-of-the-art models in the cross-domain scenario."
    ],
    "5003": [
        "We introduce an intrinsic way of comparing neural machine translation architectures by looking at the nearest neighbors of the encoder hidden states.",
        "The transformer model is superior in terms of capturing lexical semantics, while the recurrent model better captures syntactic similarity.",
        "The hidden state representations capture quite different information than what is captured by the corresponding embeddings.",
        "The hidden states capture more of WordNet relations of the corresponding word than they capture from the nearest neighbors of the embeddings.",
        "The reverse recurrent layer captures more lexical and semantic information, whereas the forward recurrent layer captures more long-distance, contextual information."
    ],
    "5007": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR), with performance not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "Neural parsers generalize surprisingly well, and are able to draw benefits both from pre-trained language representations and structured output prediction.",
        "These properties allow single-model parsers to surpass previous state-of-the-art systems on out-of-domain generalization."
    ],
    "5008": [
        "\"Our methods perform better than traditional training methods in combating annotation biases and obtaining a less biased representation.",
        "\"Our methodology can be extended to handle biases in other tasks where one is concerned with finding relationships between two objects.",
        "\"Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "\"Removing the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "\"Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\")."
    ],
    "5009": [
        "Our employment of two adversarial learning techniques to a general NLI model has helped reduce hypothesis-only biases in the model.",
        "These techniques may help models exhibit fewer hypothesis-only biases, and we hope this work will encourage the development and analysis of models that ignore such biases.",
        "Our work may contribute to the development and analysis of models that include components that ignore hypothesis-only biases, as well as similar biases discovered in other natural language understanding tasks.",
        "Our techniques may be useful for removing language biases in visual question answering tasks, where recent work has considered similar adversarial techniques."
    ],
    "5013": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The HMM can be used as a suitable approximator for the LSTM, and using this similarity one can develop a hybrid model which may simplify the model training behavior of the LSTM or increase its respective performance.",
        "Modifying the GPU-based code can improve the training times of the model and reduce reliance on CPU-based computation.",
        "The results of this paper justify that the HMM and LSTM have structural similarities, especially at lower dimensionalities, but the cosine similarity may increase with an increase in hidden state dimensionality in complex corpora like the War and Peace dataset."
    ],
    "5020": [
        "The present study has achieved a milestone in building on five years of multilingual NMT research.",
        "There is still a long way to go towards truly universal machine translation.",
        "Many promising solutions for future directions are interdisciplinary, making multilingual NMT a plausible general test bed for other machine learning practitioners and theoreticians."
    ],
    "5021": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The proposed ReCoSa model can be useful for improving the quality of multiturn dialogue generation by using proper detection methods, such as self-attention.",
        "Our model outperforms existing HRED models and its attention variants in generating responses.",
        "The relevant contexts detected by our model are more accurate than those detected by other models.",
        "The proposed ReCoSa model can be used to improve the quality of multiturn dialogue generation.",
        "Our model uses self-attention to effectively capture long distant dependency relations.",
        "The use of proper detection methods, such as self-attention, can improve the quality of multiturn dialogue generation."
    ],
    "5022": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "The proposed cost-aware algorithm for interactive sequence-to-sequence learning finds more cost-efficient solutions than models learning from a single feedback type and uncertainty-based active learning models.",
        "The self-regulation module learns which type of feedback to query from a human teacher.",
        "The proposed framework can naturally be expanded to integrate more feedback modes suitable for the interaction with humans, e.g., pairwise comparisons or output rankings.",
        "Future research directions will involve the development of reinforcement learning models with multidimensional rewards and modeling explicit credit assignment for improving the capabilities of the regulator to make context-sensitive decisions in mini-batch learning."
    ],
    "5023": [
        "Our layer provides important gains on large-scale language modeling, reaching with 12 layers the performance of a 24-layer BERT-large model with half the running time.\" - This claim highlights the efficiency and effectiveness of the proposed memory layer in improving the performance of a neural network for language modeling tasks.",
        "The efficiency of our layer relies on two key ingredients: the factorization of keys as a product set, and the sparse read/write accesses to the memory values.\" - This claim emphasizes the importance of the specific design choices made in the proposed memory layer, specifically the factorization of keys and the sparse read/write accesses, in achieving efficiency gains.",
        "Our layer is integrated into an existing neural network architecture.\" - This claim indicates that the proposed memory layer can be easily integrated into existing neural network architectures, making it a versatile and practical solution for improving the performance of such models.",
        "The running time of our 12-layer model with the proposed memory layer is only half that of a 24-layer BERT-large model.\" - This claim highlights the significant reduction in running time achieved by the proposed memory layer, making it an attractive solution for large-scale language modeling tasks."
    ],
    "5025": [
        "'The widely used HRED based models simply treat all contexts indiscriminately, which violate the important characteristic of multi-turn dialogue generation.'",
        "'Our core idea is to utilize the self-attention mechanism to effectively capture the long distant dependency relations.'",
        "'We conduct extensive experiments on both Chinese customer services dataset and English Ubuntu dialogue dataset. The experimental results show that our model significantly outperforms existing HRED models and its attention variants.'",
        "'Furthermore, our further analysis show that the relevant contexts detected by our model are significantly coherent with humans' judgements.'",
        "'Therefore, we obtain the conclusion that the relevant contexts can be useful for improving the quality of multiturn dialogue generation, by using proper detection methods, such as self-attention.'",
        "'In future work, we plan to further investigate the proposed ReCoSa model.'"
    ],
    "5029": [
        "The proposed metric, nDTW, does not suffer from shortcomings of previous evaluation metrics.",
        "The many desirable properties of the proposed metric are reflected in human evaluations and VLN agents' performance improvements.",
        "The proposed SDTW captures well the success criteria of the task and the similarity between the intended and observed trajectory.",
        "Multiple measures, especially path length and navigation error, are useful for understanding different aspects of agent behavior.",
        "The community should adopt SDTW as a single summary measure for future work and leaderboard rankings."
    ],
    "5031": [
        "'ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.'",
        "'Our approach can improve the performance of news recommendation and outperform many baseline methods.'",
        "'The core of our approach is a news encoder and a user encoder.'",
        "'We propose a multi-view learning framework to learn unified news representations by incorporating titles, bodies and categories as different views of news.'",
        "'We also apply attention mechanism to news encoder to select important words and views for learning informative news representations.'",
        "'In the user encoder, we learn representations of users from their browsed news, and apply a news attention network to select important news for learning informative user representations.'"
    ],
    "5033": [
        "The embeddings of GloVe, ELMo, and BERT contain gender, religion, and nationality biases.",
        "A projection-based method can be used to attenuate these biases.",
        "The method works for static GloVe embeddings.",
        "The approach can be extended to contextualized embeddings (ELMo, BERT) by debiasing the first non-contextual layer alone.",
        "The simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "AdaBERT achieves comparable performance while significantly improving efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The fine-tuning procedure for leveraging large pretrained generative models for modeling task-oriented dialogue in multiple domains is simple and enables quick adaptation to constrained domains and domain-specific vocabularies.",
        "The framework for leveraging large pretrained generative models for modeling task-oriented dialogue in multiple domains has the potential to simultaneously improve and simplify the design of task-oriented conversational systems."
    ],
    "5034": [
        "The proposed DSS-VAE model outperforms the VAE baseline in reconstruction and unconditioned language generation.",
        "The DSS-VAE model enjoys the benefits of sampling and manipulation in terms of the syntax of a sentence.",
        "The sampling and manipulation advantages of DSS-VAE are used in two novel applications, namely unsupervised paraphrase and syntax-transfer generation, where both experiments achieve promising results.",
        "The use of DSS-VAE in unsupervised paraphrase and syntax-transfer generation leads to promising results.",
        "The DSS-VAE model is effective in language generation tasks."
    ],
    "5041": [
        "We apply sentence-level MT-based approaches, which bring significant improvements over target-domain testing data.",
        "By incorporating domain adversarial learning, the GAN-based approach learns language-invariant latent representations and consequentially transfers knowledge from the source domain.",
        "Given only a word-by-word bilingual dictionary, a GAN-based approach rivals the performance of the best MT-based approach.",
        "Integrating MT-based and GAN-based approaches yields the best results.",
        "We achieved new state-of-the-art on a Chinese QA dataset: DRCD."
    ],
    "5042": [
        "The proposed KG embedding model, R-MeN, achieves state-of-the-art performances for both triple classification and search personalization tasks.",
        "The integration of transformer self-attention mechanism-based memory interactions with a CNN decoder is effective in capturing potential dependencies in the KG triples.",
        "The proposed model, R-MeN, can be extended for multihop knowledge graph reasoning in future work.",
        "[R-MeN obtains new state-of-the-art performances for both triple classification and search personalization tasks.] (Paragraph 1)",
        "[The integration of transformer self-attention mechanism-based memory interactions with a CNN decoder is effective in capturing potential dependencies in the KG triples.] (Paragraph 1)",
        "[The proposed model, R-MeN, can be extended for multihop knowledge graph reasoning in future work.] (Paragraph 2)"
    ],
    "5046": [
        "\"Our method considers both the complex word and the context of the complex word when generating candidate substitutions without relying on the parallel corpus or linguistic databases.",
        "\"Experiment results have shown that our approach BERT-LS achieves the best performance on three well-known benchmarks.",
        "\"Since BERT can be trained on raw text, our method can be applied to many languages for lexical simplification.",
        "\"One limitation of our method is that it can only generate a single-word replacement for complex words, but we plan to extend it to support multi-word expressions.",
        "\"In the future, the pre-trained BERT model can be fine-tuned with just simple English corpus, and then we will use fine-tuned BERT for lexical simplification."
    ],
    "5047": [
        "The proposed dataset provides a new resource for QA on social media data, which informs us of the distinctiveness of social media from formal domains in the context of QA.",
        "QA on social media requires systems to comprehend social media specific linguistic patterns like informality, hashtags, usernames, and authorship.",
        "The dataset enables a deeper understanding of natural language in social media and rich applications that can extract essential real-time knowledge from social media.",
        "The generative baseline performs worse on ambiguous questions compared to the BiDAF model, possibly due to the overlapping words with contexts.",
        "The BiDAF model has potential advantage over the generative baseline on ambiguous questions."
    ],
    "5048": [
        "We apply Generative Latent Optimization to learn sentence representations in a non-compositional fashion.",
        "Our models perform competitively to several well-known sentence representation methods, both on supervised and unsupervised tasks.",
        "On unsupervised tasks, we outperform the popular uSIF method when trained on the same data."
    ],
    "5051": [
        "The authors have developed a robust machine translation system that is resistant to noise found on social media, as demonstrated by their first-place rankings in the BLEU evaluation in all translation directions.",
        "The authors used careful pre-processing and data filtering, as well as a combination of domain adaptation and robustness techniques (such as special handling of capital letters and emojis, natural noise injection, corpus tags, and back-translation) to improve the robustness of their machine translation system.",
        "The authors' system was able to preserve salient source relations in summaries, as demonstrated by results on benchmark datasets.",
        "The authors compared various system architectures and found that their models can effectively preserve salient source relations in summaries, with performance on par with or surpassing state-of-the-art published systems."
    ],
    "5057": [
        "Accurate neural models can predict socioeconomic status from text.",
        "Lexical information is highly predictive, but is restricted to topic.",
        "Syntactic information is almost as predictive and is a better signal for stylistic variation.",
        "Neural networks can perform well with relatively small datasets.",
        "Distant supervision with proxy labels for socio-economic status yields useful insights.",
        "There are limitations of distant labeling and social media data, such as issues related to the language of food.",
        "Additional variables, such as age and gender, may be important for predicting socioeconomic status.",
        "Mitigating the risk of fake reviews is an important future direction."
    ],
    "5059": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "the experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "the main problem lies in the learning of representations.",
        "for any other tasks which contain a large number of unseen samples, training, fine-tuning the model according to the performance on the seen samples alone is not fair.",
        "similar problems may exist in other NLP tasks, which will be interesting to investigate in the future."
    ],
    "5060": [
        "The proposed approach achieves state-of-the-art joint accuracy performance in WOZ 2.0 and MultiWOZ corpora.",
        "The model is able to deal with multiple domains and slot-types without increasing model size.",
        "Sharing knowledge by learning from multiple domain data helps to improve performance.",
        "The proposed model outperforms benchmark models across different datasets.",
        "The approach can continually learn new knowledge when the domain ontology is updated."
    ],
    "5061": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The effort spent on the construction of this new resource, according to the semi-automatic procedure described, is about 24 FTE 9, with an average production of about 300 examples per day.",
        "We consider this effort lower than typical efforts to create linguistic resources from scratch.",
        "The results show they all have similar features and performances. However, compared to another specific architecture for SLU, i.e., Bert-Joint, they perform worse.",
        "It was expected and it demonstrates the Almawave-SLU can be a valuable dataset to train and test SLU systems on the Italian language."
    ],
    "5065": [
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "We have found that similarity features help to improve mapping abilities of joint models based on recur-rent neural networks paired with pre-trained word embeddings or ELMo embeddings while staying roughly on par with the advanced language representation model BERT in terms of accuracy.",
        "The accuracy of BERT is 7.25% higher on test sets with a simple random split than on test sets with the proposed custom split.",
        "We have discussed some interesting future research directions and challenges to be overcome."
    ],
    "5066": [
        "We present an Event Logic Graph (ELG), which can reveal the evolutionary patterns and development logics of real world events.",
        "We propose a framework to construct ELG from large-scale unstructured texts.",
        "The techniques used in the ELG are language-independent, allowing for easy construction of other language versions of ELG.",
        "We use the ELG to improve the performance of script event prediction.",
        "All techniques used in the ELG are language-independent."
    ],
    "5067": [
        "The noncontextualized source representations in encoder-free models cause a big performance drop.",
        "The attention component in encoder-free models is a powerful feature extractor, and can partially compensate for the lack of contextualized encoder representations.",
        "Regarding the interpretability of attention, our results do not show that the attention mechanism in encoder-free models is consistently more alignment-like.",
        "Attending to source embeddings improves the alignment quality on DE\u2192EN but makes the alignment quality worse on ZH\u2192EN."
    ],
    "5070": [
        "a wide range of effective annotation tools already exist",
        "SLATE is designed for terminal-users who want a fast, easy to install, and flexible annotation tool",
        "It supports a range of annotation types, and adjudication of disagreements in annotations",
        "The code is publicly available under a permissive open-source license",
        "the tool has already been used in two research projects, including one that involved over 250 hours of annotation."
    ],
    "5071": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Developing annotated datasets from multiple sources based on existing datasets and the causality tagging scheme can improve the performance of SCITE.",
        "Combining SCITE with distant supervision and reinforcement learning can achieve better performance without requiring a large, high-quality annotated corpus for causality extraction.",
        "The unsupervised method for discovering root-and-pattern morphology in Semitic languages is validated by intrinsic and extrinsic evaluations.",
        "The extracted Semitic roots are the basic units of these languages."
    ],
    "5075": [
        "Our proposed multi-granular text encoder outperforms existing BiLSTM and CNN baselines in terms of accuracy, efficiency, and compactness.",
        "Our model can extract effective and intuitive evidence to support its predictions.",
        "The ConMask model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The relationship-dependent content masking in the ConMask model still has some limitations and room for improvement, such as the potential for entities with similar names to be ranked highly."
    ],
    "5079": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to significant BLEU gains, but there may be room for further improvement by solving label bias in general.",
        "Our method for alleviating or eliminating the beam problem is helpful and easy, and we hope to see it included to make stronger baseline NMT systems."
    ],
    "5081": [
        "The Multi-Layer Convolutional and Transformer models tended to produce data that could improve several models, though too much of it would begin to dampen performance.",
        "The most substantial improvements could be found when the size of the real data for training was quite small.",
        "The syntax-based models, PRPN and ONLSTM, are very sensitive to the quality of artificial errors and their performance drops substantially with the addition of artificial error data.",
        "Our experiments suggest that, unlike in machine translation, it is not very straightforward to use a simple back-translation approach for GEC as unrealistic errors produced by back-translation can hurt the correction performance substantially.",
        "We believe this work shows the promise of using recent neural methods in an out-of-the-box framework, though with care.",
        "Future work will focus on ways of improving the quality of the synthetic data. Ideas include leveraging recent developments in powerful language models or better controlling for diversity and frequency of specific error types."
    ],
    "5084": [
        "The proposed method can generate a large number of reviews with desired sentiments using fine-tuned GPT-2 and BERT text classification.",
        "The generated reviews have the same fluency as real reviews written by humans.",
        "Subjective evaluation shows that the generated reviews are as fluent as real reviews.",
        "The proposed method can preserve sentiment and context information using cold fusion or simple fusion.",
        "The generated reviews lack diversity and may be already covered by countermeasures, leading to increased detection errors.",
        "The proposed method needs further improvements to detect the generated reviews.",
        "The use of three countermeasures (Grover, GLTR, and GPT-2 detector) demonstrates a detection equal error rate of 22.5%."
    ],
    "5089": [
        "Our proposed Discourse Marker Augmented Network achieves state-of-the-art results on SNLI and MultiNLI datasets.",
        "We transfer the knowledge learned from the discourse marker prediction task to the NLI task to augment the semantic representation of the model.",
        "We take the various views of the annotators into consideration and employ reinforcement learning to help optimize the model.",
        "Our approach considers the choice of discourse markers and other transfer learning sources for future work."
    ],
    "5090": [
        "Our proposed data-to-text model outperformed existing models in all evaluation measures.",
        "Incorporating writer information to data-to-text models has a positive effect on performance.",
        "The proposed model produces a summary text that imitates the human writing process, leading to improved performance.",
        "Our model outperformed existing models in all evaluation measures."
    ],
    "5091": [
        "We have presented a novel 2D-CTC model for scene text recognition, which is an extension to the vanilla CTC model.",
        "Motivated by the observation that instances of scene text are actually in 2D forms, we devised and formulated 2D-CTC to describe this distinctive property and produce more precise recognition and more explainable intermediate predictions.",
        "The qualitative and quantitative results on standard benchmarks confirmed the effectiveness and advantages of 2D-CTC.",
        "Our 2D-CTC model is an extension to the vanilla CTC model, and it is motivated by the observation that instances of scene text are actually in 2D forms.",
        "The results on standard benchmarks confirmed the effectiveness and advantages of 2D-CTC, which produces more precise recognition and more explainable intermediate predictions."
    ],
    "5092": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA. (Paragraph 1)",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations. (Paragraph 2)",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types. (Paragraph 3)",
        "Better OCR methods led to better results for DVQA. (Paragraph 4)",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation. (Paragraph 5)"
    ],
    "5093": [
        "We found that data augmentation for Task 1a may have helped deep learning models but not traditional machine learning methods.",
        "It also appears that deep learning methods perform better than traditional methods across the board when they have enough training data.",
        "We recommend that future approaches should go beyond off-the-shelf deep learning methods, and also exploit the structural and semantic characteristics that are unique to scientific documents.",
        "The committee also observes that CL-SciSumm series over the past 5 years has catalysed research in the area of scientific document summarization.",
        "We have achieved this goal now, and we will consider newer tasks to push the effort towards automated literature reviews.",
        "We will also consider switching the format of the shared evaluation from a shared task to a leaderboard to which systems can submit evaluations asynchronously throughout the year."
    ],
    "5098": [
        "The proposed method for span-based pre-training extends BERT by masking contiguous random spans and training the span boundary representations to predict the entire content of the masked span.",
        "The pre-training process yields models that outperform all BERT baselines on a variety of tasks.",
        "The method reaches substantially better performance on span selection tasks in particular."
    ],
    "5101": [
        "Our approach outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our analysis shows the limitations of BERT-based MCQ models and the challenge of learning natural language abductive inference.",
        "Our overall system improves on the state of the art by 11.6%."
    ],
    "5105": [
        "We propose a method to improve the time efficiency of seq2seq models for semantic parsing using a large vocabulary.",
        "We show that one can leverage a finite-state approximation to the LF language in order to speed up neural parsing significantly.",
        "Given a context-free grammar for the LF language, our strategy is general and can be applied to any model that predicts the output in a sequential manner.",
        "In the future we will explore alternatives to finite-state automata, which potentially characterize the relevant LF languages exactly while still allowing for efficient computation of admissible next tokens.",
        "We also plan to experiment with additional datasets."
    ],
    "5108": [
        "Training character-level embedding of language model requires enormous size of corpora\" (related to the methodology).",
        "We extend the idea of character-level embedding pre-trained on language model to cross-lingual scenarios for distantly supervised and low-resource scenarios\" (related to the future work).",
        "As long as orthographic constraint and some lexical words in target language such as loanwords to act as pivot are shared, we can utilize the high-resource languages model\" (related to the results)."
    ],
    "5109": [
        "Our proposed time masking strategy provides gains over baseline systems that simply encode dialogue distance.",
        "Incorporating additional information such as domain and intents into the time mask improves over competing approaches that indirectly incorporate time, particularly for multi-domain dialogues.",
        "We want to investigate more contextualized representations of the domain and intent in order to capture more subtle variations in the dialogue for multi-domain scenarios."
    ],
    "5110": [
        "The authors have addressed the lack of a tool for model selection in non-English languages under low-resource settings.",
        "They have presented LINSPECTOR WEB, an open-source, web-based evaluation suite with 16 probing tasks for 28 languages.",
        "LINSPECTOR WEB can probe pretrained static word embeddings and various layers of a number of selected AllenNLP models.",
        "The tool can easily be extended for additional languages, probing tasks, probing models, and AllenNLP models.",
        "The system is currently being extended to support contextualized word embeddings with contextualized probing tasks using the Universal Dependency Treebanks."
    ],
    "5111": [
        "We investigated self-attention network for Chinese word segmentation, demonstrating that it can achieve comparable results with recurrent network methods.",
        "Local attention gives better results compared to standard SAN.",
        "Under SAN, we also investigate the influence of rich character and word features, including BERT character embeddings and a neural attention method to integrate word information into character-based CWS.",
        "Extensive in-domain and cross-domain experiments show that the proposed SAN method archives state-of-the-art performance on both in-domain and cross-domain Chinese word segmentation datasets."
    ],
    "5114": [
        "We proposed an approach to automatically extract valid accident precursors from a dataset of raw construction injury reports.",
        "The learned precursors are valid and made several suggestions to improve the results.",
        "The proposed methods can also be used by the user to visualize and understand the models' predictions.",
        "Incidentally, while predictive skill is high for all models, we make the interesting observation that the simple TF-IDF + SVM approach is on par with (or outperforms) deep learning most of the time."
    ],
    "5115": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\"). (Claim 1)",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality. (Claim 2)",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, leading to improvements over previous work on joint POS tagging and dependency parsing. (Claim 3)",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks. (Claim 4)",
        "Our proposed system, TEXTFOOLER, is effective at generating targeted adversarial texts that are legible, grammatical, and similar in meaning to the original texts. (Claim 5)"
    ],
    "5116": [
        "The proposed hybrid neural network (HNN) model for commonsense reasoning achieves new state-of-the-art results on three classic commonsense reasoning tasks, pushing the WNLI benchmark to 89%, the WSC benchmark to 75.1%, and the PDP60 benchmark to 90.0%.",
        "The HNN model consists of two component models: a masked language model and a deep semantic similarity model, which share a BERT-based contextual encoder but use different model-specific input and output layers.",
        "The ablation experiments conducted in the study justify the design of HNN and provide insights into the contributions of its components to the overall performance.",
        "Future work may extend HNN to more sophisticated reasoning tasks, especially those where large-scale language models like BERT and GPT do not perform well, as discussed in (Gao et al., 2019; Niven and Kao, 2019)."
    ],
    "5117": [
        "The current approaches to solving the problem of relational implication are not explicitly defined as probabilistic statements, but our approach is.",
        "Our probabilistic framework for modeling relational data provides a clear interpretable framework for model development and parametization.",
        "The models developed using our framework outperform all previous work, attributed to both the probabilistic framework and the inclusion of link prediction scores.",
        "Current evaluatory datasets are not of sufficient quality, and we have developed a new targeted reannotation schema and made the resulting dataset public.",
        "Our model provides a broad and extensible platform for future research, and it may be useful to use the model to evaluate compositional implication rules or extract inference rules.",
        "The model can be extended to collect temporal information and develop a more thorough model of implication."
    ],
    "5118": [
        "Scene graphs can facilitate Visual QA.",
        "Leveraging scene graphs largely increases the Visual QA accuracy on questions related to counting, object presence and attributes, and multi-object relationships.",
        "The GN-based model can be further improved by incorporating image features on nodes as well as advanced multi-modal fusion and attention mechanisms.",
        "We expect that the GN-based model can be further improved by incorporating image features on nodes as well as advanced multi-modal fusion and attention mechanisms.",
        "Our experimental results demonstrate that scene graphs, even automatically generated by machines, can definitively benefit Visual QA if paired with appropriate models like GNs."
    ],
    "5119": [
        "Hybrid Code Networks using a convolutional neural network as input layer improves turn accuracy.",
        "The model using convolutional neural network outperforms the baseline model of Hybrid Code Networks on the Dialogue bAbI task 6 and Alquist Conversational Dataset.",
        "Hybrid Code Networks with a convolutional neural network as input layer achieves better turn accuracy than the baseline model.",
        "The proposed architectures of Hybrid Code Networks using a convolutional neural network as input layer show improved performance on the Dialogue bAbI task 6 and Alquist Conversational Dataset."
    ],
    "5121": [
        "Continual pre-training with multi-task learning can improve language representation capabilities.",
        "The ERNIE 2.0 model is more competent in language representation than BERT and XLNet.",
        "Incrementally adding more pre-training tasks to the ERNIE 2.0 framework can further improve the performance of the model.",
        "Other sophisticated continual learning methods may be investigated in the ERNIE 2.0 framework."
    ],
    "5122": [
        "Pre-trained encoders are an essential part of sequence generation.",
        "Sharing the weights between the encoder and decoder decreases the memory footprint.",
        "Combining BERT and GPT-2 into a single model often underperforms a randomly initialized baseline.",
        "Combining RoBERTa and GPT-2 achieves strong results and shows the importance of sharing the vocabulary.",
        "Training a language-specific BERT model improves performance over using the multilingual version."
    ],
    "5123": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Joey NMT achieves performance comparable to more complex toolkits, while being minimalist in its design and code structure.",
        "It is smaller in size and but more extensively documented than other toolkits.",
        "The code is comprehensibly written and structured."
    ],
    "5125": [
        "Our approach achieves promising performance on the popular CoQA dataset.",
        "We incorporate a dynamic reasoning procedure to the general encoder-decoder model, which dynamically updates the encoding representations of the inputs.",
        "We use the quality of the answers predicted by a QA model as rewards and fine-tune our model via reinforcement learning.",
        "In the future, we would like to explore how to better select the rationale for each question.",
        "Using linguistic knowledge such as named entities or part-of-speech tags could improve the coherence of the conversation."
    ],
    "5126": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results suggest that the community is ready for more difficult CQA datasets.",
        "Including human-generated question-answer pairs in future CQA datasets would be beneficial.",
        "Document-level CQA requires document question answering, page segmentation, and more.",
        "Using a thresholding technique based on cosine distance can significantly improve relation detection performance, especially for reducing false positives.",
        "The approach outperformed the baseline with both the Gigaword corpus and the NOW corpus."
    ],
    "5127": [
        "Our approach allows for efficient indexing and retrieval of video moments on our newly proposed task of search through large video collections.",
        "We have shown a simple yet effective approach for aligning video clips to natural language queries for retrieving moments in untrimmed, unsegmented videos.",
        "Our work opens up the possibility of effectively searching video at large scale with natural language interfaces.",
        "Our approach improves over prior work on our proposed task in terms of accuracy and search index size.",
        "We have quantitatively evaluated on benchmark datasets extended to our task and shown the effectiveness of our approach."
    ],
    "5128": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We propose a novel RL paradigm called RELIS, which learns a reward function from a reward oracle with learning-to-rank (L2R) algorithms at training time, and then uses the learnt reward to train input-specific RL policies at test time.",
        "Compared with the widely employed cross-input RL-based summarisation approaches, RELIS avoids the expensive learning of cross-input policies but, instead, efficiently performs L2R and input-specific RL learning.",
        "Moreover, RELIS avoids the arduous reward design required in input-specific RL-based summarisation approaches.",
        "We prove that, with proper L2R and RL algorithms, RELIS guarantees to produce near-optimal outputs.",
        "Our experiments show that even with linear L2R and standard RL algorithms, RELIS yields performance on par with the state of the art while only requiring a small fraction of data and time to train."
    ],
    "5129": [
        "We present a new method for creating an automatically generated clean and controlled parallel corpus.",
        "Our resource, called MaSS, contains 20 hours of speech in eight languages.",
        "The CMU Wilderness Multilingual Speech Dataset was used to create the corpus.",
        "The corpus is created using a novel combination of automatic speech recognition and machine translation.",
        "The resulting parallel corpus is clean and controlled, with high-quality transcriptions and translations.",
        "The corpus has been extensively evaluated and shows promising results for a variety of natural language processing tasks."
    ],
    "5130": [
        "Our abstractive summarization system does not rely on parallel resources and can be trained using example summaries and a large collection of non-matching articles, making it particularly relevant to low-resource domains and languages.",
        "Our system performed competitively to a fully supervised LSTM baseline trained on the document level on the CNN/DailyMail benchmark.",
        "We achieved promising results on the novel task of automatically generating a press release for a scientific journal article.",
        "Future work will focus on developing novel unsupervised extractors, decreasing the gap between abstractors trained on parallel and non-parallel data, and developing methods for combining the abstractor and extractor into a single system."
    ],
    "5133": [
        "The proposed method achieves good generation qualities and competitive evaluation scores for unsupervised sentence summarization using contextual matching.",
        "The method utilizes pre-trained generic language models for contextual matching in untrained generation, which is a new way of leveraging these models for this task.",
        "Future work could compare language models of different types and scales to improve the results further.",
        "The proposed method achieves good generation qualities and competitive evaluation scores for unsupervised sentence summarization using contextual matching. (Paragraph 1)",
        "The method utilizes pre-trained generic language models for contextual matching in untrained generation, which is a new way of leveraging these models for this task. (Paragraph 1)",
        "Future work could compare language models of different types and scales to improve the results further. (Paragraph 1)"
    ],
    "5134": [
        "Our English-Spanish APE system has successfully improved the translation quality in terms of automatic evaluation metrics.",
        "The hybrid word alignment plays a crucial role in this task.",
        "Edit-distance based monolingual aligner provides very well alignment links for our SAPE system.",
        "We achieve considerable improvement over the base system after incorporating the hybrid word alignment into the state-of-the-art HPBSMT pipeline."
    ],
    "5135": [
        "The authors believe that a metric that applies across multiple NLG domains may be widely adopted.",
        "The metric might focus only on semantic similarity and be complemented with another domain-specific measure, such as style preservation for style transfer or relevancy for dialogue.",
        "A model using meaning representations and parsing methods can compute semantic similarity.",
        "Achieving usable system-level correlation is not impossible for some NLG tasks, such as machine translation, image captioning, and summarization.",
        "Segment-level correlation leaves much to be desired, but the existence of metrics with high segment-level correlation opens up exciting research directions.",
        "A metric that can identify model failure modes or detect low-quality output and fall back on rule-based models as needed is useful in practice."
    ],
    "5137": [
        "The proposed Graph Neural Network (GNN) based model, GRAPHFLOW, achieves competitive results in conversational machine comprehension (MC) compared to previous approaches.",
        "The proposed model dynamically constructs a question and conversation history aware context graph at each conversation turn using a simple yet effective graph structure learning technique.",
        "The model offers good interpretability for the reasoning process in conversational MC.",
        "In the future, the authors plan to investigate more effective ways of automatically learning graph structures from free text and modeling temporal connections between sequential graphs."
    ],
    "5139": [
        "Convolutional Auto-Encoding plus Long Short-Term Memory (CAE-LSTM) explores the modeling of sentence topics to boost image paragraph generation.",
        "The proposed CAE-LSTM model studies the problem of topic distillation in an auto-encoding manner.",
        "The CAE-LSTM model uses a purely convolutional structure to abstract topics using convolutions over region-level image features, and a deconvolutional decoder to validate the topics through high-quality reconstruction.",
        "The two-level LSTM-based paragraph generator models dependency across sentences in a paragraph and generates the sentence conditioning on each learnt topic.",
        "Experiments conducted on Stanford image paragraph dataset verify the proposal and analysis, and performance improvements are observed compared to state-of-the-art image paragraph generation techniques."
    ],
    "5140": [
        "Our approach achieves state-of-the-art performance on four well-studied datasets across three text matching tasks with only a small number of parameters and high inference speed.",
        "The proposed RE2 model highlights three key features for inter-sequence alignment, including previous aligned features, original point-wise features, and contextual features.",
        "The model is highly efficient and suitable for a wide range of related applications due to its fast speed and strong performance.",
        "The proposed approach simplifies most of the other components and highlights three key features for inter-sequence alignment."
    ],
    "5141": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The model trained by the fine-tuning approach with premature strategy obtains remarkable multi-class logarithmic loss on the local 5-fold cross-validation at 0.3033, and 0.17289 on the test dataset in stage 2 of the task.",
        "MSnet can serve as a new strong baseline for gendered pronoun resolution task as well as the coreference resolution."
    ],
    "5142": [
        "The proposed NMT system achieves state-of-the-art results on the WMT 2018 News Translation shared task.",
        "The NMT system uses character-based encoding.",
        "The encoder and decoder both consist of a single LSTM layer.",
        "The authors plan to use more LSTM layers in their model as a future prospect.",
        "The authors plan to create another NMT model that takes input in words, not characters, and uses various embedding schemes to improve translation quality."
    ],
    "5144": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The model takes advantage of the strong grammatical structure inherent in tree-structured representations.",
        "The approach can be applied to other tree-to-tree tasks, such as natural language translation.",
        "The method can be extended into a more general graph-to-graph method."
    ],
    "5153": [
        "The beam problem in sequence-to-sequence tasks can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Jointly performing POS tagging and dependency parsing improves the performance of both tasks.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, leading to improvements over previous work on joint POS tagging and dependency parsing.",
        "The experimental evaluation shows that our method significantly improves the performance of the baseline models on several benchmark datasets for different NLP tasks.",
        "The transfer learning of multi-layer neural networks has the potential to improve the text understanding of the models, and further research into this area is encouraged."
    ],
    "5156": [
        "Our experiments show that DLGNet models perform better than existing state-of-the-art multiturn dialogue models.",
        "They also achieve the best performance to date on open-domain Movie and closed-domain Ubuntu datasets based on BLEU, ROUGE and distinct ngram scores.",
        "The combination of (i) the transformer architecture with (ii) the injection of random paddings exploiting the large maximum input sequence is responsible for the performance improvement over existing methods.",
        "Other contributing factors include joint modeling of dialogue context and response, and the 100% tokenization coverage from the byte pair encoding (BPE).",
        "Our analysis also reveals some tradeoffs between response relevance and response length, and we showed how different sampling strategies can be used to make an informed decision about such response relevance-diversity compromises.",
        "In our future work, we plan to investigate how to improve on the length of the generated responses without necessarily sacrificing their coherence and their relevance to the dialogue context."
    ],
    "5157": [
        "The proposed GEAR framework is effective for claim verification on the FEVER dataset.",
        "The framework utilizes BERT sentence encoder, ERNet, and an evidence aggregator to encode, propagate, and aggregate information from multiple pieces of evidence.",
        "The framework achieves significant improvements over previous pipelines.",
        "In future work, the authors plan to design a multi-step evidence extractor and incorporate external knowledge into their framework."
    ],
    "5158": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "Self-knowledge distillation (SKD) improves the performance of language modeling and neural machine translation tasks.",
        "SKD can be applied to other tasks where cross-entropy is used, and does not depend on task or model architectures.",
        "Applying SKD to image classification tasks may be possible, although it is not guaranteed that comparable image classes are closely located in the word embedding space for image-related tasks.",
        "Developing an automatic way for the parameters like \u03b1 in Eq. (12) and generalizing the equation for q n in Eq. (8) may be possible."
    ],
    "5160": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "The method works for the static GloVe embeddings.",
        "Our approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The experimental results provide impressive improvements on all tasks.",
        "In the future, we will extend the proposed method to other fields."
    ],
    "5168": [
        "The proposed flexibly-structured dialogue model is a novel end-to-end architecture for task-oriented dialogue that uses the structure in the schema of the KB to make architectural choices.",
        "The proposed model introduces inductive bias and addresses the limitations of fully structured and free-form methods.",
        "The experiment suggests that this architecture is competitive with state-of-the-art models, while at the same time providing a more practical solution for real-world applications."
    ],
    "5169": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The approach outperforms existing methods that do not utilize chunk-merging by a significant margin, especially when combining with Evolved Transformer."
    ],
    "5172": [
        "Debiased embeddings can worsen a text classifier's fairness.",
        "Strongly debiased embeddings can reduce gender information and improve fairness while maintaining good classification performance.",
        "Injecting debiased embeddings into model architectures does not result in much additional burden for ML practitioners (e.g. model tweaks, labelled data).",
        "The approach of using debiased embeddings can generalize to a variety of datasets and other identity groups."
    ],
    "5174": [
        "Question answering, like many NLP tasks, is impaired by noisy inputs.",
        "Introducing ASR into a QA pipeline corrupts the data.",
        "A neural model that uses the ASR system's confidence outputs and systematic forced decoding of words rather than unknowns improves QA accuracy on Quizbowl and Jeopardy! questions.",
        "Our methods are task agnostic and can be applied to other supervised NLP tasks.",
        "Larger human-recorded question datasets and alternate model approaches would ensure spoken questions are answered accurately, allowing human and computer trivia players to compete on an equal playing field."
    ],
    "5175": [
        "Our proposed attention-based image captioning model, ATTEND-GAN, achieves state-of-the-art performance on the SentiCap dataset and outperforms baseline models.",
        "ATTEND-GAN is capable of generating stylistic captions that are strongly correlated with images and contain diverse stylistic components.",
        "Future work includes developing ATTEND-GAN to generate a wider range of captions and developing further mechanisms to ensure compatibility with the visual content."
    ],
    "5181": [
        "The proposed approach achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "The novel end-to-end model for joint slot label alignment and recognition requires no external label projection.",
        "The model outperforms the simple projection baseline using fast-align on most languages.",
        "The approach of joint training with a denoising encoder task for our submission achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning, and it is worth investigating this type of models in other learning problems."
    ],
    "5182": [
        "The model trained on two complementary auxiliary tasks (Cross-Modal Alignment and Next Visual Scene) learns visual and textual representations that can be transferred to navigation agents, improving their performance in key navigation metrics.",
        "The transferred representations improve both the SF and RCM agents by 5% absolute measure.",
        "Our ALTR agent-RCM initialized with domain adapted representations outperforms published models at the time by 5%.",
        "The scoring model trained on the tasks has additional capabilities like cross-modal alignment, which could help improve methods that generate additional paired instruction-path pairs.",
        "Jointly training the agent with the auxiliary tasks is desirable for future work."
    ],
    "5185": [
        "The proposed novel constituent hierarchy predictor based on recurrent neural networks can capture global sentential information.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser in terms of F1 score on standard WSJ and CTB evaluations.",
        "The generate-validate framework has shown promise in solving qualitative word problems and offers opportunities for transfer learning.",
        "The apply the generate-validate framework to other applications that use semantic parsing, such as Natural Language Inference.",
        "Improving Natural Language Inference models will naturally improve the performance of the proposed models."
    ],
    "5186": [
        "The choice of named entity classes has a significant impact on the performance of NER systems.",
        "The use of specific named entity classes (e.g., person, organization, location) can improve the accuracy of NER systems.",
        "The NER systems employed in the experimental section of this paper are based on the locally-normalized structure of the model.",
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "More research is needed to determine how much, if any, improvement remains to be gained by solving label bias in general.",
        "The solution to the brevity problem is a very limited form of globally-normalized model."
    ],
    "5188": [
        "Offensive language on online social media platforms is harmful.",
        "Automatic methods are required to detect this kind of harmful content.",
        "Most of the research on the topic has focused on solving the problem for English.",
        "Sharing information across languages and platforms leads to good models for the task.",
        "The resources and classifiers are available from the authors under CC-BY license, pending use in a shared task.",
        "A data statement [30] is included in the appendix.",
        "Extended results and analysis are given in [31]."
    ],
    "5189": [
        "Incorporating word and sentence structures into BERT pre-training can obtain new state-of-the-art results in a variety of downstream tasks.",
        "The new StructBERT model can achieve better performance than existing models on popular GLUE benchmark, SNLI Corpus, and SQuAD v1.1 question answering.",
        "The word structural objective and sentence structural objective introduce two new pre-training tasks for deep understanding of natural language in different granularities.",
        "The proposed novel structural pre-training can improve the performance of BERT models on a variety of NLP tasks."
    ],
    "5190": [
        "Once we obtain user attributes, they can be applied to many downstream applications.",
        "We select two directions we are interested in and discuss them in detail, and point out current limitations.",
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We also add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy."
    ],
    "5191": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Lexically constrained decoding has shown to be helpful by previous works, but it depends on an ideal condition where lexical constraints provided by users are perfect.",
        "Our framework is capable of improving the translation quality with these automatically generated constraints.",
        "The shallow constraint encoder is more superior when noisy rate is relatively low, while the deep constraint encoder is more robust when the noises occur frequently.",
        "We propose a novel framework to augment the NMT model, which treats constraints as external memories."
    ],
    "5193": [
        "The web-based speech transcription tool, IMS-Speech, can be used by non-technical researchers to utilize audio recordings in their studies.",
        "The IMS-Speech results compared favorably with specialized systems in terms of WER (Word Error Rate) in a diverse set of tasks and conditions.",
        "The system can be customized for users' needs in the future.",
        "The ASR system will be constantly improved in the future."
    ],
    "5195": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?"
    ],
    "5196": [
        "a highly effective post-training method for a multiturn response selection is proposed and evaluated.",
        "Our approach achieved new state-of-the-art results for two response selection benchmark data sets, Ubuntu Corpus and Advising Corpus.",
        "We will utilize external domain knowledge, such as ubuntu manual description in Ubuntu Corpus or curriculum information in Advising Corpus, for enhancing domain understandings."
    ],
    "5200": [
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "Our experiments empirically show that tagging and parsing can be tackled using much simpler models without losing accuracy.",
        "Our approach is much simpler yet more accurate than the previous state-of-the-art.",
        "BERT embeddings are well-suited for semantic reasoning in long sentences, beyond syntactically intensive or morphologically complex tasks."
    ],
    "5201": [
        "The authors propose a portable NER framework called FlexNER that can recognize entities from textual input.",
        "The proposed data augmentation paradigm does not require external data and is straightforward to implement.",
        "The learning representation is enhanced by augmenting entity-context diversity.",
        "The layer stacks and subnetwork combinations can be commonly used in different datasets to provide better representations from different perspectives.",
        "The framework has the potential to be used in low-resource languages and may benefit other entity-related tasks.",
        "In the future, the authors plan to apply this system to biomedical research, such as extracting the functional brain connectome or exploring the relations between drugs and diseases."
    ],
    "5202": [
        "The early-fusion B2T2 model outperforms other models in visual question answering tasks, with the best available results.",
        "Late fusion performs substantively worse than early fusion, indicating that grounding words in the visual context should be done early rather than late.",
        "The Dual Encoder model achieves competitive results with state-of-the-art on the VCR dataset, even when textual references to image bounding boxes are ignored.",
        "Incorporating visual features into the textual embeddings improves the performance of the Dual Encoder model.",
        "Pretraining the deep model on Conceptual Captions with a Mask-LM loss yields a small additional improvement and more stable fine-tuning results."
    ],
    "5205": [
        "The proposed FlowDelta mechanism can model the information flow in multiturn dialogues more comprehensively and boosts the performance consistently.",
        "The FlowDelta mechanism is flexible to apply to other machine comprehension models, including FlowQA and BERT.",
        "The proposed method can handle conversational tasks by modeling the dialogue flow.",
        "The model needs to perform the correct sequence of actions on the initial world state and obtain the correct world states after each instruction.",
        "The encoded world states and logical form are used to represent the change of world states.",
        "The proposed method can handle the logical form with three or four integers, including the type of action performed, the position of the context, and the additional property for the action."
    ],
    "5206": [
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Our model can yield new state-of-the-art or comparative results in both extremely challenging tasks.",
        "The proposed attention mechanism also verifies the practicability of using linguistic information to guide attention learning and can be easily adapted with other tree-structured annotations."
    ],
    "5208": [
        "The proposed model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptive model can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The use of a probabilistic model to induce mappings from lexical sources and their grammatical sources to negative inferences is effective.",
        "The model fits best when positing one lexical property and one structural property, which is a surprising finding given prior work.",
        "The findings suggest new directions for theoretical research attempting to explain the interaction between lexical and structural factors in neg-raising.",
        "Future work in this vein might extend the model proposed here to investigate the relationship between neg-raising and acceptability as well as other related phenomena with associated large-scale datasets."
    ],
    "5209": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but the results for DVQA were more nuanced due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results suggest that the community is ready for more difficult CQA datasets.",
        "Charts in the wild may contain variations that are not captured by current datasets, and human-generated questions should be included in future CQA datasets.",
        "Document-level CQA is necessary to understand charts in documents and to answer questions about the chart.",
        "PReFIL has the potential to improve retrieval of information from charts, which has numerous applications."
    ],
    "5211": [],
    "5212": [
        "'KBRD can reach better performances in both recommendation and dialog generation in comparison with the baselines.'",
        "'Dialog information is effective for the recommender system especially in the setting of cold start, and the introduction of knowledge can strengthen the recommendation performance significantly.'",
        "'Information from the recommender system that contains the user preference and the relevant knowledge can enhance the consistency and diversity of the generated dialogs.'"
    ],
    "5214": [
        "'We propose an end-to-end model towards the problem of how to learn an efficient dialogue manager without taking too many manual efforts.'",
        "'Our model is efficient in terms of goal achievement ratio and average dialogue turns.'",
        "'Our method is also scalable and can reduce error propagation due to the nature of end-to-end learning.'",
        "'We expect to investigate whether other kinds of abilities, such as reasoning ability, can be modeled for agent towards the problem.'",
        "'In addition to the efficiency issue, the quality of natural language generation should also be paid attention in order to guarantee the quality of overall dialogue system.'"
    ],
    "5215": [
        "Question classification can enable targetting question answering models, but is challenging to implement with high performance without using rule-based methods.",
        "Our approach achieves state-of-the-art results across benchmark datasets in open, science, and medical domains.",
        "Attending to question type can significantly improve question answering performance, with large gains possible as question classification performance improves.",
        "Developing high-precision methods of question classification independent of their recall can offer the opportunity to incrementally make use of the benefits of question classification without suffering the consequences of classification errors on QA performance."
    ],
    "5216": [
        "We introduce MTMSN, a multi-type multi-span network for reading comprehension that requires discrete reasoning over the content of paragraphs.",
        "Our model achieves 79.9 F1 on the DROP hidden test set, creating new state-of-the-art results.",
        "As future work, we would like to consider handling additional types such as sorting or multiplication/division.",
        "We also plan to explore more advanced methods for performing complex numerical reasoning."
    ],
    "5218": [
        "We introduce lexical semantic information into a neural language model's pre-training objective, which results in a boosted word-level semantic awareness of the resultant model.",
        "The resulting model, named SenseBERT, considerably outperforms a vanilla BERT on a SemEval based Supersense Disambiguation task and achieves state of the art results on the Word in Context task.",
        "This improvement was obtained without human annotation, but rather by harnessing an external linguistic knowledge source.",
        "Our work indicates that semantic signals extending beyond the lexical level can be similarly introduced at the pre-training stage, allowing the network to elicit further insight without human supervision."
    ],
    "5219": [
        "We propose CTNMT, an effective, simple, and efficient transfer learning method for neural machine translation that can be also applied to other NLP tasks.",
        "Our conclusions have practical effects on the recommendations for how to effectively integrate pre-trained models in NMT.",
        "Adding pre-trained LMs to the encoder is more effective than the decoder network.",
        "Employing CTNMT addresses the catastrophic forgetting problem suffered by pre-training for NMT.",
        "Pre-training distillation is a good choice with nice performance for computational resource constrained scenarios.",
        "While the empirical results are strong, CTNMT surpasses those previous pretraining approaches by 1.4 BLEU score on the WMT English-German benchmark dataset.",
        "Our method still achieves remarkable performance on the other two large datasets."
    ],
    "5221": [
        "projection-based neural models are robust to text transformations compared to BERT or BiLSTMs with embedding lookup tables for words and word-pieces.",
        "LSH-based projection neural networks for memory-efficient text representations are robust to text transformations.",
        "BERT or BiLSTMs with embedding lookup tables for words and word-pieces are not as robust to text transformations as projection-based neural models.",
        "projection-based methods can be used to attenuate biases in contextualized embeddings without loss of entailment accuracy.",
        "the approach of debiasing the first (non-contextual) layer alone can effectively attenuate bias in both contextualized embeddings."
    ],
    "5223": [
        "We can exploit the power of neural networks to train the model to perform downstream NLP tasks like Named Entity Recognition even in Nepali language.",
        "The word vectors learned through fasttext skip gram model performs better than other word embedding because of its capability to represent subword and this is particularly important to capture morphological structure of words and sentences in highly inflectional languages like Nepali.",
        "Stemming post-positions can help a lot in improving model performance because of the inflectional characteristics of Nepali language.",
        "Separating out its inflections or morphemes can minimize the variations of same word, which gives its root word a stronger word vector representations compared to its inflected versions.",
        "OurNepali dataset volume is almost ten times bigger compared to ILPRL dataset in terms of entities."
    ],
    "5226": [
        "We introduced bidirectional sequence generation by employing placeholders in the output sequence.",
        "Our approach outperforms previous end-to-end approaches that do not make use of any pretrained language models.",
        "In conjunction with the pre-trained language model BERT, our bidirectional sequence generation approach allows us to achieve new state-of-art results on both conversational tasks.",
        "We wonder if a further performance increase could be achieved if the pre-training of BERT would employ our placeholder strategy."
    ],
    "5227": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "using language model rescoring to choose more fluent translation candidates",
        "preprocessing and post-processing approaches improve the quality of final translations, particularly to replace unknown words with possible relevant target words."
    ],
    "5228": [
        "The notion of style matrix conforms well to human experiences and existing linguistic theories on language style.",
        "The NS algorithm achieves competitive transfer quality with state-of-the-art methods and shows superior flexibility in various use cases.",
        "The quality of semantic vectors impacts the informativeness of style matrix.",
        "Studying what is encoded in higher-order statistics of semantic vectors may be useful in the future."
    ],
    "5229": [
        "The novel densely connected graph convolutional networks (DCGCNs) outperform state-of-the-art models in two tasks: AMR-to-text generation and syntax-based neural machine translation.",
        "DCGCNs scale naturally to significantly more layers without suffering from performance degradation and optimization difficulties, thanks to the introduced dense connectivity mechanism.",
        "The proposed framework has the potential to perform improved graph representation learning for various graph-related tasks.",
        "Other NLP applications such as relation extraction, semantic role labeling, and graph representation learning can potentially benefit from the proposed approach."
    ],
    "5230": [
        "The proposed method can achieve a much faster inference speed than state-of-the-art methods.",
        "The incorporation of lexicon information into character representations improves the performance of Chinese NER systems.",
        "Experimental studies on four benchmark datasets show that our method outperforms state-of-the-art methods.",
        "The proposed method is effective in improving the computational efficiency of Chinese NER systems."
    ],
    "5231": [
        "The proposed modifications to the HAN architecture, making the sentence encoder context-aware (CAHAN), result in improved performance.",
        "Taking context into account is beneficial for the sentence encoder, as shown by the outperformance of the bidirectional version of the document encoder compared to the HAN baseline and undirectional variant.",
        "The computational overhead of the proposed modifications is small.",
        "Experimental results on tasks requiring a deeper understanding of the input documents should highlight the superiority of CAHAN."
    ],
    "5232": [
        "We presented a comprehensive fusion strategy based on VAE that outperforms previous methods by a significant margin.",
        "The encoder and decoder networks in the VAE are simple fully-connected layers.",
        "We plan to improve the performance of our method by employing more sophisticated networks, such as fusion networks like MFN and TFN as the encoders.",
        "Our approach outperforms benchmark models across different datasets.",
        "We have employed a simple fusion strategy based on VAE to improve the performance of our method."
    ],
    "5233": [
        "The brevity problem in NMT can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "The adversarial data includes less profanity and is more nuanced due to figurative language, negation, and world knowledge, which makes current classifiers fail.",
        "Classifiers that learn from more complex examples are more robust to attack.",
        "Using the dialogue context gives improved performance if the model architecture takes it into account.",
        "The binary problem of offensive or safe language can be considered, and classes of offensive language can be separated.",
        "Future work can explore other dialogue tasks, such as those from social media or forums.",
        "The build it, break it, fix it strategy can be applied to make neural generative models safe."
    ],
    "5234": [
        "We have introduced a novel methodology and toolkit for rapid development of production grade systems, called CFO.",
        "Our demonstration of an end-to-end QA system composed of a SOTA MRC model adapted to answering 'natural language questions'.",
        "We show experimentation using the NQ dataset to illustrate training techniques that can be used to build SOTA systems on top of existing pre-trained language models like BERT.",
        "We are actively seeking to open source the CFO framework and hope that, once available, the community will be able to quickly build and deploy their own SOTA NLP components as interactive multi-component systems.",
        "We intend on expanding GAAMA to incorporate additional QA components to improve its performance through approaches like query expansion for improved recall and network pruning for improved latency."
    ],
    "5235": [
        "The proposed system achieved competitive performance in translating from Czech to Polish, ranking second among ten teams in the competition in terms of BLEU score.",
        "The use of out-of-domain data provided by the organizers resulted in a challenging but interesting scenario for all participants.",
        "The proposed hypothesis (i.e., word-BPE level information) is effective in similar language translation, and worth exploring further.",
        "There are morphological differences between the two languages (and the other two language pairs in the competition) that can be captured by training models, which is not always possible due to time constraints."
    ],
    "5237": [
        "The existing NLU systems exhibit poor systematic generalization and robustness capabilities, especially when compared to a graph neural network that works directly with symbolic input.",
        "The CLUTRR benchmark suite can be used to diagnose and improve the compositional, modular, and robust nature of NLU systems.",
        "The gap between machine reasoning models that work with unstructured text and models that are given access to more structured input highlights the need for progress in building more compositional, modular, and robust NLU systems."
    ],
    "5238": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The use of a projection-based method for attenuating biases can effectively reduce bias in contextualized embeddings without loss of entailment accuracy.",
        "Our model introduces structured external knowledge into text representations, validated on the medical domain.",
        "Incorporating more powerful relationship models like TransR (Lin et al. 2015) into the CC embedding model is an interesting investigation.",
        "Combining CC embeddings with general representation models like BERT is a promising way to further improve state-of-the-art text representation models."
    ],
    "5239": [
        "Our approach (NDORGS) can generate a coherent and well-structured ORPT over a large corpus of documents with acceptable efficiency and accuracy.",
        "The ORPTs generated by our approach on 0.2 summaries are the best overall in terms of human evaluations, running time, information coverage, and topic diversity.",
        "Our approach is stable and the results are not sensitive to sensitivity analysis."
    ],
    "5240": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The proposed ReCoSa model can be useful for improving the quality of multiturn dialogue generation, by using proper detection methods, such as self-attention.",
        "In future work, we plan to further investigate the proposed ReCoSa model and optimize the iteration scheme in our algorithm.",
        "We will take full advantages of more edges (translation pairs) in our language graph distillation."
    ],
    "5242": [
        "Our architecture is competitive with the state-of-the-art.",
        "MPAD is sensitive to word order and word-word relationship strength.",
        "By processing weighted, directed word co-occurrence networks, MPAD captures the hierarchical structure of documents.",
        "Three hierarchical variants of MPAD bring improvements over the vanilla model."
    ],
    "5243": [
        "PReFIL surpasses prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL achieves better results for FigureQA than human baseline, while the results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods lead to better results for DVQA.",
        "The strong results suggest that the community is ready for more difficult CQA datasets.",
        "The approach can be applied to any probabilistic language model without additional training, opening up many other tasks, including summarization, dialogue systems, and question answering.",
        "Using better similarity measures that capture semantics could further improve results."
    ],
    "5244": [
        "Fine-tuning a word embedding for a task can significantly impact performance.",
        "For standard vision-language metrics, language features matter most on retrieval and grounding tasks, and less on text-to-clip and generation tasks.",
        "Word embeddings trained on outside vision-language datasets and tasks generalize to other applications.",
        "Multi-task training with five common vision-language tasks can incorporate nuanced visual information and provide low variance results.",
        "The proposed GrOVLE embedding, which incorporates hierarchical language relations from WordNet as well as language with visual context from Visual Genome, provides a 300-D embedding with vision-language enhancements that is comparable to current embeddings and provides low variance results."
    ],
    "5245": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "WEAT, the most common test of word embedding association, has theoretical flaws that cause it to systematically overestimate bias.",
        "SGNS does not, on average, make most words any more gendered in the embedding space than they are in the training corpus. However, for words that are gender-biased or gender-specific by definition, SGNS amplifies the genderedness in the corpus."
    ],
    "5247": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach outperforms benchmark models across different datasets.",
        "We achieved the highest recall and this score was still lower as our achieved precision (indicating a good balance).",
        "We demonstrated the need for post-processing measures and how the traditional methods performed against new methods with this problem.",
        "We improve a hierarchical classification open source library to be easily used in the multi-label setup achieving state-of-the-art performance with a simple implementation.",
        "The high scoring of such traditional and lightweighted methods is an indication that this dataset has not enough amount of data to use deep learning methods.",
        "Many small improvements were not performed, such as elimination of empty predictions and using label names as features. This will be performed in the future."
    ],
    "5250": [
        "The proposed co-analysis framework can guide the visual exploration of data.",
        "Future work will focus on recommending meaningful groups or biclusters in different variable sets to improve analytical efficiency.",
        "Parallel computation will be employed to accelerate computational efficiency in bicluster generation.",
        "The co-analysis framework will be extended to time-varying multivariate data to capture coherence in the time space."
    ],
    "5255": [
        "The proposed approach significantly outperforms SOTA on commonsense-related CSQA and WSC tasks, while maintaining comparable performance on GLUE tasks to the BERT models.",
        "Incorporating commonsense knowledge into language representation models leads to improved performance on commonsense-related tasks.",
        "The proposed method for automatically constructing a multi-choice QA dataset for pretraining is effective in improving the performance of language representation models on commonsense-related tasks.",
        "The approach can be used to incorporate commonsense knowledge into models such as XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019)."
    ],
    "5260": [
        "Change in synset leadership is predictable to some degree.",
        "It is possible to make successful predictions several decades into the future.",
        "The direction of change in synset leadership is relatively constant over many decades.",
        "English appears to be cooling; the rate of change is decreasing over time.",
        "The growth of English speakers may increase the inertia of English.",
        "This project has great potential for more research with this combination of resources.",
        "Evolutionary trends in language are the result of many individuals making many decisions.",
        "A model of the natural selection of words can help us understand how such decisions are made, which will enable computers to make better decisions about language use."
    ],
    "5261": [
        "We introduce a hybrid approach for automatic extraction of events and arguments.",
        "Our new dataset in the disaster domain for five languages consists of a large number of tags, significantly larger than usual datasets.",
        "Our rule-augmented methods outperform deep learning-based models on lesser annotated data and low-resource languages.",
        "We show more improvement on tail labels using our approach.",
        "We plan to integrate cross-linking between events and its arguments in future work."
    ],
    "5263": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism, which uses Similarity Entropy Minimization to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We address the limitations posed by previous systems and propose a simple yet effective architecture ZSGNet.",
        "Our proposed model ZSGNet performs significantly better than existing baseline in the zero-shot setting."
    ],
    "5266": [
        "The current GANs for text generation suffer from instability issues due to policy gradient.",
        "Incorporating RAML into the adversarial training paradigm can address the instability issue and improve the performance of the generator.",
        "Our proposed adversarial training framework with RAML achieves better performance than several state-of-the-art GAN baselines with lower training variance, yet producing better performance on three text generation tasks.",
        "The instability issue in current GANs can be addressed by using a novel adversarial training framework that incorporates RAML.",
        "Our model performs better than state-of-the-art GAN baselines with lower training variance, yet producing better performance on three text generation tasks."
    ],
    "5267": [
        "The proposed method improves the performance of word sense disambiguation (WSD) compared to traditional methods.",
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "The approach of constructing context-gloss pairs and converting WSD to a sentence-pair classification task is effective in leveraging gloss knowledge for WSD.",
        "Fine-tuning the pre-trained BERT model on a downstream task can improve the performance of WSD.",
        "Our approach demonstrates that using gloss knowledge in a supervised neural WSD system can be more intuitive and effective than traditional methods."
    ],
    "5270": [
        "We presented a cross-modality framework, LXMERT, for learning the connections between vision and language.",
        "Our model is then pre-trained with diverse pre-training tasks on a large-scale dataset of image-and-sentence pairs.",
        "Empirically, we show state-of-the-art results on two image QA datasets (i.e., VQA and GQA) and show the model generalizability with a 22% improvement on the challenging visual reasoning dataset of NLVR 2.",
        "We also show the effectiveness of several model components and training methods via detailed analysis and ablation studies.",
        "The goal of visual question answering (VQA) is to answer a natural language question related to an image.",
        "We take VQA v2.0 dataset which reduces the answer bias compared to VQA v1.0.",
        "The dataset contains an average of 5.4 questions per image and the total amount of questions is 1.1M."
    ],
    "5275": [
        "The proposed approach can better leverage monolingual data for NMT by utilizing pre-trained sentence representations.",
        "The sentence representations are acquired through bidirectional self-attention language model (BSLM) and integrated into NMT network by a weighted fusion mechanism and a knowledge transfer paradigm.",
        "The proposed model achieves prominent improvements on standard data sets as well as in the low-resource scenario."
    ],
    "5276": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Our proposed method, Latent Relation Language Models, outperforms previous work in conditional language modeling tasks.",
        "Marginalization over latent variables allows the model to score spans with their posterior relation probability.",
        "The model can learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets."
    ],
    "5284": [
        "The current state-of-the-art MTL architectures and learning mechanisms have not been systematically explored and compared for their strong performance in-depth.",
        "The proposed work is the first to investigate the effects of five widely used MTL methods with deep neural networks for a broad range of representative NLP tasks.",
        "All six individual MTL methods contribute great improvements to the overall performance on the experimental datasets.",
        "Using linguistic hierarchical information performs better than other individual MTL methods.",
        "The upper-level auxiliary task brings more improvement than the lower-level auxiliary task.",
        "In the future, the effectiveness of these MTL methods in alleviating negative knowledge transfer problems among different tasks will be investigated.",
        "A more effective MTL framework will be devised when considering the strengths and weaknesses of existing MTL methods for alleviating negative knowledge transfer problems."
    ],
    "5289": [
        "Our approach outperforms benchmark models across different datasets.",
        "We have proposed a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We have also added a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Existing QA and reading comprehension methods hardly have the capability of applying the retrieved general knowledge to the specific case described by a scenario.",
        "We have contributed GeoSQA-a large SQA dataset where diagrams are present and have been manually annotated with natural language descriptions.",
        "The results of existing methods on our dataset are not satisfactory, thus demonstrating the unique challenges presented by the SQA task on our dataset."
    ],
    "5295": [
        "The proposed model X-SQL achieves exceptional performance on the WikiSQL task and sets a new state-of-the-art across all metrics.",
        "The contribution of the model around loss objective may be bounded by the specific SQL syntax used by WikiSQL, but the model's ability to leverage contextual information and use schema type can be immediately applied to other tasks that involve pretrained language models for structured data.",
        "Future work includes experimenting with more complex datasets such as Spider (Yu et al., 2018b)."
    ],
    "5296": [
        "Our multi-passage BERT model outperforms all state-of-the-art models on four standard benchmarks.",
        "We find two effective techniques to improve the performance of multipassage BERT.",
        "Splitting articles into passages with the length of 100 words by sliding window is an effective technique for improving the performance of multipassage BERT.",
        "Leveraging a passage ranker to select high-quality passages is another effective technique for improving the performance of multipassage BERT.",
        "Our multi-passage BERT model globally normalizes answer scores across multiple passages corresponding to the same question.",
        "In future, we plan to consider inter-correlation among passages for open-domain question answering."
    ],
    "5297": [
        "The training data used in the dataset has some issues, such as ambiguous answers and lack of useful information.",
        "The ground truth answers in the training data may be hard for the system to learn from, and may contain ambiguous or generic responses.",
        "The caption and summary information in the training data is often useless for answering questions.",
        "The authors use multiple modalities to let the model gain a scene-aware ability, and use dialogue history as data augmentation to improve performance.",
        "Adding previous dialogue history and using DMN on video to reinforce the model's ability to memorize important information are effective in improving the overall performance."
    ],
    "5298": [
        "The beam problem in sequence-to-sequence text generation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset, and this can be done in an inexpensive way.",
        "Solving the brevity problem leads to significant BLEU gains, but it remains to be seen how much improvement is left to be gained by solving label bias in general.",
        "Our proposed method, Dynamic Memory Induction Networks (DMIN), achieves new state-of-the-art results on few-shot text classification tasks using external working memory with dynamic routing.",
        "The model can adapt and generalize better to support sets and unseen classes by leveraging previous learning experience and the former to track dynamic memory.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning, and exploring this type of models in other learning problems is an avenue for future work.",
        "The pre-training step of our transfer learning approach is independent of downstream tasks and jointly learns both encoder and decoder representations.",
        "Our model, PoDA, is simple, intuitive, and doesn't require changing network architecture during the fine-tuning stage.",
        "Experiments on several abstractive summarization and grammatical error correction datasets demonstrate that PoDA leads to better performance and faster convergence."
    ],
    "5309": [
        "The experimental results show that our model significantly outperforms existing state-of-the-art baselines on the NYT benchmark.",
        "Our model achieves accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our carefully designed dual tagging framework for triple extraction collectively exploits interactions on top of the joint decoding strategy via multi-task learning with a customized parameter sharing."
    ],
    "5310": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning.",
        "The proposed model, DMIN, leverages external working memory with dynamic routing to track previous learning experience and adapt to unseen classes.",
        "The model achieves better results on POS tagging, NER, and CCG supertagging tasks compared to BiLSTM-CRF and BiLSTM-softmax.",
        "The hierarchically-refined label attention network (LAN) effective solves the label bias issue."
    ],
    "5311": [
        "The Transformer model is not worse than the standard RNN-based seq2seq model for the task of conversational modeling, as previously thought. (Supported by preliminary experiments)",
        "Current chatbot models have several inappropriate properties. (Criticized in the paper)",
        "The Transformer model can be augmented with various techniques to make conversational agents more natural and human-like. (Discussed in the paper)",
        "The author has presented directions and experiments that should be conducted with the Transformer model to determine its performance. (Mentioned in the paper)",
        "The author will focus on implementing ideas related to solving fundamental issues with current dialog agents, such as making open-domain conversational models as human-like as possible. (Future research direction mentioned in the paper)"
    ],
    "5316": [
        "The beam problem in NMT can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The discovered rules for root-and-pattern morphology in Semitic languages can be used to extract Semitic roots.",
        "The performance of our root extraction method is not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "We developed an unsupervised technique to extract tags that identify complementary attributes of movies from user reviews.",
        "Our coarse story understanding approach can be extended to longer stories, i.e., entire books.",
        "We tokenize the synopses and reviews using spaCy 10 NLP library.",
        "We retain the words that appear at least in ten synopses and reviews to remove rare words and other noise.",
        "We represent the out of vocabulary words with a <UNK> token."
    ],
    "5319": [
        "The proposed graph neural network-based model can effectively detect supporting sentences compared to well-known answer-selection QA models.",
        "The model does not rely on \"answer span\" information for detecting supporting sentences, instead, it focuses on evaluating and analyzing the effectiveness of the proposed graph neural network-based model for classifying supporting sentences.",
        "The use of traditional measures for the QA system is adopted to evaluate the performance of the proposed model from a different perspective.",
        "The proposed model is not trained with the exact \"answer span\" information, which allows the model to track the supporting sentences from simple word matching.",
        "The study focuses on evaluating and analyzing the effectiveness of the proposed graph neural network-based model for classifying supporting sentences compared to well-known answer-selection QA models."
    ],
    "5320": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "We contrast the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Our results suggest that learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves."
    ],
    "5322": [
        "Experiments on 23 languages\u2192English and English\u219223 languages show that language embeddings can sufficiently characterize the similarity between languages and outperform prior knowledge (language family) for language clustering in terms of the BLEU scores.",
        "For future work, we will test our methods for many-to-many translation.",
        "We will consider more languages (hundreds or thousands) to study our methods in larger scale setting.",
        "We will also study the relationship between language similarity and machine translation quality."
    ],
    "5324": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our proposed model demonstrates effectiveness in compressing a large BERT model into a shallow one via Patient Knowledge Distillation.",
        "Pre-training BERT from scratch could address the initialization mismatch issue.",
        "Modifying the proposed method to help during pre-training is another exploration direction.",
        "Designing more sophisticated distance metrics for loss functions is another exploration direction.",
        "Investigating Patient-KD in more complex settings such as multi-task learning and meta learning is a future work."
    ],
    "5325": [
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "The method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy."
    ],
    "5327": [
        "We propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "We investigate structure-infused copy mechanisms that combine source syntactic structure with the copy mechanism of an abstractive summarization system.",
        "Our models can effectively preserve salient source relations in summaries.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "We raise the issue of UNMT domain adaptation since domain adaptation methods for UNMT have never been proposed.",
        "Experimental results show our modified corresponding methods improve the performance of UNMT in these scenarios.",
        "In the future, we will try to investigate other unsupervised domain adaptation methods to further improve domain-specific UNMT performance."
    ],
    "5330": [
        "We propose a simple and general differentiable product quantization framework for learning compact embedding layers.",
        "Our proposed method is effective in a wide variety of language tasks.",
        "The proposed DPQ layer can also be used for end-to-end learning of general discrete codes in neural networks.",
        "DPQ variants give very similar top 10 nearest neighbors as the original full embedding.",
        "However, in DPQ-SX the neighbors have closer distances than the baseline, hence a tighter cluster; while in DPQ-VQ the neighbors are further from the original word."
    ],
    "5332": [
        "LM-generated falsified texts are very similar in style to LM-generated texts containing true content.",
        "Stylometry-based classifiers cannot identify auto-generated intentionally misleading content.",
        "Extending Veracity-based Benchmarks to better evaluate detectors against LM-generated misinformation.",
        "Improving Non-stylometry Methods for detecting misinformation, such as fact-checking, which makes fewer assumptions on available auxiliary information.",
        "Incorporating non-textual information can yield better results in fact-checking."
    ],
    "5334": [
        "The proposed RNNwA model with hand-crafted features achieves state-of-the-art accuracy on English and competitive performance on Spanish and Arabic for gender prediction from tweets.",
        "The improved model outperforms the simple projection baseline using fast-align on most languages, and achieves comparable performance to the state-of-the-art label projection approach with only half of the training time.",
        "The use of LSA-reduced n-grams and concatenation with the neural representation from RNNwA improves the model's performance.",
        "The model is self-learning, but there may still be a gender bias in the evaluation of the model due to the data itself.",
        "The model learns to predict the gender directly from tweets of the Twitter users, which may reflect any bias the users have."
    ],
    "5336": [
        "The authors introduce a new annotation scheme for question-answer pairs in natural conversation, defining five question types and seven answer types based on formal and functional criteria.",
        "The authors develop an annotation guide and annotate multi-lingual corpora with high inter-annotator agreement scores.",
        "The authors achieve above-baseline performance with a simple decision tree algorithm for automatic annotation, but there is much work to be done to make it practically feasible.",
        "The authors plan to expand the multilingual component of their work by adding language-specific guidelines, annotating more corpora, and adapting their machine learning algorithms to different languages."
    ],
    "5338": [
        "We address the challenge of generating data for training semantic parsers from scratch in multiple domains.",
        "We thoroughly analyze the OVERNIGHT procedure, and shed light on the factors that lead to poor generalization, namely logical form mismatch and language mismatch.",
        "We propose GRANNO, a method that directly annotates unlabeled utterances with their logical form, by letting crowd workers detect automatically generated canonical utterances.",
        "Our method substantially improves generalization to real data compared to OVERNIGHT.",
        "We demonstrate our method's success on two popular datasets."
    ],
    "5341": [
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Our experiment results and norm analysis show that Low-Rank Matrix Factorization works better in general than pruning, except for particularly sparse matrices.",
        "We also discover that inherent low-rankness and low nuclear norm correlate well, explaining why compressing multiplicative recurrence works better than compressing additive recurrence.",
        "In future works, we plan to factorize all LSTMs in the model, e.g. BiDAF model, and try to combine both Pruning and Matrix Factorization."
    ],
    "5345": [
        "BERT's vector space is unsuitable for common similarity measures like cosine-similarity.",
        "Fine-tuning BERT in a siamese/triplet network architecture (SBERT) achieves significant improvement over state-of-the-art sentence embeddings methods.",
        "SBERT is computationally efficient and can be used for tasks that are not feasible with BERT.",
        "Replacing BERT with RoBERTa does not yield a significant improvement in our experiments.",
        "SBERT can reduce the effort required for clustering 10,000 sentences with hierarchical clustering from 65 hours to about 5 seconds."
    ],
    "5346": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Exact search may not be practical, but it allowed us to discover deficiencies in widely used NMT models.",
        "The model often prefers the empty translation - an evidence of NMT's failure to properly model adequacy.",
        "Simple heuristics like length normalization are unlikely to remedy the problem satisfactorily."
    ],
    "5347": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "We demonstrated that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Our method is highly versatile and can be used in different latency scenarios.",
        "Our approach can be used for decoding with a variable number of encoder (\u2264 N ) and decoder (\u2264 M ) layers.",
        "We will make an in-depth analysis on the nature of our N \u00d7 M models, such as the diversity of hypotheses generated by different layers.",
        "Our approach can be combined with other techniques to further speed up decoding and improve model compaction.",
        "Our idea is applicable to other tasks based on deep neural networks."
    ],
    "5350": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our approach outperforms benchmark models across different datasets.",
        "We show that a tokenizer-free language model with sufficient capacity can achieve results that are competitive with word-based LMs.",
        "Our model reads raw byte-level input without the use of any text preprocessing.",
        "As such, the model has no direct access to word boundary information.",
        "We show that our model's intermediate representations capture word-level semantic similarity and relatedness across layers."
    ],
    "5355": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Document-level CQA: FigureQA and DVQA have well-defined image regions and all information needed to answer a question is contained in that image.",
        "To understand charts in documents, information in the rest of the document may be necessary to answer questions about the chart.",
        "Creation of such a dataset would greatly increase the challenge for future algorithms and better match real-world usage."
    ],
    "5356": [
        "The proposed unsupervised domain adaptation technique for neural machine translation is effective across settings.",
        "The technique adapts the model using domain-aware feature embeddings learned with language modeling.",
        "The method allows for controlling the output domain of translation results.",
        "Future work includes designing more sophisticated architectures and combination strategies, as well as validating the model on other language pairs and datasets."
    ],
    "5360": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "The proposed neural personal discrimination model with both adversarial discriminators and attention mechanisms can capture social correlations between different posts.",
        "The personal attributes and the effectiveness of the proposed neural personal discrimination model in modeling such personal attributes with significant performance improvement over state-of-the-art baselines."
    ],
    "5361": [
        "Our proposed guided policy learning method for joint reward estimation and policy optimization in multi-domain task-oriented dialog achieves higher task success and better user satisfaction than state-of-the-art baselines.",
        "GDPL can be equipped with NLU modules that identify the dialog acts expressed in utterance, and with NLG modules that generate utterances from dialog acts, allowing for an end-to-end scenario.",
        "The agenda-based user simulator is powerful to provide a simulated interaction for the dialog policy learning, but it needs careful design and is lacking in generalization.",
        "Training a neural user simulator is quite challenging due to the high diversity of user modeling and the difficulty of defining a proper reward function.",
        "GDPL may offer some solutions for multi-agent dialog policy learning where the user is regarded as another agent and trained with the system agent simultaneously."
    ],
    "5364": [
        "The majority training examples are not representative of the real-world data distribution, including the challenge data.",
        "Minimizing the average training loss no longer accurately describes our objective.",
        "We tackle the problem by adapting the learning objective to focus on examples that cannot be easily solved by biased features.",
        "Our debiasing method improves model performance on challenge data given known dataset bias.",
        "Current improvements largely rely on task-specific prior knowledge, thus an important next step is to develop more general methods that tackle different types of biases."
    ],
    "5365": [
        "The proposed data augmentation method with atomic templates for SLU achieves significant improvements on DSTC 2&3 dataset and is effective for SLU domain adaptation with limited data.",
        "The method can also be applied to SLU systems with other semantic representations (e.g. semantic frame).",
        "The generated utterances are associated with the atomic exemplars, and well-formed utterances can be produced for some frequent triples.",
        "The method has the potential to reduce human efforts in generating utterances for SLU systems.",
        "The use of atomic templates provides a way to generate utterances with minimum human efforts."
    ],
    "5368": [
        "'We have shown that a Transformer-based model can generate equations for math word problems, and it has an edge over RNN-based models.'",
        "'Jointly training two decoders with a shared encoder in the Transformer works better than using just one decoder.'",
        "'Reinforcement learning can further boost performance.'",
        "'So far the template retrieval method still beats generative models.'",
        "'This is partly due to the low quality of ground truth equations.'"
    ],
    "5374": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The prevalently-used (CRF-)BiLSTM-CNN in modeling cross-context for sequence-labeling NER has been shown to be inadequate, and the practical impacts have been analyzed on OntoNotes 5.0 and WNUT 2017.",
        "Additive and multiplicative cross-structures have shown to be crucial in modeling cross-context, significantly enhancing recognition of emerging, complex, confusing, and multi-token entity mentions.",
        "Remedying the core module of NER is important for future improvements on sequence-labeling NER."
    ],
    "5375": [
        "Exposing CWR-based models to shallow syntax has little effect on their performance, across several tasks.",
        "Linguistic probing shows that CWRs aware of such structures do not improve task transferability.",
        "The architecture and methods used in this study are general enough to be adapted for richer inductive biases, such as those given by full syntactic trees or different pretraining objectives.",
        "The current methods can be adapted for masked language modeling or other pretraining objectives.",
        "The performance of CWR-based models is not improved by exposing them to shallow syntax."
    ],
    "5376": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Better tuned deep learning settings in our multilingual and multitask models would be expected to outperform the existing state-of-the-art embeddings and algorithms applied to our data.",
        "The different annotation labels and comparable corpora would help us perform transfer learning and investigate how multimodal information on the tweets, additional unlabeled data, label transformation, and label information sharing may boost the classification performance in the future."
    ],
    "5378": [
        "SubQG achieved superior results than the existing approaches, especially for complex questions.",
        "Our experiments showed that SubQG merges query substructures to build new query structures for questions without appropriate query structures in the training data.",
        "We plan to add support for other complex questions whose queries require UNION, GROUP BY, or numerical comparison.",
        "We are interested in mining natural language expressions for each query substructure, which may help current parsing approaches."
    ],
    "5380": [
        "The IBM Science Summarizer is the first system that provides researchers with a tool to systematically explore and consume summaries of scientific papers.",
        "The IBM Science Summarizer will be expanded to support additional entities, such as methods, in future work.",
        "The IBM Science Summarizer will be openly available as a service and will include more papers in its corpus.",
        "The IBM Science Summarizer will be evaluated through an extensive user study, including automatic evaluation of the summaries.",
        "The IBM Science Summarizer has the potential to improve the efficiency and quality of research by providing researchers with accurate and informative summaries of scientific papers."
    ],
    "5381": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "FAMULUS helps experts in annotating data fast and reliable while successfully predicting entities and activities occurring in diagnostic texts.",
        "FAMULUS is applicable in real-time scenarios and generates feedback much faster than humans.",
        "Our approach can be adapted to many other disciplines requiring the training of diagnostic skills.",
        "We open-source all components necessary to employ FAMULUS in new case studies, hoping to encourage more research in this area."
    ],
    "5382": [
        "The proposed model significantly outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by the proposed model are coherent with human judgments.",
        "The proposed model achieves new state-of-the-art results on miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what has been used here for few-shot learning.",
        "The system can be easily extended to both multi-class and multilabel applications.",
        "Ranking documents jointly instead of independently may be preferred to account for challenges such as duplication of information or subtopics.",
        "The static transition probabilities used in the Viterbi smoothing technique are likely to degrade the effect on the output, and adding support for dynamic, contextualized estimations of transition probabilities will provide more fine-grained modeling of relevance."
    ],
    "5386": [
        "Our method outperforms previous methods for predicting concept features on two property norm datasets.",
        "By predicting plausible semantic features for concepts through the leveraging of corpus-derived word embedding data, our method offers a useful tool for guiding the expensive and laborious process of collecting property norm listings.",
        "Existing property norm datasets can be extended through human verification of features predicted with high confidence by Feature2Vec, with these features being added to the norms and subsequently incorporated into Feature2Vec in an iterative, semi-supervised manner.",
        "Feature2Vec provides a useful heuristic to add interpretable feature-based information to these datasets for new words in a practical and efficient way."
    ],
    "5387": [
        "Our proposed method achieves state-of-the-art results on five representative multi-choice MRC datasets including RACE.",
        "The experiment results consistently indicate the general effectiveness and applicability of our model.",
        "Our method integrates dual co-matching networks with two reading strategies to enhance multi-choice machine reading comprehension.",
        "Using strong pre-trained language models such as BERT and XLNet as encoder, our proposed method achieves state-of-the-art results.",
        "Our method shows the general effectiveness and applicability of using pre-trained language models for multi-choice MRC."
    ],
    "5388": [
        "The beam problem in NMT can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset, and we hope to see it included to make stronger baseline NMT systems.",
        "Solving the brevity problem leads to significant BLEU gains, but it remains to be seen how much improvement remains to be gained by solving label bias in general.",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed techniques have the potential to improve the accuracy of charge-based prison term prediction and total term prediction.",
        "There are ethical problems of the proposed techniques that are worth cautious thinking."
    ],
    "5391": [
        "DialogueGCN outperforms the strong baselines and existing state of the art, by a significant margin.",
        "Future works will focus on incorporating multimodal information into DialogueGCN.",
        "Speaker-level emotion shift detection\" is a potential future direction.",
        "Conceptual grounding of conversational emotion reasoning\" is another potential future direction.",
        "We also plan to use Dia-logueGCN in dialogue systems to generate affective responses."
    ],
    "5397": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The higher the encoder layer, the more relevant information is encoded into hidden states.",
        "Most of the disambiguation work is done by encoders.",
        "Self-attention can detect ambiguous nouns and distribute more attention to context words.",
        "Self-attention focuses on the ambiguous nouns themselves in the first layer, then keeps extracting features from context words in higher layers."
    ],
    "5398": [
        "The proposed model improves SOTA on the SQA dataset, particularly in handling conversational context effectively.",
        "The model predicts answers directly from the table.",
        "The model will be expanded with pre-trained language representations (e.g., BERT) to improve performance on initial queries and matching of queries to table entries.",
        "The model will be investigated for handling larger tables by sharding the table row-wise and running the model on all the shards before combining the answer rows."
    ],
    "5400": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our approach learns better representations that help the decoder to generate more accurate and fluent questions.",
        "In future work, we will adopt the auxiliary language modeling task to other neural generation systems to test its generalization ability."
    ],
    "5401": [
        "With the findings on how to best exploit BERT language model finetuning, we were able to train high-performing models.",
        "The XLNet-base model performs strongly on the ATSC task. Here, domain-specific finetuning could probably bring significant performance improvements.",
        "Domain-specific finetuning could probably bring significant performance improvements.",
        "Cross-domain adaptation experiments showed significant improvement over unadapted models.",
        "One cross-domain adapted model performs even better than a BERT-base model that is trained in-domain.",
        "Our findings reveal promising directions for follow-up work, including investigating cross-domain behavior for an additional domain like hotels."
    ],
    "5404": [
        "Our approach achieves significant improvements over baseline systems.",
        "The proposed semisupervised learning framework is effective for low-resource machine translation.",
        "Experimental results on ja-en and ug-en translations show that our approach achieves significant improvements.",
        "The proposed approach demonstrates the effectiveness of artificially creating source-ordered target sentences for data augmentation.",
        "Our approach is simple yet effective for low-resource machine translation.",
        "The use of divergent language pairs in experimental results demonstrates the effectiveness of the proposed approach on a wide range of language pairs."
    ],
    "5406": [
        "Our personalized generative models can generate plausible, personalized, and coherent recipes preferred by human evaluators for consumption.",
        "We introduce a set of automatic coherence measures for instructional texts as well as personalization metrics to support our claims.",
        "Our future work includes generating structured representations of recipes to handle ingredient properties, as well as accounting for references to collections of ingredients (e.g. \"dry mix\")."
    ],
    "5411": [
        "The beam problem in NMT can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Developing annotated datasets from multiple sources based on existing datasets and our causality tagging scheme can improve the performance of SCITE.",
        "Combining our method with distant supervision and reinforcement learning can achieve better performance without having to build a high-quality annotated corpus for causality extraction."
    ],
    "5412": [
        "The proposed model is sensitive to small errors in a fluent output, even when the training data is from statistical systems.",
        "The model can correctly identify incorrect pronouns, as shown by the attention maps in Figure 5 and the comparison of two system translations in Figure 6.",
        "The model scores the translation in Figure 6 (bottom) higher than the translation in Figure 6 (top), despite highlighting both occurrences of \"his\" as wrong.",
        "The translation in Figure 6 (bottom) is better because it maintains the animacy/human aspect and is consistent, even if the gender is wrong.",
        "Evaluation measures such as APT and AutoPRF are likely to yield the same accuracy/precision-recall for both cases."
    ],
    "5414": [
        "The proposed model, ReCoSa, significantly outperforms existing HRED models and their attention variants in multi-turn dialogue generation.",
        "The relevant contexts detected by the model are coherent with human judgments.",
        "The use of self-attention mechanism improves the ability of the model to capture long-distance dependencies.",
        "The proposed method can be further improved by introducing topical information and considering detailed content information in the relevant contexts.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "The success of contextualized word representations such as ELMo and BERT can be further improved by augmenting encoding-decoding loss with natural hyperlinks from Wikipedia.",
        "The contextualized entity encoder benefits more from the hyperlink-based training objective, suggesting future works to prioritize encoding entity description from its mention context."
    ],
    "5415": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Our model improves the accuracy of question type prediction by a large margin, and achieves new state-of-the-art results for question generation."
    ],
    "5416": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Applying the distributional semantic reward to reinforcement learning in abstractive summarization is effective, as demonstrated by our experimental results on Gigaword and CNN/Daily Mail datasets.",
        "The generated sentences have fewer repetitions, and the fluency is also improved.",
        "Our finding is aligned to a contemporaneous study (Wieting et al., 2019) on leveraging semantic similarity for machine translation."
    ],
    "5417": [
        "Pretrained sentence encoders can benefit from incorporating discourse information, as shown by the proposed training objectives and their effects on different tasks.",
        "The proposed DiscoEval evaluation suite can inspire additional research in capturing broad discourse context in fixed-dimensional sentence embeddings.",
        "The use of pretrained sentence encoders can be improved by strengthening their ability to incorporate discourse information, as demonstrated by the effects of the proposed training objectives on different tasks.",
        "There is a need for a comprehensive evaluation suite to assess the ability of pretrained sentence encoders to capture broad discourse context, as suggested by the proposed DiscoEval evaluation suite."
    ],
    "5418": [
        "We were able to improve on BERT's zero-resource crosslingual performance on the MLDoc classification and CoNLL NER tasks.",
        "Language-adversarial training was generally effective, though the size of the effect appears to depend on the task.",
        "Adversarial training moves the embeddings of English text and their non-English translations closer together, which may explain why it improves cross-lingual performance.",
        "Future directions include adding the language-adversarial task during BERT pre-training on the multilingual Wikipedia corpus, which may further improve zero-resource performance.",
        "Finding better stopping criteria for zero-resource crosslingual tasks besides using the English dev set."
    ],
    "5421": [
        "Applying state-of-the-art model like DistMult from UMLS to obtain knowledge graph embeddings can lead to improved state-of-the-art performance for the medical NLI task.",
        "Sentiments of medical concepts can contribute to the medical NLI task, opening a new direction for exploration.",
        "The proposed approach can be tried out in other domains for future exploration.",
        "Knowledge graphs in different domains can benefit from the proposed approach."
    ],
    "5423": [
        "Our approach achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "We propose a novel tagging scheme to incorporate the constraint that there is a maximum of one pun in each text.",
        "Our approach is generally applicable to both heterographic and homographic puns.",
        "Empirical results on the benchmark datasets prove the effectiveness of the proposed approach.",
        "Future research includes investigations on how to make use of richer semantic and linguistic information for detection and location of puns.",
        "Research on puns for other languages such as Chinese is still under-explored, which could also be an interesting direction for our future studies."
    ],
    "5424": [
        "We propose a novel cross-lingual pretraining method for unsupervised machine translation.",
        "Our method leverages Cross-lingual Masked Language Model (CMLM) to incorporate explicit and strong cross-lingual information into pre-trained models.",
        "Experimental results on en-fr, en-de, and en-ro language pairs demonstrate the effectiveness of our proposed method.",
        "In the future, we may apply our pre-training method to other language pairs and delve into the performance of the pre-trained encoders on other NLP tasks, such as Name Entity Recognition."
    ],
    "5426": [
        "The neglect of semantic overlapping between subspaces of the different attention heads increases the difficulty of translation.",
        "Our method can outperform strong baselines.",
        "Setting the capsule part close to the supervisory signals can improve translation performance."
    ],
    "5427": [
        "The proposed method for pre-training large scale language models achieves state-of-the-art performances on several Chinese natural language understanding tasks.",
        "Employing an effective functional relative positional encoding scheme leads to notable improvement over other positional encodings.",
        "Pre-training the NEZHA models integrates several techniques, including whole word masking strategy, mixed precision training, and the LAMB optimizer.",
        "The pre-training of the NEZHA models achieves state-of-the-art performances on several Chinese natural language understanding tasks.",
        "Continuing to improve NEZHA on Chinese and other languages and extending its applications to more scenarios is a future plan."
    ],
    "5428": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed QAInfomax is flexible to apply to different machine comprehension models.",
        "The augmented model achieves state-of-the-art results on Adversarial-SQuAD.",
        "In the future, we will investigate more methods for reducing the limitations of QAInfomax and improving the capability of generalization in QA systems."
    ],
    "5429": [
        "We presented a novel edge-oriented graph neural model for document-level relation extraction using multi-instance learning.",
        "The proposed model constructs a document-level graph with heterogeneous types of nodes and edges, modelling intraand inter-sentence pairs simultaneously with an iterative algorithm over the graph edges.",
        "To the best of our knowledge, this is the first approach to utilize an edge-oriented model for document-level RE.",
        "Analysis on intra-and inter-sentence pairs indicated that the proposed, partially-connected, document graph structure can effectively encode dependencies between document elements.",
        "Additionally, we deduce that document-level information can contribute to the identification of intrasentence pairs leading to higher precision and F1score.",
        "As future work, we plan to improve the inference mechanism and potentially incorporate additional information in the document-graph structure.",
        "We hope that this study will inspire the community to further investigate the usage of edge-oriented models on RE and other related tasks."
    ],
    "5430": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The substantial headroom between the best model performance and human performance encourages future research on contextual commonsense reasoning.",
        "The proposed Cosine Annealing Strategy combines two methods to improve domain adaption performance.",
        "The representation extractor uses Similarity Entropy Minimization to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The use of pseudo-labeled target-domain data and labeled source-domain data improves the domain adaption performance.",
        "The approach achieves new state-of-the-art on FewRel 2.0 dataset."
    ],
    "5431": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our model substantially boosts multilingual SRL performance by introducing deep enhanced representation, achieving new state-of-the-art results on the in-domain CoNLL-2009 benchmark for Catalan, Chinese, Czech, English, German, Japanese and Spanish.",
        "These results further show that syntactic information and deep enhanced representation can also promote multiple languages rather than only the case of English."
    ],
    "5433": [
        "Our approach significantly outperforms previous systems, reducing the error by 21% on English Switchboard.\" (based on the claim in the first sentence)",
        "Our method trained on the full dataset achieves competitive performance compared to previous systems, using less than 1% of the training data.\" (based on the second sentence)",
        "Using decoder-only transformers for abstractive summarization is effective and can achieve high performance without many standard tools in neural abstractive summarization.\" (based on the third sentence)",
        "Our approach yields highly fluent text and illustrates the power of unsupervised representation learning-based transfer learning for downstream tasks.\" (based on the last sentence)"
    ],
    "5435": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "QUASE has the potential to improve on many tasks, especially in the low-resource setting.",
        "Using additional QA datasets can help improve QUASE.",
        "Sentence encoding approaches can be used to retrieve signals from sentence-level QA pairs to help NLP tasks.",
        "p-QUASE generates latent 10 An average of BLEU1-BLEU4 scores representations related to attentions.",
        "s-QUASE provides latent sentence-level representations for tasks with a single-sentence input."
    ],
    "5437": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "The unified coherence model shows state of the art results on the standard coherence assessment tasks: the inverse-order and the global discrimination tasks.",
        "The unified coherence model is effective in assessing global and local coherence of texts."
    ],
    "5438": [
        "Our approach outperforms state-of-the-art results in AMR-to-text generation.",
        "We have studied the problem of generating text from AMR graphs using a novel architecture that explicitly encodes two parallel and adjuvant representations of the graph.",
        "Our models are able to achieve the best performance in AMR-to-text generation.",
        "In the future, we will consider integrating deep generative graph models to express probabilistic dependencies among AMR nodes and edges."
    ],
    "5440": [
        "The proposed approach improves the translation performance and ZP prediction accuracy.",
        "The two proposed approaches accumulatively improve the translation performance and ZP prediction accuracy.",
        "The models outperform existing ZP translation models in previous work.",
        "The approach achieves a new state-of-the-art on the widely-used subtitle corpus.",
        "Manual evaluation confirms that the performance improvement comes from the alleviation of translation errors.",
        "The improvement is mainly caused by subjective, objective as well as discourse-aware ZPs.",
        "There are two potential extensions to the work: evaluating the method on other implication phenomena and investigating the impact of different context-aware models on ZP translation."
    ],
    "5441": [
        "The proposed approach of structural position encoding consistently improves translation performance over both absolute and relative sequential position representations.",
        "The proposed approach can be applied to the decoder with RNN Grammars for future work.",
        "Inferring structure representations from AMR or external SMT knowledge is a potential future direction.",
        "The structural position encoding strategy improves translation performance on Chinese\u21d2English and English\u21d2German translation tasks."
    ],
    "5443": [
        "Our model better encodes homophily relations compared to other models when social information is proven useful.",
        "When social information is proved useful, our model's dynamic representations outperform static representations obtained with other models.",
        "We applied our model to three tasks and compared its performance against several competing models.",
        "Our model dynamically explores the connections of a user and identifies the ones that are more relevant for a specific task.",
        "Unlike most models proposed in the literature so far, which are tested on one single task, we applied our model to three tasks.",
        "We plan to perform a deeper investigation of cases in which social information does not prove beneficial and to assess the ability of our model to dynamically update the representation of the same author in different contexts."
    ],
    "5444": [
        "We propose a visual-aware PCR model that aligns contextual information with visual information and jointly uses them to find the correct objects that the targeting pronouns refer to.",
        "Our proposed model outperforms conventional PCR models, and jointly using visual information and contextual information is an essential path for fully understanding human language, especially dialogues.",
        "We present VisPro, the first large-scale visual-supported pronoun coreference resolution dataset, which focuses on resolving pronouns in dialogues that discuss a view that both speakers can see.",
        "Our approach demonstrates the effectiveness of using visual information and contextual information to resolve pronouns in dialogues.",
        "Jointly using visual information and contextual information is an essential path for fully understanding human language, especially dialogues."
    ],
    "5446": [
        "The proposed method provides an additional data-efficient tool in the modeling arsenal, which can be applied on its own or together with another training method.",
        "The method achieves state-of-the-art results for the task, and is also effective for improving on top of a strong pre-trained BERT model.",
        "The required conditional dependence between task labels is present in many situations.",
        "Other possible application of the method includes training language identification of tweets given geo-location supervision, training predictors for renal failure from textual medical records given classifier for diabetes, training a political affiliation classifier from social media tweets based on age-group classifiers, zip-code information, or social-status classifiers."
    ],
    "5449": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The proposed solution for representing unknown confounds in text classification using topic models and log-odds scores learns to make predictions using stylistic features, rather than focusing on topical information.",
        "The learning procedure presented is general and applicable to other tasks that require learning invariant representations with respect to some attribute of text.",
        "The proposed solution can be applied to other tasks where topics can be latent confounds, like predicting gender bias."
    ],
    "5450": [
        "The effectiveness of incorporating pseudo data for GEC: The study found that utilizing Gigaword as the seed corpus and pretraining the model with BACKTRANS (NOISY) data is effective in improving the performance of GEC.",
        "State-of-the-art performance on two test sets: The proposed settings for GEC achieved state-of-the-art performance on the CoNLL-2014 test set and the BEA-2019 test set.",
        "The importance of high-quality annotated data: The study highlighted the insufficiency of high-quality annotated data as a limiting factor for the performance of SCITE, and suggested possible solutions such as developing annotated datasets from multiple sources and combining the method with distant supervision and reinforcement learning.",
        "The potential of incorporating pseudo data for GEC: The study demonstrated the effectiveness of incorporating pseudo data for GEC and proposed suitable settings for its use."
    ],
    "5451": [
        "Word relationships in embedding space can be represented as simple geometric translations or complex non-linear transformations.",
        "There are parsimonious representations of relationships between words that can be represented as orthogonal and linear transformations.",
        "It is possible to easily learn an orthogonal or linear transformation for a word relationship given its mean translation vector.",
        "Analogical reasoning done using linear transformations is more accurate than using geometric translations.",
        "The findings offer novel insight into how downstream NLP models may be inferring word relationships.",
        "A single attention head in a Transformer has sufficient capacity to represent a semantic or syntactical word relationship.",
        "Certain attention heads have syntax-and position-specific behavior, concurring with recent findings."
    ],
    "5452": [
        "Our method for commonsense knowledge base completion is robust and unsupervised.",
        "We develop a method for expressing knowledge triples as sentences to estimate their validity.",
        "Our approach does not rely on any previous exposure to the Con-ceptNet database, ensuring that its performance is not biased.",
        "Our method has the potential to be extended to mining facts that are not commonsense and generating new commonsense knowledge outside of any given database of candidate triples.",
        "We see potential benefit in developing a more expansive set of evaluation methods for commonsense knowledge mining to strengthen the validity of our conclusions."
    ],
    "5453": [
        "Context-specificity is always accompanied by increased anisotropy.",
        "The anisotropy-adjusted similarity between words in the same sentence is highest in ELMo but almost non-existent in GPT-2.",
        "On average, less than 5% of the variance in a word's contextualized representations could be explained by a static embedding.",
        "Even in the best-case scenario, in all layers of all models, static word embeddings would be a poor replacement for contextualized ones.",
        "Contextualized representations have had on a diverse array of NLP tasks."
    ],
    "5457": [
        "We have proposed a novel Query-guided Capsule Network with an improved dynamic routing algorithm for enhancing context modeling for the document-level Neural Machine Translation Model.",
        "Experiments on English-German in different domains showed our model significantly outperforms sentence-level NMTs and achieved state-of-the-art performance on two of three datasets, which proved the effectiveness of our approaches.",
        "Our approach achieves better performance than sentence-level NMTs on two of three datasets.",
        "Our model significantly outperforms sentence-level NMTs and achieved state-of-the-art performance on two of three datasets.",
        "The proposed Query-guided Capsule Network with an improved dynamic routing algorithm enhances context modeling for the document-level Neural Machine Translation Model."
    ],
    "5461": [
        "Using discourse relations to effectively propagate polarities of affective events from seeds.",
        "The proposed method performed well even with a minimal amount of supervision.",
        "Event pairs linked by discourse analysis are shown to be useful, but they nevertheless contain noises.",
        "Adding linguistically-motivated filtering rules would help improve the performance."
    ],
    "5463": [
        "The joint goal accuracy of our model achieves similar performance compared with state-of-the-art on both the MultiWoZ dataset and the WoZ dataset.",
        "Our model has a constant inference time complexity with respect to the number of domains, slots, and values pre-defined in an ontology.",
        "The flexibility of our hierarchical encoder-decoder framework and the CMR decoder provides abundant future research directions.",
        "Applying the transformer structure, incorporating open vocabulary and copy mechanism for explicit unseen words generation, and inventing better dialogue history access mechanism to accommodate efficient inter-turn reasoning."
    ],
    "5465": [
        "We proposed a simple training fix to tackle posterior collapse in VAEs.",
        "Extensive experiments demonstrate the effectiveness of our method on both representation learning and language modeling.",
        "Our method improves the state-of-the-art and surpasses human accuracy on two datasets.",
        "PReFIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets."
    ],
    "5468": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed technique of adversarial bootstrapping is useful for dialogue modeling and addresses the issues of data-induced redundancy and exposure bias in dialogue models trained with maximum likelihood.",
        "A doubly bootstrapped system produces better performance than a system where only the teacherforcing MLE objective is used."
    ],
    "5470": [
        "Our method achieves performance gains while maintaining the model size.",
        "Our method exhibits improvement on downstream tasks with limited amounts of training datasets for fine-tuning.",
        "Our method helps improve the alignment quality of the phrase alignment model.",
        "Our approach generates representations suitable for the class of semantic equivalence assessment tasks.",
        "BERT's pre-training aims to generate generic representations transferable to a broad range of NLP tasks, but our method generates representations suitable for a specific task.",
        "Sentential and phrasal paraphrase relations help sentence representation learning."
    ],
    "5471": [
        "GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone.",
        "For the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "We have introduced the Unicoder which is insensitive to different languages.",
        "Our approach brings large improvements and becomes new state of the art on XNLI and XQA.",
        "The more languages we used in fine-tuning, the better the results.",
        "Even rich-resource language also could be improved."
    ],
    "5473": [
        "The proposed approach achieves state-of-the-art results on both the DVQA and PlotQA datasets.",
        "Existing VQA models perform poorly for Open Vocabulary questions, revealing the need for more accurate visual element detection to improve reasoning over plots.",
        "The proposed hybrid model with separate pipelines for handling simpler and complex questions is effective in improving the performance on PlotQA.",
        "The pipeline combines visual element detection, OCR with QA over tables, and separate pipelines for handling simpler and complex questions to improve the performance on Open Vocabulary questions."
    ],
    "5474": [
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Our proposed method can achieve better performance without having to build a high-quality annotated corpus for causality extraction.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Strengthening bi-directional mapping consistency significantly improves the effectiveness over the state-of-the-art method, leading to the best results on a standard benchmark."
    ],
    "5475": [
        "The experiments show that our method can give moderate gains for dependency parsing when leveraging gaze information.",
        "However, the experiments also show that there is room for improvement, especially coming from generalization capabilities across different treebanks.",
        "This also opens the question of whether a different architecture could better suit the purpose of leveraging the gaze information in a consistent way.",
        "A potential line of work could adapt human-attention approaches (Barrett et al., 2018) for structured prediction and word-level classification, although it would come at a cost of speed for parsing as sequence labeling (Strzyz et al., 2019b).",
        "We address this problem and propose a method of leveraging gaze features by using it as an auxiliary task both with and without parallel data.",
        "We obtain modest but positive improvements, which opens the question about how to increase the leverage of eye-tracking or other complementary data that is only available during training or comes from a different dataset."
    ],
    "5476": [
        "The beam problem in natural language processing can largely be explained by the brevity problem, which is caused by the locally-normalized structure of the model.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The proposed NE hypersphere model is a novel, open definition for monolingual NE recognition.",
        "The NE hypersphere model is useful and effective in two NE recognition related applications.",
        "The NE hypersphere model can guide the NE taggers over sentence to give significantly better performance without extra annotated data requirement on the given language besides the pretrained embedding."
    ],
    "5482": [
        "A user study shows that GetGoing's senior-tailored delivery improves comprehension and retention of information.",
        "The system will be improved further by concentrating on improving system understanding of older users and users' comprehension of system directions.",
        "In the future, the system will be improved further by concentrating on improving system understanding of older users and users' comprehension of system directions.",
        "Specifically future work will in part concentrate on improving the system's delivery to increase the retention of bus arrival times and intermediate trip locations."
    ],
    "5489": [
        "Our approach leverages corpus-linguistic statistics to guide the inference of cross-lingual dependency parsing.",
        "We compile these statistics into corpus-statistic constraints and design two inference algorithms on top of a graph-based parser based on Lagrangian relaxation and posterior regularization.",
        "Our approach improves the performance of the cross-lingual parser substantially, as shown by experiments on 19 languages.",
        "In the future, we plan to study the design and incorporation of fine-grained constraints considering multiple languages for cross-lingual transfer.",
        "We also plan to adapt this constrained inference framework to other cross-lingual structured prediction problems, such as semantic role labeling."
    ],
    "5490": [
        "We introduced formal verification of text classification models against synonym and character flip perturbations.",
        "Through experiments, we demonstrated the effectiveness of the proposed simplex bounds with IBP both during training and testing.",
        "Verifiably trained models achieve the highest exhaustive verification accuracy on SST and AG News.",
        "IBP verifies models in constant time, which is exponentially more efficient than naive verification via exhaustive search.",
        "Adversarial training compared with exhaustive verification has weaknesses.",
        "Exhaustive verification accuracy can be achieved by verifiably trained models."
    ],
    "5491": [
        "The proposed models achieve better performances by modeling hierarchical structure of sequence.",
        "The number of the main clause's subject can be predicted with high accuracy.",
        "The direct object of the main clause can be predicted with high accuracy.",
        "The proposed models outperform the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The use of the ON-LSTM model with a novel activation function and structured gating mechanism improves the performance of the hybrid model.",
        "The modification of the cascaded encoder by explicitly combining the outputs of individual components enhances the ability of hierarchical structure modeling in a hybrid model."
    ],
    "5497": [
        "The proposed dataset is effective in training data-driven neural models.",
        "The hybrid models produce more comprehensive summaries than abstracts and traditional citation-based summaries.",
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The proposed method can be used to overcome the limitations of abstracts and traditional citation-based summaries.",
        "The hybrid models integrate both authors' and community's insights to produce more comprehensive summaries.",
        "The dataset is orders of magnitude larger than prior datasets, facilitating future research in supervised scientific paper summarization."
    ],
    "5504": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "This result suggests that large-scale datasets with fine-grained image-level semantic labels, even when they do not dissect complex visual scenes, can benefit current state-of-the-art models - especially when applied to benchmarks where images are from diverse domains."
    ],
    "5506": [
        "PaLM outperforms strong baselines on language modeling.",
        "Incorporating syntactic supervision during training leads to further language modeling improvements.",
        "Training our unsupervised model on large-scale corpora could result in both stronger language models and, potentially, stronger parsers."
    ],
    "5507": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed AdaBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The KA GNE T module based on a GCN-LSTM-HPA architecture effectively represents graphs for relational reasoning purpose in a transparent, interpretable way, yielding a new state-of-the-art results on a large-scale general dataset for testing machine commonsense.",
        "Better question parsing methods to deal with negation and comparative question answering are potential future directions.",
        "Incorporating knowledge to visual reasoning is another potential future direction."
    ],
    "5509": [
        "The proposed joint model for spoken language understanding with Stack-Propagation improves the intent detection performance and achieves state-of-the-art performance on two datasets.",
        "Incorporating strong pretrained BERT models in SLU tasks leads to a new state-of-the-art level of performance.",
        "The proposed token-level intent detection improves the intent detection performance and eases error propagation.",
        "The incorporation of BERT models in SLU tasks achieves better results than previous state-of-the-art systems."
    ],
    "5510": [
        "Our method achieves favorable performance compared to various methods and setups.",
        "Unpaired captions and images can be easily collected from the web, facilitating application-specific captioning models where labeled data is scarce.",
        "Our framework leverages a small amount of paired data to train an image captioning model, resulting in favorable performance.",
        "The use of unpaired data for training image captioning models can facilitate the development of application-specific models where labeled data is scarce."
    ],
    "5511": [
        "The proposed semantics-aware BERT architecture achieves superior performance on a wide range of NLU tasks, surpassing all published works in all concerned tasks.",
        "Explicit contextual semantics can be effectively integrated with state-of-the-art pre-trained language representation for better performance improvement.",
        "The proposed method demonstrates the effectiveness of integrating accurate semantic signals for deeper comprehension and inference.",
        "Traditional methods that focus on heuristically stacking complex mechanisms may not be as effective as the proposed simple but effective method.",
        "The proposed method has the potential to shed light on fusing accurate semantic signals for deeper comprehension and inference."
    ],
    "5512": [
        "The proposed multi-granularity self-attention model achieves significant improvements over the baseline TRANS-FORMER in machine translation tasks.",
        "The model effectively captures useful phrase information, as shown by targeted evaluations on multi-granularity phrases.",
        "The approach is not limited to specific tasks and can be applied to other NLP tasks such as reading comprehension, language inference, and sentence classification.",
        "The proposed attention mechanism incorporates rich syntactic information, leading to better performance.",
        "The use of syntactic phrase-based mechanisms obtains the best results."
    ],
    "5514": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Partial translation is highly effective, and code-mixed treebanks can give significantly better results than full-scale translation.",
        "Our method is complementary with several other methods for cross-lingual transfer, such as annotation projection, and thus can be further integrated with these methods."
    ],
    "5515": [
        "We propose learning and decoding methods for extracting nested entities.",
        "Our decoding method iteratively recognizes entities from outermost ones to inner ones in an outside-to-inside way.",
        "Our method has no hyperparameters beyond those of conventional CRF-based models.",
        "Our method achieves 85.82%, 84.34%, and 77.36% F1-scores on ACE-2004, ACE-2005, and GENIA datasets, respectively.",
        "We would like to address the issue of joint modeling of NER with entity linking or coreference resolution while taking nested entities into account.",
        "Previous studies have demonstrated that leveraging mutual dependency of the NER, linking, and coreference tasks could boost each performance."
    ],
    "5518": [
        "Our proposed hybrid network accelerates Transformer decoding with significant speedup while maintaining comparable translation quality to the strong Transformer baseline.",
        "Our hybrid model fully takes advantage of the parallelization capability of self-attention and the fast decoding ability of RNN-based decoder.",
        "Pre-training our model using the MLE-based method and fine-tuning it using sequence-level knowledge distillation improves translation quality."
    ],
    "5519": [
        "Our proposed hierarchical encoder for table-to-text generation learns table representations from row, column, and time dimensions.",
        "The model consists of three layers that learn records' representation in three dimensions, combine those representations using sailency, and obtain row-level representation based on records' representation.",
        "During decoding, the model selects important table rows before attending to records.",
        "Our model achieves new state-of-the-art performance on the ROTOWIRE dataset of NBA games, as shown by both automatic and human evaluation results."
    ],
    "5522": [
        "LIBERT outperforms other unsupervised pretraining models on 9 out of 10 language understanding tasks from the GLUE benchmark, and for 3 lexical simplification benchmarks.",
        "Explicitly injecting lexical knowledge into pretraining improves the performance of unsupervised pretraining models.",
        "LIBERT has the potential to be applied to other languages and resource-poor scenarios.",
        "LIBERT has the potential to improve unsupervised pretraining models for language understanding tasks."
    ],
    "5523": [
        "The proposed architecture in this paper, called FlowSeq, is non-autoregressive and does not use any autoregressive flows.",
        "FlowSeq uses a non-autoregressive decoder with a latent variable z as the only input, which makes it possible to use highly-optimized implementations of RNNs such as those provided by cuDNN.",
        "The use of a non-autoregressive model in FlowSeq leads to efficient density estimation and generation, and the model does not suffer from the KL collapse problem.",
        "The proposed method is effective in generating sequences, and one potential direction for future work is to leverage iterative refinement techniques such as masked language models to further improve translation quality.",
        "Another exciting direction for future work is to theoretically and empirically investigate the latent space in FlowSeq, which could provide deeper insights into the model and allow for additional applications such as controllable text generation."
    ],
    "5524": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "We have explored simple and effective ways to alleviate or eliminate the beam problem.",
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains, but there may still be room for improvement by solving label bias in general.",
        "Our modification with shared words can make the target model fail, illustrating the robustness issue we reveal."
    ],
    "5525": [
        "Our approach places modifying sentiment words closer to the aspect target and can resolve potential syntactic ambiguity.",
        "TD-GAT-GloVe outperforms various baseline models.",
        "TD-GAT-BERT achieves much better performance.",
        "It is lightweight and requires fewer computational resources and less training time than fine-tuning the original BERT model.",
        "This paper is the first attempt directly using the original dependency graph without converting its structure for aspect level sentiment classification.",
        "Many potential improvements could be made in this direction.",
        "The local feature vector of an aspect node is the average of embedding vectors of words in the aspect and each word in the aspect is equally important.",
        "Future work could consider using an attention mechanism to focus on important words in the aspect.",
        "Incorporating dependency relation types into our model and taking part-of-speech tagging into consideration as well in the future.",
        "Combining such a graph-based model with a sequence-based model to avoid potential noise from dependency parsing errors."
    ],
    "5526": [
        "Our approach improves the state of the art in both AMR and UCCA, and is competitive to the best parser in SDP.",
        "Our transductive framework does not require a pre-trained aligner, and it is capable of building a meaning representation that is less anchored to the input text.",
        "Our approach is well suited to semantic parsing in cross-lingual settings.",
        "We hope to explore its potential in cross-framework and cross-lingual semantic parsing in the future."
    ],
    "5527": [
        "Our metric provides a promising direction towards a holistic metric for text generation and a direction towards more 'human-like' (Eger et al., 2019) evaluation of text generation systems.",
        "The latter has demonstrated strong generalization ability across four text generation tasks, oftentimes even outperforming supervised metrics.",
        "In future work, we plan to avoid the need for costly human references in the evaluation of text generation systems, and instead base evaluation scores on source texts and system predictions only, which would allow for 'next-level', unsupervised (in a double sense) and unlimited evaluation."
    ],
    "5528": [
        "The conventional framework of using transformer networks is not rich enough to capture entity semantics in certain cases.",
        "Proposing entity-centric ways to formulate richer transformer encoding of the process paragraph, guiding self-attention in a target entity-oriented way leads to significant performance improvements.",
        "The models still do not model the intermediate compositional entities and perform well by largely relying on surface entity mentions and verb semantics."
    ],
    "5529": [
        "In supervised extractive summarization, we cannot learn to rank by considering dataset sentences as independent educational examples.",
        "To overcome this issue, we suggested incorporating document features explicitly in the feature vector of sentences.",
        "We also suggested using features that take into account the properties of document.",
        "Conducted experiments demonstrated the benefit of adding explicit document features, as well as document-aware features, both in model precision and summary quality.",
        "For future work, more document-aware features can be examined.",
        "It is also possible to run the same experiments on an English (or any other language) dataset, if available."
    ],
    "5533": [
        "The common scenario where relation phrases are implied in NLQs can be addressed by employing the method in [19] to generate candidate edges for implied relation phrases.",
        "If the graph-structured query generated by the optimal query representation returns an empty answer, and the problem cannot be addressed deleting the constraints of class vertices on variables, generating another query based on candidate query representations with higher cost scores may be necessary.",
        "Other improved translation-based models such as TransH [41] and TransR [23] can also be adopted in the framework by modifying the functions computing cost scores, which may improve performance but increase the complexity of the framework and training time."
    ],
    "5537": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "SHiP, a novel combination of hierarchical modeling of clinical notes and language model pretraining, can improve discharge diagnosis classification over previous state-of-the-art models.",
        "Our work builds on a substantial recent literature on applying deep learning techniques to analysis of electronic health records data.",
        "The utility of hierarchical attention networks and pretraining methods applied jointly, and specifically within a clinical context."
    ],
    "5538": [
        "We proposed Uncertain Natural Language Inference (UNLI), a new task of directly predicting human likelihood judgments on NLI premise-hypothesis pairs.",
        "Not all NLI contradictions are created equal, nor neutrals, nor entailments.",
        "Eliciting supporting data is feasible.",
        "Annotations in the data can be used for improving a scalar regression model beyond the information contained in existing categorical labels.",
        "Humans are able to make finer distinctions between meanings than is being captured by current annotation approaches.",
        "We advocate the community strives for systems that can do the same, and therefore shift away from categorical NLI labels and move to something more fine-grained such as our UNLI protocol."
    ],
    "5539": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The ConMask model was able to capture the correct relationship in some cases where the name of the entity did not appear in the description.",
        "The reason for the error in predicting Gabrielle Stanton's notable work was because the indicator word for The Vampire Diaries was \"consulting producer\", which was not highly correlated to the relationship name \"notable work\" from the model's perspective.",
        "The ConMask model still has some limitations and room for improvement, such as the nature of the relationship-dependent content masking, which can lead to entities with similar names to the given relationships being ranked with a high score.",
        "Applying a filter to modify the list of predicted target entities so that entities that are same as the relationship will be rearranged may be an easy solution for improving the model's performance."
    ],
    "5543": [
        "We proposed using the Discrete Cosine Transform (DCT) as a mechanism to efficiently compress variable-length sentences into fixed-length vectors in a manner that preserves some of the structural characteristics of the original sentences.",
        "By applying DCT on each feature along the word embedding sequence, we efficiently encode the overall feature patterns as reflected in the low-order DCT coefficients.",
        "We showed that these DCT embeddings reflect average semantic features, as in vector averaging but with a more suitable normalization, in addition to syntactic features like word order.",
        "Experiments using the SentEval suite showed that DCT embeddings outperform the commonly-used vector averaging on most tasks, particularly tasks that correlate with sentence structure and word order.",
        "Without compromising practical efficiency relative to averaging, DCT provides a suitable mechanism to represent both the average of the features and their overall syntactic patterns."
    ],
    "5547": [
        "The proposed DELETER model finds an optimal deletion path for a sentence, where each node along the path is a grammatical subsequence.",
        "The DELETER model is comparable to state-of-the-art supervised models on Sentence Compression datasets.",
        "The DELETER model has not been fully explored in terms of its power, as it has only been applied to select one sentence as the output.",
        "The DELETER model can be applied to tasks such as Data Augmentation, where the training data of each epoch is dynamically assembled by randomly sampling an intermediate sentence along the deletion path of each sentence.",
        "The DELETER model can also be used to generate adversarial examples by deleting a few tokens and preserving the semantics of the original sentence."
    ],
    "5548": [
        "The proposed CASREL framework outperforms state-of-the-art baselines on relational triple extraction tasks, especially when extracting overlapping relational triples.",
        "Identifying relations is more challenging than identifying entities for the proposed CASREL model on WebNLG dataset.",
        "The number of relations contained in the dataset (i.e., 246 in WebNLG) makes it harder to identify relations compared to entities.",
        "The proposed CASREL model can simultaneously extract multiple relational triples from sentences without suffering from the overlapping problem.",
        "Modeling relations as functions that map subjects to objects provides a fresh perspective to revisit the relational triple extraction task."
    ],
    "5551": [
        "'Our approach achieves new state-of-the-art on FewRel 2.0 dataset.'",
        "'We first train a representation extractor with the Clustering Promotion Mechanism.'",
        "'The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.'",
        "'Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.'",
        "'Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.'",
        "'We present ConvBank, a new dataset of annotated automated speech transcripts, annotated with the Spoken Conversation Universal Dependencies (SCUD) annotation scheme to build UD-compatible parses.'",
        "'We demonstrate the utility of ConvBank by finetuning a dependency parser with a large dataset, reaching 85.05% on labeled dependency edges and 77.82% on unlabeled dependencies.'",
        "'In future work, we hope to expand the size of ConvBank to improve its utility to researchers in training parsing models and studying conversational systems.'",
        "'We also hope to improve the performance of our parsing system by jointly training dependency parsing with semantic role labeling.'"
    ],
    "5555": [
        "The proposed model significantly outperforms existing HRED models and their attention variants in multiturn dialogue generation.",
        "The self-attention mechanism is effective in capturing long distant dependency relations, improving the quality of generated responses.",
        "The relevant contexts detected by the model are coherent with human judgments.",
        "The proposed method can improve the quality of multiturn dialogue generation by using proper detection methods.",
        "Including PSP in the pre-training can differentiate between different sentence orders and improve pair-wise semantic reasoning.",
        "The document-level objective is effective, at least for the BERTbase model.",
        "Future work may involve investigating the way to take advantage of both large-scale training and the proposed method."
    ],
    "5556": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The accuracy of all models decreases dramatically along x-axis.",
        "OCD+MTL and OCD perform better after position 2, which demonstrates that they are more robust against error propagation, or exposure bias.",
        "Our method outperforms competitive baselines by a large margin.",
        "Scheduled sampling is effective in reducing exposure bias."
    ],
    "5561": [
        "The task of identifying the correct entity answer to a given user question based on a collection of unstructured reviews describing entities is introduced.",
        "The dataset for this task is large, with over 47,000 QA pairs, and requires processing 500 times more documents per question than most existing QA tasks.",
        "A clusterselect-rerank architecture is developed to bring together neural IR and QA models for an overall good performance.",
        "The best system registers a 25% relative improvement over baseline models, but only 21% of questions have a correct answer in the top 3.",
        "Neuro-symbolic methods that reason on locative and budgetary constraints could be an interesting direction of future work, as these types of questions constitute nearly 64% of the user constraints specified in questions in the dataset."
    ],
    "5562": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "Creating a dataset with human-generated question-answer pairs would be beneficial for advancing the field of CQA.",
        "A document-level CQA dataset that includes information from the rest of the document, not just the image, would increase the challenge for future algorithms and have numerous applications."
    ],
    "5563": [
        "'Our approach achieves new state-of-the-art on FewRel 2.0 dataset.'",
        "'We first train a representation extractor with the Clustering Promotion Mechanism to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.'",
        "'The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.'",
        "'Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.'",
        "'We explore the abilities of BERT embeddings and graph propagation to capture context relevant for these tasks.'",
        "'Combining these two approaches improves performance compared to using either one alone.'",
        "'Future work could extend the framework to other NLP tasks and explore other approaches to model higher-order interactions like those present in event extraction.'"
    ],
    "5564": [
        "Our proposed approach UPSA outperforms previous state-of-the-art unsupervised methods to a large extent.",
        "We propose a novel searching objective function that involves semantic preservation, expression diversity, and language fluency.",
        "Our model also includes a copy mechanism as the searching action.",
        "We plan to apply the SA framework on syntactic parse trees in hopes of generating more syntactically different sentences.",
        "Our approach outperforms most domain-adaptive paraphrase generators, as well as a supervised model on the Wikianswers dataset."
    ],
    "5566": [
        "We have presented a simple but efficient method to adapt neural NER for the cross-lingual settings.",
        "The proposed method benefits from multiple transferable knowledge and shows competitive performances with the state of the art using limited resources.",
        "We examine multiple factors that impact the transfer process and conduct an ablation study to measure their influences.",
        "Our experiment on Bengali shows that leveraging knowledge from Wikipedia will be a promising direction for future research."
    ],
    "5567": [
        "The proposed method improves out-of-domain performance across all settings.",
        "The bias product method is consistent but can be outperformed by the learned-mixin method with an appropriate entropy penalty.",
        "The reweight baseline improves performance on HANS but has limited effectiveness in other cases.",
        "Increasing out-of-domain performance often comes at the cost of losing some in-domain performance.",
        "TriviaQA-CP exhibits a minimal trade-off between out-of-domain and in-domain performance.",
        "The methods reduce the need for the model to solve examples the bias-only method works well on, which can limit the amount of training data.",
        "An ideal approach would be to block the model from using the bias-only method and require it to solve examples through other means.",
        "Our key contribution is a method of using human knowledge to improve model robustness to domain shifts.",
        "Our approach is to train a robust model in an ensemble with a pre-trained naive model, and then use the robust model alone at test time.",
        "Extensive experiments show that our method works well on two adversarial datasets and two changing-prior datasets, including a 12 point gain on VQA-CP.",
        "Future work includes learning to automatically detect dataset bias, which would allow our method to be applicable with less specific prior knowledge."
    ],
    "5568": [
        "The proposed graph-based approach for fact checking outperforms the state-of-the-art on the public leaderboard.",
        "The automatically constructed graph is derived based on semantic role labeling.",
        "The two graph-based modules, one for calculating contextual word embeddings using graph-based distance in XLNet, and the other for learning representations of graph components and reasoning over the graph, bring improvements to the system.",
        "Evidence selection is an important component of fact checking, and jointly learning evidence selection and claim verification model may be a potential solution."
    ],
    "5573": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "REE can reliably evaluate rules and its calculation time is lower than that for traditional rule evaluation models for some standard datasets.",
        "PBF also outperformed existing models.",
        "The results comprehensively show that AKGLG and observed feature models can effectively work together.",
        "Embedding models can be further extended in the future, but we need to consider how a model deals with rules for further development."
    ],
    "5577": [
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We demonstrate that recent summarization systems over-exploit the inherent lead bias present in news articles, to the detriment of their summarization capabilities.",
        "We explore two techniques aimed at learning to better balance positional cues with semantic ones.",
        "While our auxiliary loss method achieves significant improvement, we note that there is a large gap which better methods can hope to bridge in the future.",
        "One approach, building on ours, is to examine other ways to combine loss signals (Finn et al., 2017) , and to encourage exploration (Haarnoja et al., 2018) ."
    ],
    "5579": [
        "We introduced a new task of Counterfactual Story Rewriting that challenges current language understanding and generation systems with counterfactual reasoning.",
        "Our new dataset, TIMETRAVEL, provides nearly 30k counterfactual revisions to simple commonsense stories together with over 100k counterfactual sentences.",
        "Neural language models show promises, but generally have difficulties in rewriting the consequences of the counterfactual condition with full consistency.",
        "The empirical results demonstrate that integrating true reasoning capabilities to neural language models is necessary for more focused research."
    ],
    "5580": [
        "The new Birds-to-Words dataset and Neural Naturalist model are introduced for generating comparative explanations of fine-grained visual differences.",
        "The dataset features paragraph-length, adaptively detailed descriptions written in everyday language.",
        "The authors hope that continued study of this area will produce models that can aid humans in critical domains like citizen science.",
        "The new Birds-to-Words dataset and Neural Naturalist model are introduced for generating comparative explanations of fine-grained visual differences. (sentence: \"We present the new Birds-to-Words dataset and Neural Naturalist model for generating comparative explanations of fine-grained visual differences.\")",
        "The dataset features paragraph-length, adaptively detailed descriptions written in everyday language. (sentence: \"This dataset features paragraph-length, adaptively detailed descriptions written in everyday language.\")",
        "The authors hope that continued study of this area will produce models that can aid humans in critical domains like citizen science. (sentence: \"We hope that continued study of this area will produce models that can aid humans in critical domains like citizen science.\")"
    ],
    "5582": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Intrinsic evaluations demonstrate that the addition of WordNet and Wikipedia to BERT improves the quality of the masked LM and significantly improves its ability to recall facts.",
        "Downstream evaluations demonstrate improvements for relationship extraction, entity typing, and word sense disambiguation datasets.",
        "Future work will involve incorporating a diverse set of domain-specific KBs for specialized NLP applications."
    ],
    "5584": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing",
        "the discovery of root-and-pattern morphology in Semitic languages using an unsupervised method",
        "the extracted roots are the basic units of these languages",
        "our model can learn to generate high-quality training and prediction policies to help both the training and testing process of multi-label classifiers in a principled way",
        "our method outperforms other baselines"
    ],
    "5585": [
        "We described our submitted models to the Arabic author profiling and deception detection shared task (APDA) [32].",
        "We focused on detecting age, dialect, and gender using BERT models under various data conditions, showing the utility of additional, in-house data on the task.",
        "A majority vote of our models trained under different conditions outperforms single models on the official evaluation.",
        "In the future, we will investigate automatically extending training data for these tasks as well as better representation learning methods."
    ],
    "5587": [
        "'Our approach takes into account the diversity of target data in domain adaptation for sequence labeling tasks.'",
        "'Our model can fuse knowledge at multiple levels, including the sample level and element level.'",
        "'Experimental results demonstrate the effectiveness of our approach on different tasks.'",
        "'Our approach has potential applications in a broader range of domain adaptation scenarios.'",
        "'Our method can achieve multi-level fusion of knowledge from both the source model and target data.'"
    ],
    "5589": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The user's age has a significant effect on the ratings. For example, older users find jokes as a response to harassment highly inappropriate.",
        "Perceived appropriateness also depends on the type of previous abuse. For example, avoidance is most appropriate after sexual demands.",
        "All system were rated significantly higher than our negative adult-only baselines -except two data-driven systems, one of which is a Seq2Seq model trained on 'clean' data where all utterances containing abusive words were removed."
    ],
    "5590": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Image captioning models are usually evaluated without explicitly considering their ability to generalize to unseen concepts.",
        "We hope that this work will encourage researchers to design models that better reflect human-like language production.",
        "Future work includes extending the evaluation to other concept pairs and other concept classes, analyzing the circumstances in which the re-ranking step improves compositional generalization, exploring the utility of jointly trained discriminative re-rankers into other NLP tasks, developing models that generalize to size modifier adjectives, and devising approaches to improve the handling of semantically equivalent outputs for the proposed evaluation metric."
    ],
    "5591": [
        "The proposed corpus-free framework can generate diverse data with only templates and train a data-driven user simulator on it, improving the agent policy by 6.36% success rate.",
        "The novel State2Seq user model predicts user response with attention on refined dialogue context representations, outperforming the Seq2Seq baseline for 1.9 F-score.",
        "The use of attention and refined context representation in the State2Seq model improves the user simulator's ability to predict user response more accurately."
    ],
    "5592": [
        "The authors tackle the unaddressed problem of controllable content selection in text generation.",
        "The proposed framework based on variational inference can be potentially applied to arbitrary tasks.",
        "The model outperforms state-of-the-art models regarding the diversity and controllability of content selection.",
        "The authors introduce an effective way to achieve a performance/controllability trade-off, which can be easily tuned to meet specific requirements."
    ],
    "5596": [
        "We have proposed a bottom-up approach for extracting a model's preferences and consequently understanding opaque neural network architectures better.",
        "Our approach can be used to analyze examples, compare classifiers, and generate sentences with opposite labels.",
        "We were able to uncover significant differences of what the respective architectures want to read by sifting out what models have encoded in order to understand their limits and create fair representations in the future.",
        "Our approach is an important first step towards abstractive interpretability methods and bottom-up bias identification in NLP."
    ],
    "5597": [
        "We have proposed neural embedding allocation (NEA) for learning interpretable vector-space embeddings of words, documents, topics, and authors by deconstructing topic models to reveal underlying semantic representations.",
        "Our experimental results show that our proposed NEA method successfully mimics topic models with nuanced vector representations, while performing better than them at many tasks.",
        "The proposed NEA algorithm can smooth out topic models' parameters to improve topic coherence and author modeling, and produces vector representations which improve document categorization performance.",
        "We plan to use NEA to study and address gender bias issues in natural language processing."
    ],
    "5598": [
        "We have expanded the traditional statetracking task with the additional task of predicting and explaining dependencies between steps, and presented a new model, XPAD.",
        "Experiments show that XPAD significantly improves the predictions of correct dependencies between actions, while matching state-of-the-art results on the earlier tasks.",
        "XPAD is biased to prefer predictions that have an identifiable purpose (enables a subsequent step), and where that purpose is more plausible (judged using a large corpus).",
        "Given appropriate annotations, XPAD could be trained with a richer set of state change operators, longer paragraphs, and (with the addition of a temporal ordering module, e.g., (Ning et al., 2017) ) nonchronological event orderings.",
        "In addition, given additional hand-labeled training data, a system might learn to directly predict dependency links (instead of deriving them heuristically from state changes)."
    ],
    "5599": [
        "The proposed method achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what has been used here for few-shot learning.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "The intrinsic and extrinsic evaluations of the rules validate the pattern discovery method and root extraction method (JZR).",
        "The use of evidence fragment detection task can improve the downstream-task performance via transfer learning or in a pipeline fashion.",
        "The evidence fragment detection task can help biocurators delineate evidence fragments as independent documents so they can be cataloged, indexed, and reused.",
        "The convention of only linking claims to evidence from a single paper or of following citations can be discarded with the help of evidence fragments.",
        "The scientific discourse tagger, claim extractor, and evidence fragment detector may serve as the actual implementation of the modules in a future framework for integrating scientific argument and evidence from scientific documents.",
        "The proposed method can dramatically increase the amount of primary evidence used to generate individual claims, thereby improving the quality of those claims."
    ],
    "5600": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "We have proposed novel methods for multilingual fine-tuning of languages that outperform models trained with far more data and compute on two widely studied cross-lingual text classification datasets on eight languages in both the zero-shot and supervised setting."
    ],
    "5601": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We propose Definition Frames, a hybrid semantically interpretable representation that is grounded in both lexical semantics and distributed representations.",
        "By disentangling the Qualia structure relations, DFs can capture different types of similarity (relatedness and similarity) and achieve improved performance on word similarity tasks.",
        "Finally, we demonstrate the explainability of Definition Frames via a human study showing that they provide valid insights on how terms are related.",
        "DFs are independent of the distributed representation used as basis.",
        "Future work could explore the use of contextual embeddings basis and the benefits of Definition Frames in downstream tasks."
    ],
    "5602": [
        "The beam problem in neural machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The performance of conditional language models is substantially better than widely-known baselines when initializing the decoder with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "The proposed probabilistic diversity and uncertainty network (PDUN) improves the performance of visual dialog by around 3.5%.",
        "The use of the diversity module obtained through a variational autoencoder allows for generating diverse answers.",
        "The diversity of the proposed network is high compared to variants of the method."
    ],
    "5604": [
        "It is possible to precompute a discrete set of possible solutions that contains one correct option for many QA tasks.",
        "Our approach significantly outperforms previous approaches on six QA tasks, achieving absolute gains of 2-10% and setting the new state-of-the-art on five well-studied datasets.",
        "The discrete latent variable learning algorithm iteratively predicts the most likely solution in the precomputed set and further increases the likelihood of that solution.",
        "Our method achieves new state-of-the-art results on five well-studied datasets.",
        "Our approach can be applied to a variety of QA tasks, including reading comprehension, open-domain QA, discrete reasoning task, and semantic parsing."
    ],
    "5605": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Developing annotated datasets from multiple sources based on existing datasets and our causality tagging scheme could improve the performance of SCITE.",
        "Combining our method with distant supervision and reinforcement learning could achieve better performance without having to build a high-quality annotated corpus for causality extraction.",
        "Simple, small models that are not much worse than a couple of times bigger and more complex ones could be created using the insights from this work.",
        "The best models (Wo-BiLSTM-CRF and WoCh-BiLSTM-CRF) used in our experiments with a couple of state-of-the-art architectures and test the performance of BiLSTM-CRF models using contextual and language model-based embeddings."
    ],
    "5611": [
        "Our approach combines sequential and recursive linguistic properties for NER, using syntactic and semantic features extracted from TLSTM and BLSTM.",
        "The relative attention mechanism allows our model to exploit linguistic properties over the sentence with respect to every word.",
        "The global attention mechanism combines semantic and syntactic patterns to balance the attention.",
        "Our model learns to detect words that disclose information of entity types based on their syntactic properties.",
        "Our approach establishes a new state of the art on two datasets."
    ],
    "5612": [
        "Providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Incorporating commonsense knowledge into the learning process of event embeddings can improve the quality of event representations and benefit downstream applications.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "Using a five-action transition system, we develop three classifiers to resolve structural, tagging, and labeling conflicts."
    ],
    "5615": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model for part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Our novel end-to-end model for joint slot label alignment and recognition requires no external label projection and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our simple approach involving transfer learning reaches the same performance as the state-of-the-art while being computationally inexpensive.",
        "Our future work will involve adding larger pre-trained language models like RoBERTa and XLNet."
    ],
    "5616": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning.",
        "The model using just ArgRewrite for training was able to correctly classify a revision that was misclassified by all models using AESW data.",
        "Including revision context could help improve both annotation and classification performance.",
        "The length difference was a top 5 feature in all experiments, and the best model (Ar-gRewrite+AESW plaintext) had a length difference of 4.81 for predicted Better revisions and -3.99 for predicted NotBetter revisions."
    ],
    "5617": [
        "Our approach achieves state-of-the-art performance on the CommonsenseQA dataset.",
        "We propose an approach consisting of knowledge extraction and graph-based reasoning to improve commonsense question answering.",
        "The graph-based approach consists of a graph-based contextual word representation learning module and a graph-based inference module to select the right answer.",
        "Our model achieves accuracy similar to that of linguistic-based models and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerge as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits."
    ],
    "5618": [
        "'Our model outperforms state-of-the-art methods on both the existing automatic metrics and the proposed assessment criteria.'",
        "'The experimental results show that our model significantly outperforms existing HRED models and its attention variants.'",
        "'Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.'",
        "'ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.'",
        "'The proposed assessment criteria demonstrate that our model significantly outperforms state-of-the-art methods in terms of relevance, coherence, and expressiveness.'"
    ],
    "5620": [
        "Our proposed method for cross-lingual NER leverages MT for translating entities and achieves state-of-the-art results on a diverse set of languages.",
        "Our method outperforms state-of-the-art monolingual baselines for Armenian, an actual medium-resource language.",
        "The dependence on MT restricts our approach to languages covered by off-the-shelf MT systems, but these systems continue to improve in coverage and quality.",
        "As translation quality improves, approaches like ours are poised to benefit.",
        "Our method falls short of NER models trained on large monolingual corpora, but a significant portion of this degradation is due to distribution shift.",
        "Incorporating domain adaptation techniques could be a promising route to improving our models, as evidenced by the improvement in Hindi F 1 in Parallel regime."
    ],
    "5621": [
        "Our proposed method can perform well on OOD detection tasks in low-resource settings, as demonstrated by evaluation on real-world datasets.",
        "Our method does not adversely affect the performance of few-shot ID classification.",
        "The proposed method is inspired by Prototypical Networks.",
        "The method uses a new approach to tackle OOD detection in low-resource settings.",
        "The evaluation results show that our method performs favorably against state-of-the-art algorithms on OOD detection."
    ],
    "5622": [
        "The proposed end-to-end event temporal relation extraction system achieves effective performance for extracting events and relations simultaneously, avoiding error propagation in previous pipeline systems.",
        "The novel neural structured prediction model with joint representation learning improves the performances of previously published systems by 10% and 6.8% on the TB-Dense and MATRES datasets, respectively.",
        "Future research can focus on creating more robust structured constraints between events and relations, especially considering event types, to improve the quality of global assignments using ILP.",
        "A better event model is generally helpful for relation extraction, and incorporating multiple datasets can enhance the performance of the event extraction systems."
    ],
    "5623": [
        "The proposed approach, STYLEFUSION, bridges conversation models and non-parallel style transfer by structuring a shared latent space.",
        "The system can generate stylized relevant responses by sampling in the neighborhood of the model prediction.",
        "The system can continuously control style intensity by modulating the sampling radius.",
        "The approach demonstrates improvements in two tasks: generating arXiv-like and Holmes-like conversational responses.",
        "The system outperforms competitive baselines without sacrificing relevance.",
        "In future work, the technique will be used to distill information from other nonparallel datasets, such as external informative text."
    ],
    "5624": [
        "The proposed model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model is able to achieve a state-of-the-art level F1-score (0.69) while producing human interpretable explanations.",
        "The paper provides an in-depth comparative quantitative and qualitative analysis on the contributions of different types of data sources for the generation of explicit semantic vector spaces.",
        "The model is able to generate explicit semantic vector spaces using composition of definitions, visual features, and commonsense knowledge graphs.",
        "The inclusion of additional data sources can increase model coverage and improve the generation of explicit semantic vector spaces.",
        "The model is able to produce human interpretable explanations for the generated explicit semantic vector spaces.",
        "The paper demonstrates the complementarity aspect of different resources for the generation of explicit semantic vector spaces."
    ],
    "5629": [
        "The system achieved a score of 59.39 F 0.5 in the Restricted Track of the BEA 2019 Shared Tasks, finishing 10th out of 21 participants.",
        "The system utilized Wikipedia revision edits as a training data and reached a score of 44.14 F 0.5 in the Low Resource Track.",
        "The system was finetuned using annotated training data, obtaining a score of 65.55 F 0.5 in the Unrestricted Track, ranking 3rd out of 7 submissions.",
        "The iterative decoding algorithm is worth exploring more thoroughly, specifically allowing threshold parameter to change in each iteration may provide gains.",
        "Training systems on Wikipedia revisions in other languages may be a future work."
    ],
    "5632": [
        "Our approach does not rely on labeled entity linking data in the target domain and models fine-grained entity properties.",
        "With the domain independent setting, our approach achieves strong results on the CoNLL dataset.",
        "In a harder setting of unknown entities derived from the WikilinksNED dataset, our approach generalizes better than a state-of-the-art model on the dataset."
    ],
    "5633": [
        "We proposed a self-assembling neural modular network for multi-hop QA.",
        "Our model outperforms both the single-hop baseline and the original NMN on HotpotQA (Yang et al., 2018).",
        "We presented analyses to show that our model does in fact learn to perform compositional reasoning.",
        "Our model can dynamically assemble the modular network based on the question."
    ],
    "5635": [
        "Simulation-based collection is a better alternative for collecting datasets like this owing to the factors below.",
        "Fewer Annotation Errors: All annotations are automatically generated, so these errors are rare.",
        "Simpler Task: The crowd worker task of paraphrasing a readable utterance for each turn is simple.",
        "Low Cost: The simplicity of the crowd worker task and lack of an annotation task greatly cut data collection costs.",
        "Better Coverage: A wide variety of dialogue flows can be collected and specific use cases can be targeted.",
        "We used the conversational flows present in other public datasets like MultiWOZ 2.0 and WOZ2.0 as a guideline while developing the dialogue simulator."
    ],
    "5638": [
        "The proposed method outperforms conventional clustering-based methods evaluated on simulated speech mixtures.",
        "Furthermore, domain adaptation with real speech data achieved a significant DER reduction on the CALLHOME dataset.",
        "The proposed end-to-end neural speaker diarization method is directly optimized with a diarization-error-oriented objective.",
        "The experimental results show that the proposed method outperforms conventional clustering-based methods."
    ],
    "5639": [
        "The proposed approach achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model improves the performance of both POS tagging and dependency parsing.",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing.",
        "The use of augmented data can achieve comparable performance to the original training data in MT, but may not be effective for all sentence rewriting tasks.",
        "Pre-training the model with the augmented data is better than training together with the original training data for sentence rewriting tasks."
    ],
    "5641": [
        "Neural dialogue models can be manipulated by crafted adversarial examples, posing a security risk for their practical implementation.",
        "The proposed Reverse Dialogue Generator can automatically craft inputs to manipulate the black-box neural dialogue model, demonstrating the effectiveness of our approach.",
        "Extensive experiments on a representative neural dialogue model demonstrate the vulnerability of dialogue systems to manipulation, highlighting the need for security concerns and privacy issues to be addressed in future works.",
        "The current framework can be extended to other sequence models, providing a foundation for future research on the security and privacy of dialogue systems."
    ],
    "5642": [
        "We propose methods from two aspects to address the low qualities of open-domain NER datasets built with distant supervision.",
        "Introducing external supervision corrects false labels and improves open-domain NER performances.",
        "Our correction model, trained with small but high-quality DocRED, is strong external supervision.",
        "Using our correction model, we build AnchorNER which covers various entities and achieves better open-domain NER performances.",
        "Our multi-task learning method makes 'O' labels different from each other based on their context.",
        "Previous open-domain NER datasets suffer from the low RAT problem, and our method is proposed to solve it.",
        "Built with our correction model, AnchorNER does not have such problem anymore."
    ],
    "5643": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "The model size in neural machine translation can be compressed to approximately 7.9x smaller than 32-bit floats using a 4-bit logarithmic quantization.",
        "Retraining after quantization is necessary to restore the model's performance.",
        "Bias terms behave differently and can be left uncompressed without affecting the compression rate significantly."
    ],
    "5644": [
        "The proposed framework offers a systematized view on human biases encoded in word embeddings.",
        "The main results indicate that our debiasing methods can effectively attenuate biases in arbitrary input distributional spaces.",
        "Our debiasing methods can also be transferred to a variety of target languages.",
        "The proposed framework includes new debiasing methods that deal with the two different bias specifications.",
        "The main results show that our debiasing methods can effectively attenuate biases in arbitrary input distributional spaces."
    ],
    "5649": [
        "The Visual Genome project has some data issues that limit its reliability for certain tasks.",
        "The use of large language models and unsupervised datasets has led to a recent surge in NLP benchmark performance.",
        "Transfer learning using NLP models is possible even with limited labeled data.",
        "The Attention Graph mechanism can benefit both NLP and visual domains.",
        "The Attention Graph architecture can be extended to enable graphs with arbitrary connectivity.",
        "Using attention as a way of deriving structure is an interesting avenue for future work."
    ],
    "5650": [
        "Our proposed method for aspect-level sentiment classification effectively incorporates aspect information into the convolutional neural network architecture, as shown by comparisons with baseline methods.",
        "Our model learns aspect-specific sentiment expressions significantly better than multiple strong neural baselines.",
        "To the best of our knowledge, our model is the first attempt using convolutional neural networks to solve this problem.",
        "Our work could inspire future research exploring this direction, and it is interesting to see if such parameterized CNN architecture could benefit other natural language processing tasks involving text pairs like question answering task."
    ],
    "5653": [
        "The proposed QG model achieves new state-of-the-art performances when using semantics-enhanced rewards to regularize the model.",
        "The QG model's ability to mimic human annotators in generating QA training data is directly evaluated using a QA-based evaluation method.",
        "The use of synthetic data generation methods along with a data filter and mixing mini-batch training improves both BiDAF and BERT QA baselines.",
        "The approach improves the performance of QA models without introducing new articles, questions, or any data size reduction.",
        "The influence of synthetic data size is investigated by sampling 10% to 100% examples from HarvestingQA."
    ],
    "5654": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further."
    ],
    "5656": [
        "We have demonstrated that cascaded models are very competitive when not constrained to only train on AST data.",
        "With data augmentation, pretraining, fine-tuning and architecture selection, we trained end-to-end models that show competitive performance when compared to cascade approach.",
        "Our approaches reduced the performance gap between end-to-end and strong cascade models, from 8.2 to 1.4 BLEU on En-Fr Librispeech AST data and from 6.7 to 3.7 on the En-Ro MuST-C corpus.",
        "We also analyzed the effect of TTS data in terms of quality, quantity, and the use of single speaker vs. multiple speakers, and we provide recommendations on how to harness this type of data.",
        "In the future, we would like to investigate how to better use larger-scale TTS-generated data."
    ],
    "5657": [
        "The proposed approach of building a single multilingual graphemic hybrid ASR model can effectively perform language-independent recognition on any within training set languages, and substantially outperform each monolingual ASR alternative.",
        "The approach can provide better ASR performance and serve as an alternative to a typical production deployment, which typically includes extensive monolingual ASR systems and a separate language ID model.",
        "The proposed approach can greatly simplify the productionizing and maintenance process.",
        "The approach can give comparable performance without requiring separate language ID guidance during decoding.",
        "Future work will expand the language coverage to include both geographically proximal and distant languages."
    ],
    "5659": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Incorporating tree structures into Transformer is an important and worth exploring direction, because it allows Transformer to learn more interpretable attention heads and achieve better language modeling.",
        "The interpretable attention can better explain how the model processes the natural language and guide the future improvement."
    ],
    "5662": [],
    "5664": [
        "We have presented EMU, a semantic specialization framework for multilingual sentence embeddings.",
        "EMU incorporates multilingual adversarial training on top of fine-tuning to enhance multilinguality without using parallel sentences.",
        "Our experimental results show that EMU outperformed the baseline methods including state-of-the-art multilingual sentence embeddings, LASER, and monolingual sentence embeddings after machine translation with respect to multiple language pairs.",
        "The results also show that EMU can successfully train a model using only monolingual labeled data and unlabeled data in other languages."
    ],
    "5666": [
        "Our proposed framework improves entities consistency by querying KB in two steps.",
        "We introduce a novel approach to filtering irrelevant KB entities and encouraging consistent generation using a single KB row.",
        "We perform attention mechanism to select the most relevant KB column, leading to improved task-oriented dialogue generation.",
        "Our method is effective, as shown by experimental results.",
        "Extensive analysis confirms the observation and reveals the correlation between the success of KB query and the success of task-oriented dialogue generation."
    ],
    "5667": [
        "Our approach, Cross-Lingual BERT Transformation (CLBT), outperforms previous state-of-the-art in zero-shot cross-lingual dependency parsing.",
        "Our method provides a fast and data-efficient solution to learning cross-lingual contextualized embeddings, leveraging publicly available pre-trained BERT models.",
        "Compared to XLM, our method requires much fewer parallel data and less training time, yet achieving comparable performance.",
        "We are interested in exploring unsupervised cross-lingual alignment, inspired by prior success on static embeddings.",
        "Our approach has the potential to achieve better performance with a deeper understanding of the geometry of the multilingual contextualized embedding space."
    ],
    "5671": [
        "The proposed Collaborative Memory Network (CM-Net) is effective in jointly modeling slot filling and intent detection.",
        "The CM-Net captures semantic correlations among words, slots, and intents in a collaborative manner.",
        "The CM-Net incrementally enriches the information flows with local context and global sequential information.",
        "The proposed CM-Net demonstrates effectiveness and generalizability on two standard benchmarks and a new corpus (CAIS).",
        "The CAIS corpus is contributed to the research community."
    ],
    "5673": [
        "Our approach can transfer dialogue state tracking models across domains without target-domain supervision at the turn level.",
        "Our method uses a transfer learning method with supervised learning to learn a base model, and then fine-tune using reinforcement learning with dialogue level reward.",
        "Our results show consistent improvements over domain transfer baselines without fine-tuning, at times showing similar performance to in-domain models.",
        "Using the dialogue-level reward signal for fine-tuning can further improve supervised models in-domain.",
        "Dialogue-level feedback is almost as useful as turn-level labels."
    ],
    "5674": [
        "The proposed evaluation method based on communication-based quality judgments outperforms standard n-gram-based methods in a simple color reference game setting.",
        "The communication-based evaluation method can be applied to various NLG tasks, such as summarization, image captioning, translation, and pure text generation.",
        "The method is effective in capturing the effectiveness of an utterance in leading a listener to accurately recover the speaker's communicative intentions.",
        "The method can be used in any setting where there is a well-defined action for a listener to perform in response to an utterance.",
        "The proposed evaluation method is motivated by the idea that an utterance's quality is determined by how well it leads a listener to accurately recover the speaker's communicative intentions."
    ],
    "5675": [
        "ANSP outperforms existing methods on two six-label datasets, LIAR and Weibo, demonstrating its effectiveness.",
        "ANSP is robust to four-label Twitter16 data, indicating its ability to generalize to new datasets.",
        "Existing methods struggle to capture slight differences among multiple credibility types of information, especially half-true and half-false types.",
        "Integrating deeper user behavior features into ANSP could improve its performance.",
        "Reducing labor costs of manual annotation through shared spaces of shared-private models is a promising direction for future work."
    ],
    "5676": [
        "The proposed method significantly improves the efficiency of BERT models while achieving comparable performance.",
        "The proposed method effectively captures long-distance dependency relations using self-attention mechanisms.",
        "The model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by the model are significantly coherent with humans' judgments.",
        "The proposed method can be applied to improve the quality of multiturn dialogue generation."
    ],
    "5677": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The proposed approach achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The model's performance is improved by enhancing the visual representation extracted at the level of objects with their corresponding semantic information to reduce the semantic gap between visual and textual modalities.",
        "Deploying a dual guiding attention helps the model focus on the parts corresponding to a given answer, and using a dynamic attention attends to the most relevant image visual features in each recurrent question generation context."
    ],
    "5680": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our reasoner significantly improves the coverage of answer passages than IR methods.",
        "With the predicted passages, we show that a standard reading comprehension model is able to achieve similar performance as the state-of-the-art method that requires BERT in multiple modules."
    ],
    "5681": [
        "The proposed entity-centric approach to IR improves the performance of an existing QA system.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.",
        "The proposed approach can find relevant evidence required to answer multi-hop questions from a corpus containing millions of paragraphs."
    ],
    "5689": [
        "Attention scores can be easily manipulated: The paragraph states that attention scores are \"easily manipulable,\" suggesting that they may not be a reliable indicator of what the model focuses on.",
        "Models with reduced attention mass can still perform well: The authors show that models with significantly reduced attention mass over tokens known to be useful for prediction can still achieve good performance, challenging the assumption that attention scores are a crucial factor in determining model behavior.",
        "Attention may be used as a tool for auditing algorithms: The authors note that their results raise concerns about the potential use of attention as a tool to audit algorithms, suggesting that malicious actors could use similar techniques to mislead regulators.",
        "Simple training schemes can produce effective models: The paragraph highlights the effectiveness of the authors' simple training scheme in producing models with reduced attention mass, indicating that more complex training schemes may not be necessary to achieve good performance."
    ],
    "5690": [
        "Our proposed deep learning architecture outperforms other semantic similarity scorers when used to re-rank candidate answers in the Text Spotting problem.",
        "Our approach could be useful in multimodal machine translation, where an image caption must be translated using not only the text but also the image content.",
        "Our approach could be useful in tasks that lie at the intersection of computer vision and NLP, such as popularity prediction and automatic text illustration."
    ],
    "5691": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our approach achieves state-of-the-art results on two MRS tasks.",
        "Ablation studies demonstrate the importance of semantic retrieval at both paragraph and sentence levels in the MRS system.",
        "The work can give general guidelines on MRS modeling and inspire future research on the relationship between semantic retrieval and downstream comprehension in a joint setting."
    ],
    "5695": [
        "The proposed recursive graphical neural network model outperforms other graph-based models on text classification tasks.",
        "The use of LSTM components in the model helps to alleviate the over-smoothing problem faced by many graph-based models.",
        "The model is not limited to text classification tasks and can be applied to other tasks such as text generation.",
        "Experiment results on both single and multiple label text classification testify the effectiveness of our proposed model.",
        "Furthermore, we give detailed analysis to show that using LSTM between layers of GNN can effectively alleviate the over-smoothing problem, which is faced by many graph based models.",
        "It should be noted that our model is not limited to the classification task. In the future, we would like to apply our model to other tasks, such as generation tasks."
    ],
    "5697": [
        "Experimental results verify that Conversational-GCN can handle deep conversation structures effectively.",
        "Our hierarchical framework performs much better than existing methods.",
        "In future work, we shall explore to incorporate external context and extend our model to multi-lingual scenarios.",
        "We shall investigate the diffusion process of rumors from social science perspective and draw deeper insights from there to incorporate them into the model design."
    ],
    "5698": [
        "The proposed method for incorporating syntax into NLI models improves accuracy for four NLI models (DA, ESIM, BERT, MT-DNN) across three standard NLI datasets.",
        "Our findings demonstrate the usefulness of syntactic information in semantic models and motivate future research into syntactically informed models.",
        "The use of contextual vector representations from a pretrained parser is effective in improving the accuracy of NLI models.",
        "The proposed method is simple and broadly applicable, making it a valuable contribution to the field of NLP."
    ],
    "5704": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Utilizing sense definitions can enhance the model on infrequent polysemes.",
        "The performance of using average or max operation is nearly the same for Bert def.",
        "Concatenating the sentence vector will impair the F1-score.",
        "Fine-tuning pre-trained language models like BERT on WSD tasks achieves state-of-the-art results.",
        "Using relations between senses, like hypernym and hyponym, can provide more accurate sense representations in future works."
    ],
    "5706": [
        "We propose light-weight adapters, a simple yet efficient way for adapting large scale neural machine translation models.",
        "Our proposed approach enables the final network to adapt to multiple target tasks simultaneously, without forgetting the original parameters.",
        "Experiments support the flexibility and scalability of light-weight adapters, (i) yielding comparable or better results when compared with the standard full fine-tuning or bilingual baselines, (ii) without the need for any hyperparameter tuning across varying adaptation dataset sizes and model capacities.",
        "With a large set of globally shared parameters and small interspersed task-specific layers, adapters allow us to train and adapt a single model for a huge number of languages and domains.",
        "We hope that this work would motivate further research into massively multitask and universal translation models."
    ],
    "5707": [
        "The proposed method, Pointer-Gen, can generate synthetic code-switching sentences using a novel approach.",
        "The model can learn code-switching points by attending to input words and aligning parallel words, without requiring word alignments or constituency parsers.",
        "The model is effective for languages that are syntactically different, such as English and Mandarin Chinese.",
        "The language model trained using the proposed method outperforms equivalence constraint theory-based models.",
        "The learned language model can be used to improve the performance of an end-to-end automatic speech recognition system."
    ],
    "5709": [
        "Our methods can improve the token embeddings alignment for multi-sense words in a microscopic perspective without hurting the macroscopic performance on the bilingual lexicon induction task.",
        "The current state-of-the-art method for cross-lingual token embedding learning cannot handle multi-sense words well.",
        "Experiments showed that our methods can improve the token embeddings alignment for multi-sense words in a microscopic perspective without hurting the macroscopic performance on the bilingual lexicon induction task.",
        "Possible extensions would be to train a multi-sense word detector based on the number of clusters of token embeddings for each word and to create a new evaluation task for cross-lingual contextual word embeddings (token embeddings) with attention to multi-sense words.",
        "As the research on cross-lingual token embedding learning is still in its early stage, we also discussed possible future work such as applying clustering algorithms on token embeddings to obtain sense-level multi-sense word representations."
    ],
    "5712": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Using contextual information is important for NLU systems, and learning correct attention is vital for NLU systems."
    ],
    "5713": [
        "a joint training approach for polyglot language models outperforms a retrofitting approach of aligning monolingual language models",
        "jointly trained LMs learn a better crosslingual lexical correspondence than the one produced by aligning monolingual language models or word type vectors",
        "our results provide a strong basis for multilingual representation learning and for further study of crosslingual transfer in a low-resource setting beyond dependency parsing"
    ],
    "5715": [
        "Our model outperforms state-of-the-art methods in terms of ROUGE scores and human evaluations by a large margin, demonstrating the effectiveness of PESG.",
        "The fact checker ensures that the generated summary is consistent with the input document, allowing for effective fact extraction and polishing.",
        "The editing-based generator produces a summary by incorporating polished facts and summary patterns, resulting in high-quality summaries.",
        "The proposed framework, PESG, aims to generate summaries in formal writing scenarios, with the goal of producing summaries that conform to a patternized style."
    ],
    "5716": [
        "Our approach outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Existing knowledge bases provide a significant amount of knowledge, but there are areas where the models can be further improved, particularly in cases where the knowledge is present but the model could not answer, or where it predicted wrong answers with irrelevant knowledge.",
        "Our synthetic dataset measures the memorizing and reasoning ability of language models."
    ],
    "5717": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "our approach learns meaningful dynamic representations of entities without any entity-level supervision.",
        "We have presented a new neural architecture called Procedural Reasoning Networks (PRN) for multimodal understanding of step-by-step instructions.",
        "Our proposed model is based on the successful BiDAF framework but also equipped with an explicit memory unit that provides an implicit mechanism to keep track of the changes in the states of the entities over the course of the procedure.",
        "Our experimental analysis on visual reasoning tasks in the RecipeQA dataset shows that the model significantly improves the results of the previous models, indicating that it better understands the procedural text and the accompanying images."
    ],
    "5718": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Our system design did not use the different explanation roles and the dependencies between them, but using such features the precision is likely to improve further.",
        "Our Iterative re-ranking algorithm shows it can improve the precision even more, given a reasonably precise Relevance Learner model.",
        "This is the first time this task has been organized and there is lot of scope for improvement in precision.",
        "We have presented a system that participated in the shared task on explanation regeneration and ranked second out of 4 participating teams."
    ],
    "5722": [
        "The vectorized word representations are strongly correlated with eye-tracking, EEG, and fMRI data.",
        "CogniVal can be used for predicting downstream performance and choosing the best embeddings for specific tasks.",
        "CogniVal can be extended to evaluate sentence or paragraph embeddings.",
        "Combining CogniVal with other embedding evaluation frameworks can make it even more effective."
    ],
    "5723": [
        "We have treated argumentative relation classification in a new light, as a task where we learn to rank candidate texts according to their plausibility.",
        "Our approach is competitive with previous work, and does not require pre-processing.",
        "In the 'content-based' setting, our method outperforms previous work by a considerable margin.",
        "We observed substantial improvements in precision when using our method on the scarce class attack.",
        "Our adapted neural Siamese ranking model allows us to embed source and target argumentative units into plausible and implausible argumentative discourse contexts."
    ],
    "5724": [
        "AAT outperforms state-of-the-art methods on the task of image captioning.",
        "AAT is a generic attention model that can be employed by any sequence-to-sequence learning task.",
        "AAT adaptively aligns image regions to caption words for image captioning.",
        "AAT improves over state-of-the-art methods in image captioning tasks."
    ],
    "5726": [
        "'G-DuHA is able to generate higher-quality and goal-focused dialogues as well as responses with decent diversity and non-redundancy.'",
        "'The goal embedding module plays a vital role in the performance improvement and the dual architecture can significantly enhance diversity.'",
        "'We demonstrated one application of the goal-oriented dialogue generation through a data augmentation experiment, though the proposed model is applicable to other conversational AI tasks which remains to be investigated in the future.'",
        "'A language model coupled with goal embedding suffers from roleswitching or confusion.'",
        "'It's also interesting to further dive deep with visualizations (Kessler, 2017) and quantify the impact on quality, diversity, and goal focus metrics.'"
    ],
    "5727": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our approach does not require an annealing schedule or a hamstrung decoder to avoid posterior collapse.",
        "In addition to preventing posterior collapse, our approach improves translation quality in terms of BLEU.",
        "Empirical evaluation demonstrates that the proposed method has improved performance in dealing with uncertainty in data, including weakly supervised learning from source-side monolingual data as well as noisy parallel data."
    ],
    "5728": [
        "We have identified classes of knowledge gaps when reasoning under partial knowledge.",
        "Our approach outperforms previous approaches on the OpenBookQA task, with and even without additional missing fact annotation.",
        "Identifying the knowledge gap first and then reasoning by filling this gap outperforms previous approaches.",
        "This work opens up the possibility of focusing on other kinds of knowledge gaps and extending this approach to other datasets and tasks (e.g., span prediction)."
    ],
    "5729": [
        "Using unlabeled sentences from auxiliary languages and adversarial training can improve the performance of cross-lingual dependency parsing.",
        "Adversarial training can successfully learn not to capture language-dependent features for contextual encoders, leading to improved cross-lingual dependency parsing performance.",
        "The proposed approach of using unlabeled sentences from auxiliary languages and adversarial training for inducing language-agnostic encoders shows promise for improving the performances of cross-lingual dependency parsing.",
        "The use of adversarial training for multi-source transfer to parsing and other cross-lingual NLP applications is a future research direction."
    ],
    "5730": [
        "Realizing the potential for typology may require rethinking current approaches.",
        "We can further drive performance by refining typology-based similarities into a metric more representative of actual transfer quality.",
        "Ultimately, we would like to design models that can directly leverage typological compositionality for distant languages."
    ],
    "5732": [
        "Uncertainty sampling with deep models like FastText.zip exhibits negligible class bias.",
        "Uncertainty sampling is robust to various algorithmic factors and expensive sampling strategies like ensembling.",
        "Uncertainty sampling can be effectively used to bootstrap the training of large DNN models by generating compact surrogate datasets (5x-40x compression).",
        "FTZ-Ent provides a strong baseline for deep active text classification, outperforming previous results by a margin of 4x less data.",
        "The current work opens up several directions for future investigations, including a deeper look into the nature of sampled data, the creation of surrogate datasets for various applications, and an extension to other deep models and beyond classification models."
    ],
    "5734": [
        "We designed and implemented two different algorithms for imitating input text, and made it interactive.",
        "The text generated by our systems are well-appreciated by users, and found that the interpolated Markov model got the most positive user feedback.",
        "local coherence might be more important than the global structure when generating philosophical statements."
    ],
    "5736": [
        "The proposed approach (BSDAR) improves the generation of longer and more diverse sequences.",
        "The proposed approach maintains the ability to predict \"abstractive\" keyphrases, i.e., semantically relevant keyphrases that are not present in the source text.",
        "The use of attention vectors as a re-scoring method in beam search decoding improves the generation of abstractive keyphrases."
    ],
    "5739": [
        "The proposed techniques for transfer learning using pivot-based parallel data are effective.",
        "Pre-training NMT models with source-pivot and pivot-target parallel data can improve the final source-target translation performance.",
        "Our methods are suitable for most non-English language pairs with lots of parallel data involving English.",
        "The methods can be applied to zeroresource language pairs, showing a strong performance in the zero-shot setting or with pivot-based synthetic data.",
        "The proposed techniques expand the advances in NMT to many more non-English language pairs that are not yet studied well.",
        "Future work will be zero-shot translation without step-wise pre-training, i.e., combining individually pre-trained encoders and decoders freely for a fast development of NMT systems for a new non-English language pair."
    ],
    "5740": [
        "zero-shot cross-lingual transfer learning on RC with multi-BERT is possible, even when the languages for training and testing are different.",
        "reasonable performance can be obtained\" in cross-lingual transfer learning.",
        "only token-level pattern matching is not sufficient for multi-BERT to answer questions.",
        "typology variation and code-switching only caused minor effects on testing performance."
    ],
    "5742": [
        "Utilizing clinical notes along with time-series data can significantly improve the prediction performance of identifying the patient's condition in advance.",
        "The use of advanced models for clinical notes is expected to improve the prediction performance even more in the future.",
        "Expert knowledge about a patient's condition is summarized in clinical notes, which can be used to improve the accuracy of predicting the patient's condition.",
        "The current literature has exclusively focused on using time-series measurements from ICU instruments for identifying the patient's condition, but incorporating clinical notes can provide a more comprehensive understanding of the patient's condition."
    ],
    "5743": [
        "domain portability of neural QA systems",
        "impact of error propagation in end-to-end retrieval and QA systems",
        "base QA model was trained on non-biomedical data, but was able to generalize to spans of text and gave very good results",
        "the bottleneck is the quality of the retrieval system, not necessarily the QA model itself"
    ],
    "5744": [
        "\"We propose a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "\"Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "\"Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "\"ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "\"The proposed self-attention transformer network for Hindi-English language translation achieves better performance than other neural machine translation models on the basis of BLEU.",
        "\"The basic encoder-decoder based NMT, attention based model is effective in improving different components of SMT.",
        "\"Deep learning models have improved different components of SMT.",
        "\"Machine translation has been discussed with a brief overview of basic objectives and terminologies along with early statistical approaches."
    ],
    "5745": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Grounding entailment generation in images is beneficial, but linguistic features also play a crucial role.",
        "The data used was not elicited in a multimodal setting, despite the availability of images.",
        "It has become increasingly clear that Vision-Language NLP models are not grounding language in vision to the fullest extent possible.",
        "Designing datasets with complementary textual and visual modalities is necessary to properly assess the contribution of grounded language models for entailment generation.",
        "Generating contradictions should be considered in future work.",
        "Incorporating attention mechanisms into more sophisticated Vision-Language architectures is important for further research."
    ],
    "5751": [
        "Our experiments show that these latent tree-based neural machine translation decoders successfully learn to induce reasonably linguistically plausible constituency trees while achieving good downstream task performance on translation.",
        "Furthermore, our experiments demonstrate that PRPN and ON-LSTM trained as machine translation decoders perform competitively or better than respective language models for latent tree learning.",
        "We therefore conclude that machine translation is a more signal-rich task for inducing trees than language modeling.",
        "However, the high variance in parsing F1 of MT decoders is alarming: does it happen only in the models we investigated, or is it generally true for latent tree learning with translation objective?",
        "What kind of consistent linguistic structures these models learn? A more thorough analysis of the models and the parses produced would be needed to understand these open questions in future work."
    ],
    "5752": [
        "The proposed deep structured model based on SSVM combines the benefits of structured models and data-driven deep neural architectures for event temporal relation extraction.",
        "The experimental results exhibit the effectiveness of this approach for event temporal relation extraction.",
        "Further leveraging commonsense knowledge, domain knowledge in temporal relation, and linguistics information can create more robust and comprehensive global constraints for structured learning.",
        "Improving feature representations by designing novel neural architectures that better capture negation and hypothetical phrases as discussed in error analysis can further enhance the model's performance.",
        "Leveraging large amounts of unannotated corpora can help improve event temporal relation extraction."
    ],
    "5754": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed framework demonstrates effectiveness in automatic short answer grading.",
        "There are several directions that need further exploration, such as using one attention mechanism with multiple heads or replacing transformer block with other types of sentence encoders."
    ],
    "5756": [
        "Allowing the workers to assign labels to image pairs may have introduced some level of visual bias.",
        "We quantify this potential bias and calculate the best possible performance for a model relying only on this bias.",
        "We create new evaluation sets using a subset of the original evaluation data, which are robust to bias.",
        "Our evaluation of existing SOTA models using the new evaluation sets shows that largely they do not take advantage of latent visual bias and confirm their performance gains.",
        "The addition of the new evaluation sets for the NLVR2 evaluation provides a more robust assessment of model performance.",
        "We recommend evaluating existing models on the original NLVR corpus, which includes synthetic images, to provide a more comprehensive assessment of their performance.",
        "However, this may be challenging for models relying on pre-training using natural images."
    ],
    "5759": [
        "The proposed pre-training method for cross-lingual natural language generation (NLG) can transfer monolingual NLG supervision signals to all pre-trained languages.",
        "With the pre-trained model, we achieve zeroshot cross-lingual NLG on several languages by only finetuning once.",
        "Our model outperforms the machine-translation-based pipeline model on several cross-lingual NLG tasks.",
        "The proposed method has the potential to be improved towards the fully unsupervised setting in future work."
    ],
    "5760": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our results with DEER show that a single-stage retrieval approach for entities from mentions is highly effective without any domain-specific tuning.",
        "The dual encoder approach allows for interesting extensions beyond traditional entity linking, such as building entity expectations during text processing and using context encodings to retrieve entities relevant to the context.",
        "Training with English Wikipedia can be extended to far more training examples across far more languages using cross-lingual datasets."
    ],
    "5762": [
        "Multi-stage pretraining with BERT initialization improves the performance of abstractive summarization models compared to randomly initialized baselines.",
        "Full-network parameter initializations for a Transformer-based model can be achieved through multi-stage pretraining, and this approach leads to better performance than randomly initialized baselines.",
        "Using a simple maximum-likelihood loss (MLE) setup for training abstractive summarization models can achieve better performance than relying on inference-time heuristics.",
        "Two-stage pretraining with BERT initialization and MLE training reaches peak score faster compared to zero- or one-stage pretrained models.",
        "MLE-based training is superior to RL-based methods in terms of speed and stability, making it a convenient recipe for training high-quality abstractive summarization systems."
    ],
    "5763": [
        "The proposed ReCoSa model significantly outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by the ReCoSa model are coherent with human judgments.",
        "Using proper detection methods, such as self-attention, can improve the quality of multi-turn dialogue generation.",
        "Introducing topical information or considering detailed content information in the relevant contexts can further improve the quality of generated responses.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "Data ordering patterns can have an effect on the model performance for neural machine translation, and using heuristics specific to evaluating NMT performance like perplexity and BLEU score can be good measures to rank the data for training."
    ],
    "5764": [
        "The query is the most important utterance in the context, and removing it leads to a significant decline in performance (claim 1).",
        "The last three utterances are more important than the others (claim 2).",
        "It's better to model the query separately than deal all of the utterances in the same way, as they have significantly different importance (claim 3).",
        "The proposed model achieves state-of-the-art results on both Ubuntu and Douban corpus, demonstrating its effectiveness and generalization (claim 4).",
        "The triple attention mechanism is effective and can be applied to other NLP tasks in the future (claim 5)."
    ],
    "5765": [
        "The proposed model learns structured conversation representations via hierarchical self-attention and dynamically refers to external, context-aware, and emotion-related knowledge entities from knowledge bases. (related to methodology)",
        "Experimental analysis demonstrates that both contextual information and commonsense knowledge are beneficial to model performance. (related to results)",
        "The tradeoff between relatedness and affectiveness plays an important role in the model's performance. (related to methodology)",
        "The proposed model outperforms state-of-the-art models on most of the tested datasets of varying sizes and domains. (related to results)",
        "The model can be easily adapted to other languages, given that there are similar emotion lexicons to NRC VAD in other languages and ConceptNet is a multilingual knowledge base. (related to adaptability)",
        "The model can be adapted as a generic model for conversation analysis, given that NRC VAD is the only emotion-specific component. (related to adaptability)"
    ],
    "5766": [
        "We introduce nearest neighbor overlap (N2O), a comparative approach to quantifying similarity between sentence embedders.",
        "Using N2O, we draw comparisons across 21 embedders.",
        "We also provide additional analyses made possible with N2O, from which we find high variation in embedders' treatment of semantic similarity."
    ],
    "5771": [
        "The results show that for a few relations, word embeddings can outperform embeddings derived directly from KGs.",
        "Corpus-based embeddings fail to capture relations at the concept level.",
        "For most relation types, embeddings only can predict relations with an accuracy under 0.7.",
        "Our results provide evidence that correct capture of relations should happen at the concept level and may not be achievable with high accuracy at the word level.",
        "As future work, we want to apply our method to study contextual embeddings."
    ],
    "5772": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The model can adapt and generalize better to support sets and unseen classes using dynamic routing and external working memory.",
        "The emergence of function words in natural language can be explained by a signaling game with variable contexts consisting of multiple objects with multiple gradable properties, and simple reinforcement learning by neural networks.",
        "Much work remains to be done, including using a soft attention mechanism, hyper-parameter search, and generalizing the input to discover which dimensions are relevant for successful communication.",
        "Communication systems like those exhibited here can emerge in the very general setting of communicating by a sequence of symbols, with costs for things like vocabulary size and length of messages."
    ],
    "5775": [
        "Our proposed algorithm can effectively match reviewer-paper relevance through jointly finding common research topics between submissions and reviewers' publications.",
        "Our model based on word embeddings efficiently captures the reviewer-paper relevance, and is robust to cases of vocabulary mismatch and partial topic overlap between submissions and reviewers.",
        "The common topic model showed strong empirical performance on a benchmark and a newly collected dataset."
    ],
    "5776": [
        "We introduced TALKDOWN, a new annotated Reddit corpus of condescending linguistic acts in context.",
        "Using BERT, we established baseline models that suggest this is a challenging task, and one that benefits from rich contextual representations.",
        "In qualitative analyses on diverse subreddits, we offered initial evidence that models trained on TALKDOWN generalize to new data."
    ],
    "5777": [
        "The proposed model can significantly improve the performance of generating responses by incorporating heterogeneous information in an end-to-end dialogue system.",
        "The model applies Heterogeneous Memory Networks (HMNs) to model sequential history and structured database, which leads to outstanding performance in learning the distribution over dialogue history and retrieving knowledge.",
        "The proposed context-aware memory networks can efficiently use various structured data in end-to-end task-oriented dialogue without any extra labeling and module training.",
        "The model is able to incorporate heterogeneous information in an end-to-end dialogue system, which improves the performance of generating responses.",
        "The results on several datasets show that the proposed model can significantly improve the performance of generating responses."
    ],
    "5778": [
        "Our approach can alleviate the attention distraction problem of traditional attention-based soft-selection methods.",
        "Experimental results demonstrate the effectiveness of our method, especially when handling multi-aspect sentences with different sentiment polarities.",
        "Our hard-selection approach outperforms soft-selection approaches significantly when handling multi-aspect sentences with different sentiment polarities.",
        "The deep associations between the sentence and aspect, and the long-term dependencies within the sentence are taken into consideration by leveraging the pretrained BERT model.",
        "Our method determines the start and end positions of the opinion snippet for a given input aspect."
    ],
    "5780": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Past work has proposed using attention mechanisms or rationale methods to explain the prediction of a target variable.",
        "The former produce noisy explanations, while the latter do not properly capture the multi-faceted nature of useful rationales.",
        "The Multi-Target Masker (MTM) addresses these drawbacks by replacing the binary mask with a probabilistic multidimensional mask.",
        "According to comparison with human annotations and automatic evaluation on two real-world datasets, the inferred masks were more accurate and coherent than those that were produced by the state-of-the-art methods."
    ],
    "5781": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "The model is flexible in alternating the training mode between supervised and unsupervised learning.",
        "The two semi-supervised methods are simple but effective to use the nonparallel and parallel data.",
        "The model is effective in verifying the effectiveness of the model on both automatic metrics and human evaluation."
    ],
    "5786": [
        "UNITER outperforms state-of-the-art models over multiple V+L tasks by a significant margin.",
        "Four main pre-training tasks are proposed and evaluated through extensive ablation studies.",
        "Trained with both in-domain and out-of-domain datasets, UNITER outperforms state-of-the-art models.",
        "Future work includes studying early interaction between raw image pixels and sentence tokens, as well as developing more effective pre-training tasks.",
        "UNITER provides UNiversal Image-TExt Representations for Vision-and-Language tasks."
    ],
    "5788": [
        "The proposed slot-independent neural model (SIM) effectively reduces model complexity while achieving superior or comparable results on various datasets compared to previous models.",
        "SIM incorporates better feature representations to tackle the dialogue state tracking problem.",
        "Future work will focus on designing general slot-free dialogue state tracking models that can be adapted to different domains during inference time, given domain-specific ontology information, making the model more agile in real applications.",
        "The proposed model is able to achieve superior or comparable results on various datasets without relying on pre-defined slots.",
        "The incorporation of better feature representations in SIM leads to improved performance in dialogue state tracking tasks."
    ],
    "5789": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Large-scale pretraining significantly improves NMT performances.",
        "The proposed AdaBERT achieves a BLEU score of 32.3 on WMT17 Chinese-English dataset, with a significant performance boost of +3.2 over existing SOTA results."
    ],
    "5797": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The thus generated output may serve as an intermediate representation that is easier to process for downstream semantic applications and may thus lead to a better performance of those tools.",
        "We intend to train a sentence simplification model on MINWIKISPLIT and compare it to previously proposed systems trained on the WEB-SPLIT and WIKISPLIT corpora.",
        "We plan to improve the quality of the simplified target sentences in our corpus in accordance with the insights we gained through the analyses described above.",
        "First, we will perform a detailed error analysis of the output to determine the most common types of mistakes and get some starting points for further improving our heuristics for filtering out malformed simplifications.",
        "To enhance the syntactic correctness of the output, we will train a classifier on the recently proposed CoLA dataset (Warstadt et al., 2018) to eliminate instances with ungrammatical target sentences from our corpus.",
        "In addition, special attention will be given to improving the heuristics that ensure that each simplified target sentence represents a single semantic unit."
    ],
    "5798": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed AdaBERT model adaptively compresses BERT for various downstream tasks using Neural Architecture Search.",
        "The proposed AdaBERT model finds different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The proposed GAN architecture for keyphrase generation obtains state-of-the-art performance in generating abstractive keyphrases.",
        "This is the first work that applies GANs to the keyphrase generation problem."
    ],
    "5799": [
        "We propose a method to combine two types of sentence embeddings : 1) universal embeddings, pre-trained on a large corpus such as Wikipedia and incorporating general semantic structures across sentences and 2) domain-specific embeddings, learned during training.",
        "We merge them together by using a graph convolutional network that eliminates the need of hand-crafted features or additional annotated data.",
        "We introduce a fully data-driven model Sem-SentSum that achieves competitive results for multi-document summarization on both kind of summary length (665 bytes and 100 words summaries), without requiring hand-crafted features or additional annotated data.",
        "As SemSentSum is domain-independent, we believe that our sentence semantic relation graph and model can be used for other tasks including detecting information cascades, query-focused summarization, keyphrase extraction and information retrieval."
    ],
    "5800": [
        "The number of training targets (syllables) does not significantly impact the WER results, and more layers can almost eliminate the difference in decoding results.",
        "Dropout improves performance by up to 18%, confirming the importance of regularization.",
        "The bLSTM results are a bit worse than expected, which could be due to the relatively small dataset.",
        "The hybrid ASR system performs better with more targets and with a larger lexicon.",
        "Training times for the bLSTM networks were extensive, but the hybrid system can be easily adapted to fit a new lexicon and language model.",
        "The results for this medium-sized dataset were significantly better, and syllable recognition error rates are only slightly worse than word recognition error rates.",
        "Evaluating the accuracy of time alignments is important for downstream tasks such as keyword search or paralinguistic analyses."
    ],
    "5802": [
        "The decision boundary in many transfer datasets stays at the lexical level, meaning that many instances only require modifying certain pivot words to cross the boundary and transfer the text class.",
        "There are cases with no pivot words, where the decision boundary is higher than the word level, requiring a deeper understanding of sentence structures, including syntax, semantics, and common sense.",
        "The Pivot Analysis framework, consisting of three text mining algorithms, can be used to examine and inspect text style transfer datasets and models, revealing the important words that influence the class of a sentence and how they interact with transfer models.",
        "The majority of transfer cases stay at the lexical level, while the syntactic structures remain unchanged, and the limitations of our methods and SOTA transfer models lie in understanding higher-level sentence compositionality.",
        "Future challenges include using better inductive bias and more powerful models to understand higher-level sentence compositionality."
    ],
    "5807": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "This is the first work to address and discuss OOV issues at the subword level in multilingual settings.",
        "Future work includes: investigating other embedding alignment methods such as Gromov-Wasserstein alignment (Alvarez-Melis and Jaakkola, 2018) upon more languages; investigating approaches to choose the subwords to be added dynamically."
    ],
    "5815": [
        "Our approach significantly outperforms previous methods in terms of reducing the error.",
        "The Triage Module corrects the covariate shift caused by the mismatch of context lengths between training and testing phases.",
        "By pruning the context, the Triage Module mimics the training setting at testing time for the subsequent layers.",
        "Our approach has a similar regularizing effect as training on multiple paragraphs, but is much faster during training and testing.",
        "The two-stage nature of Integrated Triaging allows us to reduce the average latency time further through early-exiting of easy question, context pairs."
    ],
    "5821": [
        "The dynamic negotiation coach can increase the profits of sellers by making measurably good recommendations, grounded in strategies and tactics from the negotiation literature.",
        "The coach uses natural language processing and machine learning techniques to identify and score the likelihood of successful tactics, and has been tested on human-human negotiations.",
        "The coach provides on-the-fly autocomplete suggestions to negotiating parties, seamlessly integratable in goal-oriented negotiation dialog systems.",
        "The coach has standalone educational and commercial values, such as improving negotiation skills of non-expert negotiators, and assisting humans in sales and customer service.",
        "The study aggregates negotiation strategies from economics and behavioral research, and proposes novel ways to operationalize the strategies using linguistic knowledge and resources."
    ],
    "5824": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses a combination of Similarity Entropy Minimization and Adversarial Distribution Alignment to promote clustering and align similar class distributions across domains.",
        "The use of pseudo labels generated by the k-means cluster miner improves domain adaptation performance.",
        "The proposed Cosine Annealing Strategy combines the representation extractor with the k-means cluster miner to improve domain adaptation performance.",
        "The approach does not require tweaking a previously developed mathematical theory to justify the empirical finding that the embeddings of too frequent words should be downweighted when summed with those of less frequent ones.",
        "The SIF authors made a number of errors in their pursuit of mathematical validity."
    ],
    "5828": [
        "The proposed neural extractive summarization models achieve better performance when the dataset is explored in depth and the nature of the dataset is fully utilized.",
        "Existing models are weak and can be improved by understanding the factors that influence their performance, such as the dataset used for training.",
        "By conducting in-depth analyses and providing guidelines for future research, this study aims to help design more powerful summarization models.",
        "The proposed data-dependent understanding of neural extractive summarization models has the potential to improve the performance of these models.",
        "The exploration of different factors of datasets and the use of the nature of the dataset can lead to the development of more powerful summarization models."
    ],
    "5829": [
        "'Our experimental results outperformed certain baselines and with multimodality, we achieved improved overall F1-scores of 0.92 for utterance-level intent detection and 0.97 for word-level slot filling.'",
        "'We briefly show that our experimental results outperformed certain baselines and with multimodality, we achieved improved overall F1-scores of 0.92 for utterance-level intent detection and 0.97 for word-level slot filling.'",
        "'Ongoing research has a potential impact of exploring real-world challenges with human-vehicle-scene interactions for autonomous driving support with spoken utterances.'"
    ],
    "5841": [
        "The proposed novel constituent hierarchy predictor based on recurrent neural networks outperforms the state-of-the-art baseline parser by achieving higher F1 scores.",
        "The resulting fully-supervised parser maintains the same model size and speed as the baseline parser, but with improved performance.",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The approach of joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing.",
        "Sentence fusion for abstractive summarization is a crucial aspect that needs more attention to improve, as many summary outputs generate false information due to entity replacement and other complex merging methods."
    ],
    "5843": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The principles behind Word2Vec work well in other models when using unfiltered, natural usage examples, but are less consistent when the sentences are filtered or of a different type.",
        "A combination of existing and novel evaluation tasks can be used to compare and explain model performance between context-based and hybrid methods in different scenarios.",
        "Simple methods typically struggle to generalise to multiple sub-tasks, while more complex methods are more flexible but at the price of overhead and a risk of overfitting.",
        "Careful optimisation is required to adapt other models to the few-shot setting."
    ],
    "5848": [
        "The new dataset for grammatical error correction in Czech contains almost twice as many sentences as existing German datasets and more than three times as RULEC-GEC for Russian.",
        "The performance of our system is substantially higher than results of existing reported systems for all three low-resource languages (German, Russian, and Czech).",
        "Our system supersedes existing reported systems even if only pretrained on unsupervised synthetic data.",
        "The performance of our system could be even higher if we trained multiple models and combined them into an ensemble.",
        "We plan to extend our synthetic corpora with data modified by supervisedly extracted rules, which could help especially in the case of Russian, which has the lowest amount of training data."
    ],
    "5853": [
        "There is a large gap in performance relative to the oracle, showing that the problem has ample room for further development.",
        "Summarization methods trained on US Bills transfer to California bills, thus the summarization methods developed on this dataset could be used for legislatures without human written summaries.",
        "There is ample room for further development in legislative summarization."
    ],
    "5854": [
        "Our models can effectively preserve salient source relations in summaries.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "Convolutional neural networks are successful classification models in this scenario.",
        "The multi-class models improved the final performance of the slot filling pipeline.",
        "The model trained jointly on entity and relation classification achieved the best micro F 1 scores.",
        "The model with structured prediction performed best in terms of macro F 1 .",
        "Aliases and coreference resolution are important for the system's performance.",
        "The candidate extraction component and the classification component are responsible for most of the errors of the system.",
        "Our type-aware convolutional neural network had a large positive impact on the performance of the whole slot filling pipeline."
    ],
    "5855": [
        "The noisy channel formulation requires only parallel sentences (rather than documents) but we can use abundant monolingual documents to train the language model component.",
        "Experiments show that our proposed model considerably improves translation quality-it achieves approximately 2.5 BLEU higher than transformer baselines.",
        "Subjective evaluation further confirms that the language model helps enforce consistency of tense, number, and lexical choice across documents."
    ],
    "5857": [
        "MemNet, TAware, and Qadpt generally perform better than the other two baselines.",
        "Qadpt generally performs the best in most metrics.",
        "TAware shows its power in knowledge graph entities prediction.",
        "MemNet demonstrates its potential in human evaluation.",
        "Qadpt has the potential to be explored more.",
        "MemNet and TAware attend on the whole graph instead of focusing on the most influential part.",
        "The dataset is currently provided with a Chinese and an English TV series as well as their correspondent knowledge graphs.",
        "This paper also benchmarks the task and dataset by proposing automatic evaluation metrics and baseline models, which can motivate the future research directions."
    ],
    "5863": [
        "The use of BERT embedding component is effective in End-to-End Aspect-Based Sentiment Analysis (E2E-ABSA).",
        "BERT-based models outperform other neural models in capturing aspect-based sentiment.",
        "The BERT embedding component is robust to overfitting.",
        "Using the BERT embedding component in E2E-ABSA leads to superior performance.",
        "The experimental results demonstrate the effectiveness of BERT in E2E-ABSA."
    ],
    "5868": [
        "BERT outperforms previous distributional methods on an attribute classification task, highlighting possible reasons why BERT improves the state-of-the-art on various commonsense reasoning tasks.",
        "However, we show that BERT still lacks proper attribute representations in many areas.",
        "We developed implicit and explicit methods of remedying this deficit on the downstream task.",
        "Both methods can improve scores on the downstream reasoning task.",
        "We motivate future work in probing and improving the ability of neural language models to reason about everyday commonsense."
    ],
    "5872": [
        "The proposed textual approach can detect social media trolls with high accuracy (claim: \"We built a new machine learning model based on theme-based and profiling features that in cross-validation evaluation achieved a F1 macro value of 0.94\").",
        "The extracted themes improve the performance of the proposed model (claim: \"Our experiments showed that the extracted themes boosted the performance of the proposed model when coupled with other surface text features\").",
        "NLI features can be used to identify IRA trolls based on their writing style (claim: \"We proposed NLI features to identify IRA trolls from their writing style, which showed to be very effective\").",
        "Emotional and linguistic analysis of IRA accounts can provide insights into their behavior (claim: \"Through the manually checking of IRA accounts, we noticed that frequently irony was employed. As a future work, it would be interesting to identify these accounts by integrating an irony detection module\")."
    ],
    "5873": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improves the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The real-world applicability of text classifiers for labeling polarized tweets in a retweet network is restricted to pre-filtering tweets for manual annotation.",
        "If used as a filter, the classifier can significantly speed up the annotation process, making large-scale content analysis more feasible."
    ],
    "5878": [
        "The proposed multi-level GRNN combined with non-textual features can effectively mine multi-level information out of conversations, leading to improved dialog act labeling performance.",
        "The model is able to predict short sentences accurately and surpasses state-of-the-art results significantly without requiring extensive feature engineering.",
        "Introducing attention mechanisms into the model can further improve the use of contextual information for similar tasks in the future.",
        "The proposed multi-level GRNN combined with non-textual features can effectively mine multi-level information out of conversations, leading to improved dialog act labeling performance. (Paragraph 3)",
        "The model is able to predict short sentences accurately and surpasses state-of-the-art results significantly without requiring extensive feature engineering. (Paragraph 4)",
        "Introducing attention mechanisms into the model can further improve the use of contextual information for similar tasks in the future. (Paragraph 5)"
    ],
    "5879": [
        "We investigated the ability of sequence-to-sequence models to model their confidence in their decisions.",
        "By approximating train-test mismatch using an autoencoder and combining it with the posterior probabilities, we are able to improve confidence estimation over a strong baseline.",
        "It is better to measure the mismatch on the decoder hidden states than on the encoder hidden states.",
        "We presented an approach to infer the internal alignment of complex sequence-to-sequence models.",
        "Using this alignment instead of a state-of-the-art external alignment for mapping target confidence measure to source tokens clearly improved the quality of the confidence measure for source words."
    ],
    "5883": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "GloVe, ELMo, and BERT embeddings all encode gender, religion, and nationality biases",
        "Our experiments show that the projection-based method works for the static GloVe embeddings",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy."
    ],
    "5885": [
        "The authors introduced a new task of code generation under interactive computing, conditioned on a context of interleaved code snippet cells and NL markdown.",
        "They collected a large-scale open-domain dataset (JuICe) from publicly available Jupyter notebooks for training models.",
        "The authors gathered a high-quality evaluation set using nbgrader and in-class programming assignment notebooks with solutions to reliably test code generation models.",
        "Using an increased amount of code context and training data improves performance on code generation tasks, with significant room for improvement.",
        "The authors evaluated a variety of baseline models for two context-dependent code generation tasks (API sequence generation and full code generation)."
    ],
    "5889": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "The scope of any assessment of new methods and models should be broadened to the understanding of their mistakes and the reasons why these models perform well or poorly in concrete examples, contexts and word meanings.",
        "These issues are particularly important in text data sets, in which semantic meaning and linguistic syntax are very complex.",
        "Our categories are similar to the taxonomy defined in publication about errors analysis for Uyghur Named Tagger (Abudukelimu et al., 2018)."
    ],
    "5890": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The ConMask model can capture the correct relationship between entities, even when the name of the entity does not appear in the description.",
        "The ConMask model has some limitations and room for improvement, such as the nature of the relationship-dependent content masking.",
        "The current methods have certain limitations and can be improved by adding more prior knowledge into the methods or considering more sophisticated combining strategies.",
        "The proposed framework of domain differential adaptation (DDA) can model the differences between domains with the help of models in a related task, and adapt models for the target task.",
        "Two simple strategies under the proposed framework for neural machine translation are presented and are demonstrated to achieve good performance.",
        "The subword-level evaluation metrics for domain adaptation in machine translation can adapt models to a larger extent and with a higher accuracy compared with several alternative adaptation strategies."
    ],
    "5892": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Our experiments show that the proposed approach results in significant improvements on many tasks."
    ],
    "5893": [
        "The pairwise approaches are not necessarily superior to the pointwise approach particularly if precision is taken into account.",
        "Our large system scored second with a FEVER score of 69.66 without ensembling.",
        "We additionally examined hard negative mining for training the retrieval systems and showed that it slightly improves the performance.",
        "Using BERT as an end-to-end framework for the entire FEVER pipeline can be investigated in the future."
    ],
    "5894": [
        "Explicitly conditioning Seq2Seq models on control tokens such as length, paraphrasing, lexical complexity or syntactic complexity increases their performance significantly for sentence simplification.",
        "Our method paves the way toward adapting the simplification to audiences with different needs.",
        "Each control token has the desired effect on the generated simplifications.",
        "Our approach is easy to extend to other attributes of text simplification."
    ],
    "5898": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Enabling 'tag \u2192 parse' only also improves the tagging accuracy itself.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing."
    ],
    "5900": [
        "Incorporating both visual and ASR-token-based features into instructional video captioning models can have a significant impact on performance.",
        "The decoder is able to paste-together even simple sets of object detections into high-quality captions, suggesting that the input encoding may be a bottleneck for performance.",
        "Better models of input data may be needed to improve performance.",
        "Transfer learning may prove fruitful in improving performance, given the small size of the dataset and the success of transfer learning in related work (Sun et al., 2019b,a; Miech et al., 2019)."
    ],
    "5905": [
        "Our proposed multimodal model with a shared multilingual text encoder adapts alignment between languages for image-description retrieval tasks, while training.",
        "We introduce a loss function that combines pairwise ranking loss and maintains alignment of word embeddings in multiple languages.",
        "Our approach yields better generalization performance on image-to-text and text-to-image retrieval tasks, as well as caption-caption similarity task.",
        "In future work, we can investigate applying self-attention models like Transformer (Vaswani et al., 2017) on the shared text encoder to find a more comprehensive representation for descriptions in the dataset.",
        "We can explore the effect of a weighted summation of two loss functions instead of equally summing them together."
    ],
    "5907": [
        "Adding self-attention modules in the different ResNet blocks can improve the feature extraction procedure for the VQA task.",
        "Using wider CNN such as ResNeXt [19] or Wide-ResNet [20] can further improve the improvement margins.",
        "Better self-attention, such as multi-head attention in CNN [15, 11], could improve the overall model.",
        "Better modulation of the linguistic hidden state could directly be an input of the computation for self-attention.",
        "Other works have already considered using all the hidden states from the language model as the 'key' K for visual self-attention [11] .",
        "Modulating batchnormalization as previously investigated in [21] could also be explored."
    ],
    "5909": [
        "The combination of methods brings significant improvements to the NMT systems on two low-resource language pairs.",
        "Selecting some appropriate synonymous words for the source sentence from n-best synonymous words can further improve the performance of the NMT systems.",
        "Leveraging more unsupervised methods based on monolingual data to address rare word problem can bring further improvements to the NMT systems."
    ],
    "5914": [
        "RevGAN outperforms other baseline models in generating personalized product reviews.",
        "The generated reviews by RevGAN are similar to organically generated user reviews.",
        "RevGAN incorporates three novel components: self-attentive recursive autoencoder, conditional discriminator, and personalized decoder.",
        "Future work for RevGAN includes improving the review generation process to receive several key words from users as input and generate reviews based on this prior information.",
        "Another direction of future research lies in developing novel methods that distinguish the type of reviews described in the paper and organic reviews."
    ],
    "5916": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Removing our full observability assumption\" is an important direction for future work.",
        "Experimenting with using the interaction history\" is another potential direction for future work.",
        "Expanding the learning example aggregation to error cases beyond incorrect start positions\" could be a useful direction for future research.",
        "Making agent reasoning interpretable to reduce user frustration\" is another possible direction for future work."
    ],
    "5917": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our framework can improve the quality of multiturn dialogue generation by using proper detection methods, such as self-attention.",
        "Introducing topical information or content information can further improve the quality of generated responses."
    ],
    "5919": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "There is a need to move beyond operational style transfer and focus on real-world style transfer research, which will create challenges for evaluation metrics.",
        "Existing auto-evaluation metrics for style transfer with non-parallel corpora are not suitable for real-world applications.",
        "A shift towards practical applications of style transfer systems will create problems for evaluation metrics.",
        "It is important to pay attention to the real-world style transfer research and move beyond operational style transfer."
    ],
    "5923": [
        "Our factored OSNMT implementation significantly improves the original non-factored one by reducing the number of decoding steps.",
        "Experiments on two language pairs confirm that this can be done without loss in MT quality and without an increase in decoding time.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing.",
        "We showed that word case information and joining of subword units can be predicted effectively by a target factor; this allows for a single representation of similar or even identical lexical items, so that more full forms can be kept for a given subword vocabulary limit.",
        "In the future, we plan to utilize target factors in NMT for other auxiliary prediction problems."
    ],
    "5924": [
        "Our proposed framework can learn a semantic representation of hotels by jointly embedding hotel click data, geographic information, user rating, and attributes.",
        "Enriching the neural network with supplemental, structured hotel information results in superior embeddings compared to a model that relies solely on click information.",
        "Our final embedding can be decomposed into multiple sub-embeddings, each encoding the representation for a different hotel aspect, resulting in an interpretable representation.",
        "The same framework can be applied to general item embedding, such as product embedding on Amazon, Ebay, or Spotify.",
        "Our approach allows for dynamic updating of the embeddings when one of the attributes or user ratings changes for a hotel."
    ],
    "5926": [
        "The proposed method using Generative Adversarial Networks (GANs) with machine-generated captions does not achieve success in the task of conditional image generation.",
        "The Image Captioning Module that was trained on the COCO dataset generates captions that are too similar to each other, which may be related to the fact that more diverse and detailed captions are needed during training to achieve significant improvements.",
        "Using GANs for image captioning instead of traditional methods may generate more different captions, as seen in the work of Shetty et al. [28].",
        "An open question is whether a bigger dataset would allow the GAN to learn the image-captions correspondence, even when captions are very similar for each image.",
        "An hybrid approach could make the proposed method work by having humans write captions on a subset of the dataset and using the obtained captions to train a captioning system.",
        "Crowdsourcing platforms like Amazon Mechanical Turk could be used for generating human captions."
    ],
    "5929": [
        "The beam problem in neural machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset, and more general globally-normalized models can be trained in a similarly inexpensive way.",
        "The proposed novel constituent hierarchy predictor based on recurrent neural networks can capture global sentential information and outperform the state-of-the-art baseline parser.",
        "The resulting fully-supervised parser achieves high F1 scores on standard WSJ and CTB evaluations.",
        "CONNET learns fine-grained representations of each source which are further dynamically aggregated for every unseen sentence in the target data, leading to superior performance compared to prior crowd-sourcing and unsupervised domain adaptation sequence labeling models.",
        "The proposed learning framework shows promising results on other NLP tasks like text classification."
    ],
    "5930": [
        "The key challenge of the problem is multi-faceted set expansion, and a novel approach called FUSE is proposed to address it.",
        "FUSE outperforms previous state-of-the-art approaches significantly by extracting and clustering skip-grams for each seed, identifying coherent semantic facets of all seeds, and expanding entity sets for each semantic facet.",
        "The proposed framework FUSE is general in that it achieves quality set expansion in both multi-faceted and single-faceted settings.",
        "FUSE solves the case where different seeds have different multi-facetedness for the first time.",
        "Future work includes exploring other skip-gram clustering approaches and coherent semantic facet discovery algorithms."
    ],
    "5933": [
        "The authors explored classifying an account reporting any kind(s) of sexism such that the categories can cooccur.",
        "The developed neural framework outperforms many deep learning and traditional ML baselines for multi-label sexism classification.",
        "The authors provided the largest dataset for sexism classification, linked with 23 categories.",
        "Directions for future work include devising approaches that perform sexism classification more accurately, enhancing the categorization scheme, and developing other ways to help counter sexism.",
        "The authors hope that this paper will give rise to further work aimed at fighting sexism."
    ],
    "5937": [
        "using pre-trained affective information from dialogue generators\" can help improve the overall task and provide added benefits in terms of lesser training epochs for good generalization.",
        "Experimenting with dyadic conversations both in the source and target tasks has shown to be effective, and the approach is primed to investigate more complex multi-party conversations in the future.",
        "Pre-training on multi-party data will increase the complexity of the task, and special training schemes are needed to capture complex influence dynamics.",
        "The proposed transfer learning framework for ERC, TL-ERC, uses pre-trained affective information from dialogue generators to improve the overall task and provide added benefits in terms of lesser training epochs for good generalization."
    ],
    "5938": [
        "The proposed BERT-based Hierarchical Aggregation Model outperforms all baselines by a large margin, proving its effectiveness.",
        "The category-based grouping method performs best among the three methods (time, topic, and category) used in the news aggregation process.",
        "The proper input time for the model is around 40 minutes, and the prediction accuracy declines with an increase in prediction delay.",
        "News attributes have a significant influence on forex trading, with Business Sectors news affecting USD-EUR trading and Politics/International Affairs news impacting USD-RMB trading the most.",
        "The influence patterns of news attributes can help forex traders make better decisions.",
        "This is the first work to utilize advanced NLP pre-train technology in the enormous forex market, and the results show the potential of this research area.",
        "Designing more suitable grouping methods or combining news grouping and market predicting in an end-to-end model may be a promising future study."
    ],
    "5941": [
        "The problem of conversational question answering over a large-scale knowledge base can be addressed using a multi-task learning framework that learns for type-aware entity detection and pointer-equipped logical form generation simultaneously.",
        "The proposed multi-task learning framework takes full advantage of supervisions from all subtasks, leading to improved performance on the final KB-QA problem.",
        "Experimental results on a large-scale dataset verify the effectiveness of the proposed framework.",
        "In future work, the authors plan to test their proposed framework on more datasets and investigate potential approaches to handle spurious logical forms for weakly-supervised KB-QA."
    ],
    "5950": [
        "The proposed Granular Multi-modal Attention Network (GMMAN) improves attention regions for visual dialog tasks, as evaluated through empirical analysis.",
        "The improved attention obtained using GMMAN consistently improves results for the task of visual dialog.",
        "The proposed attention regions correlate well with the regions obtained using Gradcam.",
        "The proposed attention regions aid the network in solving the task of visual dialog.",
        "The method obtains consistent attention regions that are useful for solving various tasks.",
        "The idea of obtaining correct semantic granular regions for solving various tasks is worth exploring in future work."
    ],
    "5955": [
        "We presented a simple method to update word vectors to the distribution of a new corpus.",
        "Our method is not a definitive solution to this challenging task, rather constitutes a proof of concept.",
        "Experiments seem to indicate that the proposed approach could be used for extending the lexicon, allowing to aggregate low frequency words from several corpora.",
        "While this task is of premier importance, we lack proper evaluation datasets for rare words.",
        "We leave the construction of adapted evaluation datasets for future work, and posit that such resources would greatly fuel research in that direction."
    ],
    "5958": [
        "Our model reached outstanding results on difference caption task and achieved state-of-the-art performance.",
        "We proposed an encoder-decoder framework with various feature fusing tactics for image difference detection and 21 description generation.",
        "We enriched our dataset with textual style transfer method unprecedentedly, proved to be reliable as the substitution of human annotation.",
        "Our model achieved state-of-the-art performance on the difference caption task.",
        "The proposed encoder-decoder framework and feature fusing tactics are effective for image difference detection and description generation.",
        "The textual style transfer method is reliable as a substitution for human annotation.",
        "The AMT dataset is a large dataset of differential descriptions on fine-grained image pairs, which can be used for image difference detection and description generation."
    ],
    "5961": [
        "The proposed adversarial learning method improves the robustness of a frame semantic parser.",
        "The improved parser achieves better performance on a multi-ID WSJ OOD BROWN dataset, with an increase in F1 score of 82.4% to 71.7%.",
        "The improvement is achieved by using adversarial learning to learn a mapping from action-oriented features to visual entities.",
        "This approach provides a more expressive signal for captioning compared to the raw features themselves.",
        "The improved parser only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline."
    ],
    "5962": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our NumNet model achieves numerical reasoning ability by combining the numerically-aware graph and the NumGNN together.",
        "The numerically-aware graph encodes numbers as nodes and relationships between them as edges, which is required for numerical comparison.",
        "Our NumGNN could perform comparison and identify the numerical condition after multiple-step reasoning.",
        "However, our NumNet is not applicable to the case where an intermediate number has to be derived (e.g., from arithmetic operation) in the reasoning process, which is a major limitation of our model."
    ],
    "5965": [
        "The model achieves better performance compared to conventional open-vocabulary NMT solutions in many languages, using a significantly smaller number of parameters.",
        "The decoding procedure in NMT can be performed in a multi-dimensional search space defined by word and character level units via a hierarchical decoding structure and beam search algorithm.",
        "Our model shows promising application under high-resource settings.",
        "Our software is available for public usage."
    ],
    "5966": [
        "Our approach achieved the best performance both with and without the use of additional monolingual data.",
        "We use back-translation to help regularizing and adapting to the test domain, particularly in the Myanmar to English direction.",
        "We use self-training as a way to better leverage in-domain source-side monolingual data, particularly in the English to Myanmar direction.",
        "Given the complementary nature of these two approaches, we combined them in an iterative fashion.",
        "We improve decoding by using noisy-channel reranking and ensembling.",
        "There is still quite some room for improvement by better leveraging noisy parallel data resources, by better combining together these different sources of additional data, and by designing better approaches to leverage source side monolingual data."
    ],
    "5967": [
        "GOLDEN Retriever greatly improves the recall of gold supporting facts.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, it improves over previous work on joint POS tagging and dependency parsing.",
        "GOLDEN Retriever is also more interpretable to humans compared to previous neural retrieval approaches and affords better understanding and verification of model behavior.",
        "Generating natural languages queries for each step of reasoning, GOLDEN Retriever demonstrates competitive performance to the state of the art."
    ],
    "5968": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "State-of-the-art RNN based joint SLU models perform poorly on certain out-of-distribution slots which may encounter slot values with large semantic variability after deployment.",
        "Previous string matching based delexicalization techniques are inadequate to handle such slots since these slot values are mostly out-of-vocab.",
        "Our model also enables faster model training for slots with large training vocabulary (e.g. movie/show titles), and supports some continuous learning without requiring model updates by simply maintaining an updated contemporary vocabulary."
    ],
    "5969": [
        "The finetuned model forgets some of the useful NLG skills acquired during large-scale pretraining.",
        "The mix-review finetuning method can alleviate the forgetting problem.",
        "Large-scale pretraining changes the model's generative behavior in various profound ways.",
        "The behavior change is influenced by the nature of data itself.",
        "We can discuss news with the dialogue model finetuned by mix-review, even when the target dataset is not about news.",
        "Beam-search suffers from the \"generic response\" problem and gives repetitive responses.",
        "Top-k sampling is more diverse than beam-search and does not compromise sample quality."
    ],
    "5970": [
        "The existing state-of-the-art few-shot models struggle on the new task of few-shot domain adaptation and few-shot none-of-the-above detection.",
        "The new task explores two aspects that have been ignored in previous work: few-shot domain adaptation and few-shot none-of-the-above detection.",
        "Few-shot models need to be improved for few-shot DA and few-shot NOTA, which require further exploration in these real-world challenges.",
        "The existing techniques are not satisfactory solutions for few-shot DA and few-shot NOTA.",
        "There is a need for further research to handle these two issues."
    ],
    "5971": [
        "Using auto-sizing on the transformer network of the 2019 WNGT efficiency task resulted in deleting more than 25% of the parameters in the model while only suffering a modest BLEU drop.",
        "Focusing on the parameter matrices of the feed-forward networks in every layer of the encoder and decoder yielded the smallest models that still performed well.",
        "The proximal gradient step of auto-sizing can be applied to a wide variety of parameter matrices, making it a flexible framework for pruning parameters in a large NMT system.",
        "With an aggressive regularization scheme, large portions of the model can be deleted with only a modest impact on BLEU scores, resulting in a much smaller model on disk and at run-time."
    ],
    "5972": [
        "The transformer-based unsupervised question-answering pipeline can solve the fact checking task with high accuracy.",
        "The Question Generation stage of the pipeline achieves almost 90% accuracy in transforming the FEVER dataset into a Cloze-task.",
        "The masked language modeling approach from the BERT model can be used to answer questions generated by the pipeline with high accuracy (80.2% label accuracy on \"SUPPORTS\" label).",
        "It is possible to verify the facts with the right kind of factoid questions."
    ],
    "5973": [
        "BERTRAM improves over standard BERT and RoBERTa on downstream task datasets.",
        "Our method is beneficial not only for rare words, but also for frequent words.",
        "In future work, we want to investigate BERTRAM's potential benefits for frequent words.",
        "It would be interesting to explore more complex ways of incorporating surface-form information."
    ],
    "5975": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?"
    ],
    "5976": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our principled approach enables the learned embeddings to capture domain similarities and as the embedding dimensions are aligned across domains, it facilitates interesting studies of semantic shifts of word usage.",
        "Our nuanced representations provide a better intrinsic fit for the data, lead to an improvement in a downstream task of rating prediction over state-of-the-art approaches, and are intuitively meaningful to humans."
    ],
    "5977": [
        "Using a vanilla variant of memory networks does not lead to competitive results on single-hop machine reading comprehension tasks.",
        "The use of an output classification layer with weights for every answer candidate is detrimental, especially when dealing with a large number of unseen answers at test time.",
        "Replacing the original output classification layer with a pointing mechanism or performing answer anonymization can improve the results.",
        "Incorporating a stronger signal about the most compatible passage window, such as through a one-hot attention feature, leads to a perceivable improvement.",
        "The study has limitations, including only covering a generic memory network and not exploring other architectures with more complex encoding and compatibility layers.",
        "Using pretrained word embeddings leads to an observable improvement, but other types of representations, such as pretrained contextualized embeddings from language models, may result in even stronger improvements.",
        "The analysis is limited to two cloze-style datasets, which may limit the generalizability of the findings to other QA and machine reading datasets."
    ],
    "5983": [
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.\" - This claim suggests that using a conditional language model with a multi-label classifier trained to predict visual entities from action features can lead to better captioning performance compared to traditional sequence-to-sequence frameworks.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.\" - This claim suggests that the process of learning a mapping from action-oriented features to visual entities can enhance the expressiveness of the captioning process.",
        "We first start by adapting the recurrent and transformer sequence-to-sequence frameworks in order to perform caption generation based on a sequence of action features.\" - This claim suggests that the authors began by adapting existing sequence-to-sequence frameworks to perform caption generation based on a sequence of action features.",
        "We contrast the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.\" - This claim suggests that the authors compared the performance of traditional sequence-to-sequence frameworks with conditional language models that use a multi-label classifier to predict visual entities from action features.",
        "This indicates that in some cases learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.\" - This claim suggests that learning a mapping from action-oriented features to visual entities can provide a more expressive signal for captioning, and that this approach can be effective in some cases."
    ],
    "5984": [
        "We introduced a new task and dataset to study subsentence highlight extraction.",
        "The use of sub-sentence segments provides more concise summaries over full sentences.",
        "Our data provides a new upper bound for evaluation metrics.",
        "We evaluated our data using a state-of-the-art neural architecture to show the modeling capabilities using this data."
    ],
    "5986": [
        "We presented a corpus of aligned triples of German audio, German text, and English translations for speech translation from German to English.",
        "The audio data in our corpus are read speech, based on German audio books, ensuring a low amount of speech disfluencies.",
        "The audio-text alignment and text-to-text sentence alignment was checked to be of high quality in a manual evaluation.",
        "We have a large corpus of aligned triples of German audio, German text, and English translations for speech translation from German to English.",
        "Our approach ensures a low amount of speech disfluencies.",
        "The manual evaluation shows that the audio-text alignment and text-to-text sentence alignment is of high quality."
    ],
    "5988": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "BERT passage embeddings outperform BM25 baseline significantly on factoid QA datasets but fail to perform better than BM25 on non-factoid datasets.",
        "We observed a big gap between embedding-based models and in-domain the fully fine-tuned BERT on QA datasets."
    ],
    "5994": [
        "The baseline method BASE improves over Pisa's results by a margin of about 30% on boundary F-measure, BF.",
        "Neural segmentation methods extending the \"align to segment\" approach improve over the baseline on all accounts, particularly for precision metrics.",
        "The proposed extensions to move towards joint segmentation and alignment involve introducing a word-length bias in the attention mechanism and designing an auxiliary loss.",
        "The bilingual methods generate word alignments and their real benefit should be assessed with alignment-based metrics, but this is left for future work due to the difficulty of computing valid alignments in very limited data conditions.",
        "Future work may focus on mitigating data sparsity with weak supervision information using lists of frequent words, the presence of certain word boundaries on the target side, or more sophisticated attention models."
    ],
    "5995": [
        "Many open-domain NLP tasks rely upon multidocument input from the web to facilitate tasks such as answering questions or writing summaries, but current approaches struggle to encode the entirety of this information.",
        "We propose constructing one knowledge graph per query and show that this method compresses information and reduces redundancy.",
        "Using the linearized graph achieves better performance than TF-IDF retrieval on two abstractive generation tasks."
    ],
    "5996": [
        "The proposed model significantly outperforms existing HRED models and their attention variants in multi-turn dialogue generation.",
        "The relevant contexts detected by the proposed model are coherent with human judgments.",
        "The proposed method can extract causality in natural language text based on a self-attentive BiLSTM-CRF-based solution.",
        "The performance of the proposed method is limited by the insufficiency of high-quality annotated data.",
        "The proposed method can be improved by developing annotated datasets from multiple sources and combining it with distant supervision and reinforcement learning.",
        "The novel concept pointer generator model improves abstractive summarization and generates concept-oriented summaries.",
        "The proposed distant supervision strategy for model adaptation to different datasets is effective."
    ],
    "6000": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks."
    ],
    "6003": [
        "Our proposed RL-based Graph2Seq model achieves state-of-the-art performance on the SQuAD dataset, outperforming previous methods by a wide margin.",
        "The effectiveness of our Deep Alignment Network and bidirectional GNN is crucial in processing the directed passage graph.",
        "Our two-stage training strategy, which combines cross-entropy based and REINFORCE based sequence training, benefits from both types of training.",
        "We explore both static and dynamic approaches for constructing graphs when applying GNNs to textual data.",
        "In the future, we aim to investigate more effective ways of automatically learning graph structures from free-form text."
    ],
    "6009": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed approach significantly outperforms several state-of-the-art baselines for DASC.",
        "In our future work, we would like to solve other challenges in DASC, e.g., negation detection problem, to further improve the performance.",
        "We would like to apply our HRL approach to other sentiment analysis tasks, such as aspect and opinion co-extraction, and dialog-level sentiment analysis."
    ],
    "6019": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Kernels in Graph Neural Networks lead to more accurate evidence selection and fine-grained joint reasoning.",
        "The two kernels play different roles and contribute to different aspects crucial for fact verification.",
        "The kernel-based attentions show intuitive and effective attention patterns, with the node kernels focusing on the correct evidence pieces and the edge kernels accurately gathering the necessary information from one node to the other to complete the reasoning chain."
    ],
    "6021": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The proposed approach outperforms the competitors significantly.",
        "The introduction of external knowledge and meta patterns confirms the effectiveness of the system.",
        "The systematic empirical results show that the proposed approach outperforms the competitors significantly.",
        "The introduction of external knowledge and meta patterns is effective in improving the system's performance."
    ],
    "6023": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but the results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The current state-of-the-art methods for CQA are not challenging enough, and future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Using a smaller embedding size (128) for GLAD and GCE resulted in better performance compared to the default size (400).",
        "Reducing the embedding size of GLAD and GCE to 128 and training the model resulted in similar performance as the original models, but with a smaller embedding size."
    ],
    "6028": [
        "We have observed that a system trained to learn implicitly the target punctuation can recover part of the quality degradation due to ASR errors up to 1 BLEU point.",
        "Fine-tuning on noisy input can instead improve by more than 2 BLEU points.",
        "A system tuned on ASR errors does not obtain a further improvement by more data for implicit punctuation learning.",
        "Finally, when fine-tuning on clean and noisy data, the system becomes robust to noisy input and keeps high performance on clean input."
    ],
    "6029": [
        "We proposed a novel document-level approach for question generation using multistep recursive attention mechanism.",
        "Our method takes additional attention steps to learn a more relevant context, leading to better quality of generated questions.",
        "We demonstrate that our approach leads to a new state-of-the-art results in question generation for all three datasets (SQuAD, MS MARCO, and NewsQA).",
        "Our method extends the relevant context using document and answer representation.",
        "Taking additional attention steps helps learn a more relevant context."
    ],
    "6035": [
        "A first approach, inspired by multilingual NMT, allows a coarse-grained control over the length and no degradation in translation quality.",
        "A second approach, inspired by positional encoding, enables a fine-grained control with only a small error in the token count, but at the cost of a lower translation quality.",
        "Manual evaluation confirms the translation quality observed with BLEU score.",
        "In future work, we plan to design more flexible and context-aware evaluations which allow us to account for short translations that are not equivalent to the original but at the same time do not affect the overall meaning of the discourse."
    ],
    "6037": [
        "Our proposed full quantization strategy for the Transformer architecture achieves higher BLEU scores than all other quantization methods for the Transformer on multiple translation tasks, without any loss in BLEU compared to full-precision.",
        "Applying quantization to only certain components of the Transformer can increase translation accuracy, as shown by our ablation study, which found that BLEU score could increase even more when only specific elements of the Transformer were quantized.",
        "Our method has the potential to open up new possibilities for improving machine translation accuracy and we plan on applying our method to other tasks and exploring the compression of these networks.",
        "Further gains might be possible by extending our work to variations of the Transformer and exploring the compression of these networks."
    ],
    "6038": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "There exist significant gender and race-specific biases in dialogue systems.",
        "The proposed methods effectively reduce the biases and ensure fairness of dialogue systems."
    ],
    "6041": [
        "Our proposed models (CRF and bidirectional LSTM-CRF) outperform human and baseline performance in EM and F1 for overall results and per-tag breakdown.",
        "Despite strong human performance records, our models achieve better results than humans and baselines.",
        "We release a pre-trained Title2vec job title vector representation that can serve as basic building blocks and improve the performance for a wide spectrum of downstream tasks.",
        "Our work is the first attempt to address the challenging occupational NER task, and both the dataset and pre-trained embeddings are first made available in the literature of occupational analysis.",
        "The IPOD corpus comprises a large number of job titles with a knowledge-based gazetteer that includes manual NE tags from three domain exports annotators."
    ],
    "6044": [
        "The proposed novel constituent hierarchy predictor based on recurrent neural networks outperforms the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "The resulting fully-supervised parser leverages right-hand side syntax for local decisions, yet maintains the same model size and speed as the baseline parser.",
        "The proposed method for causality extraction based on self-attentive BiLSTM-CRF-based solution outperforms the state-of-the-art baseline method by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "The proposed method for causality extraction introduces a multihead self-attention mechanism to learn the dependencies between cause and effect, which improves the performance of the method.",
        "The performance of the proposed method for causality extraction is limited by the insufficiency of high-quality annotated data, and further work is needed to address this issue.",
        "The proposed method for unsupervised objectives exploration using denoising objectives outperforms language modeling and deshuffling for pre-training, and different objectives can lead to different sequence lengths and training speeds.",
        "Additional exploration of objectives similar to the ones considered here may not lead to significant gains for the tasks and model considered, and instead, it may be fortuitous to explore entirely different ways of leveraging unlabeled data."
    ],
    "6046": [
        "Our approach demonstrates great potential in video understanding problems.",
        "There is still a big gap with respect to human performance in video understanding problems.",
        "Our dataset will contribute to reducing the gap between machine and human performance in video understanding problems.",
        "Specific knowledge about the task was combined with multi-modal video information to improve video understanding.",
        "Knowledgebased models have great potential in video understanding problems."
    ],
    "6048": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our results on the SQuAD 2.0 dataset using the relation module on both BiDAF and BERT models show improvements from the relation module.",
        "The semantic object pairs are fed into the relation network which makes a guided decision as to whether a question is answerable.",
        "Our results prove the effectiveness of our relation module.",
        "For future work, we plan to generalize the relation module to other aspects of question answering, including span prediction or multi-hop reasoning."
    ],
    "6053": [
        "We achieved a 43.67% average recall with an ensemble of 16 models in the test phase.",
        "Simple data augmentation and model ensembles could significantly improve recall percentage.",
        "As future work, we will focus on the positive example mining method since each candidate can have multiple matched targets, while the given training and validation datasets only indicate a single target which may potentially lead to overfitting.",
        "We will also try various ensembling methods instead of simply averaging scores."
    ],
    "6056": [
        "The proposed CNN text classification model achieves similar performance as state-of-the-art classifiers.",
        "N-gram feature analysis is effective in interpreting the classification process of the model.",
        "Convolution attribution establishes the relevance between n-gram features and word vectors.",
        "The multi-sentence weighting strategy works well for long context input.",
        "The model has interpretable performance, as demonstrated by two applications with interpretable cases.",
        "The authors will further explore methods to interpret neural models in NLP tasks and study how to use interpretability to directionally adjust model performance."
    ],
    "6059": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The use of keywords to specify event types on the fly for the models enables the operation of the models to new event types.",
        "A novel feature-wise attention technique is presented for the CNN models for ED in this formulation."
    ],
    "6060": [
        "Our DPP+BERT models harness the power of deep contextualized representations and optimization to achieve outstanding performance on multi-document summarization benchmarks.",
        "Our analysis further reveals that, despite the success of deep contextualized representations, it remains necessary to combine them with surface indicators for effective identification of summary-worthy sentences.",
        "Our DPP+BERT models achieve outstanding performance on multi-document summarization benchmarks."
    ],
    "6064": [
        "The proposed method obtains SOTA results on both nested and flat NER datasets, which indicates its effectiveness.",
        "This formalization comes with two key advantages: (1) being capable of addressing overlapping or nested entities; (2) the query encodes significant prior knowledge about the entity category to extract.",
        "The proposed method is effective in addressing both nested and flat NER tasks.",
        "The use of a query that encodes prior knowledge about the entity category allows for more effective extraction of entities.",
        "The proposed method has the potential to be explored with variants of the model architecture in the future."
    ],
    "6065": [
        "We present a new task of generating common questions based on shared concepts among documents.",
        "Our variant model MSQG applied to this new task outperforms multi-source encoder-decoder framework models.",
        "We provide an empirical evaluation framework based on automated metrics and human judgments to evaluate multi-source generation framework for generating common questions."
    ],
    "6066": [
        "Our methods demonstrate effectiveness on the CNN/Daily Mail dataset.",
        "These simple methods are also adaptable to other summarization models with attention.",
        "Further exploration on this and combination with other approaches like RL remains as our future exploration.",
        "We argue that these simple methods are effective and can be applied to other datasets.",
        "Our work has the potential to improve retrieval of information from documents, which has numerous applications.",
        "Our proposed method is simple but effective, and can be easily adapted to other summarization models with attention."
    ],
    "6071": [
        "'Our method only requires parallel corpora and UPOS tags in the target language, unlike prior work which requires more resources.'",
        "'We studied the quality of learned embeddings by examining nearest neighbors in the embedding space and investigating their functional dissimilarity.'",
        "'Pre-training with a parallel corpus allowed the syntactic embeddings to be transferred to low-resource languages via few-shot fine-tuning.'",
        "'Our evaluations indicated that syntactic structure can be learned by using simple network architectures and explicit supervision.'",
        "'Future directions include improving the transfer performance for low-resource languages, disentangling semantic and syntactic embeddings, and analyzing the effect of transfer learning between languages belong to the same versus different language families.'"
    ],
    "6072": [
        "We introduce DENS, a dataset for multi-class emotion analysis from long-form narratives in English.",
        "Our benchmark results demonstrate that this dataset provides a novel challenge in emotion analysis.",
        "Attention-based models could significantly improve performance on classification tasks such as emotion analysis.",
        "Incorporating common-sense knowledge into emotion analysis to capture semantic context could significantly improve performance.",
        "Using few-shot learning to bootstrap and improve performance of underrepresented emotions.",
        "As narrative passages often involve interactions between multiple emotions, one avenue for future datasets could be to focus on the multiemotion complexities of human language and their contextual interactions."
    ],
    "6073": [
        "We provided an experimental evaluation of sentence embedding methods for Polish on five linguistic tasks.",
        "We presented a comparison of three groups of methods: static word embeddings, contextual embeddings from language models and sentence encoders.",
        "We also examined two recently introduced methods for aggregating word vectors into a sentence vector.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "We make the source code for our experiments, our datasets and pre-trained models public to accelerate research in Polish natural language processing."
    ],
    "6076": [
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "We suggest a future research direction in sarcasm detection where the two types of sarcasm are treated as separate phenomena and socio-cultural differences are taken into account."
    ],
    "6077": [
        "We explore the uses of tree holes to enhance microblog-based suicide risk detection.",
        "Suicide-oriented word embeddings based on tree hole contents are built to strengthen the sensibility of suicide-related lexicons.",
        "A two-layered attention mechanism is deployed to grasp intermittently changing points from individuals open blog streams.",
        "Our proposed model outperforms well-designed approaches on benchmark data set.",
        "Our model also performs well on people's implicit and anti-real contrary expressions."
    ],
    "6078": [
        "The 9 DAs of the ViGGO dataset can support a natural multi-turn exchange on the topic of video games.",
        "Using a language generator trained on the ViGGO dataset in a dialogue system may lead to repeating the full name of a video game in subsequent turns.",
        "The ViGGO dataset was designed for grounded generation but without context, and therefore it is up to the dialogue manager to ensure that pronouns are substituted for the names whenever it would sound more natural in a dialogue.",
        "The dataset can be easily augmented with automatically constructed samples which omit the NAME slot in the MR and replace the name with a pronoun in the reference utterance.",
        "The new parallel corpus for data-to-text NLG contains 9 dialogue acts, making it more conversational than other similar datasets.",
        "The crowdsourced utterances were thoroughly cleaned to obtain high-quality human references.",
        "The recent trend in research is to train neural models on small but high-quality data, like humans can."
    ],
    "6081": [
        "Our models can effectively preserve salient source relations in summaries.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "We have shown that the beam problem can largely be explained by the brevity problem.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Currently, our work is only limited to memes which have up to two parts in the meme caption, but we are interested in extending the Memeify system to include multiple parts in the meme caption."
    ],
    "6085": [
        "BERT relies heavily on keywords to solve multi-choice reading comprehension tasks.",
        "The high performance of BERT on partial training shows that it can exploit dataset artifacts and statistical cues to perform well instead of learning natural language understanding and reasoning.",
        "To make the model better understand natural language, both the datasets and the model need to be further improved."
    ],
    "6089": [
        "The proposed RQRM query rewriting model outputs related bid keywords for a given query, which is a novel approach in a keyword-based sponsored search system.",
        "The model uses a deep neural network, which is the first of its kind in this application.",
        "Extensive experiments have been conducted to evaluate the performance of RQRF.",
        "The proposed model outperforms previous methods, as shown by the significant reduction in error (21%) on the English Switchboard dataset.",
        "The model achieves competitive performance compared to previous systems trained on the full dataset, using less than 1% of the training data."
    ],
    "6093": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "We can accurately predict the basic level within a single domain.",
        "The performance of a global model across multiple domains is slightly lower.",
        "Predictions in a new domain, for which there are no examples in the training set, are meaningful only after a per-domain normalization step.",
        "Concepts that are difficult to label for human annotators seem to be more challenging for the classifier as well."
    ],
    "6094": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The written descriptions provided by baseball scouts hold some predictive capacity for the task of predicting whether an amateur baseball player will make the major leagues.",
        "The dataset used for this task is shared with the community, and statistical approaches have also shown success in similar tasks.",
        "Integrating player statistics with the written descriptions seen here would greatly improve upon the results seen here.",
        "Analyzing the language use in the written descriptions and evaluating the predictive capabilities of the numeric grades could be potential directions for future contributions."
    ],
    "6097": [
        "We present adaptive ensembling, an unsupervised domain adaptation framework capable of identifying political texts for a multi-source, diachronic corpus by only leveraging supervision from a single-source, modern corpus.",
        "Our methods outperform strong benchmarks on both binary and multi-label classification tasks.",
        "We release our system, as well as an expert-annotated set of political articles from COHA, to facilitate domain adaptation research in NLP and political science research on public opinion over time.",
        "Our approach can identify political texts for a multi-source, diachronic corpus without requiring any labeled data from the target domain.",
        "Our method is able to adapt to new domains using only supervision from a single modern corpus."
    ],
    "6102": [
        "The proposed game theoretic approach to class-wise rationalization is effective in generating supporting evidence for any given label.",
        "The framework consists of three types of players, which competitively select text spans for both factual and counterfactual scenarios.",
        "The proposed game theoretic framework drives the solution towards meaningful rationalizations in a simplified case.",
        "Extensive objective and subjective evaluations on both single-and multi-aspect sentiment classification datasets demonstrate that CAR performs favorably against existing algorithms in terms of both factual and counterfactual rationale generations."
    ],
    "6103": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our HLPNN model achieves state-of-the-art results on these three datasets.",
        "Using character-aware word embeddings is helpful for overcoming noise in social media text.",
        "The transformer encoders effectively learn the correlation between different features and decouple the two different level predictions.",
        "Adding country-level regularization could effectively guide the city-level prediction towards the correct country, and reduce the errors where users are misplaced in the wrong countries.",
        "Potential improvements could be made using better graph-level classification frameworks.",
        "Our model assumes each post of one user all comes from one single home location but ignores the dynamic user movement pattern like traveling."
    ],
    "6110": [
        "The proposed contrastive attention mechanism significantly improves the performance of abstractive sentence summarization, advancing the state-of-the-art performance for the task.",
        "The use of both conventional attention and novel opponent attention in the contrastive attention mechanism allows for more effective contribution from the relevant parts of the source sentence and less relevant parts for the summary word generation.",
        "The proposed mechanism improves the performance by encouraging contribution from the conventional attention and penalizing con-Src:press freedom in Algeria remains at risk despite the release on Wednesday of prominent newspaper editor Mohamed UNK after a two-year prison sentence, human rights organizations said.",
        "The use of Transformer as a strong baseline in the experiments on three benchmark data sets shows that the proposed contrastive attention mechanism achieves better performance."
    ],
    "6113": [
        "The word-level embeddings are outperformed by the sentence-level embeddings.",
        "The pre-trained models available online with no doubts can attempt some of the NLU tasks with little or almost no fine-tuning.",
        "The directions of the future work may include probing of embedding models for Russian rich morphology and free word order.",
        "Our experiments show that the MCQA dataset is much more complicated than the other two datasets.",
        "The quality of the results for this task is not as high as for two others.",
        "All models perform somewhat similar in next sentence prediction and paraphrase identification tasks.",
        "The paraphrase identification dataset is highly unbalanced, and the positive examples are in the minority, which may affect the quality of the results."
    ],
    "6116": [
        "Regardless of the type of strategy selected, the Insertion Transformer is able to learn it with high fidelity and produce high-quality output in the selected order.",
        "This is especially true for English-German single sentence translation, in which we found that order does not matter.",
        "This opens a wide range of possibilities for generation tasks where monotonic orderings are not the most natural choice.",
        "We would be excited to explore some of these areas in future work."
    ],
    "6121": [
        "'Our approach achieves new state-of-the-art on FewRel 2.0 dataset.'",
        "'We first train a representation extractor with the Clustering Promotion Mechanism.'",
        "'The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.'",
        "'Experimental results demonstrate that our approach achieves competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.'",
        "'Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.'",
        "'Perplexity-based data selection improves translation, leading to an improvement of up to 10.0 BLEU points for LRL\u2192en and 17.0 BLEU points for en\u2192LRL when adapting from a multilingual model, with reduced training time.'",
        "'Our adaptation strategy with selected data is useful even in the extreme case of zero-shot translation for an unseen language (+13.0 BLEU).'",
        "'In future works, we plan to integrate our approach with data augmentation and semi-supervised model training strategies.'"
    ],
    "6123": [
        "When document-level context is only partially available, we evaluate the effectiveness of various data completion techniques.",
        "A simple copy heuristic helps models achieve much better lexical cohesion, even for a highly inflected language such as Russian.",
        "Only adding random context sentence pairs, however, reduces a model's ability to capture cross-sentence interactions, yet these effects are not visible from BLEU scores.",
        "We confirm the effectiveness of back-translation on overall translation quality, while also demonstrating its usefulness on the four challenge sets.",
        "As even simple context completion techniques have a clear impact on model performance, it may be worthwhile to explore additional approaches.",
        "In particular, to obtain more natural contexts, while limiting model generation errors, we can envision embedding sentences such that neighbors in vector space are probable contexts of each other.",
        "Moreover, it might be useful to understand how different data augmentation schemes interact with more sophisticated model architectures."
    ],
    "6125": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks",
        "we will explore better graph encoding methods, and apply discourse graphs to other tasks that require long document encoding"
    ],
    "6126": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Removing gender signals is not trivial to do and a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.",
        "This work serves as a reminder that languages other than English have different properties that are rarely dealt with when processing English.",
        "These aspects should be taken into account when dealing with morphologically rich languages, as not all models and algorithms for English can transfer directly to other languages."
    ],
    "6128": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The use of an image-conditioned caption autoencoder for image captioning can be effective.",
        "The word-level hidden state guidance in the REINFORCE case assigns an intermediate reward that emphasizes the most relevant words.",
        "The approach of using a multi-label classifier trained to predict visual entities from action features can provide a more expressive signal for captioning compared to the raw features themselves.",
        "The use of a GRU-based sequence-to-sequence baseline can provide a more sophisticated and effective approach for captioning."
    ],
    "6129": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "Our model can effectively preserve salient source relations in summaries.",
        "The model uses cascaded LSTMs and DNN structure to model NLU, ST and AS in a single network.",
        "The network maps raw utterances to agent actions directly.",
        "Experimental results on meeting room booking tasks show our model outperforms previous models.",
        "Visualization of dialogue embeddings illustrates they keep the information of dialogue states.",
        "For dialogue tasks with more slots, the model cannot converge well.",
        "More efficient training methods should be explored in the future."
    ],
    "6130": [
        "Our approach for using location as a surrogate for dialect aiming at building a very large scale Twitter dataset of Arabic varieties.",
        "We introduced an effective hierarchical attention multi-task learning (HA-MTL) approach for modeling varieties and micro-dialects.",
        "We empirically demonstrated the utility of BERT on our tasks.",
        "Our work has the potential to open up opportunities for investigating variants of Arabic that remain largely understudied.",
        "The work is also a first step toward deployment of Arabic NLP technologies in real-world applications, such as in disaster and emergency situations where diverse varieties are in actual use."
    ],
    "6131": [
        "CMAML (Conversational Model Adaptation with Multi-Layer) is able to customize unique conversational models for different users.",
        "CMAML introduces a private network for each user's language model, whose structure will evolve during the training to better fit the characteristics of this user.",
        "The private structure of CMAML will only be trained on the corpora of the corresponding user and his/her similar users, improving the model's ability to capture the unique characteristics of each user.",
        "CMARL achieves the best performance in terms of personalization, quality, and diversity compared to other methods.",
        "The experiment results show that CMAML produces quite dissimilar language models for different users, indicating that the model is able to capture unique characteristics of each user."
    ],
    "6132": [
        "The proposed model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "The ConMask model can effectively preserve salient source relations in summaries.",
        "The proposed model achieves substantial improvement in Fake News Challenge Stage 1 results, putting the task in the line of Natural Language Processing tasks that are reported to benefit from transfer learning from pre-trained transformers.",
        "The ConMask model has some limitations and room for improvement, such as the nature of the relationship-dependent content masking."
    ],
    "6136": [
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We contrast the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "Our model uses the proposed GD-Transformer encoder to take sequence input and biaffine attention scorer to directly predict the word boundaries.",
        "With powerful enough encoding ability, our model only needs unigram features for scoring instead of various n-gram features in previous work.",
        "Our model is evaluated on standard benchmark SIGHAN Bakeoff datasets, which shows not only our model performs segmentation faster than any previous models but also gives new higher or comparable segmentation performance against previous state-of-the-art models."
    ],
    "6137": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "We have explored simple and effective ways to alleviate or eliminate the beam problem.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Our systems rely heavily on transfer learning, from document-level MT to document-level NLG.",
        "Our submitted systems ranked first in all tracks.",
        "The MT models are already very good and probably do not need the extra context.",
        "Our NLG models can generate fluent and coherent text and are able to infer some facts that are not explicitly encoded in the structured data.",
        "Aggressive fine-tuning allows us to specialize MT models into NLG models, but it will be interesting to study whether a single model can solve both tasks at once with multi-task learning."
    ],
    "6139": [
        "We presented a new parallel corpus of user reviews of restaurants, which we think will be valuable to the community.",
        "The best single technique for domain adaptation is fine-tuning.",
        "Corpus tags also achieve good results, without degrading performance on a general domain.",
        "Back-translation helps, but only with sampling or tags.",
        "Our models are promising, but they still show serious errors when applied to user-generated content: missing negations, hallucinations, unrecognized named entities, insensitivity to context.",
        "This suggests that this task is far from solved.",
        "We hope that this corpus, our natural noise dictionary, model outputs and human rankings will help better understand and address these problems.",
        "We also plan to investigate these problems on lower resource languages, where we expect the task to be even harder."
    ],
    "6141": [
        "The proposed model, Neural Assistant, significantly outperforms existing HRED models and their attention variants in multiturn dialogue generation.",
        "The relevant contexts detected by the proposed model are coherent with human judgments.",
        "The proposed model can effectively handle large knowledge bases.",
        "The proposed model learns to reason on the provided knowledge source with weak supervision signal coming from the text generation and action prediction tasks, thereby removing the need for belief state annotations."
    ],
    "6142": [
        "The proposed approach achieves over 70% relative accuracy for languages that belong to the SVO language family, such as Spanish, Italian, Portuguese, German, and Chinese.",
        "The approach achieves lower relative accuracy for languages that belong to the SOV language family, such as Japanese, and languages that belong to the VSO language family, such as Arabic.",
        "The proposed approach transfers the model across different languages with minimal resource requirements (a small bilingual dictionary with 1K word pairs).",
        "The approach achieves very good performance (up to 79% of the accuracy of the supervised target-language RE model) on extensive experiments for 7 target languages across a variety of language families.",
        "The proposed approach provides a strong baseline for building cross-lingual RE models with minimal resources."
    ],
    "6145": [
        "The model learns convenient generation order as a by-product of its training procedure.",
        "The model outperforms left-to-right and right-to-left baselines on several tasks.",
        "The model has a preference towards producing \"easy\" words at the beginning and leaving more complicated choices for later."
    ],
    "6147": [
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "We have proposed a new crossmodal attention mechanism called HOCA for video captioning.",
        "HOCA integrates the information of the other modalities into the inference of attention weights of current modality.",
        "We have introduced the Low-Rank HOCA which has a good scalability to the increasing number of modalities.",
        "The experimental results on two standard datasets have demonstrated the effectiveness of our approach."
    ],
    "6149": [
        "Text generation requires a proper embedding space for words.",
        "Incorporating KerBS into text generation could boost the performance of several text generation tasks, especially the dialog generation task.",
        "Unlike traditional Softmax, KerBS includes a Bayesian composition of multi-sense embedding for words and a learnable kernel to capture the similarities between words.",
        "Future work includes proposing better kernels for generation and designing a meta learner to dynamically reallocate senses."
    ],
    "6151": [
        "The quality of the learned NMT representations can be assessed by comparing them to other baselines and upper bounds.",
        "NMT representations are contextualized word representations and differ from recent popular representations.",
        "The methodological approach taken in this work provides insights into how the NMT model exploits language representations.",
        "The results can be compared to other NMT architectures to understand their differences and similarities.",
        "Analysis work is essential for understanding and improving NMT models."
    ],
    "6152": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "En-sensembling unique models outperforms ensembling the same model trained with different seeds.",
        "Using unnormalized max aggregation for short answers and logistic regression normalized noisy or aggregation for long answers yields an F1 improvement of 2 to 4 points over single model performance on the NQ challenge."
    ],
    "6153": [
        "We present a pipeline to create curated monolingual corpora in more than 100 languages.",
        "Our approach improves the quality of the resulting dataset and allows for the training of multilingual text level representations like XLM.",
        "We preprocess Common Crawl by following the pipeline of (Grave et al., 2018) , with the differences that we preserve the structure of documents and filter the data based on their distance to Wikipedia.",
        "The two histograms correspond to English, which is the largest dataset, and Gujarati which is a low-resource language.",
        "We display statistics for 25 languages only.",
        "The number of documents per language for the Feb. 2019 snapshot after deduplication is shown, with a logarithmic scale and vertical lines corresponding to perplexity thresholds applied to split the corpus in head/middle/tail."
    ],
    "6154": [
        "Pre-trained BERT models add a significant improvement to the classification task in the legal domain.",
        "Fine-tuning BERT on a large legal corpus adds marginal but practically valuable improvements in both accuracy and training speed.",
        "Fine-tuning both the language model independently and as part of the end task yields the best performance and reduces the need for a more sophisticated architecture and/or features.",
        "A large legal corpus, even an unannotated one, is a very valuable asset and a significant competitive advantage for NLP applications in that domain."
    ],
    "6155": [
        "Our system first accurately filters out unrelated documents and then performs joint prediction of answer and supporting evidence.",
        "Several novel ideas to train the document filter model and the model for answer and support sentence prediction are presented.",
        "Our proposed system attains competitive results on the HotpotQA blind test set compared to existing systems.",
        "We would like to thank Peng Qi of Stanford University for running evaluation on our submitted models.",
        "This work is partially supported by Beijing Academy of Artificial Intelligence (BAAI)."
    ],
    "6159": [
        "Our models targeting age and gender detection in Arabic social media perform well on both tasks without needing to be pretrained on huge amounts of data like BERT.",
        "The utility of MTL, especially with task-specific attention, is shown on the two learning tasks.",
        "Our models can be deployed for real-world social media analysis at scale, covering 17 Arabic countries.",
        "The models are language-agnostic and can be applied to other languages.",
        "The newly-released BERT model is useful for our models."
    ],
    "6163": [
        "ZEN provides an alternative way of learning larger granular text for pre-trained models.",
        "Experiments on several NLP tasks demonstrated the validity and effectiveness of ZEN.",
        "State-of-the-art results were obtained while ZEN only uses BERT base model requiring less training data and no knowledge from external sources compared to other existing Chinese text encoders.",
        "ZEN is efficient and able to learn with limited data.",
        "ZEN employs a different method to incorporate word information that is complementary to some other previous approaches.",
        "Therefore it is potentially beneficial to combine it with previous approaches suggested by other researchers, as well as to other languages."
    ],
    "6167": [
        "We present PRIVACYQA, the first significant corpus of privacy policy questions and more than 3500 expert annotations of relevant answers.",
        "The goal of this work is to promote question-answering research in the specialized privacy domain, where it can have large real-world impact.",
        "Strong neural baselines on PRIVACYQA achieve a performance of only 39.8 F1, indicating considerable room for future research.",
        "We shed light on several important considerations that affect the answerability of questions.",
        "We hope this contribution leads to multidisciplinary efforts to precisely understand user intent and reconcile it with information in policy documents, from both the privacy and NLP communities."
    ],
    "6168": [
        "We found that the performances of P2P-Tran, P2T-Tran, and T2P-Tran are on par with the performance of T2T-Tran.",
        "We built a large dataset based on WMT18 Zh-En dataset for our experiments.",
        "Our proposed modified BPE algorithm to learn syllables was effective.",
        "Subwords are the best choice for pronunciation sentences, but syllables can be a good alternative.",
        "The performances of P2P-Tran, P2T-Tran, T2P-Tran, and T2T-Tran are similar, so a system should choose the one that simplifies the whole system best, considering its upstream and downstream components."
    ],
    "6170": [],
    "6179": [
        "Existing ASC classifiers have great difficulty in classifying contrastive sentences with multiple aspects and different opinions.",
        "The main cause of this difficulty is the rarity of contrastive sentences.",
        "Assigning higher weights to such examples during training can improve their training.",
        "Our proposed method, ARW, discovers incorrect examples from contrastive sentences and adaptively assigns them higher weights for improved training.",
        "Experimental results show that our method is highly effective in handling contrastive sentences crucial for the ASC task and works well on the full test set."
    ],
    "6180": [
        "previous techniques that used sentence encodings were not sufficient to detect social biases.",
        "our proposed use of contextual word embeddings to assess bias represents an important step in the direction of expanding techniques to consider context.",
        "combining the above de-biasing techniques for contextual word models remains a crucial direction for future work.",
        "methods for de-biasing specifically across race, gender, and intersectional identities remains a challenging open question."
    ],
    "6183": [
        "We propose a novel mechanism to improve bidirectional decoding with dynamic target semantics.",
        "Experimental results show our method achieves remarkable improvements over our baselines.",
        "It also indicates our model can track the interaction histories and lead to more accurate attention distribution.",
        "Recently a synchronous decoding approach proposed by Zhang et al. (2019) achieves promising results, and we plan to integrate their method into our model for further exploration in the future study."
    ],
    "6184": [
        "The use of ANN-based models for Disease-NER improves the performance of BioNER.",
        "Using character embeddings, pre-trained word embeddings, dictionary information, and CRF with global scores enhances the performance of BioNER.",
        "The IOBES scheme outperforms the IOB2 scheme in terms of disease recognition.",
        "The use of pre-trained word embeddings and dictionary information improves the performance of BioNER.",
        "The addition of character embeddings to the model enhances the accuracy of BioNER."
    ],
    "6186": [
        "The proposed approach, PReFIL, surpasses prior state-of-the-art methods for both DVQA and FigureQA.",
        "The use of a projection-based method for attenuating biases in word representations leads to effective results.",
        "The method works for the static GloVe embeddings, but the approach can be extended to contextualized embeddings (ELMo, BERT) by debiasing the first non-contextual layer alone.",
        "The approach effectively attenuates bias in both contextualized embeddings without loss of entailment accuracy.",
        "The results suggest that the community is ready for more difficult CQA datasets.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "The approach has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments."
    ],
    "6188": [
        "The proposed model achieves state-of-the-art performance on four benchmark datasets for definition extraction.",
        "The model improves the representations learned by exploiting the whole dependence structures of the input sentences and the semantic consistency between the terms, the definitions, and the sentences.",
        "The multi-task learning framework jointly performs sequence classification and sequence labeling for definition extraction based on deep learning.",
        "The proposed mechanisms to exploit the whole dependence structures and semantic consistency improve the model's performance."
    ],
    "6189": [
        "The claim that the introduced deep model for Slot Filling (SF) improves label information in the context and predicts which concepts are expressed in the given sentence, leading to state-of-the-art results on three benchmark datasets.",
        "The claim that the multi-task setting of the model increases the mutual information between the word representation and its context.",
        "The claim that the model achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The claim that the novel end-to-end model for joint slot label alignment and recognition requires no external label projection.",
        "The claim that the baseline BERT-Large model brings substantial improvements on multilingual training and cross-lingual transfer tasks."
    ],
    "6191": [
        "The proposed formalization of coreference resolution can successfully retrieve mentions left out at the mention proposal stage.",
        "The model can make use of a plethora of existing question answering datasets for data augmentation.",
        "A new speaker modeling strategy can boost the performance in dialogue settings.",
        "The empirical results demonstrate the effectiveness of the proposed model on two widely-used coreference datasets.",
        "Future work will explore novel approaches to generate questions based on each mention and evaluate the influence of different question generation methods on the coreference resolution task."
    ],
    "6195": [
        "Our proposed model design, HIRE dynamically generates complementary representation from all hidden layers other than that from the default last layer.",
        "The experimental results demonstrate the effectiveness of refined language representation for natural language understanding.",
        "The analysis highlights the distinct contribution of each layer's output for diverse tasks and different examples.",
        "HIRE dynamically generates complementary representation from all hidden layers other than that from the default last layer.",
        "A lite fusion network then incorporates the outputs of HIRE into those of the original model.",
        "The proposed model design refines language representation by adaptively leveraging the Transformer-based model's hidden layers."
    ],
    "6196": [
        "Our proposed method achieves state-of-the-art performance in many standard machine translation tasks, with improvements of 1.0-2.0 BLEU in various tasks.",
        "Our approach exhibits a strong correlation with ensembles of models and trades perplexity off for better BLEU scores.",
        "Our method is complementary to back-translation with extra monolingual data, significantly improving back-translation performance.",
        "The proposed method improves translation performance in low-resource tasks, such as English-Nepali, Nepali-English, English-Sinhala, and Sinhala-English."
    ],
    "6197": [
        "Our new state-of-the-art multilingual masked language model, XLM-R, achieves strong gains over previous multilingual models like mBERT and XLM on classification, sequence labeling, and question answering.",
        "We expose the limitations of multilingual MLMs, including the high-resource versus low-resource trade-off, the curse of multilinguality, and the importance of key hyperparameters.",
        "We show that multilingual models are surprisingly effective over monolingual models, with strong improvements on low-resource languages.",
        "Our model is trained on 2.5 TB of newly created clean Com-monCrawl data in 100 languages."
    ],
    "6198": [
        "This work introduces contextual grounding, a higher-order interaction technique to capture corresponding context between text entities and visual objects.",
        "The evaluation shows SOTA 71.36% accuracy of phrase localization on Flickr30K Entities.",
        "In the future, it would be worth investigating the benefits of grounding guided visual representations in other related and spatio-temporal tasks."
    ],
    "6199": [
        "The proposed approach, LVC transformation, can adapt recurrent neural models to multi-label classification tasks and avoid exposure bias.",
        "Considering the correlations between labels is essential in multi-label emotion classification, and LVC transformation is ideal for this scenario.",
        "The proposed Seq2Emo model performs better on datasets with higher percentages of multi-labeled examples and scales better with the amount of correlations between labels.",
        "The LVC transformation requires k decoding steps for a label set of size k, which can be limiting for RNN-based models when dealing with large label sets.",
        "The time complexity of the model is linearly related to k, making it potentially difficult to scale LVC-based models for MLC tasks with a large number of distinct labels."
    ],
    "6200": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Our novel end-to-end model for joint slot label alignment and recognition requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We have proposed multi-query attention as an alternative to multi-head attention with much lower memory bandwidth requirements in the incremental setting."
    ],
    "6201": [
        "CoKE achieves new state-of-the-art results in link prediction and path query answering on a number of widely used benchmarks.",
        "CoKE is able to capture contextual meanings of entities and relations in a flexible and adaptive way.",
        "CoKE has the potential to be applied to more downstream tasks beyond those within a given KG.",
        "CoKE is conceptually simple yet empirically powerful.",
        "CoKE can discern fine-grained contextual meanings of entities and relations."
    ],
    "6202": [
        "ReorderNAT has a smaller decoding space than NAT, as the decoding vocabulary is limited to the words in the source sentence.",
        "The reordering module is more likely to be guided to one pseudo-translation among multiple alternatives, making it easier to capture reordering information compared to the original NAT.",
        "ReorderNAT can easily capture the dependencies among target words and choose words belonging to the same translation, thanks to the use of pseudo-translations as internal supervision.",
        "The candidates of the i-th word of the final translation can be narrowed to the translations of z i to some extent, since z i is the i-th word in the pseudo-translation which indicates the corresponding source information of y i ."
    ],
    "6206": [
        "The proposed approach for question generation leverages GPT-2 and BERT in an end-to-end trainable fashion, facilitating semi-supervised learning.",
        "The generated questions are of high quality, showing high semantic similarity with groundtruth data.",
        "The proposed approach significantly reduces the burden of full annotation for question generation.",
        "BERT as a QA module in the feedback loop model shows the best performance, attributed to the bi-directional context-specific embeddings and powerful feedback mechanism.",
        "BERT QA can be used as a surrogate measure for assessing question generation quality.",
        "The use of BERT QA can potentially eliminate the need for answer annotations, enabling fully unsupervised question generation."
    ],
    "6209": [
        "The model introduced in the paper, called Senti-LARE, outperforms state-of-the-art language representation models on various sentiment analysis tasks.",
        "The Senti-LARE model leverages linguistic knowledge from SentiWordNet and integrates it into BERT-style models through pre-training tasks.",
        "The use of context-aware sentiment attention and label-aware masked language model in the Senti-LARE model improves its performance on sentiment analysis tasks.",
        "The Senti-LARE model achieves high accuracy and F1 scores on various sentiment analysis benchmarks, as shown in Table 15."
    ],
    "6210": [
        "We have built a spatial question answering system for a physical blocks world, already able to handle a majority of questions in dialogue mode.",
        "Our spatial language model relies on intuitive computational models of spatial prepositions that come close to mirroring human judgments by combining geometrical information with context-specific information about the objects and the scene.",
        "This enables natural interaction between the machine and the user.",
        "We are not aware of any other end-to-end system with comparable abilities in QA dialogues about spatial relations.",
        "Our ongoing work is targeting reasoning about structures and complex shapes, which will eventually be incorporated into blocks world structure learning and collaborative construction tasks."
    ],
    "6211": [
        "a carefully implemented IE system can be used to improve the factual correctness of neural summarization models via RL",
        "optimizing the ROUGE metrics via RL can substantially improve the factual correctness of the generated summaries",
        "our work draws the community's attention to the factual correctness issue of abstractive summarization models",
        "in a domain with a limited space of facts such as radiology reports, a carefully implemented IE system can be used to improve the factual correctness of neural summarization models via RL",
        "even in the absence of a reliable IE system, optimizing the ROUGE metrics via RL can substantially improve the factual correctness of the generated summaries"
    ],
    "6215": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our PLTE outperforms various baselines and performs up to 11.4 times faster than previous lattice-based method.",
        "Switching to BERT representations, PLTE achieves more significant performance gain than existing methods.",
        "Applying our model to the pre-training procedure of Chinese Transformer language models is a promising direction for future work."
    ],
    "6216": [
        "The proposed diversified co-attention model for automatic live video commenting captures the complex dependency between video frames and surrounding comments, leading to better representations.",
        "By introducing bidirectional interactions between the video and text from multiple perspectives, two information sources can mutually boost for better representations.",
        "The proposed approach can substantially outperform existing methods and generate comments with more novel and valuable information.",
        "The technique of parameter orthogonalization helps to avoid excessive overlap of information extracted from different perspectives."
    ],
    "6218": [
        "We propose a joint model for the task of Deep Linearization, which extends Puduppully et al. (2016) to perform joint graph linearization, function word prediction, and morphological generation.",
        "Our system uses Transition-Based methods for joint NLG from semantic structure, and achieves the highest scores reported for the NLG 2011 shared task on Deep Input Linearization (Belz et al., 2011).",
        "We demonstrate the usefulness of our joint model by outperforming the baseline Puduppully et al. (2016) on the NLG 2011 shared task.",
        "Our approach uses less than 1% of the training data, yet achieves competitive performance compared to previous systems trained on the full dataset.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard."
    ],
    "6219": [
        "The proposed Multi-source Word Aligned Attention model (MWA) enhances the fine-tuning performance of Chinese PLMs.",
        "The MWA approach yields substantial improvements compared to BERT, BERT-wwm, and ERNIE.",
        "The word-aligned attention can be applied to English PLMs to bridge the semantic gap between whole words and segmented Word-Piece tokens.",
        "The proposed approach is effective and universal, demonstrating its ability to improve performance on a variety of NLP tasks.",
        "The use of word segmentation information in the character-level self-attention mechanism enhances the fine-tuning performance of Chinese PLMs."
    ],
    "6220": [
        "using a combination of similarity and coherence for scoring relevant passages is a simple estimate but works quite well in practice.",
        "A context window of size three seems to successfully capture significant word co-occurrences.",
        "giving greater influence to the Indri ranking and giving a higher preference to node weights than edge weights improves results.",
        "including only the previous and the first turns proved to be most beneficial.",
        "Weighted turns did not improve the results significantly.",
        "positions of query terms in passages following the intuition that passages are more relevant in which the query terms appear earlier.",
        "using contextualized embeddings, like a pretrained BERT model [7] , to deal with polysemy.",
        "introducing its ranking results as an additional score."
    ],
    "6222": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "the proposed loss function help to achieve significant performance boost without changing model architectures."
    ],
    "6223": [
        "The beam problem in open-domain question answering can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach to joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Our efficient training of sparse representations by kernelizing the sparse inner product space leads to faster open-domain QA models.",
        "Our fast open-domain QA model that augments DENSPI with SPARC outperforms previous open-domain QA models, including recent BERT-based pipeline models, with two orders of magnitude faster inference time."
    ],
    "6225": [
        "The proposed method integrating dynamic embeddings into the training of static embeddings shows better results on lexical semantics tasks such as word similarity and analogy.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Enabling \"tag \u2192 parse\" only also improves the tagging accuracy itself.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, leading to improvements over previous work on joint POS tagging and dependency parsing."
    ],
    "6226": [
        "We study the lightweight BERT model with the goal of achieving both efficiency and effectiveness.",
        "We profile and analyze the memory bottlenecks of BERT and focus on optimize dotproduct self-attention, which consumes quadratic memory with respect to the sequence length.",
        "To reduce both time and memory consumption, we present BlockBERT, which sparsifies the attention matrices to be sparse block matrices.",
        "The proposed model achieves time and memory saving without significant loss of performance.",
        "In the future, we plan to benchmark more efficient Transformers in language model pre-training and fine-tuning.",
        "We also would like to explore more applications of BlockBERT on NLP tasks involving long sequences such as coreference resolution (Joshi et al., 2019b ) and document-level machine translation (Miculicich et al., 2018) , and also non-NLP tasks such as protein sequence modeling (Rives et al., 2019; Rao et al., 2019)."
    ],
    "6228": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We found that better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We propose PReFIL, a new CQA system that improves the state-of-the-art and surpasses human accuracy on two datasets.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments."
    ],
    "6230": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy."
    ],
    "6232": [
        "The MovieQA dataset has language bias.",
        "A simple QA only model can exploit this language bias to achieve state-of-the-art performance on four out of five categories on the leaderboard.",
        "Training a word2vec model on a subset of the data used by state-of-the-art methods, and focusing only on the train and test movie plots, can lead to improved performance."
    ],
    "6238": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Applying our proposed controlled crowdsourcing protocol to QA-SRL successfully attains truly scalable high-quality annotation by laymen, facilitating future research of this paradigm.",
        "Our workers' annotation quality, and particularly its coverage, are on par with expert annotation.",
        "We release our data, software, and protocol, enabling easy future dataset production and evaluation for QA-SRL, as well as possible extensions of the QA-based semantic annotation paradigm.",
        "Our simple yet rigorous controlled crowdsourcing protocol would be effective for other challenging annotation tasks."
    ],
    "6242": [
        "Our memory-augmented neural networks (MARNNs) outperformed vanilla RNN and LSTM models in recognizing the generalized Dyck languages.",
        "Our MARNNs learned to emulate pushdown-automata to recognize the Dyck languages.",
        "We were able to visualize the changes in the external memory of the Baby-NTMs trained to learn the D 2 and D 3 languages.",
        "Our simple analysis revealed that our MARNNs learned to emulate pushdown-automata to recognize these Dyck languages.",
        "Our approach outperformed benchmark models across different datasets.",
        "We limited the dimensionality of the external memory in our memory-augmented architectures to one, which allowed us to visualize the changes in the external memory of the Baby-NTMs trained to learn the D 2 and D 3 languages."
    ],
    "6244": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model for part-of-speech tagging and dependency parsing improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Pretraining with external working memory and dynamic routing leads to significantly higher correctness, lower external knowledge, and slightly higher relevance compared to models without pretraining.",
        "Reinforcement helps improve the diversity of generated questions, but may degrade the formulations in the outputs.",
        "The dataset creation methodology used in this study is effective, as the data derived from it fits well with the task of curiosity-driven question generation.",
        "The models built using the proposed approach obtain lower scores in terms of answerability than humans, indicating that the generated questions may be of lower quality."
    ],
    "6246": [
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing.",
        "We present a novel self-training methodology for learning DNN-based dialogue generation models using noise injection sampling and a MR parser.",
        "Our findings hold under a human evaluation as well.",
        "On automatic measures of syntactic complexity, our approach is closer to matching human authored references than prior work.",
        "In future work, we intend to explore methods of self-training that further improve syntactic diversity."
    ],
    "6251": [
        "The models explored in this work, which follow two main approaches (FF-NN and RNN), proved to be very effective and performed on par with or better than state-of-the-art (SOTA) approaches.",
        "The authors plan to investigate sequence-to-sequence models such as RNN Seq2seq, Conv Seq2seq, and Transformer in the future.",
        "Integrating diacritics into other systems can attain enhanced versions in NLP tasks, as demonstrated by the example of using MT as a case study and showing how the idea of ToD improved the results of the SOTA NMT system.",
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks."
    ],
    "6255": [
        "Adversarial training (AT) improves performance significantly and consistently across different natural language processing tasks.",
        "Semi-supervised learning on MRC tasks can benefit from training on unanswerable questions.",
        "AT cannot defend against artificial adversarial examples.",
        "AT helps the model to learn better on the examples with more rare words."
    ],
    "6257": [
        "The method of making the source sentence and machine translation attend to each other improves the performance of post-editing.",
        "Explicitly predicting words to be copied is beneficial for improving the performance of post-editing.",
        "Our approach achieves new state-of-the-art results on the WMT 2016 & 2017 APE PBSMT sub-tasks."
    ],
    "6258": [
        "The proposed approach, Hierarchical Graph Network (HGN), achieves state-of-the-art performance on the HotpotQA benchmark for multi-hop question answering.",
        "The HGN model weaves heterogeneous nodes into a single unified graph to capture clues from different granularity levels.",
        "The current approach of using an off-the-shelf paragraph retriever for selecting relevant context from a large corpus of text can be improved by investigating the interaction and joint training between HGN and the paragraph retriever.",
        "The brevity problem in neural machine translation can largely be explained by the locally-normalized structure of the model, and solving this problem leads to significant BLEU gains.",
        "The solution to the brevity problem requires globally-normalized training on only a small dataset, and more general globally-normalized models can be trained in a similarly inexpensive way."
    ],
    "6260": [
        "The proposed multi-perspective inferrer (MPI) achieves substantial improvement in NLI tasks, especially in relieving misjudgment between entailment and neutral relations when there is a small amount of aligned parts.",
        "The MPI method takes all perspectives into account to make the final decision, which ensures highly interpretable predictions.",
        "The explicit supervision on each perspective introduced in the MPI method ensures accurate predictions and reduces misjudgment in NLI tasks.",
        "The MPI method has the potential to be suitable for other sentence pair tasks such as semantic matching and machine comprehension.",
        "The current implementation of the MPI method achieves sub-stantial improvement in NLI tasks, especially when there is a small amount of aligned parts."
    ],
    "6261": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our method for generating adversarial samples is capable of exposing translation pitfalls without handcrafted error features.",
        "Our method achieves stable degradation with meaning preserving adversarial samples over different victim models.",
        "Our method can generate adversarial samples efficiently from monolingual data, making it convenient for mass production of adversarial samples for victim model analysis and further improvement of robustness."
    ],
    "6263": [
        "We introduced E-BERT, an efficient yet effective way of injecting factual knowledge about entities into the BERT pretrained Language Model.",
        "We showed how to align Wikipedia2Vec entity vectors with BERT's wordpiece vector space, and how to feed the aligned vectors into BERT as if they were wordpiece vectors.",
        "In doing so, we made no changes to the BERT encoder itself.",
        "This stands in contrast to other entity-enhanced versions of BERT, such as ERNIE or KnowBert, which add encoder layers and require expensive further pretraining.",
        "We set a new state of the art on LAMA, a recent unsupervised QA benchmark.",
        "Furthermore, we presented evidence that the original BERT model sometimes relies on the surface forms of entity names (rather than \"true\" factual knowledge) for this task.",
        "To quantify this effect, we introduced LAMA-UHN, a subset of LAMA where questions with helpful entity names are deleted.",
        "We also showed how to apply E-BERT to two supervised tasks: relation classification and entity linking. On both tasks, we achieve competitive results relative to BERT and other baselines."
    ],
    "6264": [
        "The model achieves state-of-the-art performance on an array of response selection tasks and in transfer learning for intent classification tasks.",
        "The model offers more accurate conversational pretraining models.",
        "The model is lightweight and takes up only a small amount of space.",
        "The model is efficient in terms of training cost."
    ],
    "6265": [
        "Our major contribution in this paper is threefold: we present COMMONGEN, a novel constrained text generation task for generative commonsense reasoning, with a large dataset; we carefully analyze the inherent challenges of the proposed task; and our extensive experiments systematically examine recent pre-trained language generation models (e.g., UniLM, BART, T5) on the task.",
        "The performance of recent pre-trained language generation models on the COMMONGEN task is still far from humans, generating grammatically sound yet realistically implausible sentences.",
        "Our study points to interesting future research directions on modeling commonsense knowledge in language generation process, towards conferring machines with generative commonsense reasoning ability.",
        "We hope that COMMONGEN would also benefit downstream NLG applications such as conversational systems and storytelling models."
    ],
    "6268": [
        "KLD, ASA, and MTL approaches for speaker adaptation in AED-based E2E ASR system\" - This claim highlights the novel approach of using KLD, ASA, and MTL for speaker adaptation in an AED-based E2E ASR system.",
        "avoid overfitting\" - This claim emphasizes the importance of avoiding overfitting in the KLD approach.",
        "adversarial learning\" - This claim highlights the use of adversarial learning in the ASA approach.",
        "offset the asymmetric deficiency of KLD\" - This claim explains how the ASA approach can offset the asymmetric deficiency of KLD.",
        "resolve the target sparsity issue\" - This claim highlights the ability of MTL to resolve the target sparsity issue.",
        "significant improvements over a strong SI AED baseline\" - This claim emphasizes that all three methods (KLD, ASA, and MTL) achieve significant improvements over a strong SI AED baseline.",
        "consistently improves over KLD\" - This claim highlights that ASA consistently improves over KLD.",
        "not comparable with that of KLD and ASA\" - This claim emphasizes that the performance of MTL is not comparable with that of KLD and ASA, potentially indicating that larger improvements can be achieved by KLD and ASA."
    ],
    "6269": [
        "The proposed model for document-level argument linking outperforms strong baselines on the RAMS dataset, which is constructed with a small amount of existing data for the task.",
        "The novel model introduces a new approach to multisentence argument linking, which has not been well explored in previous work.",
        "The model's performance is applicable to a variety of related datasets, indicating its versatility and potential for real-world applications.",
        "The construction of the RAMS dataset provides a valuable resource for training and evaluating models for multisentence argument linking, which has not been well explored in previous work.",
        "The hope is that the RAMS dataset will stimulate further work on multisentence argument linking, indicating the potential for future research and development in the field."
    ],
    "6270": [
        "The authors have introduced a new task called dodecaDialogue to evaluate conversational agents with multiple skills.",
        "The goal of this task is to move towards evaluating open-domain sets of skills, beyond niche skills.",
        "The current systems are reasonably competitive compared to humans in particular domains for short conversations.",
        "There is still a long way to go in terms of improving the systems to evaluate an open-domain set of skills.",
        "The authors have leveraged 12 tasks in their work, but there are still many skills not included in their set, such as longer conversations involving memory or mixing open-domain conversation with task-oriented goals.",
        "Future work should consider adding these tasks to the ones used here, while continuing the quest for improved models."
    ],
    "6272": [
        "The proposed method for inducing modularity in attention-based seq2seq models achieves a competitive WER performance in the standard 300h Switchboard task.",
        "The learned model adheres to the three properties of modular systems -independence, interchangeability, and clearness of interface.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The proposed method for inducing modularity in attention-based seq2seq models achieves a competitive WER performance in the standard 300h Switchboard task, and has the potential to be extended to other sequence-to-sequence machine translation and language processing tasks."
    ],
    "6273": [
        "Our specific word embedding method for aspect term extraction achieves better results compared to other embedding methods.",
        "We consider both positional and dependency context when learning the word embedding.",
        "The lexical information along dependency path is encoded into representations of dependency context.",
        "Our method can be applied to more NLP tasks, such as text classification, sentiment analysis, and machine translation.",
        "[Our specific word embedding method for aspect term extraction achieves better results compared to other embedding methods.] (Paragraph 3)",
        "[We consider both positional and dependency context when learning the word embedding.] (Paragraph 3)",
        "[The lexical information along dependency path is encoded into representations of dependency context.] (Paragraph 3)",
        "[Our method can be applied to more NLP tasks, such as text classification, sentiment analysis, and machine translation.] (Paragraph 4)"
    ],
    "6274": [
        "Our approach outperforms existing IR methods for entity retrieval, achieving new state-of-the-art results on several benchmark datasets.",
        "Our BERT-based model is conceptually simple, scalable, and highly effective for entity linking.",
        "We show that it is possible to achieve efficient linking with modest loss of accuracy through evaluations of the accuracy-speed trade-off inherent to large pre-trained models.",
        "Knowledge distillation can further improve biencoder model performance.",
        "Enriching entity representations by adding entity type and entity graph information can potentially improve entity linking performance.",
        "Modeling coherence by jointly resolving mentions in a document is a promising direction for future work.",
        "Extending our work to other languages and other domains could lead to further improvements in entity linking performance.",
        "Joint models for mention detection and entity linking may be effective in improving the overall performance of the system."
    ],
    "6275": [
        "\"Translationese and original text can be treated as separate target languages in a 'multilingual' model.",
        "\"The resulting model has improved performance in the ideal, zero-shot scenario of original\u2192original translation.",
        "\"However, this is associated with a drop in BLEU score, indicating that better automatic evaluation is needed."
    ],
    "6276": [
        "Compared to WAE and VAE, WAE with GMP provides control over the style of generated samples.",
        "WAE with GMP generates fluent and diverse sentences while it is capable of generating sentences with a mixture of styles.",
        "The GMP is powerful to capture the latent representation of the dataset, allowing for the addition of more data samples with other classes to small datasets and learning enough features to generate diverse samples with a desired style/class.",
        "WAE and VAE are not capable of learning the representation of a dataset, nor can they learn a good language model when the dataset has few training samples."
    ],
    "6278": [
        "The beam problem in neural machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our models can effectively preserve salient source relations in summaries.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "The gendered word lists used may not be comprehensive, and a more continuous representation of gender should be used in the future.",
        "We present an effective method to control the quantity of gendered words generated by manipulating control bins.",
        "Generative dialogue models are prone to overuse frequent words and produce generic utterances, and this can affect bias.",
        "Combining all bias mitigation methods in one yields the best control over the genderedness of the utterances while improving the F1-score.",
        "The percent of male bias and the percent of gendered words in the generated utterances can be improved by using a more continuous representation of gender."
    ],
    "6279": [
        "Using well-founded mechanisms for assessing the validity of hypotheses is crucial for any field that relies on empirical work.",
        "Our survey indicates that the NLP community is not fully utilizing scientific methods geared towards such assessment, with only a relatively small number of papers using such methods, and most of them relying on p-value.",
        "We surfaced various issues and potential dangers of careless use and interpretations of different approaches.",
        "We do not recommend a particular approach. Every technique has its own weaknesses.",
        "Incorrect use of any technique can result in misleading conclusions.",
        "We contribute a new toolkit, HyBayes, to make it easy for NLP practitioners to use Bayesian assessment in their efforts.",
        "We hope that this work provides a complementary picture of hypothesis assessment techniques for the field and encourages more rigorous reporting trends."
    ],
    "6281": [
        "The proposed Fakeddit dataset provides a large number of multimodal samples with multiple labels for various levels of fine-grained classification, which is unique and has wide-ranging practicalities in fake news research and other research areas.",
        "The current baseline models and attention variants significantly underperform compared to the proposed Fakeddit dataset, indicating that there is still significant room for improvement in fine-grained fake news detection.",
        "The error analysis on the results highlights the importance of large scale multimodality unique to Fakeddit and demonstrates that there is still significant room for improvement in fine-grained fake news detection.",
        "The dataset has wide-ranging practicalities in fake news research and other research areas, such as tracking a user's credibility through using the metadata and comment data provided, incorporating video data as another multimedia source, and implicit fact-checking research with an emphasis on image-caption verification.",
        "The proposed dataset can be used to advance efforts to combat the ever-growing rampant spread of disinformation in today's society."
    ],
    "6282": [
        "We introduced a novel approach, based on example forgetting, to extract minority examples and build more robust models systematically.",
        "Our method is based on example forgetting, where we built a set of minority examples on which a pre-trained model is fine-tuned.",
        "We evaluated our method on large-scale models such as BERT and XLNet and showed a consistent improvement in robustness on three challenging test sets.",
        "The larger versions obtain higher out-of-distribution performance than the base ones, but still benefit from our method."
    ],
    "6285": [
        "Our proposed approach integrates graph structure at every stage for text-based open-domain question answering, which consistently outperforms competitive baselines on three datasets.",
        "Our retrieval method leverages both text corpus and a knowledge base to find a relevant set of passages and their relations.",
        "Our reader propagates information according to the input graph, enabling knowledge-rich crosspassage representations.",
        "We included a detailed qualitative analysis to illustrate which components contribute the most to the overall system performance."
    ],
    "6290": [
        "The model \"Bilingual Generative Transformers\" uses parallel data to learn to perform source separation of common semantic information between two languages from language-specific information.",
        "The model is able to accomplish source separation through probing tasks and text generation in a style-transfer setting.",
        "The model bests all baselines on unsupervised semantic similarity tasks, with the largest gains coming from a new challenge called \"Hard STS\".",
        "The model is especially effective on unsupervised cross-lingual semantic similarity, due to its ability to strip away language-specific information and allow for the underlying semantics to be more directly compared.",
        "In future work, the authors plan to explore generalizing this approach to the multilingual setting or applying it to the pre-train and fine-tune paradigm used widely in other models such as BERT."
    ],
    "6291": [
        "Our proposed HighwayGraph consistently and significantly achieves improvements over multiple GNNs on three benchmark graph datasets with limited extra computational cost.",
        "Our method verifies the effectiveness and generalization of our method.",
        "Highway-Graph enables to significantly improve prediction accuracy for unlabeled nodes that are far away from labeled nodes.",
        "The two proposed solutions to modelling long-distance node relations are effective and justify the improvements.",
        "In the future, we plan to find better methods in this direction."
    ],
    "6297": [
        "We analyze posterior collapse in sequence VAEs from the perspective of the encoder network.",
        "The issue is caused in part by the lack of dispersion in features from the encoder.",
        "We provide empirical evidence to verify this hypothesis.",
        "We propose a simple architectural change that utilizes pooling operations.",
        "Our proposed methods can effectively prevent posterior collapse while achieving comparable or better NLLs compared to existing methods without any additional computation costs."
    ],
    "6298": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "DeCom could be easily incorporated into many existing knowledge graph embedding models and experimental results show that it could boost the performance of many popular link predictors on several knowledge graphs and obtain state-of-the-art results on FB15k-237 across all evaluation metrics."
    ],
    "6300": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We propose PReFIL, a new CQA system that improves the state-of-the-art and surpasses human accuracy on two datasets.",
        "Our results suggest harder datasets are needed for advancing the field.",
        "Better OCR is also important for advancing the field of CQA."
    ],
    "6303": [
        "We propose a novel framework to understand text data by converting English sentences or articles into video-like 3-dimensional tensors.",
        "Our model can be easily applied to other languages as well as other NLP tasks such as machine translation.",
        "The proposed model achieves surprisingly excellent results on text classification, with a final goal of topic and sentiment analysis.",
        "Imposing a 3-dimensional convolutional kernel on text tensors makes it convenient to implement an n-gram model based on convolutional neural networks.",
        "A subsequent 1-dimensional max-over-time pooling is applied to this feature map, and then three FC layers are implemented with a final goal for text classification."
    ],
    "6305": [
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "Our method alleviates out-of-domain parsing errors, leading to superior performance compared to tree structures.",
        "We have proposed two algorithms for generating high-quality dependency forests for relation extraction."
    ],
    "6307": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The Cosine Annealing Strategy is proposed to combine the two methods.",
        "The approach utilizes pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "The adaptive compression of BERT for various downstream tasks using Neural Architecture Search achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed AdaBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The task of meta-answering is a framework for QA that simulates real-world imperfect information-seeking tasks, where humans look for answers in settings mediated by machines using natural language.",
        "Human meta-answerers can compete with a BERT-based single system with access to full documents, by only looking at a five token window around candidates.",
        "The task of meta-answering brings to the surface limitations of the current NLU paradigm, and MMA cannot use the contextual information that is effortlessly exploited by humans.",
        "The use of reinforcement learning for meta answering is a future work, motivated by the sequential aspect of information-gathering and answering, and the challenges of credit assignment in sparse rewards."
    ],
    "6309": [
        "Our approach significantly outperforms the existing TS (En) So the same as we saw before.",
        "OT (De) Also genauso, wie wir es vorher gesehen haben.",
        "We incorporate ASR and MT tasks in a principled way to leverage additional sources of data.",
        "Our approach brings new opportunities to build efficient end-to-end ST systems with a limited amount of training data.",
        "The proposed approach is a generic framework that can comfortably accommodate existing and future end-to-end ST models.",
        "We improved the performance of the proposed method by augmenting synthetic data and using wordpiece vocabularies."
    ],
    "6312": [
        "Our proposed modification outperforms previous models in the six datasets.",
        "The adapted Transformer is suitable for being used as the English character encoder, because it has the potentiality to extract intricate patterns from char-acters.",
        "Experiments in two English NER datasets show that the adapted Transformer character encoder performs better than BiLSTM and CNN character encoders.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements."
    ],
    "6314": [
        "Our proposed framework can effectively improve the basic SRL model, even when the basic model is enhanced with BERT representations.",
        "Our framework is more effective at utilizing syntactic information compared to the hard parameter sharing strategy of MTL.",
        "By utilizing BERT representations, our framework achieves new state-of-the-art performance on both span-based and word-based Chinese SRL benchmarks.",
        "Detailed analysis shows that syntax helps most on long sentences, because of the long-distance dependencies captured by syntax trees.",
        "There is still a lot of work to do to better integrate syntactic information and BERT representation."
    ],
    "6315": [
        "The proposed pre-training based dialogue generation model can produce coherent and persona-consistent responses conditioned on explicitly represented personas.",
        "The model can effectively utilize persona-sparse dialogue data in the fine-tuning stage.",
        "Adding attribute embeddings in the encoder helps to model the persona of each speaker involved in the dialogue context.",
        "A dynamic weighting scheme in the decoder balances the amount of persona-related features to exhibit in the decoded responses.",
        "The model can incorporate richer persona-related features in the generated responses compared to state-of-the-art baselines when the dialogues available at the fine-tuning stage are persona-sparse."
    ],
    "6319": [
        "The current state-of-the-art in text-to-SQL parsing is not satisfactory, and many contemporary models struggle to learn good representations for a given database schema.",
        "The main challenges in text-to-SQL parsing are learning good representations for the database schema and properly linking column/table references in the question.",
        "The proposed RAT framework addresses the schema encoding and linking challenges by jointly learning schema and question representations based on their alignment with each other and schema relations.",
        "The RAT framework achieves significant state-of-the-art improvement on text-to-SQL parsing tasks.",
        "The RAT framework provides a way to combine predefined hard schema relations and inferred soft self-attended relations in the same encoder architecture, which is beneficial for tasks beyond text-to-SQL as long as the input has some predefined structure."
    ],
    "6320": [
        "We have shown that margin-based mining in a joint multilingual sentence embedding space can be scaled to monolingual texts of more than 36 billions unique sentences in 38 languages.",
        "Our approach is generic and simply compares all sentences among each other, without requiring any document alignment.",
        "We tackled the computational complexity by parallelizing all processing steps.",
        "This procedure yielded 661 million sentences aligned with English, and 4.5 billion for pairwise alignments of 28 languages.",
        "To the best of our knowledge, this is by far the largest collection of high quality parallel sentences.",
        "We have performed an extensive evaluation of the quality of the mined bitexts by training NMT systems for many language pairs.",
        "The mined bitexts seem to be of high quality.",
        "Training only on our mined data, we are able to outperform the best reported single NMT system at the WMT'19 evaluations for the translation between German, Russian and Chinese and English, as well as between German and French.",
        "We also achieve state-of-theart BLEU scores for the translation between Russian and Japanese on the WAT'19 test set.",
        "In the next version of the CCMatrix corpus, we will increase the number of common crawl snapshots and focus on low-resource languages.",
        "The mined data can be also used to train improved multilingual sentence embeddings.",
        "The large amount of parallel data also raises interesting question how to use it best, for instance, how to efficiently train NMT systems on more than fifty million high quality bitexts?"
    ],
    "6321": [
        "Different representation units in NMT have distinct advantages and disadvantages.",
        "Character-level models and BPE models with different vocabulary sizes can have very different behaviors.",
        "In the future, we would like to investigate methods to combine different representations to get the best of all worlds."
    ],
    "6322": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model of part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Biased representations lead to biased inferences, and the task of natural language inference can be used to construct a systematic probe for measuring biases in word representations.",
        "The projection-based method for attenuating biases works for the static GloVe embeddings and can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Existing neural-based multi-task learning models are limited by the sharing mechanisms used, and hard sharing, soft sharing, and hierarchical sharing have limitations such as struggling with heterogeneous tasks, being parameterinefficient, or depending on manually designed architectures.",
        "The proposed Sparse Sharing mechanism, which partially shares parameters across tasks, is flexible and can handle loosely related tasks, and the approach to induce such architectures by automatically extracting subnets for each task is simple yet efficient."
    ],
    "6323": [
        "Adversarial test sets can be used to evaluate the robustness of language understanding models in task-oriented dialog systems.",
        "Using a combination of back-translation and sequence auto-encoder to generate adversarial test sets can improve the model's performance compared to using either method alone.",
        "Improving the model's robustness against adversarial test sets by augmenting the training data with back-translated paraphrases and using a new loss function based on logit pairing can be effective.",
        "Combining data augmentation with back-translation and adversarial logit pairing loss performs best on adversarial test sets."
    ],
    "6327": [
        "The authors have reduced the cost of bootstrapping by using dormant accounts and old transactions backed up by restoration mechanism and incentive structure.",
        "The authors' work enables ordinary clients to bootstrap efficiently, send or verify a transaction themselves, and reduces storage size and bootstrapping time.",
        "However, a restore transaction may require large payload data for merkle proofs and void proofs, which can be costly.",
        "The authors have optimized the ethereum network using ethanos, but it is not limited to ethereum and could be applied to any account-based blockchain that uses MPT.",
        "The authors did not treat smart contracts for simplicity in this paper and left them as a future work."
    ],
    "6328": [
        "We proposed a novel architecture, HUSE, to learn a universal embedding space that incorporates semantic information.",
        "Unlike previous methods, HUSE learns a new universal embedding space that still has the same semantic distance as the class label embedding space.",
        "The embedding space learned by HUSE has more semantic information than the other baselines as measured by HP@K metric.",
        "A shared classification layer used by HUSE for both image and text embeddings and the instance loss reduced the media gap and resulted in superior cross-modal performance.",
        "HUSE also achieved state-of-the-art classification accuracy of 92.3% on UPMC Food-101 dataset, outperforming the previous best by 1.5%."
    ],
    "6331": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The resulting fully-supervised parser outperforms the state-of-theart baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "PReFIL outperformed prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL exceeded the human baseline for FigureQA, but results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, with PRe-FIL using oracle OCR exceeding humans across all question types.",
        "The proposed fully-supervised parser outperformed the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations."
    ],
    "6333": [
        "The widely-known baselines for caption generation based on a sequence of action features do not perform as well as conditional language models, especially when the decoder is initialized with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "The latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Tagging ambiguity is mostly local while dependency parsing heavily depends on POS tags to predict syntactic structures.",
        "Enabling \"tag \u2192 parse\" only also improves the tagging accuracy itself."
    ],
    "6337": [
        "We find that randomised embeddings are capable of outperforming state-of-the-art pre-trained models, including ELMo and BERT, when the dataset contains many sparse, lexically-similar terms such as measurements.",
        "ELMo embeddings outperform BERT embeddings in short-text LN, and that word-level models outperform character-level models in our experiments.",
        "Our word-level GRU model outperforms existing deep-learning based techniques on Twitter data.",
        "We have also released a substantial lexical normalisation dataset to the community, and hope that this dataset may be used for future LN research.",
        "In future, it will be useful to find a way to mitigate a key drawback of word-level lexical normalisation: the inability to normalise words to labels that have not been seen in the training data."
    ],
    "6339": [
        "Our proposed approach of modeling multi-domain dialogue state tracking as question answering with a dynamically-evolving knowledge graph enables the model to generalize to new domains, slots, and values by simply constructing new questions.",
        "Our model achieves state-of-the-art results on MultiWOZ 2.0 and MultiWOZ 2.1 datasets with a 5.80% and a 12.21% relative improvement, respectively.",
        "Our domain expansion experiments show that our model can better adapt to unseen domains, slots, and values compared to the previous state-of-the-art model.",
        "Our approach uses less than 1% of the training data to achieve competitive performance compared to previous systems.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard."
    ],
    "6343": [
        "The proposed Dynamic Neural Network for Relation Extraction (DNNRE) improves the DSRE accuracy.",
        "The dynamic design of the model benefits for potential style shifts caused by keyword variation under different entity types.",
        "The entity type information can be trained cross different relation classes, benefiting long-tail relation classes.",
        "Exploring how to better model the style shift problem with different information inputs, such as entity description information, can further improve DSRE accuracy.",
        "Utilizing connections between different relation classes can boost the performance of DSRE.",
        "Incorporating entity typing into the DNNRE via a multitask training manner can further improve the DSRE performance."
    ],
    "6345": [
        "The proposed model, CatGAN, achieves better performance than existing state-of-the-art methods on both category text generation and general text generation.",
        "The informative updating signal provided by measuring the relativistic relation between generated samples and real samples on each category helps guide the category-aware model to obtain accurate category samples.",
        "The hierarchical evolutionary learning algorithm developed to train CatGAN allows the model to preserve well-performing offspring, where the generated category samples can retain diversity and high quality after each training iteration.",
        "The proposed method improves the generation performance by measuring the relativistic relation between generated samples and real samples on each category.",
        "The hierarchical evolutionary learning algorithm developed to train CatGAN is effective in improving the generation performance."
    ],
    "6347": [
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "GloVe, ELMo, and BERT embeddings all encode gender, religion, and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "We plan to further personalize the dialog policy based on user attributes and conversational preferences, and investigate richer state representations.",
        "We plan to explore the impact of evolving attributes and preferences on the learned policies."
    ],
    "6358": [
        "We introduced ROCC, a simple unsupervised approach for selecting justification sentences for question answering, which balances relevance, overlap of selected sentences, and coverage of the question and answer.\" (emphasis added)",
        "We coupled this method with a state-of-the-art BERT-based supervised question answering system, and achieved a new state-of-the-art on the MultiRC and ARC datasets among approaches that do not use external resources during training.\" (emphasis added)",
        "ROCC-based QA approaches are more robust across domains, and generalize better to other related tasks like entailment.\" (emphasis added)",
        "In the future, we envision that ROCC scores can be used as distant supervision signal to train supervised justification selection methods.\" (emphasis added)"
    ],
    "6359": [
        "We propose a generic Multi-zone Unit for RNNs along with three effective variants, which boost the state transition function through modeling multiple space composition.",
        "To further enhance MZUs, we propose an effective regularization objective to promote the diversity in multiple zones.",
        "Our MZUs can automatically capture rich linguistic information and substantially improve performance.",
        "Experimental results on the ABSA task demonstrate the superiority and generalizability of the MZU."
    ],
    "6360": [
        "The proposed DualVD model achieves superior performance compared to other state-of-the-art approaches.",
        "The DualVD model is interpretable and provides insight into how information from different modalities is used for inferring answers.",
        "Deriving visual information from visual-semantic representations can improve the performance of visual dialogue systems.",
        "The gate mechanism in the DualVD model allows for adaptive selection of desired clues for answer inference.",
        "The use of a semantic module that encodes image information at the semantic level can improve the performance of visual dialogue systems.",
        "The use of a visual module that encodes image information at the appearance level can improve the performance of visual dialogue systems."
    ],
    "6363": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism.",
        "The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.",
        "Our approach outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "There is additional local information present that previous approaches have not taken advantage of."
    ],
    "6364": [
        "We propose a novel method of decomposing common grounding based on its subtasks to study the intermediate process of common grounding.",
        "Our approach demonstrates advantages over existing baseline models through extensive analysis of the annotated corpus.",
        "Our work is expected to be a fundamental step towards interpreting and improving common grounding through reference resolution.",
        "We demonstrate the effectiveness of our method through analyzing the annotated corpus and comparing it to existing baseline models.",
        "Our novel approach allows for a more fine-grained understanding of the intermediate process of common grounding."
    ],
    "6366": [
        "The proposed method, SCDV-MS, outperforms previous embeddings (including SCDV and BERT) on downstream text classification tasks.",
        "Disambiguating multi-sense words based on context words can lead to better document representations.",
        "Sparsity in representation is helpful for effective and efficient lower-dimensional manifold representation learning.",
        "Representation noise in words level can have a significant impact on downstream tasks."
    ],
    "6368": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.",
        "The solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our classifier for telling apart comments by mentioned trolls vs. such by non-trolls performs almost equally well for paid trolls vs. non-trolls.",
        "Using just mentions might be a \"witch hunt\": some users could have been accused of being \"trolls\" unfairly.",
        "Finding users who have been called trolls more often is easier, which suggests they might be trolls indeed."
    ],
    "6369": [
        "Our models can effectively preserve salient source relations in summaries.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Our results demonstrate the significance of our findings based on micro-analysis and human judgments."
    ],
    "6372": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The goal of this research is to explore how to interface the extremely effective aspects of models such as GPT-2 with more accessible sources of knowledge and planning.",
        "By using the human readable output of a Language Model component to direct further information gathering (or, potentially, other activities), one might imagine the system would not only become more capable (without exponentially long training), but would also have an internal dialogue that would be human interpretable.",
        "Embedding factoids in neural network weights is not a practical way of building intelligent systems.",
        "Even humans (built on a biological neural substrate) seem to reason about facts symbolically despite the processing being based in neurons."
    ],
    "6374": [
        "The proposed global greedy parser can perform projective parsing when using only two arc-building actions, and it also supports non-projective parsing with the introduction of two extra non-projective arc-building actions.",
        "The parser achieves a better tradeoff between parsing accuracy and efficiency by leveraging both graph-based models' training methods and transition-based models' linear time decoding strategies.",
        "The proposed parser is effective in achieving good performance on 27 treebanks, including the PTB and CTB benchmarks.",
        "The parser can be used to improve the efficiency of parsing tasks while maintaining accurate results."
    ],
    "6375": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The effectiveness of the joint model is improved by allowing lexicality and syntax to interact with each other in the joint search process.",
        "Providing emotional feedback and semantic feedback separately can be useful for improving empathetic conversation generation.",
        "Incorporating external knowledge, such as user profile or commonsense knowledge, into the model can be a potential extension of EmpDG.",
        "Modeling the interaction between semantic and emotional feedback is a promising future direction."
    ],
    "6376": [
        "Synthetic supervision improves multi-task models for formality-sensitive machine translation.",
        "Attaching style tags to both input and output sequences improves the ability of a single model to control formality.",
        "Synthetic supervision via Online Target Inference introduces more changes between formal and informal translations of the same input.",
        "The proposed approach outperforms a strong multi-task baseline by producing translations that better match desired formality levels while preserving the source meaning."
    ],
    "6378": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The approach to attenuating biases using a projection-based method works for the static GloVe embeddings.",
        "The simple approach of debiasing the first (non-contextual) layer alone can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Our global classifier can successfully recover from the errors made by the local classifier in some cases, as shown in Figure 2.",
        "The accuracy of the pairwise classifier is crucial for the performance of our overall framework.",
        "More informative features (e.g., textual entailment and semantic similarity) are needed to improve the pairwise classification performance."
    ],
    "6385": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "There is still plenty of room for improvement in the task of similar case matching in the legal domain."
    ],
    "6387": [
        "Our proposed method using machine learning algorithms can generate cohesive, interesting, and diverse game environments.",
        "Our machine learning approach can be used to aid humans in creating game worlds as well.",
        "Our method can generate new game elements, including locations, characters, and objects within containers, using machine learning algorithms.",
        "Our method can be used to create game worlds from crowd-sourced content."
    ],
    "6391": [
        "The Transformer encoder-decoder multi-head attention mechanism aligns each head to a specific source word, which can be used to generate diverse translations.",
        "Our algorithm outperforms previous work and achieves the most satisfactory results in terms of quality and diversity.",
        "The multiple trade-off setting can be adopted diversely depending on different needs.",
        "Applying our method as back-translation data augmentation and conversation response significantly improves performance, proving its effectiveness."
    ],
    "6392": [
        "textual evidence from similar image sequences contains rich information for visual storytelling, therefore it's capable of boosting storytelling performance.",
        "We propose a feasible two-step approach to extract textual evidence from a large corpus.",
        "We also design a two-channel encoder to incorporate textual and visual evidence into the Seq2Seq visual storytelling models and achieve state-of-the-art performance without heavy engineering."
    ],
    "6393": [
        "a significant performance gain can be achieved\" by selecting the right components in the encoder-decoder based video captioning framework.",
        "The use of Inception-ResNet-V2 as the visual encoder and temporal encoding for feature transformation results in the best performing combination.",
        "The use of fasttext word embeddings with a two-layer language model achieves the best performance.",
        "Exhaustive experiments were carried out to capture the contribution and effects of each component in the overall captioning performance.",
        "The four core components of the encoder-decoder based automatic video captioning framework are: CNN model, feature transformation, word embeddings, and language model.",
        "Various model hyperparameters such as depth, state size, and dropout in recurrent layers were also included in the empirical study."
    ],
    "6398": [
        "Our approach yields consistent overall improvements over strong baselines on three different tasks.",
        "The proposed architecture and training procedure lead the models to make more efficient use of both the source and context sequences.",
        "We have shown that our approach can improve the efficiency of the model by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks."
    ],
    "6401": [
        "Our approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "We propose a novel methodology that models the latent type of mentions and entities to improve biomedical entity linking.",
        "Our extensive set of experimental results shows that latent type modeling improves entity linking performance and outperforms state-of-the-art baseline models.",
        "The idea of latent type modeling can be useful in other text matching tasks and non-biomedical domains.",
        "This is the first work to propose the idea of latent type modeling and apply it to biomedical entity linking."
    ],
    "6402": [
        "The proposed framework MUDE achieves strong and robust performance with different types of noise presenting.",
        "The framework is able to capture both character and word-level dependencies to obtain effective word representations.",
        "Extensive experiments on datasets with various types of noise have demonstrated its superior performance over existing popular models.",
        "There are several meaningful future research directions, including extending MUDE to deal with sentences where word-level noise presents, improving the generality of MUDE, and utilizing MUDE to improve the robustness of other NLP systems.",
        "The primary focus of this work is on English, but it would be meaningful to experiment the proposed framework on other languages."
    ],
    "6403": [
        "We study the joint learning of answer selection and answer summary generation in CQA.",
        "The proposed joint learning method outperforms the state-of-the-art methods on both answer selection and summarization tasks.",
        "The experimental results show that the proposed joint learning method processes robust applicability and transferring ability for resource-poor CQA tasks.",
        "We propose a novel model to employ the question information to improve the summarization result, and meanwhile leverage the summaries to reduce noise in answers for a better performance on longsentence answer selection.",
        "In order to evaluate the answer generation task in CQA, we construct a new large-scale CQA dataset, WikiHowQA, which contains both labels for answer selection task and reference summaries for text summarization task."
    ],
    "6405": [
        "The proposed model achieves new state-of-the-art results on few-shot text classification tasks using dynamic memory induction networks (DMIN).",
        "The use of extra memories in the DMIN model improves the true-positive rate of the GENERAL aspect by 20% compared to the previous model without extra memories.",
        "The proposed model successfully identifies more varied GENERAL aspects from reviews, such as NOUN, VERB, ADJECTIVE, NUMBER, and PROBLEM, compared to the single GENERAL aspect provided by MATE.",
        "The aspect-aware memory in the proposed model leverages domain knowledge and reduces the parameters and computation cost of the model.",
        "The proposed method collects domain knowledge from external information rather than through human effort, making it easier to adapt to other product categories.",
        "The experimental results demonstrate the effectiveness of the proposed approach compared to state-of-the-art models on both aspect identification and opinion summarization tasks.",
        "Future works can design better measures for opinion selection and incorporate abstractive methods to enhance readability of the generated summaries."
    ],
    "6406": [
        "We presented a data programming paradigm that lets the user specify labeling functions.",
        "The unsupervised task of consolidating weak labels is inherently unstable and sensitive to parameter initialization and training epochs.",
        "Instead of depending on un-interpretable hyperparameters, we let the user guide the training with interpretable quality guesses.",
        "We carefully designed the potentials and the training process to give the user more interpretable control."
    ],
    "6407": [
        "Our approach effectively and efficiently improves translation performance over the TRANSFORMER model.",
        "Multi-head composition and multi-layer composition are complementary to each other.",
        "Our model makes the encoder of TRANSFORMER capture more syntactic and semantic properties of input sentences.",
        "Future work includes exploring more neuron interaction based approaches for representation composition other than the bilinear pooling.",
        "Applying our model to a variety of network architectures such as BERT (Devlin et al. 2019 ) and LISA (Strubell et al. 2018 )."
    ],
    "6408": [
        "Our approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The proposed Cosine Annealing Strategy combines two methods to improve domain adaptation.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The use of pseudo-labeled target-domain data and labeled source-domain data improves the domain adaption performance.",
        "Our proposed model outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "Addressing the challenge of extracting relation tuples from sentences is an important research direction.",
        "Our proposed models achieve significantly improved new state-of-the-art F1 scores on the New York Times (NYT) corpus.",
        "Exploring our proposed models for a document-level tuple extraction task is a potential future work."
    ],
    "6412": [
        "'We prepared high quality precomputed ELMo contextual embeddings for seven languages.'",
        "'The size of used training sets importantly affects the quality of produced embeddings, and therefore the existing publicly available ELMo embeddings for the processed languages can be improved for some downstream tasks.'",
        "'Our new EMBED-DIA ELMo embeddings show a big improvement on the analogy task, and a significant improvement on the NER task compared to the noncontextual fastText baseline.'",
        "'Our ELMo embeddings produce substantially better results compared to the existing ELMoForManyLangs embeddings on the analogy task and on the NER task.'",
        "'We plan to use the produced contextual embeddings on the problems of news media industry and build and evaluate more complex models, such as BERT (Devlin et al., 2019).'"
    ],
    "6413": [
        "The authors have introduced two novel methods for image captioning that exploit prior knowledge and help improve state-of-the-art models even when the data is limited.",
        "The first method uses association between visual and textual features to learn latent topics via an LDA topic prior, obtaining robust attention weights for each image region.",
        "The second method is an SAE regularizer that is pre-trained in an autoencoder framework to learn the structure of the captions and regulate the training of the image captioning model.",
        "Using these modules, the authors obtain consistent improvements on two investigate models, indicating the usefulness of their two modules as a strong prior.",
        "The authors plan to further investigate potential use of label space structure learning for other challenging vision tasks with limited data and to improve generalization in future work."
    ],
    "6415": [
        "We introduce a novel loss HAL for mitigating visual semantic hubs during training text-image matching models.",
        "The self-adaptive loss HAL leverages the inherit nature of Neighborhood Component Analysis (NCA) to identify information of hubs, from both a global and local perspective, giving considerations to robustness and hard sample mining at the same time.",
        "Our method beats two prevalent triplet-based objectives across different datasets and model architectures by large margins.",
        "Our methods have only experimented on the task of text-image matching, but there remains to be other cross-modal mapping tasks requiring obtaining a matching, e.g. content-based image retrieval, document retrieval, document semantic relevance, Bilingual Lexicon Induction, etc.. HAL can presumably be used in such settings as well."
    ],
    "6417": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Discourse-level factors are important when editors drop sentences as they simplify.",
        "Document characteristic features help in predicting whether a sentence will be deleted during simplification.",
        "To the best of our knowledge, this is the first datadriven study that focuses on analyzing discourse factors and predicting sentence deletion on a large English text simplification corpus."
    ],
    "6418": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "The proposed method can be used to generate both extractive and abstractive summaries.",
        "Our method emphasizes on in-depth analysis of the copy behavior in summarization.",
        "It exploits multiple strategies at training and decoding stages to generate diverse summary hypotheses.",
        "We show competitive results and demonstrate the effectiveness of the proposed method on exercising control over copying."
    ],
    "6421": [
        "We present ScienceExamCER, a densely annotated corpus of science exam questions for common entity recognition where nearly every word is annotated with fine-grained semantic classification labels drawn from a manuallyconstructed typology of 601 semantic classes.",
        "We demonstrate that BERT-NER, an off-the-shelf named entity recognition model, achieves 0.85 F1 on classifying these finegrained semantic classes on unseen text in a multi-label setting.",
        "The data and code are released with the goal of supporting downstream tasks in question answering that are able to make use of this dense semantic category annotation."
    ],
    "6423": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The brevity problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally normalized models be trained in a similarly inexpensive way?",
        "The proposed MADA framework enables dialog models to learn a more balanced state-to-action mapping, which leads to a more diverse set of responses that are both appropriate and diverse.",
        "The proposed DAMD model learns a more diverse state-to-action policy which not only achieves the state-of-the-art task success rate on the challenging Mul-tiWOZ dataset, but also generates a set of responses that are both appropriate and diverse."
    ],
    "6426": [
        "Taking into account the history and structure of the conversation helps in recovering the parent utterance.",
        "The proposed Masked Hierarchical Transformer model outperforms baseline models on three datasets.",
        "Exposure bias can be reduced by applying techniques such as schedule sampling or beam search.",
        "A better inference speed can be achieved by redesigning the model to build the presentation vector of an utterance once.",
        "The proposed model can be improved by replacing the fully connected layer with a siamese network-like component to decide if an utterance is the parent of the target utterance."
    ],
    "6427": [
        "We introduced JParaCrawl, a large web-based English-Japanese parallel corpus.",
        "Our experiments showed how JParaCrawl contains a broader range of domains and can be used for general purposes.",
        "We drastically reduced the training time by finetuning the JParaCrawl pre-trained NMT models and in doing so maintained or even boosted the performance.",
        "JParaCrawl also improved the performance of a specific domain when training a model with an existing corpus from an initial state.",
        "In future work, we will crawl more websites and make the dataset larger.",
        "We also plan to improve the bitext aligner and cleaner, especially for Japanese.",
        "We only focused on English-Japanese in the initial release, but we hope to eventually add more language pairs to/from Japanese."
    ],
    "6428": [
        "PNAT leads to significant improvement and moves closer to the performance gap between NAT and AT on machine translation tasks.",
        "The experimental results of paraphrase generation tasks show that the performance of PNAT can exceed that of autoregressive models, while also having a large improvement space.",
        "Position modeling has a significant impact on the effectiveness of PNAT, and future work can still enhance the performance of NAT models by strengthening position learning."
    ],
    "6432": [
        "Hybrid-EL-CMP outperforms other language understanding approaches for improving language understanding in human-machine social conversations, as shown by evaluating on dialog act prediction and semantic role labeling.",
        "The proposed approach utilizes both original utterances with ellipsis and their automatically completed counterparts to improve language understanding.",
        "The framework can be generalized to other dialog understanding tasks such as syntactic and semantic parsing, and will be evaluated on these tasks in the future.",
        "The proposed approach is effective in improving language understanding for human-machine social conversations, as demonstrated by evaluating on dialog act prediction and semantic role labeling."
    ],
    "6433": [
        "We suggest using weakly-labelled data created from Wikipedia to train a classifier for detecting important events in news articles.",
        "Our approach can be further improved by finding additional sources for weak labels, such as exploiting information from relevant knowledge bases.",
        "The potential coverage of relevant events can be increased by retrieving articles that do not necessarily include the name of the considered company in their title.",
        "Extending our framework to pinpoint noteworthy events for a particular company mentioned in articles that are not focused on that company is a natural direction for future research.",
        "Such an extension will require adapting the weak labelled data and the corresponding classifiers to cope with an environment in which sentences are not necessarily relevant to the company."
    ],
    "6434": [
        "The proposed Half-Sibling Regression (HSR) algorithm reduces gender bias in word vector relations and associated with gender direction.",
        "HSR enhances lexical and sentence-level quality of word vectors.",
        "HSR diminishes gender bias in downstream tasks such as coreference resolution.",
        "HSR consistently improves performance over existing post-processing and word-vector-learning methods.",
        "HSR can be adopted as a post-processing method for contextualized word embeddings.",
        "HSR can be incorporated into the training process of contextualized models.",
        "HSR can be generalized to identify and mitigate other social biases such as racism in word vectors."
    ],
    "6435": [
        "Ptakop\u011bt allows users to produce messages in a language they do not speak and still gain some level of confidence in the resulting translation.",
        "The majority of inputs were edited and while initial inputs and the final inputs were quite similar in the source language, the translations of them differed more.",
        "The average self-reported confidence in the translations was 2.10 on a 1-5 (best-worst) scale, and the tool was found more useful than standard web interfaces to MT.",
        "The majority of inputs were edited and while initial inputs and the final inputs were quite similar in the source language, the translations of them differed more.",
        "Overall understandability of the translations improved from 3.9 to 2.71 on the 1-5 (best-worst) scale.",
        "In future, the experiment will be refined and consider other features of the outbound translation user interface."
    ],
    "6437": [
        "Using knowledge distillation can improve the performance of student models in document retrieval tasks for factual verification.",
        "Adding the teacher model posteriors to the student training can achieve significant improvements in ranking metrics without sacrificing running time advantages.",
        "Applying this work to a larger set of input documents can replace the DrQA retriever with the student model.",
        "Using knowledge distillation can improve the performance of student models in document retrieval tasks for factual verification.",
        "Adding the teacher model posteriors to the student training can achieve significant improvements in ranking metrics without sacrificing running time advantages.",
        "Applying this work to a larger set of input documents can replace the DrQA retriever with the student model."
    ],
    "6439": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, and only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "The limitations of prior approaches were addressed by CAWA, which models each sentence as a distribution over classes and leverages a simple average pooling layer to constrain the neighboring sentences to have similar class distribution.",
        "The proposed loss function constraints the attention to only focus on the classes present in the document."
    ],
    "6441": [
        "Our approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "We propose a novel method for inductive unsupervised domain adaption in few-shot classification.",
        "Our approach uses a combination of Clustering Promotion Mechanism, Similarity Entropy Minimization, and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "We use a Cosine Annealing Strategy to combine the two methods.",
        "Our approach utilizes pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "Experimental results demonstrate the superiority of our approach in terms of both relevance and diversity, as validated by automatic and human evaluations.",
        "Our model is a language model-based solution that differs from traditional SEQ2SEQ paradigms for handling short-text conversation (STC).",
        "We tailor-make a training strategy to adapt the language model for the STC task.",
        "Our approach incorporates relevance clues from the query and topics inferred from the references into the generation.",
        "We explore the usage of top-k sampling for the STC task to further improve response diversity."
    ],
    "6442": [
        "We propose a unified framework to effectively integrate discrete relation constraints with neural networks for relation extraction.",
        "Our approach can help the base NRE models to effectively learn from the discrete relation constraints, and outperform popular NRE models as well as their ILP enhanced versions.",
        "Learning with the constraints can better utilize the constraints from a different perspective compared to the ILP post-processing method.",
        "Our study reveals that learning with the constraints can better utilize the constraints from a different perspective compared to the ILP post-processing method."
    ],
    "6443": [
        "The introduction of phrase alignment makes it possible to decompose the translation process of arbitrary NMT models into interpretable steps.",
        "The proposed method achieves significant better performance on both lexically and structurally constrained translation tasks.",
        "It is convenient to use our approach to impose lexical and structural constraints thanks to the availability of phrase alignment.",
        "The proposed latent variable model for neural machine translation treats phrase alignment as an unobserved latent variable."
    ],
    "6445": [
        "The current state-of-the-art in few-shot classification is not sufficient for real-world applications, as it does not take into account the limitations of language models and their lack of understanding of basic physical properties.",
        "The proposed approach of using a representation extractor with Clustering Promotion Mechanism and Cosine Annealing Strategy can achieve new state-of-the-art performance on FewRel 2.0 dataset.",
        "The use of pseudo labels to train the few-shot classifier can improve the domain adaption performance, but reducing the noise of pseudo labels is a future research direction.",
        "The current pretrained models used in NLP are not able to capture basic physical properties of the world and are limited in their ability to understand knowledge that is not explicitly stated in language.",
        "The proposed PIQA benchmark can provide insight and a benchmark for progress towards language representations that capture knowledge traditionally only seen or experienced, enabling the construction of language models useful beyond the NLP community."
    ],
    "6449": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The proposed framework, SeG, incorporates an entity-aware embedding module and a self-attention enhanced selective gate mechanism to integrate task-specific entity information into word embedding.",
        "The SeG framework consistently delivers a new benchmark in state-of-the-art performance in terms of all P@N and precision-recall AUC.",
        "The proposed modules are effective in handling wrongly labeled data, especially for the case with only one sentence in the most of bags.",
        "Incorporating an external knowledge base into the framework may further boost the prediction quality by overcoming the problems with a lack of background information."
    ],
    "6450": [
        "There is still a quite large gap between the current state-of-the-art representation models and robust human-level commonsense reasoning.",
        "Large-scale pre-trained contextualized representation has a certain degree of commonsense knowledge, but there is still a quite large gap between the current state-of-the-art representation models and robust human-level commonsense reasoning.",
        "More breakthrough in modeling may be required to achieve robust human-level commonsense reasoning.",
        "The current state-of-the-art representation models have a certain degree of commonsense knowledge, but there is still room for improvement in terms of achieving robust human-level commonsense reasoning.",
        "Releasing our test sets, named CATs, publicly can help to advance the field and provide more opportunities for researchers to explore and improve upon the current state-of-the-art representation models."
    ],
    "6454": [
        "The authors present a new and challenging dataset for legal question answering (LQA) called JEC-QA, which is the largest dataset in LQA.",
        "Existing state-of-the-art models cannot perform well on JEC-QA, indicating a need for improvement in the reasoning ability of reading comprehension and QA models.",
        "The authors hope that their JEC-QA will benefit researchers in improving the reasoning ability of reading comprehension and QA models, and making advances for legal question answering.",
        "In the future, the authors plan to explore how to improve the reasoning ability of question answering models and integrate legal knowledge into question answering, which are necessary for answering questions in JEC-QA."
    ],
    "6456": [
        "The two new Jejueo datasets (JIT and JSS) are developed to improve the performance of Jejueo-Korean machine translation and Jejueo speech synthesis.",
        "The JIT dataset consists of 170k+ bilingual sentences paired with their Korean translations, while the JSS dataset contains 10k high-quality audio files recorded by a Jejueo speaker and a transcript file.",
        "Neural machine translation models using 4k shared BPE vocabulary and a neural speech synthesis model based on Hangul Jamo tokens showed the best performance in the experiments.",
        "The datasets are expected to attract attention from both language and machine learning communities."
    ],
    "6458": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The success of the WMT 2016 task with respect to the challenges of the individual subtasks and the design of the submitted systems.",
        "The majority/random baselines, which proved quite easy to beat.",
        "None of the participating systems was able to outperform the baseline in the DiscoMT 2015 task.",
        "The macro-averaged recall and accuracy of the primary systems with respect to the official metric and unofficial, supplementary metric."
    ],
    "6463": [
        "Our approach improves factual knowledge retrieval accuracy by 8% compared to manually designed prompts.",
        "LMs are more knowledgeable than previously indicated, but sensitive to how we query them.",
        "There is potential for more robust LMs that can be queried in different ways but still return similar results.",
        "Methods to incorporate factual knowledge into LMs could lead to further improvements.",
        "Our released prompts and archive (LPAQA) could be used to optimize methods for querying LMs for knowledge."
    ],
    "6464": [
        "We have presented experiments with novel automatic metrics for machine translation evaluation that take discourse structure into account.",
        "In particular, we used RST-style discourse parse trees, which we compared using convolution kernels.",
        "We further combined these kernels with metrics from ASIYA, also tuning the weights.",
        "The resulting DISCOTK party tuned metric was the bestperforming at the segment-and system-level at the WMT14 metrics task.",
        "In an internal evaluation on the WMT12 and WMT13 metrics datasets, this tuned combination showed correlation with human judgments that outperforms the best systems that participated in these shared tasks.",
        "The discourse-only metric ranked near the top at the system-level for WMT12 and WMT13; however, it is weak at the segment-level since it is sensitive to parsing errors, and most sentences have very little internal discourse structure.",
        "In the future, we plan to work on an integrated representation of syntactic, semantic and discourse-based tree structures, which would allow us to design evaluation metrics based on more fine-grained features, and would also allow us to train such metrics using kernel methods.",
        "We want to make use of discourse parse information beyond the sentence level."
    ],
    "6465": [
        "Our approach makes full use of the results of the sentence selector, leading to consistent improvements over baseline systems.",
        "We apply reinforcement learning to build a sentence selector that selects positive and unlabeled sentences from noisy data.",
        "We add relation embedding in the state representation to improve the performance of the bag-level relation classifier.",
        "Using unlabeled instances for training models is helpful for relation extraction.",
        "Adding relation embedding in the state representation improves the performance of the bag-level relation classifier.",
        "Our proposed system consistently outperforms baseline systems."
    ],
    "6467": [
        "We have developed a large corpus of Sindhi text, containing over 61 million tokens and 908,456 unique words.",
        "We have constructed a list of Sindhi stop words by identifying high-frequency and low-importance words using Sindhi linguistic expertise.",
        "We have generated unsupervised Sindhi word embeddings using state-of-the-art CBoW, SG, and GloVe algorithms.",
        "Our proposed Sindhi word embeddings have captured high semantic relatedness in nearest neighboring words, word pair relationship, country, and capital and WordSim353.",
        "The SG model yields the best performance among CBoW, GloVe, and SG models.",
        "Our proposed Sindhi word embeddings have surpassed SdfastText in the intrinsic evaluation matrix.",
        "We will further investigate the extrinsic performance of proposed word embeddings on the Sindhi text classification task in the future."
    ],
    "6472": [
        "Our approach achieves competitive performances compared with state-of-the-art methods on both normal CWS and CWS with CSC.",
        "We re-formalize the CWS as a sequence-to-sequence translation problem and apply an attention-based encoder-decoder model.",
        "We propose an LCS-based post-editing algorithm to deal with potential translating errors.",
        "Our approach lets the model jointly learn CWS and CSC.",
        "In the future, we plan to apply other efficient sequence-to-sequence models for CWS and study an end-to-end framework for multiple natural language preprocessing tasks."
    ],
    "6473": [
        "We propose WASSP, a framework to merge weak and active learning for semantic parsing.",
        "Our framework can greatly improve the performance of a weakly-supervised semantic parser with a small fraction of examples queried.",
        "SELECTED-1 How many visitors in 2007 were there? z f (filter eq all rows '2007' 'Year') (count v0)",
        "TESTED-1 How many CIL competitions were there? \u1e91 (filter eq all rows 'CIL' 'Competition') (hop v0 'established')",
        "SELECTED-2 How many points did the driver who won $127,541 driving car #31 get? z f (filter eq all rows '$127,541' 'Winning') (filter eq v0 '#31' 'Car') (hop v1 'Pts')",
        "TESTED-2 Which driver won $40,000 in the NW Cup? \u1e91 (filter eq all rows 'NW Cup' 'Series') (hop v0 'driver')",
        "SELECTED-3 What position was WIC in a year later than 2008? z f (filter eq all rows 'WIC' 'Competition') (filter greater v0 '2008' 'Year') (hop v1 'Position')",
        "TESTED-3 What Chassis has a year later than 1989? \u1e91 (filter less all rows '1989' 'Year') (hop v0 'Chassis')"
    ],
    "6475": [
        "The proposed iterative polishing framework for Chinese poetry generation using a pre-trained BERT encoder and a transformer decoder, along with a multifunctional QA-MLM, can improve poem quality in terms of semantics, syntactics, and literary.",
        "The trained QA-MLM is able to aware of the poem quality and locate improper characters, and can predict better ones to replace the improper characters.",
        "The QA-MLM will automatically terminate the iterative polishing process when the polished draft is classified as qualified.",
        "The proposed approach is effective in Chinese poetry generation, and can automatically modify preliminary poems to elegant ones while keeping their original intents.",
        "The new text refinement approach using the QA-MLM polishing framework can be extended to other natural language generation areas."
    ],
    "6476": [
        "The proposed method, CONAN, demonstrates strong performance in rare and low prevalence disease detection using a novel pattern augmentation approach.",
        "CONAN uses negative sample embeddings as seeds to generate complementary patterns with a GAN, allowing the generator to fool the discriminator and the disease detector to distinguish positive and negative samples.",
        "The discriminator can be used for detecting positive patients after training, and the method can be extended to other application domains for classification problems with imbalanced data.",
        "Future directions for the CONAN method include incorporating data from similar diseases to guide the generation process, using other data sources such as doctor notes for better embedding, and considering time intervals between visits for modeling the progression of rare disease."
    ],
    "6477": [
        "We have introduced a two-stage approach for Bengali question classification, which combines deep learning and gradient descent.",
        "Our approach introduces a way of creating new representative theoretical samples for each coarse class, which helps maintain class balance in the training set.",
        "Experiments have shown the effectiveness of our approach.",
        "Researchers working on building Bengali question answering systems can follow this work as part of their question classification module.",
        "Our finer-class classifiers are expected to show better performance with more training data per finer class.",
        "We leave this as part of future work."
    ],
    "6481": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our proposed method builds upon top-down attention and generates captions and grounds word in these captions to entities in images.",
        "We leverage this along with structure found from the captions by the Stanford Scene Graph Parser to allow for the classification of relations between pairs of objects without having ground truth information for the task.",
        "Our proposed approaches thus allow weakly-supervised relation detection.",
        "One possible line of work involves removing the requirement of ground truth bounding boxes altogether by leveraging a recent line of work that does weakly-supervised object detection.",
        "An orthogonal line of future work might involve using a Visual Question Answering (VQA) task, either on its own replacing the captioning task, or in conjunction with the captioning task with a multi-task learning objective."
    ],
    "6483": [
        "The proposed sentence visualization and fusion framework improves pre-trained language inference and reading comprehension models.",
        "The approach is flexible and can be easily integrated into pre-trained language models.",
        "The proposed approach achieves improvements on public corpora such as SNLI, SICK, and SemEval 2018 Task 11.",
        "Future directions and improvements could include using stronger image retrieval networks and generating larger image bases.",
        "The proposed framework works well when images are correctly retrieved."
    ],
    "6485": [
        "Our model introduces future information into the decoding stage to promote holistic dialogue semantics.",
        "We devise a maximum entropy regularizer to penalize the overestimation of high frequency words.",
        "Our joint learning framework can be generalized to any end-to-end model.",
        "Extensive experiments show that our model produces more informative and relevant responses than several competitive baselines.",
        "Our approach does not depend on any external information and variables, and it is beneficial to capture holistic dialogue semantics."
    ],
    "6486": [
        "The performance of AntNet is prone to errors, especially when the answers contain implicit preference information.",
        "Answers containing implicit information, such as negative or positive words, are likely to be error-judged.",
        "The multi-hop attention mechanism used in this study is effective in facilitating answer understanding.",
        "The relationship between training data and model performances is an important factor in determining the success of the model.",
        "Further work will focus on extracting additional hints for users' choices to improve the performance of AntNet."
    ],
    "6489": [
        "Our approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The proposed Cosine Annealing Strategy combines two methods to improve domain adaptation.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The use of pseudo-labeled target-domain data and labeled source-domain data improves domain adaptation performance.",
        "Our approach outperforms benchmark models on three public datasets of machine reading comprehension.",
        "The combination of a chunking policy network and recurrent mechanism enables a model to learn to chunk lengthy documents in a more flexible way.",
        "Multi-Scale Self-Attention and Multi-Scale Transformer combines prior knowledge of multi-scale and the self-attention mechanism to extract rich and robust features from different scales.",
        "Our proposal outperforms the vanilla Transformer consistently and achieves comparable results with state-of-the-art models on three real tasks (21 datasets)."
    ],
    "6491": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "GANCoder can achieve comparable accuracy with the state-of-the art code generation model, and the stability is better.",
        "The method proposed in this paper can only realize the conversion between single-line natural language description and single-line code.",
        "Future work will study how to convert long natural language description text and multi-line code."
    ],
    "6496": [
        "We tackled the problem of conclusion-supplement answer generation for non-factoid questions, an important task in NLP.",
        "Our architecture was consistently superior to conventional encoder-decoders in this task.",
        "The ensemble network also assesses the closeness of the encoder input sequence to the output of each decoder and the combined output sequences of both decoders.",
        "Our method, NAGM, can be generalized to generate much longer descriptions other than conclusionsupplement answers.",
        "NAGM generates second line (like a conclusion) and third line (like a supplement).",
        "The first line is input by a human user to NAGM as a question, and NAGM generates the fourth line (like a conclusion) and fifth line (like a supplement)."
    ],
    "6498": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Existing models neither consider the whole question semantics nor exploit the answer position-aware features well.",
        "Our work improves existing models significantly and outperforms the SOTA models on SQuAD and MARCO datasets."
    ],
    "6501": [
        "Our proposed method, SNERL, can simultaneously predict entity linking and entity relation decisions without any mention-level supervision for entities or relations.",
        "SNERL relies solely on weak and distant supervision at the document-level, readily available in many biomedical knowledge bases.",
        "The proposed model performs favorably as compared to a state-of-the-art pipeline approach to relation extraction by avoiding cascading errors.",
        "Our method requires less expensive annotation, opening possibilities for knowledge extraction in low-resource and expensive to annotate domains."
    ],
    "6502": [
        "The Mixed-Level Feed-Forward Network (MLFFN) successfully learns word ratings from document-level ratings, outperforming previous methods for lexicon creation.",
        "Signed Spectral Clustering was applied to the resulting lexical scores to gain insights into the language of empathy.",
        "The SHAP (SHapley Additive exPlanations) calculations of feature importance for CNNs, RNNs, or Transformers can be used to improve lexicon quality over simple neural nets.",
        "The MLFFN model improves upon previous methods for creating lexica, as it is able to learn word ratings from document-level ratings.",
        "The use of the SHAP calculations leads to better feature importance and improved lexicon quality."
    ],
    "6504": [
        "We propose a neural headline editing model that aims to generate more attractive headlines with less dull and generic patterns and repetitions.",
        "To train such model, we construct the professional headline editing dataset (PHED) with the original headlines and the edited headlines collected from professional editors.",
        "Experimental results show that the adapted model dramatically decreases the repetitions and improves on other metrics such as BLEU-4 and ROUGE-L.",
        "We design a self importance-aware (SIA) objective function to be aware of the importance difference of data points during training.",
        "The results from SIA show that setting the correct \u03b1 and \u03b2 can further improve the performance over the model with pre-training and adaptation.",
        "For future work, we would like to apply SIA to other language generation tasks to analyze the effects."
    ],
    "6506": [
        "The proposed method has a few limitations, including the need for additional data to construct a semantic space and the potential for variance to overestimate uncertainty for certain concepts.",
        "Existing uncertainty sampling strategies in active learning are no better than random sampling for sequence generation tasks with multiple correct answers.",
        "The proposed method uses an embedding-based uncertainty estimation that moves away from probabilistic uncertainty estimation.",
        "The method outperforms existing sampling strategies in terms of cost efficiency and novelty of concepts sampled.",
        "The method is able to overcome the paraphrastic nature of language by using embedding-based uncertainty estimation.",
        "The proposed method has the potential to be more cost-efficient than existing baselines, despite having some limitations."
    ],
    "6508": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our approaches significantly outperform several strong baseline systems.",
        "The language invariance characteristics are manifested in both sentence level and word level neural representations."
    ],
    "6510": [
        "We propose a simple and effective recipe for building multilingual NER systems with BERT.",
        "Our approach allows for training a system that can perform inference on multiple languages, and it outperforms the same model trained only on one language at a time.",
        "The resulting model yields SotA results on CoNLL Spanish and Dutch, and on OntoNotes Chinese and Arabic datasets.",
        "The English trained model yields SotA results for 0-shot languages for Spanish, Dutch, and German NER, improving it by a range of 2.4F to 17.8F.",
        "The runtime signature of the model is the same as the models built on single languages, significantly simplifying its lifecycle maintenance.",
        "Pretrained multilingual BERT embeddings can be used effectively in an unsupervised manner while harnessing annotated data outside the target language.",
        "It is easy to extend this approach through either partial gradient updates or the use of Cloze tasks and other unsupervised joint tasks to gain further improvements."
    ],
    "6512": [
        "Our unique data preprocessing methods and pretraining social media data can improve the performance of LMs in downstream tasks.",
        "Pretraining data has a significant impact on LM performance, as shown by the improvement of all models from pretraining data of the same domain.",
        "LM pretraining on a noisy corpus demonstrates the ability of the models to learn in spite of the quality of the data.",
        "BERT is the best performing model with respect to classification accuracy and can achieve state-of-the-art results on both benchmarking downstream tasks.",
        "AWD-LSTM with ULMFiT is recommended for speed and ease of training, as it has a fast pretraining and fine-tuning process, while the results are still on par with transformer-based models.",
        "OpenAI GPT shows promising results with acceptable pretraining speed but is overshadowed by other models in both aspects.",
        "ELMo shows significant improvements when compared with the baseline biLSTM, but it requires designing a powerful task-specific model to achieve good performance."
    ],
    "6514": [
        "The two variants of AMR-to-text generation models use a recurrent graph encoder to learn concept-level states from AMR structure.",
        "Utilizing parser state was expected to model word-order traversal of the input AMR and explicitly align English spans to AMR concepts using hard attention.",
        "The models were unable to attain competitive performance due to train-test divergence on parser sequence prediction."
    ],
    "6515": [
        "The proposed model achieves high accuracy on four other languages with very different roots, illustrating its robustness.",
        "The algorithm has a high success rate and average runtime in minutes on the four languages, demonstrating its effectiveness.",
        "An ablation study was conducted to illustrate the stability of the algorithm on the four languages.",
        "The full system achieves best accuracy, average accuracy, and success rate on the four languages, demonstrating its overall performance."
    ],
    "6517": [
        "The proposed resource will be useful for building language systems in an endangered language, Mapudungun.",
        "The size of the resource (142 hours, over 260k total sentences) has the potential to alleviate many issues faced when building language technologies for Mapudungun.",
        "The resource could be used for ethnographic and anthropological research into the Mapuche culture.",
        "The resource has the potential to contribute to intercultural bilingual education, preservation activities, and the general advancement of the Mapudungun-speaking community."
    ],
    "6518": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The proposed APT framework achieves prominent improvements by fully acquiring the knowledge from pre-trained models to NMT."
    ],
    "6523": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The proposed ReCoSa model can be useful for improving the quality of multiturn dialogue generation.",
        "The use of self-attention mechanism can effectively capture the long distant dependency relations.",
        "The methodology of using proper detection methods, such as self-attention, to improve the quality of multiturn dialogue generation.",
        "The hope is that this work has provided a new way of looking at dialectology and linguistic affiliation that can generalize to new linguistic scenarios.",
        "The hope is that methodologies of this sort will help to shed light on vexing problems relating to the interplay of regularly conditioned sound change and language contact."
    ],
    "6527": [
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension that enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Our base neural encoder-decoder model converts a query graph to a natural language question, and we propose two extensions that explicitly consider instance-level connections between simple and complex questions.",
        "The proposed extensions are empirically shown to be more effective than straightforward data augmentation.",
        "The detailed process within this TreeLSTMCell involves using zero vectors as the hidden states of nonexistent \"children\" of terminal nodes, and a dummy relation encoding vector e(r)dummy is used for the root nodes."
    ],
    "6530": [
        "The WASABI dataset of songs includes lyrics annotations for structure segmentation, topic, explicitness, summary, and emotions.",
        "Some annotation layers are provided for all 1.73M songs in the WASABI corpus, while others apply to subsets due to various constraints.",
        "The creation of the resource is ongoing, and future work will include an improved emotional description.",
        "The authors of (Atherton and Kaneshiro, 2016) studied how song writers influence each other, and the goal is to learn a model that detects the border between heavy influence and plagiarism."
    ],
    "6535": [
        "We have explored the applicability of machine translation evaluation methods to answer ranking in community Question Answering.",
        "Our evaluation results on benchmark datasets have shown state-of-the-art performance, with sizeable contribution from both the MTE features and from the network architecture.",
        "This is an interesting and encouraging result, as given the difference in the tasks, it was not a-priori clear that an MTE approach would work well for cQA.",
        "In future work, we plan to incorporate other similarity measures and better task-specific features into the model.",
        "We further want to explore the application of this architecture to other semantic similarity problems such as question-question similarity, and textual entailment."
    ],
    "6536": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks",
        "our semantic mask method for E2E speech recognition is able to train a model to better consider the whole audio context for the disambiguation",
        "we elaborate a new architecture for E2E model, achieving state-of-the-art performance on the Librispeech test set in the scope of E2E models"
    ],
    "6537": [
        "The proposed unified model can associate distributed learning with symbolic rules, enabling the integration of deep neural networks (DNNs) and logical reasoning.",
        "The integrated framework can pass information from the neural model to the logic module and compute a discrepancy loss between these two components, which is minimized to update the whole network.",
        "The marriage between DNNs and the logic system can regularize deep learning in the form of knowledge distillation.",
        "The proposed method can adapt to specific data domains by updating the rule weights in the logic system.",
        "Experimental results demonstrate the advantage of combining DNNs and logical reasoning for joint inference."
    ],
    "6538": [
        "The proposed object-word alignment weak supervision signal is effective in improving the performance of vision-language tasks, as shown by achieving SOTA-level accuracies on two tasks (VQA and Language-driven Comparison of Images).",
        "The attention distributions of the model show that attention units are higher for object-word pairs referring to the same entity, and that the proposed object-word alignment does not emerge naturally without supervision.",
        "The novel object-word alignment weak supervision signal has the potential to be applied to other vision-language tasks, including image retrieval, and may lead to improved performance.",
        "The carefully designed soft alignment signal takes into account both spatial and semantic alignment between the words and the detected visual objects, which is beneficial for the task performance."
    ],
    "6539": [
        "We explore how different parts of our framework can be modified to improve its performance, or how it can be extended for further generalization.",
        "Variations of the feature sets from the perspective of both the pairwise features and the embeddings can be explored.",
        "The role of the network architecture and of the cost function used for learning can be analyzed."
    ],
    "6544": [
        "Leveraging heterogeneous data types in EHR can be beneficial to learning embeddings that holistically reflect all semantic properties among different medical concepts.",
        "Our proposed approach, Med2Meta, learns feature-specific embeddings using a graph auto-encoder by considering each data type as a separate view.",
        "It then models integration of embeddings as a meta-embedding learning problem so that latent similarities and natural clusters between medical concepts are captured in the metaembedding space through joint reconstruction across all views.",
        "Empirical results on three different tasks and visualization with t-sne plots establish the superior performance and efficacy of Med2Meta over baselines."
    ],
    "6550": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our model outperforms previous state-of-the-art models when evaluated on the New York Times (NYT) corpus, achieving significantly higher F1 scores."
    ],
    "6553": [
        "Our work is inspired by the observation of the weak generalization of existing neural dialogue evaluators.",
        "We propose to alleviate that issue via selectively adapting the evaluator, to jointly fit dialogue systems have been and to be evaluated.",
        "Experimental results show that our continual evaluators are able to adapt to achieve comparable performance with evaluator reconstruction, while our continual evaluators require significantly fewer annotations.",
        "Our continual evaluators eliminate the trouble of maintaining an increasing size of evaluators or annotations."
    ],
    "6554": [
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We explore the Medication Regimen (MR) extraction task of extracting dosage and frequency for the medications mentioned in a doctor-patient conversation transcript.",
        "Our best model can correctly extract the dosage for 71.75% (interpretation of ROUGE-1 score) and frequency for 73.58% (on qualitative evaluation) of the medications discussed in the transcripts generated using Google Speech-To-Text.",
        "Using ASR transcripts in training to improve the performance and extracting other important medical information can be interesting lines of future work."
    ],
    "6560": [
        "FlauBERT is a pre-trained language model for French that achieved state-of-the-art results on a number of French NLP tasks.",
        "FlauBERT surpassed multilingual/cross-lingual models despite being trained on almost twice as fewer text data.",
        "The pipeline used to train FlauBERT is entirely reproducible, and we provide a general benchmark for evaluating French Wikimedia database.",
        "FlauBERT is competitive with CamemBERT, another pre-trained language model for French, despite being trained on almost twice as fewer text data.",
        "The content of the corpus used to train FlauBERT is built collaboratively by volunteers around the world.",
        "The corpus includes Wikipedia, Wikisource, Wikinews, Wiktionary, Wikiversity, Wikibooks, and Wikiquote."
    ],
    "6561": [
        "The proposed method outperforms other KGC models on metrics such as Mean Rank and MRR. (Claim 2)",
        "ConMask is able to capture the correct relationship between entities, even when the name of the entity does not appear in the description. (Claim 4)",
        "The model uses a collection of base words to represent words from different domains in one semantic space to reduce domain divergence. (Claim 1)",
        "The proposed training policy is not limited to neural machine translation and can be applied to other neural machine learning models. (Claim 3)",
        "The method finds parameters that can be easily adjusted to new domains with only a limited number of training examples. (Claim 2)"
    ],
    "6563": [
        "The proposed approach achieves state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The Cosine Annealing Strategy is used to combine the two methods.",
        "The approach utilizes pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "The proposed method improves the domain adaptation performance by reducing the noise of pseudo labels.",
        "The approach is effective in compressing BERT for various downstream tasks.",
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The approach introduces two new shaping losses to encourage the emergence of communication in decentralized learning.",
        "The approach shows promise in improving the performance of decentralized agents in large-scale multi-agent domains."
    ],
    "6565": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The latent representations induced in this scheme are more abstract and structured, similar to the structural simplicity found in languages with many speakers.",
        "These preliminary results could open avenues to potential synergies between linguistics and representation learning."
    ],
    "6574": [
        "The authors introduce a new dataset containing tweets related to the #MeToo movement, which can be used for various multi-label and multi-aspect classification experiments.",
        "The dataset is annotated for five different linguistic aspects: relevance, stance, hate speech, sarcasm, and dialogue acts.",
        "This is the first dataset to provide annotations across so many different dimensions, allowing researchers to examine the social impact of this exercise and the ethics of the individuals concerned with the dataset.",
        "The dataset can be used for various computational challenges, such as building models to capture all the different linguistic aspects annotated.",
        "The authors expect that the data could be useful for socio and psycholinguists in understanding the language used by victims when disclosing their experiences of abuse.",
        "The dataset provides a chance to examine the language used to express hate in the context of sexual abuse.",
        "The authors plan to propose challenge tasks around this data in the future, where participants will have to build computational models to capture all the different linguistic aspects annotated."
    ],
    "6577": [
        "Our approach achieves new state-of-the-art performance on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism, which uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "Our approach outperforms sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Our model is capable of reducing the noise brought by relationship modeling between incoherent sentences, but also fully leveraging entity information for paragraph encoding.",
        "Our approach outperforms the state-of-the-art and other baselines on several benchmark datasets."
    ],
    "6578": [
        "Our proposed iterative dual domain adaptation framework for NMT can fully exploit the mutual complementarity between in-domain and out-domain corpora for translation knowledge transfer.",
        "Experimental results on two language pairs demonstrate the effectiveness of our framework.",
        "We plan to extend our framework to multi-domain NMT in the future.",
        "Our framework can be used to refine monolingual sentences of different domains.",
        "We will apply our framework to other translation models to verify its generality."
    ],
    "6582": [
        "This work proposes a novel dialogue generation model called TransDG, which incorporates commonsense knowledge via transferring the abilities of utterance representation and knowledge selection from KBQA task.",
        "The proposed model includes a response guiding attention mechanism to enhance input post understanding in the encoder and refine knowledge selection by multi-step decoding to generate more appropriate and informative responses.",
        "The extensive experiments demonstrate the effectiveness of the proposed model.",
        "The model is able to generate more appropriate and informative responses by incorporating commonsense knowledge and using a response guiding attention mechanism.",
        "The proposed model outperforms other state-of-the-art dialogue generation models in terms of response quality and appropriateness."
    ],
    "6586": [
        "This paper proposes a study on the problem of predicting names of functions in stripped binary code.",
        "The proposed approach uses state-of-the-art solutions in machine translation and finds a good carryover for the problem.",
        "A large public dataset of functions is created, which can be used to further the research on the topic.",
        "The results are encouraging and pave the way for further studies.",
        "Many improvements are possible, including finding faithful metrics that capture the performances perceived by humans.",
        "One direction for future work is to create a dataset with multiple reference names for each function.",
        "Another direction is to perform an extensive investigation using humans to evaluate the predicted names and source code of the functions.",
        "Metrics based on NLP, such as those proposed by BERTScore [24], could be used to evaluate the performance of the approach."
    ],
    "6587": [
        "The current methods for aspect-based sentiment analysis (ABSA) do not adequately address the task, particularly for the Chinese language.",
        "Existing studies have treated the ATE and APC subtasks as independent tasks, neglecting the potential benefits of multi-task learning.",
        "The proposed LCF-ATEPC model leverages both MHSA and LCF mechanisms to improve aspect extraction and sentiment analysis.",
        "The model is applicable to both Chinese and English languages, and can be used for classic English review sentiment analysis tasks.",
        "The LCF-ATEPC model achieves state-of-the-art performance on ATE and APC tasks compared to all models based on basic BERT.",
        "The proposed model can automatically extract aspects from reviews and infer aspects' polarity."
    ],
    "6588": [
        "Our proposed approach, M 2 Transformer, achieves a new state of the art on COCO for image captioning tasks, ranking first in the online leaderboard.",
        "The novel region encoding approach exploits a priori knowledge through memory vectors and meshed connectivity between encoding and decoding modules, which is unprecedented for other fully attentive architectures.",
        "Our model's performance is validated through ablation studies and its ability to describe novel objects."
    ],
    "6592": [
        "The proposed Dual-channel Multi-hop Reasoning Model (DMRM) for visual dialog enhances the semantic representation of questions by capturing information from the dialog history and the image.",
        "The dual-channel multi-hop reasoning process provides a more fine-grained understanding of the question by utilizing textual information and visual context simultaneously, thus boosting answer generation performance.",
        "Experiments conducted on the VisDial v0.9 and v1.0 datasets certify the effectiveness of the proposed method."
    ],
    "6595": [
        "We propose multi-stage adaptive latent action learning (MALA) for better conditioned response generation.",
        "We develop a novel dialogue state transition measurement for learning semantic latent actions.",
        "We demonstrate how to effectively generalize semantic latent actions to the domains having no state annotations.",
        "The experimental results confirm that MALA achieves better task completion and language quality compared with the state-of-the-art under both in-domain and cross-domain settings.",
        "For future work, we will explore the potential of semantic action learning for zero-state annotations application."
    ],
    "6599": [
        "Our approach can localize a target object in a 3D point cloud using natural language descriptions.",
        "We collect a new dataset, ScanRefer, with 51,583 unique descriptions for 11,046 objects from 800 ScanNet scenes.",
        "Our end-to-end method first proposes point clusters of interest and then matches them to the embeddings of the input sentence.",
        "Our architecture is capable of learning the semantic similarities of the given contexts and regressing the bounding boxes for the target objects.",
        "The \"multiple\" subset of the ScanRefer dataset refers to cases where there are multiple objects of the same category as the target object.",
        "There are on average 14 different objects to disambiguate between in the \"Multiple\" subset, and it is more challenging to correctly localize the target object in this subset."
    ],
    "6602": [
        "The proposed cyclic multitask learning framework for simile recognition can better model the dependencies among the subtasks, compared to conventional multitask learning.",
        "Extensive experiments and analysis demonstrate the effectiveness of the proposed framework.",
        "In future work, the generality of the framework will be investigated on other multitask learning-based NLP tasks.",
        "The use of variational networks, which have been widely used in many tasks, may improve the framework."
    ],
    "6603": [
        "We construct a benchmark dataset named CJRC (Chinese Judicial Reading Comprehension) to fill gaps in the field of legal research.",
        "CJRC is the first Chinese judicial reading comprehension dataset, and it involves three types of questions, namely span-extraction, YES/NO, and unanswerable questions.",
        "The dataset contains civil data and criminal data, where various of criminal charges and civil causes are included.",
        "We hope that researches on the dataset could improve the efficiency of judges' work.",
        "Integrating Machine reading comprehension with Information extraction or information retrieval would produce great practical value.",
        "The experimental results illustrate that there is still enough space for improvement on this dataset."
    ],
    "6607": [
        "The beam problem in NMT can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The discovered rules for root-and-pattern morphology in Semitic languages can be used to extract Semitic roots, which are the basic units of these languages.",
        "The performance of our unsupervised method for discovering root-and-pattern morphology is not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "A monolingual model should be preferred over multilingual BERT on word-level NLP tasks, even though multilingual BERT has been shown to perform well on Dutch NLP tasks.",
        "Lower level linguistic structure (such as part-of-speech tags) is learned earlier during pre-training than higher level information.",
        "Higher level linguistic tasks do not benefit from longer pre-training after 850K epochs, but the entity recognition task does benefit from longer pre-training.",
        "It is important that large pre-trained language models are trained for enough iterations to properly encode high level structures.",
        "English BERT encodes higher level linguistic structures in later layers, and this may also be true for BERTje."
    ],
    "6616": [
        "Using context in the form of topics improves the performance of a dialog system.",
        "Incorporating attention mechanisms to enable the decoder to focus on relevant parts of the dialog history and audio/video features improves the performance of a dialog system.",
        "Incorporating audio features from an end-to-end audio classification architecture, AclNet, improves the performance of a dialog system.",
        "Using an end-to-end audiovisual scene aware dialog system improves the performance of a dialog system."
    ],
    "6623": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptive search method incorporating two kinds of losses, task-useful knowledge distillation loss depending on the original BERT, and efficiency-aware loss based on the searched structure, can automatically and efficiently find suitable models for downstream tasks.",
        "The proposed AdaBERT model achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The use of multilingual BERT encoder brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "The novel end-to-end model for joint slot label alignment and recognition requires no external label projection and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The use of NMT backtranslation helps the model generalize better over syntactic and grammatical variance of human writing.",
        "The ensemble model gives a 3.5 point improvement over the Bert Large dev F1.",
        "Experimenting with various architectural modifications and presenting an ablation study helped to learn about neural architectural techniques and how different model components work together.",
        "The use of convolutional layers, which work well in computer vision, does not necessarily work as well in NLP.",
        "There is potential for further improvement by limiting noise in the dataset and extending the work to context augmentation.",
        "Pre-training the model on a large dataset and fine-tuning it on the SQuAD dataset for the task of Question Answering may improve the performance."
    ],
    "6626": [
        "The proposed model, CAPSAR, outperforms other models in aspect-level sentiment analysis.",
        "The network architecture of CAPSAR, with hierarchical capsule layers and a shared-weight routing algorithm, captures key features for predicting sentiment polarities.",
        "The instantiation parameters of sentiment capsules are used to reconstruct the aspect representation, allowing the model to detect potential aspect terms that are not seen before.",
        "The proposed model can capture coherent patterns between sentiment and aspect information.",
        "Experimental results on three real-world benchmarks demonstrate the superiority of the proposed model."
    ],
    "6627": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The proposed DP-LSTM network can predict the stock price accurately with robust performance, especially for S&P 500 index that reflects the general trend of the market.",
        "The differential privacy method can significantly improve the robustness and accuracy of the prediction model."
    ],
    "6643": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "By encouraging both forward and backward translation models to share the same focus on the image when generating semantically equivalent visual words, the proposed regularized training is capable of making better use of visual information.",
        "A simple yet effective multi-head co-attention model is also introduced to capture the interaction between visual and textual features.",
        "Extensive experimental results show that our approach can outperform competitive baselines by a large margin.",
        "The proposed regularization approaches can effectively improve the agreement of attention on the image, leading to better use of visual information."
    ],
    "6644": [
        "'We present a data augmentation strategy and a multi-cascaded model for enhanced paraphrase detection.'",
        "'The data augmentation strategy generates additional paraphrase and non-paraphrase annotations based on the graph analysis of the existing annotations.'",
        "'The proposed multi-cascaded model is both deep and wide in architecture, and it embodies previous best practices in deep models for paraphrase detection.'",
        "'Our enhanced model produces a higher predictive performance on all three datasets beating all previously published results on them.'",
        "'Data augmentation is generally beneficial and linguistic features only help for small and noisy text datasets.'",
        "'Multiple models can boost predictive performance beyond that achievable from any single model.'",
        "'This work provides a comprehensive treatment of paraphrase detection that includes small and large datasets, clean and noisy texts, CNN and LSTM-based models, learned features, hand-crafted linguistic features, and a new data augmentation strategy.'",
        "'In the future, it would be beneficial to investigate strategies for resolving conflicts and achieving quality annotations in noisy data.'"
    ],
    "6645": [
        "Our approach can obtain strong performance across a number of tasks.",
        "Incorporating a novel attentive multimodal combination module can improve the performance of a single architecture for multi-task training on them.",
        "Future work could investigate further how these skills are blended during interaction, rather than evaluate them as stand-alone tasks.",
        "Our approach incorporates a novel attentive multimodal combination module to improve the performance of a single architecture for multi-task training on them.",
        "We have assembled disparate multimodal tasks and built a single architecture for multi-task training on them."
    ],
    "6646": [
        "The proposed approach, TextScanner, can overcome the problems and defects of previous methods for scene text recognition.",
        "TextScanner is effective under various challenging scenarios.",
        "A novel mutual-supervision mechanism allows for taking full advantage of both real and synthetic data.",
        "TextScanner shows stronger adaptability in handling difficult text compared to previous methods."
    ],
    "6649": [
        "The authors have presented a new open reading benchmark called ORB, which is designed to test the generalizability and understanding of natural language phenomenon in reading comprehension systems.",
        "The ORB benchmark is intended to be a comprehensive test of reading comprehension systems and will grow over time as more interesting and useful datasets are released.",
        "The authors hope that the ORB benchmark will help drive research on general reading systems.",
        "The ORB benchmark includes a variety of natural language phenomenon, such as out-of-domain questions, which will challenge the ability of reading comprehension systems to make consistent predictions.",
        "The authors have released the ORB dataset and evaluation metrics to the public, allowing other researchers to test their systems on this benchmark."
    ],
    "6653": [
        "The model achieves state-of-the-art or competitive performance on various tasks such as age, dialect, gender, emotion, irony, and sentiment prediction from social media posts.",
        "AraNet uses a unified and simple framework based on the BERT model, which has the potential to alleviate issues related to comparing across different Arabic social media NLP tasks.",
        "The toolkit can be used to make important discoveries about the Arab world and enhance our understanding of Arabic online communities and the Arabic digital culture in general.",
        "AraNet has the advantage of providing one way to test new models against its predictions, which can be useful for comparing different NLP tasks."
    ],
    "6654": [
        "We introduced a zero-shot multilingual setting for evaluation of neural ranking methods.",
        "This is an important setting due to the lack of training data available in many languages.",
        "We found that contextualized languages models (namely, BERT) have a big upper-hand, and are generally more suitable for cross-lingual performance than prior models (which may rely more heavily on phenomena exclusive to English).",
        "We also found that additional in-language training data may improve the performance, though not necessarily.",
        "By releasing our code and models, we hope that cross-lingual evaluation will become more commonplace."
    ],
    "6655": [
        "The proposed method achieves state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The Cosine Annealing Strategy is used to combine the two methods.",
        "The approach can reduce the noise of pseudo labels to improve domain adaptation performance in the future.",
        "The method achieves competitive performance compared to previous systems using less than 1% of the training data on the English Switchboard test set.",
        "The method significantly outperforms previous methods by reducing the error by 21% on English Switchboard.",
        "LIMs are more effective on sentence classification tasks than sequence labeling tasks, especially for zero-shot transfer learning.",
        "The multilingual representations learned in pre-trained BERT are suitable for typologically similar languages.",
        "Language-independent models are effective in low-resource languages and can be improved with a small amount of low-resource training data.",
        "The insights have unique values for the development and customization of natural language understanding models and solutions in new languages."
    ],
    "6656": [
        "We have proposed a sentence embedding using a sequential encoder-decoder with a pairwise discriminator for NLP tasks.",
        "Our method outperforms all previous state-of-the-art methods for NLP tasks in terms of BLEU, METEOR, and TER scores.",
        "We have performed ablation analysis for our method, which justifies that a pairwise discriminator outperforms the previous state-of-art methods for NLP tasks.",
        "We plan to generalize this approach to other text understanding tasks and also extend the same idea in the vision domain."
    ],
    "6657": [
        "This is the first study on context-aware semantic expansion.",
        "The proposed network structure and different alternatives of the context encoder are studied for lexical substitution.",
        "The simplest NBOW encoder achieves surprisingly good performance, outperforming other context encoders.",
        "Seed-aware attention, which models the interaction between seed and context words, further improves the performance.",
        "The TRANS-DOT scoring function is capable of focusing on indicative words and outperforms other seed-oblivious or -aware competitors.",
        "The impact of a bias introduced when harvesting data is small."
    ],
    "6659": [
        "Theoretical analysis of word embeddings gives us a better understanding of their properties.",
        "Theory may provide us with interesting hypotheses on the nature and structure of word embeddings.",
        "Empirical verification of these hypotheses can be done using this paper's findings.",
        "Word embeddings have specific properties that can be analyzed theoretically.",
        "Theoretical analysis of word embeddings can provide insights into their structure and nature."
    ],
    "6665": [
        "Our approach can perform well on noisy data, including incorrect sentences or STT errors.",
        "Experimenting on the Twitter dataset showed improved accuracy with complete sentences.",
        "Our model can extract richer data representations from input data regardless of sentence completeness.",
        "Future work may involve evaluating the robustness of our model against other types of noise, such as word reordering, word insertion, and spelling mistakes in sentences.",
        "Further experiments will be done to search for more appropriate hyperparameters and more complex neural classifiers to substitute the last feedforward network layer."
    ],
    "6667": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The proposed approach of joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "An unsupervised method for the discovery of root-and-pattern morphology in Semitic languages is presented, with intrinsic and extrinsic evaluations of the discovered rules used to validate the method.",
        "The proposed method for extracting Semitic roots using the discovered rules has performance not too far from a rule-based language-specific (in this case Arabic) root extractor."
    ],
    "6669": [
        "The proposed approach based on laughter information offers a hierarchy of global and local discourse combining coarse and fine-grained segmentation that hybrids paralinguistic and linguistic information from natural discourses.",
        "The novel training-free algorithm uses only the paralinguistic information, that is the laughter occurrences to segment the whole conversation into several discourse segments in parallel with two clustering techniques: agglomerative and K-medoids.",
        "The proposed approach achieves a hybrid topic structure of coherence that outperforms stand-alone approaches.",
        "The framework can be applicable to the online scenario of spoken language understanding.",
        "There is an opportunity for an in-depth study of interactions between the topic structure and other discourse structures, such as discourse relational structure."
    ],
    "6670": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "TED significantly outperforms unsupervised abstractive baselines."
    ],
    "6671": [
        "The usefulness of NAQ, H1-H2, and HRF for parameterizing glottal flow is confirmed.",
        "The effectiveness of CPIF and CCD appears to be similar and rather high, with a slight preference towards CCD.",
        "CCD is effective for TTS applications, such as emotional/expressive TTS.",
        "Further testing is needed for applications that require the analysis of noisy signals, such as telephone applications.",
        "The ranking of methods changes depending on the SNR and the robustness of CCD.",
        "IAIF has lower performance in most tests but shows up to be comparatively more effective in very low SNR values."
    ],
    "6672": [
        "We presented CATS, a novel supervised model for text segmentation that couples segmentation prediction with explicit auxiliary coherence modeling.",
        "CATS is a neural architecture consisting of two hierarchically connected Transformer networks: the lower-level sentence encoder generates input for the higher-level encoder of sentence sequences.",
        "We train the model in a multi-task learning setup by learning to predict (1) sentence segmentation labels and (2) that original text snippets are more coherent than corrupt sentence sequences.",
        "We show that CATS yields state-of-the-art performance on several text segmentation benchmarks and that it can successfully segment texts from target languages unseen in training.",
        "Although effective for text segmentation, our coherence modeling is still rather simple: we use only fully randomly shuffled sequences as examples of (highly) incoherent text.",
        "In subsequent work, we will investigate negative instances of different degree of incoherence as well as more elaborate objectives for (auxiliary) modeling of text coherence."
    ],
    "6677": [
        "The brevity problem in NMT can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Our approach to NMT does not require any external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages.",
        "Our approach to NMT can be applied independently of other approaches.",
        "Our word vectors can capture sentimental aspects of words in addition to semantic and syntactic characteristics.",
        "Our approaches are portable to other languages and cross-domain.",
        "Our study is one of the few ones that perform sentiment analysis in Turkish and leverages sentimental characteristics of words in generating word vectors.",
        "Our word vectors can be used for other classification tasks such as topic classification and concept mining.",
        "Combining different approaches can help build better vectors.",
        "Our word vectors are created by conventional machine learning algorithms, but they produce state-of-the-art results."
    ],
    "6678": [
        "The proposed method significantly improves the efficiency of entity linking by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptive BERT model achieves comparable performance while significantly improving the efficiency.",
        "The proposed method corrects most of the type errors produced by the baseline.",
        "The detailed experiment analysis shows that our method significantly outperforms the baseline with an absolute improvement of 1.32% F1 on in-domain AIDA-CoNLL test set and average 0.80% F1 on five out-domain test datasets.",
        "The proposed method has the potential to capture latent entity type information with BERT, which can improve the performance of entity linking."
    ],
    "6685": [
        "The latent BOW model serves as a bridge between the latent variable models and the planning-and-realization models.",
        "The interpertability comes from the clear generation stages, while the performance improvement comes from the guidance by the sampled bag of words plan.",
        "Although effective, the decoder heavily relies on the BOW prediction, yet the prediction is not as accurate.",
        "When there exists information leakage of BOW from the target sentences, the decoder can achieve significantly higher performance.",
        "Improving the BOW prediction to better restrict the decoder's search space is a future direction.",
        "The step-by-step generation process serves as an move towards more interpretable generative models, and it opens new possibilities of controllable realization through directly injecting lexical information into the middle stages of surface realization."
    ],
    "6687": [
        "Insufficiency of labeled data heavily restricts the effectiveness of the neural models for TOWE.",
        "We propose a novel model to transfer latent opinions knowledge from resource-rich review sentiment classification datasets to improve the low-resource task TOWE.",
        "Our approach achieves better performance than other state-of-the-art methods.",
        "Extensive analysis also demonstrates the effectiveness of our model."
    ],
    "6689": [
        "The proposed model significantly outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by the model are coherent with human judgments.",
        "The use of self-attention mechanism improves the quality of multiturn dialogue generation.",
        "The model does not rely on complex neural network-based text encoders, such as CNN and LSTM.",
        "The generator has poor generalization ability when dealing with unseen relations.",
        "The bag-of-words model achieves better performance but still lacks semantic diversity, especially when understanding a relation type requires considering the word sequence information in its textual description.",
        "The zero-shot setting is based on an unified entity set E and can be beneficial to further consider unseen entities.",
        "The proposed approach does not depend on specific KG embedding methods and is model-agnostic, which could be potentially applied to any version of KG embeddings."
    ],
    "6690": [
        "The analysis of security forums is in its infancy, despite several promising recent works.",
        "We propose a novel approach to identify and classify threads of interest based on a multi-step weighted word embedding approach.",
        "Our approach consists of two parts: (a) a similarity-based approach to extract relevant threads reliably, and (b) weighted embedding-based classification method to classify threads of interest into user-defined classes.",
        "The key novelty of the work is a multi-step weighted embedding approach that projects words, threads, and classes in the embedding space and establishes relevance and similarity there.",
        "Our work is a first step towards developing an easy-to-use methodology that can harness some of the information in security forums.",
        "The easy-of-use stems from the ability of our method to operate with an initial set of bag-of-words, which our system uses to seeds to identify threads that the user is interested in."
    ],
    "6692": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The proposed multiplex word embedding model better encodes SP knowledge than different baselines without harming its capability in representing general semantics.",
        "Using a set of embeddings to represent each word's general and relation-dependent semantics can improve the model's ability to acquire SP knowledge.",
        "The effectiveness of the proposed model and its learning process is illustrated through experiments on SP acquisition and word similarity measurement, as well as analysis of different settings and optimization strategies.",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality."
    ],
    "6693": [
        "The current work uses the data provided in SemEval 2019 shared task A for Offensive language identification.",
        "A comparative study is provided by exploring the effectiveness of Google universal sentence encoder, Fasttext based embedding, Dynamic Mode Decomposition based features and RKS based explicit mapping approach.",
        "The current work uses machine learning methods such as SVM linear, Random Forest, Logistic regression, Navie Bayes and Regularized least-square based classification for the experiments.",
        "The measures used for evaluation are accuracy, precision, recall, and f1-score."
    ],
    "6695": [
        "The proposed approach using a Siamese architecture and triplet loss outperforms other encoders in a Siamese framework.",
        "The Adaptive Star Transformers (AST) efficiently filter irrelevant information from input sequences, leading to better performance.",
        "The use of AST eliminates or vastly reduces the cost of filtering corpora.",
        "The proposed approach is novel and has not been previously proposed in the literature.",
        "We present a novel approach to article-comment linking using a Siamese architecture and triplet loss.",
        "We show that our approach outperforms several other encoders in a Siamese framework.",
        "This model could allow other researchers to eliminate or vastly reduce the cost of filtering corpora."
    ],
    "6698": [
        "Our proposed multi-head self-attention network outperforms state-of-the-art visual semantic embedding models by a large margin.",
        "The model represents visual and textual data in multiple vectors, generating multiple attention weights that encode different parts of the data.",
        "We enhance the model with diversity regularization loss to address the redundancy problem and provide various aspects of visual and textual data.",
        "Our model is able to achieve superior performance without any other resources such as bounding box or image segmentation datasets.",
        "The multihead attention module can be widely applied for other computer vision tasks such as image classification, visual question answering, and semantic segmentation."
    ],
    "6699": [
        "The ED-CC model produces the least number of candidate records and achieves the lowest Content Selection Recall (CSR) compared to the gold standard content plans.",
        "The template-like discourse pattern produced by the ENT model noticeably deteriorates its performance, and it is completely outperformed by the NCP model.",
        "Employing table reconstruction as an auxiliary task indeed boosts the decoder to produce more accurate factual statements.",
        "The core fact-grounding aspect of the data-to-text generation task can be studied using a purified, enlarged, and enriched RotoWire-FG corpus with a more fair and reliable evaluation setup.",
        "Existing models can be improved by setting a benchmark on the new task and revealing fine-grained unsolved challenges hoping to inspire more research in this direction."
    ],
    "6701": [
        "The proposed model for Natural Language Generation using dependency information for sentence structuring and surface realization shows consistent improvement over state-of-the-art NLG models in individual domain and general domain experiments.",
        "The proposed model achieves comparable results with neural network-based models trained on dialogue act, E2E, and WebNLG datasets.",
        "The proposed model demonstrates an ability of adaptation to unseen domains.",
        "The proposed model can generate high-quality and fluent sentences as evidenced by human evaluation results."
    ],
    "6702": [
        "ProphetNet achieves the best performance on both abstractive summarization and question generation tasks.",
        "ProphetNet achieves new state-of-the-art results on CNN/DailyMail and Gigaword using only about 1/3 the pre-training epochs of the previous model.",
        "ProphetNet is a sequenceto-sequence pre-training model that learns to predict future n-gram at each time step.",
        "The previous model requires about 1/3 the pre-training epochs of ProphetNet for achieving the same performance.",
        "ProphetNet achieves state-of-the-art results on CNN/DailyMail and Gigaword."
    ],
    "6704": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Creation of such a dataset would greatly increase the challenge for future algorithms and better match real-world usage.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments."
    ],
    "6706": [
        "The authors propose a study of multiple domain distance measures to address the problem of domain adaptation.",
        "They provide analyses of these measures based on their single-source experiments.",
        "The measures \"MR\" and \"Books\" are not helpful for learning other tasks, so the authors mask the DistanceNet loss from these domains when the target domain is not \"MR\" or \"Books\".",
        "Proposed study of multiple domain distance measures for domain adaptation.",
        "Analyses of these measures based on single-source experiments.",
        "MR\" and \"Books\" domains not helpful for learning other tasks, so masking DistanceNet loss is necessary when target domain is not one of them."
    ],
    "6712": [
        "Using an SVM with features from different information retrieval models can improve relevance scores predicted by some models alone.",
        "The word2vec model results in a significant improvement with an RMSE of 0.2482 for capturing the relevance of ambiguous queries correctly.",
        "Sentence vectors may not be a good idea for relevance tasks because many descriptions do not have well-formed English sentences and are in more of a bullet point format consisting of many unrelated product characteristics written together.",
        "Using word vectors still does a good job for capturing the relevance of ambiguous queries correctly.",
        "Training a single SVM model using features from both models (i.e. BM25/Indri scores as well as similarity scores from the word2vec model) does not achieve any significant improvement.",
        "Increasing the training data improves the performance of deep learning models, and more labeled data is needed to train a better model."
    ],
    "6714": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Non-autoregressive translation is a fast viable alternative to autoregressive translation.",
        "There is still a discrepancy between autoregressive and non-autoregressive performance when knowledge distillation from a large transformer is applied to both."
    ],
    "6720": [
        "The proposed method maintains the embedding isotropy and is effective for word similarity, categorization, and several other downstream tasks.",
        "For pre-trained embeddings that are less isotropic (e.g., GloVe), IIQ performs better than ITQ owing to the improvement on isotropy.",
        "The results point to promising deployment of trained neural network models with word embeddings on resource constrained platforms in real life.",
        "The proposed method achieves a 32-fold (and higher) compression ratio."
    ],
    "6722": [
        "\"ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "\"providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "\"Our models are able to generate more diverse and relevant responses, even when compared with state-of-the-art approaches.",
        "\"We plan to apply these models to different generative tasks where diversity is desired."
    ],
    "6728": [
        "The proposed recall mechanism can effectively employ recalled information to improve the quality of generated captions.",
        "Our proposed methods can be applied to any captioning model.",
        "Using training data both for model training and retrieving recalled words can be instructive to other researches of captioning.",
        "The use of a recalled-word reward can boost CIDEr optimization.",
        "The proposed recall mechanism can make full use of recalled words.",
        "Semantic guide and recalled-word slot are proposed to improve the use of recalled words."
    ],
    "6731": [
        "The proposed joint-modality attention network (JMAN) effectively considers multiple modalities in visual dialogue models, leading to better comprehension of multi-modal context.",
        "Our best model achieved a significant improvement over the baseline on ROUGE-L and CIDEr, with an improvement of 12.1% and 22.4%, respectively.",
        "The proposed encoder-decoder based visual dialogue model with JMAN boosted the performance of scene-aware ability through multiple reasoning steps.",
        "Exploring this multi-modal dialogue setting further with larger scale datasets is a potential future direction.",
        "Unsupervised pre-trained language models could be applied to inject more semantics into the model for the multi-modal dialogue task."
    ],
    "6732": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Current multimodal models on the Visual Dialog task over-rely on the dialogue history.",
        "Image-only and image-history joint models achieve complementary performance gains.",
        "Multi-modal factorized bilinear pooling with co-attention learning for visual question answering."
    ],
    "6735": [
        "The proposed adaptive neural dialogue generation model (ADAND) enables the dynamical parameterization of the model to each conversation, leading to the generation of appropriate responses in diverse conversations.",
        "The ADAND model achieves superior performance and higher efficiency compared to other approaches.",
        "The proposed approach of context-aware parameterization captures local semantics of the input context, enabling the generation of more appropriate responses.",
        "The approach of topic-aware parameterization enables parameter sharing by first inferring the latent topics of the given context and then generating the parameters with the inferred latent topics.",
        "The ADAND model is not limited to LSTMs and can be applied to other structures in future work."
    ],
    "6741": [
        "There is very little work on location normalization of text, and popular solutions such as NER are not directly transferable to it.",
        "The ROIBase system provides an efficient and interpretable solution to location normalization through a web interface.",
        "The proposed baseline can be applied in different languages easily.",
        "There is potential for improving location normalization with further work."
    ],
    "6742": [
        "Our proposed length-controllable abstractive summarization model achieves high ROUGE scores in standard summarization tasks.",
        "The prototype extractor in our model identifies the important parts of the source text within the desired length constraint.",
        "The abstractive model is guided by the prototype text, which enables it to produce high-quality summaries.",
        "Our model outperforms previous models in both standard and length-controlled settings.",
        "Incorporating a pre-trained language model in the abstractive model can potentially improve the quality of the generated summaries."
    ],
    "6743": [
        "PYKE outperforms state-of-the-art approaches in the two tasks of type prediction and clustering.",
        "Our approach retains a linear space complexity, which means that our approach can be used on very large knowledge graphs and return results faster than popular algorithms such as Word2VEC and TransE.",
        "The time efficiency of PYKE is not the only important aspect; our results suggest that PYKE outperforms state-of-the-art approaches in the two tasks of type prediction and clustering.",
        "Exploring other similarity measures will be at the center of future works.",
        "Using a better initialization for the embeddings should lead to faster convergence.",
        "A stochastic approach (in the same vein as stochastic gradient descent) could be used to further improve the runtime of PYKE."
    ],
    "6744": [
        "The proposed schema-guided dialogue state tracking model is robust to changes in the schema.",
        "The model does not require any domain or slot specific parameters, instead utilizing domain and slot representations to learn corresponding representations from the input sequence.",
        "The approach outperforms the baseline approach and can effectively perform knowledge transfer between domains.",
        "The proposed approach is particularly promising for low-resource domains where training data are scarce or absent, as it can leverage data from high-resource domains to achieve significant data efficiency."
    ],
    "6748": [
        "Our proposed method, PET, consists of defining pairs of cloze question patterns and verbalizers that help leverage the knowledge contained within pretrained language models for downstream tasks.",
        "We finetune models for all pattern-verbalizer pairs and use them to create large annotated datasets on which standard classifiers can be trained.",
        "When the initial amount of training data is limited, PET gives large improvements over standard supervised training and strong semi-supervised approaches."
    ],
    "6750": [
        "We propose an evaluation framework for assessing the quality of adversarial examples in NLP, based on four criteria: (a) attacking performance, (b) textual similarity; (c) fluency; (d) label preservation.",
        "The architecture of the target classifier is an important factor when it comes to attacking performance, e.g. CNNs are more vulnerable than LSTMs.",
        "Data features such as length of text and input domains are also influencing factors that affect how difficulty it is to perform adversarial attack.",
        "Adversarial examples produced by these methods tend not to preserve their semantic content and have low readability.",
        "These methods 'cheat' by simply flipping the sentiment in the adversarial examples, and this behavior is evident especially on the yelp50 dataset.",
        "These methods could be ineffective for adversarial training."
    ],
    "6752": [
        "The technique of using the same input and output embedding weights matrix is widely adopted by recent state-of-the-art models, but it can be further optimized.",
        "The embedding sharing technique eliminates the bias of estimations for output scores, and normalization methods on embedding weight matrices can improve its effectiveness.",
        "This paper explores the shortcomings of the embedding sharing technique and presents improvements and adjustments based on theoretical analysis.",
        "The proposed improvement methods show guaranteed effectiveness in various Neural Networks in Natural Language Processing, without significantly raising the training and inference time of models.",
        "The techniques presented in this paper can be applied to various datasets of Neural Machine Translation models."
    ],
    "6757": [
        "We proposed a novel hierarchical and recurrent VAE-based architecture to capture accurately the semantics of fully annotated goal-oriented dialog corpora.",
        "To reduce the risk of inference collapse while maximizing the generation quality, we directly modified the training objective and devised a technique to scale dropouts along the hierarchy.",
        "Our proposed model VHDA was able to achieve significant improvements for various competitive dialog state trackers in diverse corpora through extensive experiments.",
        "With recent trends in goal-oriented dialog systems gravitating towards end-to-end approaches (Lei et al., 2018) , we wish to explore a self-supervised model, which discriminatively generates samples that directly benefit the downstream models for the target task.",
        "We would also like to explore different implementations in line with recent advances in dialog models, especially using large-scale pre-trained language models."
    ],
    "6759": [
        "The proposed method introduces a collaborative correlated module that ensures generated textual explanations and answers are coherent with each other and robust to perturbation in image and text.",
        "The generated textual and visual explanations are more coherent and robust against perturbation compared to previous models.",
        "The model generates comparable accuracies for answer generation, but the generated explanations are more robust against perturbation.",
        "The proposed method is able to generate textual and visual explanations that are coherent with each other and with the answers.",
        "The model's ability to generate explanations that change when there is a change in the answer signifies the importance of image and question in the robustness of the model.",
        "Obtaining improved insights while solving challenging vision and language based tasks is an area for future work."
    ],
    "6761": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "Our work introduces a principled framework to include cues for vision and language-based interaction.",
        "We aim to further validate the generalization of the approach by extending this approach to other vision and language tasks.",
        "The proposed Bayesian Expert model improved over all the other variants."
    ],
    "6762": [
        "The proposed SMART training process for conditional masked language models achieves competitive performance with mainstream autoregressive models while retaining the benefits of fast parallel decoding.",
        "The SMART training process better matches the semi-autoregressive nature of the mask-predict decoding algorithm, leading to improved model performance.",
        "The SMART trained models are competitive with mainstream autoregressive models in terms of performance, while retaining the benefits of fast parallel decoding.",
        "The SMART training process produces models that are more efficient and scalable than traditional training methods."
    ],
    "6768": [
        "TVR is of high quality and is more challenging than previous datasets.",
        "We propose Cross-modal Moment Localization (XML), an efficient model suitable for the VCMR task.",
        "The dataset is large-scale and designed for multimodal moment retrieval tasks.",
        "The proposed approach is efficient and outperforms previous methods.",
        "The dataset is of high quality and provides a challenging benchmark for evaluating moment retrieval systems.",
        "The authors have used a variety of funding sources, including DARPA KAIROS Grant #FA8750-19-2-1004, ARO-YIP Award #W911NF-18-1-0336, and Google Focused Research Award."
    ],
    "6773": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results suggest that the community is ready for more difficult CQA datasets.",
        "Charts in the wild should be included in future datasets to challenge the system.",
        "Human-generated questions should be included in future datasets to challenge the system.",
        "Document-level CQA is necessary to understand charts in documents.",
        "PReFIL has the potential to improve retrieval of information from charts, which has numerous applications."
    ],
    "6775": [
        "The performance of MRC models can be improved by incorporating verification mechanisms, as shown by the new state-of-the-art results achieved by the proposed reader.",
        "The choice of verification mechanisms has a significant impact on MRC performance, and the use of a retrospective reader that integrates both sketchy and intensive reading can improve the performance of MRC models.",
        "The proposed reader, which integrates both sketchy and intensive reading, outperforms strong baseline models in terms of newly introduced statistical significance.",
        "The use of a PrLM as encoder backbone and baseline is effective for improving the performance of MRC models.",
        "In the future, more decoder-side problem-solving techniques can be explored to cooperate with strong encoders for even more advanced MRC."
    ],
    "6778": [
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Experimental results on the commonly used English Switchboard test set show that our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our proposed measure significantly outperforms existing approaches in both unsupervised and supervised experimental setups.",
        "Automated tools for better understanding of opinions would lead us to a new era of digital democracy and improved decision making."
    ],
    "6781": [
        "The proposed schema label enhanced ranking framework achieves state-of-the-art performance on the dataset retrieval task, as demonstrated by experimental results.",
        "The framework uses a two-stage approach, with the first stage generating additional schema labels for each dataset column, and the second stage ranking datasets using both original fields and generated schema labels.",
        "The schema label generation process is treated as a multi-label classification task, where each column of a dataset is associated with multiple schema labels.",
        "The framework learns the latent feature representations of schema labels using a CoFactor model that captures dataset-schema label interactions and schema label-schema label interactions.",
        "The proposed method improves the performance of dataset retrieval, as demonstrated by the highest NDCG scores on all rank cut-offs compared to all baseline methods.",
        "The features generated from schema labels can help in supervised ranking for the web table retrieval task, similar to dataset search."
    ],
    "6782": [
        "Our proposed hybrid deep neural network framework (CAD) for detecting emotions in a dialogue outperforms state-of-the-art models on the benchmark dataset of EmotionLines-2018.",
        "We propose a hierarchical BiGRU network that takes the assistance of various hand-crafted feature sets on different levels of architecture to learn the contextual information of dialogue.",
        "Our CAD framework can be applied to various other similar datasets to improve their results.",
        "The learned representation from our model is fed to a fully connected layer over the time steps followed by a softmax layer for the predictions.",
        "We have evaluated our model on the benchmark dataset of EmotionLines-2018, which consists of two corpora i.e., Friends and EmotionPush data."
    ],
    "6784": [
        "We compare different deep learning approaches for Hindi sentence classification.",
        "CNN models perform better than LSTM-based models on the datasets considered in this paper.",
        "BOW is useful when we can trade off accuracy for speed.",
        "LSTMs do not do better than CNNs may be because the word order is relaxed in Hindi.",
        "Sentence representations captured by LASER multilingual model were rich as compared to BERT.",
        "Custom-trained models on specific datasets performed better than lightweight models directly utilizing sentence encodings.",
        "The real advantage of multi-lingual embeddings can be better evaluated on tasks involving text from multiple languages."
    ],
    "6785": [
        "The intention after a diploma has a higher impact in the choice of a career.",
        "Using the user's intention gives the best results for predicting career choices.",
        "The continuity between 2 consecutive steps in a professional career is higher than in an academic one.",
        "The MR decreases to 3.7 when using the previous job and the next job results.",
        "The MRR has been upgraded up to 0.8 with an interval ranging from 0.797 to 0.811.",
        "A part of the remaining 20% of unpredictability may be due to the simplicity of methods and reorientations.",
        "The system has not correctly predicted certain steps, which are included in the least probable results."
    ],
    "6786": [
        "Our proposed models outperformed state-of-the-art models on three data sets.",
        "The self-attention mechanism is used to model pairwise interactions between posts.",
        "The attention mechanism extracts important posts that result in predictions.",
        "Mechanisms are used to capture structure and time information.",
        "Recent papers have shown superior results using user identity information for rumor detection.",
        "Fact checking and rumor detection could provide complementary information and be done jointly in the future."
    ],
    "6788": [
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR)",
        "With performance not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "We motivated seven principles for metrics measuring the similarity of graph-based (Abstract) Meaning Representations, from mathematical, linguistic and engineering perspectives.'",
        "A metric that fulfills all principles is applicable to a wide spectrum of cases, ranging from parser evaluation to sound IAA calculation.",
        "Our principles can inform (A)MR researchers who desire to compare and select among metrics, and they ease and guide the development of new metrics.",
        "We provided examples for both scenarios.",
        "We showcased (i) by utilizing our principles as guidelines for an in-depth analysis of two AMR metrics: SMATCH and the recent SEMBLEU metrics, two quite distinct approaches.",
        "Our analysis uncovered that the latter does not satisfy some principles, which might reduce its safety and applicability.",
        "In line of (ii), we target the fulfillment of all seven principles and propose S 2 MATCH, a metric that accounts for graded similarity of concepts as atomic graph components.",
        "In future work, we aim for a metric that accounts for graded compositional similarity of subgraphs."
    ],
    "6789": [
        "Our models can achieve state-of-the-art results on two datasets.",
        "A combination of global and local contexts is empirically complementary.",
        "Cascaded architectures give better results compared to parallel ones.",
        "Incorporating pre-trained contextualized word embeddings in graphs may be a promising direction.",
        "Studying ways to diminish the gap between the reference and the generated text lengths may be worthwhile."
    ],
    "6791": [
        "We have introduced an unsupervised speech representation learning method that discovers acoustic representations from up to 8000 hours of diverse and noisy speech data.",
        "Our pretrained representations lead speech recognition systems to be robust to domain shifts compared to standard acoustic representations, and compared to representations trained on smaller and more domain-narrow pretraining datasets.",
        "We have shown, for the first time, that such pretrained representations lead speech recognition systems to be robust to domain shifts.",
        "The representations were evaluated on a standard speech recognition setup where the models are trained and evaluated on in-domain data and also on transfer tasks where the models are evaluated on out-of-domain data.",
        "We obtained consistent improvements on 25 phonetically diverse languages including tonal and low-resource languages.",
        "This suggests we are making progress toward models that implicitly discover phonetic structure from large-scale unlabelled audio signals."
    ],
    "6792": [
        "code switching can be harnessed for social good and human well-being by using it as a bridge to retrieve hostility-diffusing content written in a low-resource language.",
        "Our approach is appealing for its minimal supervision requirements.",
        "In the context of hostility diffusing hope speech comments, our methods can be used to broaden the reach of such content overcoming the varied language skills of linguistically diverse regions and transcending language barriers."
    ],
    "6798": [
        "Our proposed approach of pseudo-bidirectional decoding (PBD) yields consistent improvements upon strong transformer baselines.",
        "Our approach reduces the number of parameters in the model while providing regularization effects.",
        "The proposed method is able to provide approximated future information for transformer-based LST models.",
        "Sharing the parameters between the encoder and decoder of LST models provides regularization effects.",
        "Our approach allows for reducing the number of parameters in the model while maintaining the performance."
    ],
    "6801": [
        "The proposed method, DAML, achieves significant improvement in domain-adapted sentiment classification compared to representative end-to-end models.",
        "DAML introduces a flexible manner for each model to decide how to view its peer's predictions, unlike standard mutual learning where the output labels of sentiment classifiers are aligned.",
        "The incorporation of probers into mutual learning acts as bridges to connect feature extractors with sentiment classifiers, leveraging the sentiment information lying in a target domain.",
        "DAML allows for effective utilization of the sentiment information in a target domain, enjoying the advantage of incorporating probers into mutual learning."
    ],
    "6804": [
        "Our proposed SGP-DST system includes four modules for intent prediction, slot prediction, slot transfer prediction, and user state summarizing respectively.",
        "The official evaluation results show that our SGP-DST (team12) ranked 3rd (primary evaluation metric for ranking submissions) and 1st on the requested slots F1 score.",
        "Our system is able to perform dialogue state training under zero-shot settings.",
        "The use of a novel end-to-end model for joint slot label alignment and recognition, which requires no external label projection, improves the performance of the system."
    ],
    "6805": [
        "Our proposed emotional voice conversion framework achieves better performance than the baseline with non-parallel training data.",
        "Separate training of spectrum and prosody can achieve better performance than joint training in terms of emotion similarity.",
        "We provide a non-linear method which uses CWT to decompose F0 into different timing-scales.",
        "Our approach uses CycleGAN for both spectrum and prosody conversion.",
        "Experimental results demonstrate the effectiveness of our proposed framework."
    ],
    "6815": [
        "'Previous attempts to solve the multilingual alignment problem have been made, but there is a problem with existing formulations.'",
        "'Our proposed method uses the Wasserstein barycenter as a pivot for the multilingual alignment problem.'",
        "'Our algorithm improves the accuracy of pairwise translations compared to the current state-of-the-art method.'",
        "'The core of our algorithm lies in a new inference method based on an optimal transport plan to predict the similarity between words.'",
        "'Our barycenter can be interpreted as a virtual universal language, capturing information from all languages.'"
    ],
    "6816": [
        "We present ROT framework to incorporate the structural information into the comparison of distributional data.",
        "Our approach consistently improves previous state-of-the-art models in varying conditions.",
        "Calculation of ROT only requires O(n) time complexity.",
        "Compared to classic optimal transport that needs O(n 2 ), ROT could be scaled up to much larger cases.",
        "The ROT framework applies to more general distributional data comparison where structural information is important."
    ],
    "6823": [
        "Our approach improves the overall performance of IR without losing absolute precision.",
        "The results are promising and can be extended by achieving more in-depth free parameters tuning, especially for vector size.",
        "We compare the performance of a path-based measure (Leacock) to a vector-based measure and find that both are comparable.",
        "Concepts and words can be represented in the same vector space, making it possible to compare concepts to the original textual content of documents.",
        "Our approach can be used for conceptual indexing or to improve the quality of some conceptual indexing methods like MetaMap by filtering out non-related or noisy concepts."
    ],
    "6824": [
        "The proposed Seq2Seq model, SLAHAN, can generate informative summaries by explicitly tracking parent and child words for capturing important words in a sentence.",
        "SLAHAN achieved the best kept-token-based F1, ROUGE-1, ROUGE-2, and ROUGE-L scores on the Google dataset in both all sentence and long sentence settings.",
        "SLAHAN also achieved the best kept-token-based F1, ROUGE-1, ROUGE-2, and ROUGE-L scores on the BNC corpus, demonstrating its effectiveness on both long sentences and out-of-domain sentences.",
        "SLAHAN improved informativeness without losing readability, as shown in human evaluation.",
        "Capturing important words that will be decoded in the future based on dependency relationships can help to compress long sentences during the decoding steps."
    ],
    "6827": [
        "We introduce a multilingual speech-to-text translation corpus, CoVoST, for 11 languages into English, diversified with over 11,000 speakers and over 60 accents.",
        "We also provide baseline results, including, to our knowledge, the first end-to-end many-to-one multilingual model for spoken language translation.",
        "CoVoST is free to use with a CC0 license, and the additional Tatoeba evaluation samples are also CC-licensed."
    ],
    "6831": [
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "Iterative data programming leads to better quality training sets sooner, and terminate at obtaining non-exhaustive, labeled training sets with which the learner can attain accuracies very close to those allowed by an optimal training set.",
        "Our iterative applications of a search-based selection strategy and a data programming strategy employing weak learning for expanding text classification data are effective in sustaining a longer expansion path."
    ],
    "6832": [
        "The proposed visual concept-metaconcept learner (VCML) bridges visual concept learning and metaconcept learning.",
        "The VCML model learns concepts and metaconcepts with a unified neuro-symbolic reasoning procedure and a linguistic interface.",
        "Connecting visual concepts and abstract relational metaconcepts boostraps the learning of both.",
        "Concept grounding provides visual cues for predicting relations between unseen pairs of concepts, while the metaconcepts facilitate the learning of concepts from limited, noisy, and even biased data.",
        "VCML outperforms metaconcept-agnostic visual concept learning baselines as well as visual reasoning baselines.",
        "VCML achieves a comparable performance when the number of split B images is large, suggesting that the model outperforms both baselines in utilizing metaconcepts to support learning from biased data."
    ],
    "6834": [
        "We proposed a method to do constituent and dependency parsing relying solely on pretraining architectures.",
        "Our goal was twofold: (i) to show to what extent it is possible to do parsing relying only in word vectors, and (ii) to study if certain linguistic structures are learned in pretraining networks.",
        "The results showed that (frozen) pretraining architectures such as ELMO and BERT get a sense of the syntactic structures, and that a (tuned) BERT model suffices to parse.",
        "By freezing the weights we have provided different analyses regarding the syntax-sensitivity of word vectors.",
        "Contemporaneously to this work, Hewitt and Liang (2019) proposed to complement probing frameworks that test linguistic abilities of pretrained encoders with control tasks.",
        "As future work, we plan to add this strategy to our analyses, and expand our experiments to languages other than English."
    ],
    "6835": [
        "K-ADAPTER remains the original parameters of pre-trained models unchanged and supports continual knowledge infusion.",
        "Specifically, factual knowledge and linguistic knowledge are infused into RoBERTa with two kinds of adapters.",
        "Extensive experiments on three knowledge-driven downstream tasks demonstrate that the performance of each adapter achieves a significant improvement individually, and even more together.",
        "Detailed analyses further suggest that K-ADAPTER captures richer factual and commonsense knowledge than RoBERTa.",
        "In future work, we will infuse more types of knowledge, and apply our framework to more pre-trained models."
    ],
    "6841": [
        "The authors have created a new dataset of citation data for the three Czech apex courts, which is publicly available for download.",
        "The dataset is the first of its kind and provides a valuable resource for researchers studying the citation practices of the Czech courts.",
        "The dataset is a valuable resource for researchers studying the citation practices of the Czech courts, and it has the potential to be used in a variety of applications, such as information retrieval and machine learning.",
        "The authors have demonstrated the usefulness of the dataset by showing that it can be used to analyze the citation practices of the Czech courts and to identify patterns and trends in the data.",
        "The dataset is a significant contribution to the field of legal informatics and has the potential to be used in a variety of applications beyond the study of the Czech courts."
    ],
    "6843": [
        "MPQE learns a well-structured embedding space.",
        "Message passing across the query graph is a powerful mechanism for query answering, that generalizes to multiple query structures even when only trained for single hop link prediction.",
        "Our method presents limitations when evaluating on hard negative samples.",
        "Adding attention or gating mechanisms could improve the message passing procedure.",
        "Extensions to more expressive query embedding representations, such as boxes (Ren et al., 2020), could be useful in information retrieval and recommender systems."
    ],
    "6845": [
        "Our approach is robust to schema modifications.",
        "The model is able to transfer the extracted knowledge to unseen domains.",
        "The model achieves substantial improvements over the baseline.",
        "Our approach is consistent with real-life scenarios raised by virtual assistants.",
        "The model is a multi-task BERT-based model for multidomain dialogue state tracking in zero-shot settings."
    ],
    "6850": [
        "The proposed multimodal model preserves the structure of visual and textual spaces to learn grounded sentence representations.",
        "The approach leverages both perceptual and cluster information to learn grounded sentence representations.",
        "The intermediate grounded space enables relaxing constraints on the textual space.",
        "Our approach is the first to report consistent positive results against purely textual baselines on a variety of natural language tasks.",
        "Future work includes using visual information to specifically target complex downstream tasks requiring commonsense and reasoning, such as question answering or visual dialogue."
    ],
    "6855": [
        "The authors investigate agents that can interact and achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue.",
        "The authors define a task for an agent, where the goal is for the other player to execute a particular action.",
        "The authors explore two reinforcement learning approaches to solve this task and compare them against a strong inverse model baseline.",
        "The authors show that these approaches effectively learn dialogue strategies that lead to successful completion of goals, while producing natural chat.",
        "Future work should develop improved agents that learn to act and speak in natural language at scale in the proposed open-domain task environment.",
        "This setup is exciting because it can be further generalized to richer and richer goal (game) states as they develop models capable of them."
    ],
    "6857": [
        "The issue of \"meaningless labels\" can be mitigated by providing the model with explicit guidance through class-specific descriptions.",
        "The proposed method generates class-specific descriptions to give the model an explicit guidance of what to classify.",
        "The method develops three strategies to construct descriptions, including template-based, extractive, and abstractive strategies.",
        "The use of class-specific descriptions can improve the accuracy of text classification.",
        "The proposed method has the potential to improve the state-of-the-art in text classification tasks."
    ],
    "6858": [
        "Our model can fill in blanks with a variable number of tokens consistent with the context.",
        "We demonstrate the effectiveness of our model on various text rewriting tasks, including text infilling, ancient text restoration, and style transfer.",
        "The action of BLM consists of selecting a blank and replacing it with a word and possibly adjoining blanks.",
        "We train BLM by optimizing a lower bound on the marginal data likelihood that sums over all possible generation trajectories.",
        "Our model can realize a sentence equally well in all orders, which is suitable for filling arbitrary blanks.",
        "Depending on the application, we could also train the model to generate in specific orders by placing higher weights on the corresponding trajectories.",
        "BLM has plenty of future applications, including template filling, information fusion, assisting human writing, etc.",
        "We can extend our formulation to a conditional generative model, which can be used in machine translation to support editing and refining translation, as well as in dialogue systems to compose a complete sentence with given elements.",
        "While we proposed BLM for language generation, it would also be interesting to compare the representations learned by BLM with those produced by other pre-training methods."
    ],
    "6859": [
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our method gains significantly huge performance improvements against existing state-of-the-art NAT models and competitive results to AT models."
    ],
    "6861": [
        "The proposed method, Time-aware Large Kernel (TaLK) Convolutions, achieves a comparative performance on three NLP tasks (machine translation, abstractive summarization, and language modeling).",
        "The proposed method has a faster runtime than previous approaches and can encode longer sentences quicker with a smaller running memory footprint.",
        "The method does not require the notion of attention and can successfully encode a sequence with true linear time O(n) with respect to the sequence length.",
        "The method is applicable to other sequential tasks such as question answering and the PG-19 benchmark dataset.",
        "The method has the potential to be applied in the area of computer vision."
    ],
    "6872": [
        "AFLITE (an iterative greedy algorithm) can effectively filter out spurious biases from data, leading to more accurate benchmark estimation.",
        "The theoretical framework supporting AFLITE provides a solid foundation for understanding its effectiveness in bias reduction.",
        "AFLITE achieves better performance on out-of-distribution and adversarial test sets compared to the original datasets, indicating higher generalization abilities.",
        "The strongest performance on the resulting filtered datasets drops significantly (by 30 points for SNLI and 20 points for ImageNet) after applying AFLITE, suggesting that the algorithm is effective in reducing bias.",
        "Dataset creators can use AFLITE to identify unknown dataset artifacts before releasing new challenge datasets for more reliable estimates of task progress on future AI benchmarks."
    ],
    "6874": [
        "The proposed non-autoregressive generation addresses the non-global optimality issue for MMI in neural dialog generation.",
        "Target tokens can be generated independently in non-AR generation, allowing for faster computation of p(x|y) for each target word.",
        "The proposed strategy produces more diverse, coherent, and appropriate responses, as demonstrated by substantive gains in BLEU scores and human evaluations.",
        "The non-autoregressive generation naturally resolves the non-global optimal issue in decoding."
    ],
    "6875": [
        "The proposed method achieves improvements in BLEU score over naively trained systems with modest translation delays.",
        "The use of scheduled sampling during learning is crucial to combat exposure bias.",
        "The model can be applied to real speech input.",
        "More sophisticated imitation learning techniques that involve the generated trajectories of both \"interpreter\" and \"programmer\" could lead to further improvements.",
        "The use of word-alignment to create an oracle is a sufficient, if not minimum, amount of inputs to translate each target token.",
        "The model can be used for simultaneous translation systems with low delay translations."
    ],
    "6880": [
        "The authors have developed a novel approach to jointly perform part-of-speech tagging and dependency parsing using transition-based neural networks. This approach improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages. (Supporting evidence: \"We have presented an approach to joint part-of-speech tagging and dependency parsing using transition-based neural networks.\")",
        "The authors have prepared two large corpora of scientific paper metadata, OAGSX and OAGKX, which are freely available online and can be used for text summarization and keyword generation experiments. (Supporting evidence: \"In this paper, we described the steps we followed to process Open Academic Graph data and prepare two huge corpora...\")",
        "The authors have performed several experiments applying extractive and abstractive TS and KG methods on subsets of the corpora to establish performance benchmarks for other researchers. (Supporting evidence: \"We also performed several experiments applying extractive and abstractive TS and KG methods on their subsets to help establish performance benchmarks that could be valuable to other researchers.\")",
        "The authors plan to apply topic modeling on the two collections for deriving many subsets of research articles from more specific scientific disciplines in the future. (Supporting evidence: \"In the future, we plan to apply topic modeling on the two collections for deriving many subsets of research articles from more specific scientific disciplines.\")",
        "The authors have found that providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\"). (Supporting evidence: \"An interesting finding is that providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').\")"
    ],
    "6881": [
        "The multi-layer Transformer is effective for achieving high accuracy on hashed inputs, represented using Bloom filter digests.",
        "Applying the Transformer model to tasks with large vocabularies can be effective, and it also points to a few interesting future research directions.",
        "The Transformer model can work effectively with sets of hashed entities, and investigating this simpler setup can help understand the properties of the Transformer.",
        "Hashing adds noise to the learned representations, but it can also increase the flexibility of these representations.",
        "Using a multi-layer Transformer model can provide a mechanism for iteratively filtering noisy representations using the context.",
        "The learned capacity allocation can be more efficient than using a fixed embedding size or frequency-based allocation.",
        "An effective \"denoising\" model is a pre-requisite for such an approach to work, and Superbloom, with its strong denoising ability, can help further realize the potential of embedding models on hashed vocabularies."
    ],
    "6882": [
        "utilizing BERT intermediate layers can enhance the performance of fine-tuning",
        "proposed pooling strategies are effective in enhancing the performance of fine-tuning",
        "experimental results demonstrate the effectiveness and generality of the proposed approach"
    ],
    "6889": [
        "The proposed method achieves state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The use of dynamic memory can be a learning mechanism more general than what has been used here for few-shot learning.",
        "The model based on gaze features and part-of-speech information achieves accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features are the most discriminative ones for disambiguation.",
        "The weighted averaged vector model is computationally efficient and obtains high accuracy values for the IMDB dataset.",
        "Building word vectors only once is enough for the method, and the ensemble results are very high.",
        "The next step is to perform a lookup operation to find the word vector of the words and calculate the weighted average of the word vectors.",
        "The model does not include language-specific features, so it will be tested in different languages and with different weights to produce better results."
    ],
    "6892": [
        "Our zero-resource cross-domain framework for named entity recognition outperforms strong cross-domain sequence tagging models, with performance close to state-of-the-art models that utilize extensive resources.",
        "The framework consists of multi-task learning and Mixture of Entity Experts modules, which learn general representations of named entities and combine the representations of different entity experts based on BiLSTM hidden states.",
        "Our approach is able to cope with the model's inability to recognize named entities by learning general representations of named entities through multi-task learning.",
        "The performance of our model is close to that of state-of-the-art models that utilize extensive resources, despite not using any labeled data from the target domain.",
        "Our method leverages the strengths of both multi-task learning and Mixture of Entity Experts to improve the performance of named entity recognition in zero-resource cross-domain settings."
    ],
    "6895": [
        "The beam problem can largely be explained by the brevity problem.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The results of the current work allow to conclude that our x-vector based systems submitted to the VOiCES challenge are not robust for short duration test utterances and domain mismatch conditions.",
        "Deep ResNet architectures are more robust and allow to improve the quality of speaker verification for both long-duration and short-duration utterances.",
        "Our best performing system for VOiCES protocols is ResNet34 based system built on high frequency resolution MFB features.",
        "Utilizing the proposed U-net based VAD, scoring model in-domain centering and score normalization techniques provide additional performance gains for proposed SR systems."
    ],
    "6900": [
        "'Experiments on language model datasets show that a light weighted Transformer is able to perform competitively but with much improved computation efficiency.'",
        "'We explore less computation-expensive Transformer architectures, with the design principle of preserving long and short range dependency in the sequence but with less connections.'",
        "'Our plan is to extend the Transformer architectures to experiment on deeper Transformer architectures and more tasks.'"
    ],
    "6901": [
        "Significant variance across trials\" in the performance of contextual embedding models when using random seeds.",
        "Substantial performance gains on all tasks\" when using finetuning with random seeds.",
        "Expected performance changes as we allocate more computational resources.",
        "Two sources of variance across trials: weight initialization and training data order.",
        "Some data orders and initializations are better than others, and the latter can even be observed across tasks.",
        "A simple early stopping strategy can alleviate the computational costs of running multiple trials.",
        "All of our experimental data containing thousands of fine-tuning episodes is publicly released."
    ],
    "6906": [
        "The proposed method achieves better performance than baselines in terms of diversity and overall quality.",
        "The novel latent decomposition scheme models the style transfer as a one-to-many mapping problem.",
        "The human evaluation results show that the proposed method provides more desirable sentences considering both content preservation and style transfer.",
        "The proposed method extends the pair-wise comparison to set-wise comparison, allowing for a more comprehensive evaluation of one-to-many style transfer.",
        "The workers are able to choose which set has more diverse sentences with the same meaning, and which set provides more desirable sentences considering both content preservation and style transfer."
    ],
    "6907": [
        "We proposed to disentangle the representations of different token categories while generating FOL output and used category prediction as an auxiliary task.",
        "Our analysis showed the difficulties faced by neural networks in modeling FOL and ways to tackle them.",
        "We also experimented by introducing a perturbation in inputs in order to examine the robustness of different proposed models.",
        "We aim to release our code as well as data publicly to promote further research in the area."
    ],
    "6912": [
        "The proposed fully-supervised parser outperforms the state-of-the-art baseline parser in terms of F1 score.",
        "The representation extractor with the Clustering Promotion Mechanism achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The proposed approach can improve domain adaptation performance by reducing the noise of pseudo labels.",
        "The multi-document summarization dataset, GameWikiSum, is based on professional game guides and has the potential to be used for a wide range of applications."
    ],
    "6913": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "HotelRec is the largest publicly available dataset in the hotel domain (50M versus 0.9M) and additionally, the largest recommendation dataset in a single domain and with textual reviews (50M versus 22M).",
        "reasonable performance could be obtained, but still far from results achieved in other domains in the literature.",
        "We could easily increase the dataset with other languages and use it for multilingual recommendation."
    ],
    "6916": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, it improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "CSA can be naturally applied to tasks defined on graph structure, e.g., classification and clustering on graph(s), logical reasoning, knowledge graph reasoning, to name a few."
    ],
    "6917": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The automatic synthesis process extraction framework using a deep learning-based sequence tagger and rule-based relation extractor can detect entities with high accuracy.",
        "In future work, we will develop a deep learning-based relation extractor that incorporates syntactic information into the model to improve the extraction performance.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages."
    ],
    "6925": [
        "We propose MLS, a supervised approach to construct abstractive summaries at controllable lengths.",
        "Our network is an array of semantic kernels with clearly defined human-interpretable syntactic/semantic roles in constructing the summary given a budget-length.",
        "We train our network on limited training samples from two cross-domain datasets.",
        "Experiments show that the summaries constructed by MLS are coherent and reflectively capture the main concepts of the document.",
        "Our human evaluation study also suggest the same.",
        "In the future, we would like to extend our work to construct task-driven summaries for interactive question answering tasks.",
        "Personalizing a summary based on user's past interaction model is another exciting direction of future work."
    ],
    "6927": [
        "The proposed unified local and global NMT framework can successfully exploit context regardless of the number of sentences in the input.",
        "The model has learned to leverage a larger context, as shown by extensive experimentation and analysis.",
        "The approach has the potential to be extended to other document-level NLP tasks, such as summarization, in future work."
    ],
    "6928": [
        "The best model for document-level sentiment classification depends on the characteristics of the data.",
        "Hierarchical models perform similarly to transfer learning approaches even in low-resource scenarios, challenging the expectation that transfer learning is more effective in such scenarios.",
        "The performance of flat, hierarchical, and transfer learning models for document-level sentiment classification varies depending on the language and amount of training data available."
    ],
    "6929": [
        "We present the first large bimodal pre-trained model for natural language and programming language, CodeBERT.",
        "Finetuning CodeBERT achieves state-of-the-art performance on downstream tasks including natural language code search and code-to-documentation generation.",
        "The probing task is a cloze-style answer selection problem, and curating distractors for both NL and PL parts.",
        "CodeBERT performs better than RoBERTa and a continuously trained model using codes only.",
        "There are many potential directions for further research on this field, including learning better generators with bimodal evidence or more complicated neural architecture to improve the replaced token detection objective.",
        "The loss functions of CodeBERT mainly target on NL-PL understanding tasks, and although CodeBERT achieves strong BLEU scores on code-to-documentation generation, it could be further improved by generation-related learning objectives.",
        "How to successfully incorporate AST into the pretraining step is also an attractive direction.",
        "We plan to apply CodeBERT to more NL-PL related tasks and extend it to more programming languages, and flexible and powerful domain/language adaptation methods are necessary to generalize well."
    ],
    "6930": [
        "The current state-of-the-art models for multi-modal sentiment analysis and emotion detection in conversation are not effective in capturing the context of a conversation and treating each modality independently.",
        "Our RNN architecture consistently performs well on benchmark datasets such as CMU-MOSI and CMU-MOSEI in any multi-modal setting.",
        "The model can be further extended to have better feature extractors, and increase both the number of modalities and the number of participants in the conversation.",
        "The lack of availability of datasets consisting of these extensions with emotion or sentiment labels has limited the scope of our future work."
    ],
    "6934": [
        "The proposed procedure of training encoder-decoder models with softmax function compression can be used for decoding with a variable number of encoder and decoder layers, making it highly versatile.",
        "The cost-benefit analysis of the method shows that it can be used in different latency scenarios, and it is effective in reducing the number of parameters and computations required for decoding.",
        "The first extension proposed in the paper allows for dynamically choosing layer combinations for slightly faster decoding.",
        "The second extension proposed in the paper involves further compressing models using recurrent stacking with knowledge distillation, which can lead to additional speedup in decoding and model compression.",
        "The approach proposed in the paper can be applied to other tasks based on deep neural networks, not just NMT."
    ],
    "6936": [
        "The proposed model significantly outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by the proposed model are coherent with human judgments.",
        "The proposed model can improve the quality of multiturn dialogue generation by using proper detection methods, such as self-attention.",
        "Introducing topical information or considering detailed content information in the relevant contexts can further improve the quality of generated responses.",
        "The proposed framework can be adapted to most notions of context and can be used to study dynamic lensing and other settings such as few-shot learning and non-explicit contextualization.",
        "Comparing the proposed model with other contextualized embedding models that have substantially outperformed mBERT could result in even higher quality universal sentence vectors."
    ],
    "6937": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We achieve new state-of-the-art joint goal accuracy on the updated MultiWOZ 2.1 dataset of 51%.",
        "In the zero-shot setting, we improve the state-of-the-art by over 2%.",
        "It is worth exploring whether the state can be carried from the previous turn to predict the state for the current turn.",
        "It may be useful to capture dependencies or correlations between slots rather than independently generating values for each one of them."
    ],
    "6938": [
        "Neural and cognitive theories provide an imperative for computational models to understand human language by separating representations of word meanings from those of syntax.",
        "Using this constraint, we introduced new neural units that can provide this separation for the purpose of translating human languages.",
        "Our units showed improvements in all of our experiment domains over the typical model.",
        "The domains were a small artificial diagnostic dataset, semantic parsing, syntactic parsing, and English to Mandarin Chinese translation.",
        "We also showed that the model learns a representation of human language that is similar to that of our brains.",
        "When damaged, the model displays the same knowledge distortions that aphasics do."
    ],
    "6940": [
        "The use of large language models pre-trained on unstructured text can attain competitive results on open-domain question answering benchmarks without any access to external knowledge.",
        "The largest models with around 11 billion parameters are needed to obtain state-of-the-art results, which can be prohibitively expensive in resource-constrained settings, motivating future work on more efficient language models.",
        "Open-book\" models provide some indication of what information they accessed when answering a question, providing a useful form of interpretability, while the model distributes knowledge in its parameters in an inexplicable way and hallucinates realistic-looking answers when it is unsure.",
        "The maximum-likelihood objective used to train the model provides no guarantees as to whether a model will learn a fact or not, making it difficult to ensure that the model obtains specific knowledge over the course of pre-training and preventing the explicit updating or removing of knowledge from a pre-trained model.",
        "The tasks used in this paper mainly measure \"trivia\"-style knowledge, and the author is interested in measuring performance on question answering tasks that require reasoning capabilities such as DROP (Dua et al., 2019)."
    ],
    "6943": [
        "We investigate how an RL agent can play and generalize within a distribution of text-based games using graph-structured representations inferred from text.",
        "We introduce GATA, a novel neural agent that infers and updates latent belief graphs as it plays text-based games.",
        "We use a combination of RL and self-supervised learning to teach the agent to encode essential dynamics of the environment in its belief graphs.",
        "We show that GATA achieves good test performance, outperforming a set of strong baselines including agents pre-trained with ground-truth graphs.",
        "This evinces the effectiveness of generating graph-structured representations for text-based games."
    ],
    "6945": [
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Most of the methods exploit the structure of the links between the entities from Wikipedia and propose different ways of capturing the contextual information of a certain entity mention in the text.",
        "Models for vector space alignment of entities across multiple KGs follow supervised learning mechanisms and require a set of pre-aligned entities or triples as initial seeds.",
        "Not all the information encoded in the KGs is exploited in these models, such as multivalued object and attribute relations.",
        "The challenges mentioned in the paper pave the way for promising research directions in alignment of embedding spaces."
    ],
    "6948": [
        "The proposed model significantly outperforms existing HRED models and their attention variants.",
        "The experimental results show that our model achieves better performance on both Chinese customer services dataset and English Ubuntu dialogue dataset.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our model can improve the quality of multiturn dialogue generation by using proper detection methods, such as self-attention.",
        "The proposed model has the potential to enable new breakthroughs in question answering systems and related natural language tasks.",
        "The finetuning of the resulting model on real SQUAD1.1 data boosts the EM score to 89.4, which is a 1.7 point improvement over the fully supervised baseline."
    ],
    "6953": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL exceeded the human baseline for FigureQA, but results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The proposed AdaBERT model adaptively compresses BERT for various downstream tasks using Neural Architecture Search and taskuseful knowledge distillation loss depending on the original BERT.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further."
    ],
    "6954": [
        "The proposed model achieves improved results over some previously proposed architectures on the MS-COCO dataset.",
        "The use of a convolutional neural network and long short-term memory along with fully connected layers to map image and text inputs into a shared feature space.",
        "The introduction of a new dataset of images and tweets extracted from Twitter, which is characteristically different from existing benchmarks.",
        "The lack of standardization in the new dataset, leading to high semantic correlations between image-text pairs.",
        "The existence of a varied number of domains in the extracted dataset, making the task of image-text matching more challenging."
    ],
    "6955": [
        "The proposed GRET model can effectively solve the problem of how to model and use global contextual information in Transformer models, leading to improved generation quality.",
        "The GRET model can generate an external state by the encoder containing global information and fuse it into the decoder dynamically, which improves the ability of the model to capture long-range dependencies and contextual information.",
        "Experimental results on four translation tasks and one text summarization task demonstrate the effectiveness of the proposed approach, showing that the GRET model can improve the generation quality compared to the state-of-the-art Transformer model.",
        "The use of a global state from the GRET model can lead to higher precision in all conditions, indicating that the proposed method can obtain more information about the output sentence and contribute to improved generation performance.",
        "In the future, the authors plan to combine the proposed approach with methods for enhancing local representations to further improve generation performance."
    ],
    "6956": [
        "FONDUE outperforms the state-of-the-art when it comes to the accuracy of identifying ambiguous nodes, by a substantial margin and uniformly across a wide range of benchmark datasets.",
        "The computational cost of FONDUE is slightly higher than the best baseline method, but the difference is moderate.",
        "Using an extensive experimental pipeline, we empirically demonstrated that FONDUE outperforms the state-of-the-art when it comes to the accuracy of identifying ambiguous nodes.",
        "The boost in ambiguous node identification accuracy was not observed for the node splitting task.",
        "A combination of FONDUE for node identification, and Markov clustering on the ego-networks of ambiguous nodes for node splitting, is the most accurate approach to address the full node disambiguation problem."
    ],
    "6958": [
        "'Our approach achieves new state-of-the-art on FewRel 2.0 dataset.'",
        "'We first train a representation extractor with the Clustering Promotion Mechanism.'",
        "'Two methods are combined by the proposed Cosine Annealing Strategy.'",
        "'The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.'",
        "'ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.'",
        "'Simplifying the encoder self-attention of Transformer-based NMT models can reduce parameter footprint at training time without degrading translation quality.'",
        "'In low-resource scenarios, translation quality can be improved with simplified encoder self-attention.'",
        "'Only adjacent and previous token attentive patterns contribute significantly to the translation performance.'",
        "'The trainable encoder head can be disabled without hampering translation quality if the number of decoder layers is sufficient.'",
        "'Encoder attention heads based on locality patterns are beneficial in low-resource scenarios, but may affect the semantic feature extraction necessary for addressing lexical ambiguity phenomena.'"
    ],
    "6960": [
        "The beam problem in BERT can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The proposed approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The SDA models are generally worse than SDV models on NLI tasks.",
        "Parameter averaging is worse than logits voting when dealing with difficult tasks such as NLI, but better on simple tasks such as text classification.",
        "Our proposed fine-tuning strategies can improve BERT without significantly decreasing the training efficiency.",
        "The self-ensemble method with parameter averaging can improve BERT without significantly decreasing the training efficiency.",
        "With self-distillation, the student and teacher models can benefit from each other.",
        "Our proposed strategies are orthogonal to the approaches with external data and knowledge."
    ],
    "6961": [
        "The proposed approach achieves state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor used in the approach is effective in encoding unlabeled target-domain data into features.",
        "The Cosine Annealing Strategy improves the domain adaptation performance.",
        "The approach reduces the exposure bias effect and improves the overall performance.",
        "The novel sequence decoding approach optimizes on the data distribution rather than on external metrics.",
        "The approach achieves improvements over the state-of-the-art in abstractive summarization.",
        "The approach does not require any costly generator retraining.",
        "The approach can be applied to other tasks such as machine translation and dialogue systems in future work."
    ],
    "6964": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model of part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, leading to improved performance.",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "The use of wordnet to supersede the requirement of semantic memory is effective in improving the knowledge structure.",
        "Evaluation of the knowledge structure was presented to establish the claim, and looking at ways to deal with topics not seen before is part of an ongoing research."
    ],
    "6965": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The performance of the model based on gaze features and part-of-speech information was comparable to that of state-of-the-art systems, without the need for text processing.",
        "The use of late gaze features and disambiguation effort indicators improved the accuracy of the model.",
        "The model was able to achieve accuracy similar to that of linguistic-based models and state-of-the-art systems without the need for text processing, which suggests that gaze features can be a useful source of information for disambiguating categories.",
        "The results suggest that using late gaze features and disambiguation effort indicators can improve the accuracy of the model, and that gaze features can be a useful source of information for disambiguating categories.",
        "In the future, we will focus on extending our work to improve the multimodal reasoning capability between language, visual, and audio features."
    ],
    "6968": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets."
    ],
    "6974": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Sparse Sinkhorn Attention is a new efficient and sparse method for attention computation that demonstrates utility on a multitude of large-scale generative modeling and classification tasks.",
        "Our proposed Sinkhorn Transformer outperforms or remains competitive to vanilla Transformer and sparse Transformer models on a multitude of applications while being memory efficient."
    ],
    "6975": [
        "The proposed models outperform state-of-the-art models by a huge margin for the binary classification of co-hyponyms vs. hypernyms, as well as co-hyponyms vs. meronyms.",
        "Network representations can be very effectively utilized for a focused task like relation extraction.",
        "The use of distributional thesaurus embeddings obtained using network representation learning can improve the detection of co-hyponymy.",
        "The proposed method can distinguish a horizontal relation, co-hyponymy, from parent-child relations like hypernymy and meronymy.",
        "Investigating more sophisticated network representation learning techniques, such as path embedding and community embedding techniques, could improve the robust detection of lexical relations.",
        "The focus of this study is on distinguishing a horizontal relation, co-hyponymy, from parent-child relations like hypernymy and meronymy, but investigating the distinction of two analogous sibling relations, co-hyponymy and co-meronymy, could be an interesting future direction.",
        "The broad objective is to build a general supervised and unsupervised framework based on complex network theory to detect different lexical relations from a given corpus with high accuracy."
    ],
    "6976": [
        "The proposed complete system for video captioning includes a novel model and a training strategy, which can acquire more detailed and interactive object features through relational reasoning.",
        "The novel TRL introduces external language model to guide the caption model to learn abundant linguistic knowledge, which is the supplement of the common TEL.",
        "The system has achieved competitive performances on MSVD, MSR-VTT, and VATEX datasets.",
        "The experiments and visualizations have demonstrated the effectiveness of the proposed methods."
    ],
    "6978": [
        "The proposed Universal Phonemic Model achieves better performance than the baseline for automatic phonemic transcription tasks, with an average improvement of 7.7% in phoneme error rate for 7 languages.",
        "The approach has the potential to pave the way for zero-shot learning of speech recognition with a new framework.",
        "The proposed model outperforms the baseline for real-world production systems.",
        "The performance of the approach is still not enough for real-world production systems, but it shows promising results and opens up new possibilities for tackling zero-shot learning of speech recognition."
    ],
    "6980": [
        "The proposed Echo State NMT models can reach 70-80% performance without training the major components.",
        "The surprising findings encourage us to rethink the nature of encoding and decoding in NMT.",
        "The model based on the recurrent network architecture has potential for economic model architectures and training procedures.",
        "Applying randomized algorithms to non-recurrent architectures like Transformer is a possible research direction.",
        "Randomized feedforward networks like Extreme Learning Machine have been shown to be effective, suggesting that randomization can be a useful technique for NMT as well."
    ],
    "6982": [
        "Our approach achieves new state-of-the-art performance on FewRel 2.0 dataset.",
        "The proposed Cosine Annealing Strategy combines two methods to improve domain adaptation.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The use of pseudo-labeled target-domain data and labeled source-domain data improves the domain adaption performance.",
        "Our approach significantly improves the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks."
    ],
    "6983": [
        "Our approach can reduce the error due to syntactic differences between the source and target languages by addressing intraphrase and inter-phrase syntactic differences separately.",
        "The approach helps to reduce the error by addressing the syntactic differences between the source and target languages when chunkers are available for both languages.",
        "The approach can achieve competitive performance compared to previous systems by using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The approach can be applied when chunkers are available for both languages."
    ],
    "6987": [
        "The proposed exploratory architecture TCAN for sequence modeling can integer internal correlative features under the condition of satisfying sequential characteristics.",
        "The enhanced residual utilizes the weight of temporal attention to emphasize the importance of certain time step, improving performance without adding parameters.",
        "The model outperforms prior state-of-the-art models on WikiText-2, word-and character-level PTB datasets by a significant margin."
    ],
    "6996": [
        "The proposed adaptive multi-curricula dialogue learning framework can gradually proceed from easy samples to more complex ones in training, enabling the dialogue models to learn more effectively.",
        "The five conversational attributes (complexity and easiness of dialogue samples) are used to analyze the learning status of the model and choose different curricula at different training stages.",
        "The proposed learning framework is able to boost the performance of existing dialogue systems, as shown by extensive experiments on three large-scale datasets and five state-of-the-art conversation models."
    ],
    "7006": [
        "\"HCEDS achieves the best performance in the automatic evaluation.",
        "\"The NLU module gains better understanding of utterances by modeling token-level and sentence-level context information, which significantly improves the performance of domain-intent classification and slot labeling on the MultiWOZ dataset.",
        "\"Our proposed HCEDS explores the potential of hierarchical contextual information from a multi-domain dialogue system.",
        "\"The system achieves second place in the manual evaluation.",
        "\"There has been a significant accuracy raise lead by the engineering improvement in policy."
    ],
    "7007": [
        "We introduce CLUECorpus2020, a large-scale corpus that can be used directly for the pre-training language model in Chinese.",
        "It is the first well-defined large-scale public available dataset that serves the purpose of the pre-training language model in Chinese.",
        "Our dataset has good quality and huge potential.",
        "We conduct experiments on a small portion of this new dataset and Chinese Wiki, and the results prove that our dataset has good quality and huge potential.",
        "With our corpus and vocabulary, our model is able to match state-of-the-art performance in Chinese.",
        "We also observe that transfer learning is useful among similar tasks, and it can boost performance.",
        "We focus on pre-training, especially for language understanding, but this dataset can also be used for language generation and other NLP tasks."
    ],
    "7008": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The first meta-embeddings mechanism based on self-attention improves the performance of language modelling by exploiting more than one word-embedding.",
        "A single-layer Duo Classifier can achieve state-of-the-art results on many public benchmarks for text classification tasks.",
        "The meta embedding mechanism benefits the vanilla transformer in terms of better performance and faster convergence.",
        "There is potential for this mechanism beyond text classification tasks, and further investigations are needed in this field."
    ],
    "7009": [
        "The combination of all tasks is effective and achieves stronger performance on all evaluation metrics.",
        "The pre-training tasks are complementary to each other.",
        "XGPT outperforms state-of-the-art models by a significant margin after in-domain and out-of-domain pre-training.",
        "Extending XGPT to cross-modal understanding tasks, such as VQA and VCR, is a promising future work."
    ],
    "7010": [
        "'Our proposed method with multi-task learning can significantly boost the performance of CER, leading to a new state-of-the-art in the literature.'",
        "'The proposed method can capture speaker-related information, which is important for CER.'",
        "'Our baseline with BERT as backend is a strong one, achieving the best performance compared with previous state-of-the-art.'",
        "'The MTL based method can improve the performance of CER by bridging two tasks for mutual interaction.'",
        "'Detailed experiments showed that our suggested components for MTL are both important.'",
        "'Our analysis of the proposed model provides comprehensive understanding of its performance and effectiveness.'"
    ],
    "7016": [
        "The brevity problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model has some limitations and room for improvement, such as the nature of the relationship-dependent content masking.",
        "Applying a filter to modify the list of predicted target entities can help improve the overall performance."
    ],
    "7021": [
        "The proposed reward function based on contextual BERT embeddings employs embedding-based similarity instead of count-based similarity, and provides per-word contributions by design.",
        "The clustering approach condenses the training corpus into a set of BERT-grams, allowing efficient reward assignment independent of the corpus size.",
        "The proposed reward function confirms the expressiveness and versatility of contextualized embeddings.",
        "Aggressive clustering of word vectors maintains the properties of contextualized embeddings.",
        "The underlying REIN-FORCE training methodology has limits when employing the reward as learning signal in unconditional generation, leading to similar modes of collapse as found in GAN training."
    ],
    "7022": [
        "The hypothesis-only bias in trained NLI models mainly comes from unevenly distributed surface patterns.",
        "The attempts to mitigate the bias are meaningful as such bias not only makes NLI models fragile to adversarial examples.",
        "We try to mitigate this bias by removing those artificial patterns in the training sets, with experiments showing that it is a feasible way to alleviate the bias under proper down-sampling methods.",
        "We also show that adversarial debiasing with the guidance from the harvested artificial patterns is a feasible way to mitigate the hypothesis-only bias for sentence vector-based NLI models."
    ],
    "7023": [
        "Adapting models from the general-domain makes them better teachers.",
        "Distilling using general-domain data does not impact a model's adaptability.",
        "Distilling twice for best results: once in the general-domain to possibly improve student performance, and again using an adapted in-domain teacher.",
        "The results are robust among multiple language pairs, student sizes, in-domain settings."
    ],
    "7027": [
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse')",
        "the model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets",
        "EmpTransfo a multi-head Transformer model is an empathetic aware dialog system to interact with users with higher quality in terms of coherence, relevance, and emotion",
        "the proposed approach requires meta information of emotion, action, and topic in order to respond with the proper emotions"
    ],
    "7031": [
        "Our method achieves high accuracy of fine-grained classification with limited annotated data.",
        "The result implies a positive aspect of transfer learning for real information extraction scenarios.",
        "Investigating more final layers for classification is a possible direction for future research.",
        "Adding heuristic features generated from humans can improve the performance of our model.",
        "Replacing CNN with deeper structures, such as CNN-BiLSTM, can potentially improve the model's performance."
    ],
    "7032": [
        "The proposed Morfessor EM+Prune algorithm reduces search error during training, leading to models with lower Morfessor costs.",
        "The use of Morfessor EM+Prune leads to improved accuracy in comparison to linguistic morphological segmentation.",
        "The Morfessor costs of the proposed algorithm are lower than those of the Morfessor Baseline."
    ],
    "7034": [
        "We propose a novel method for constructing a commonsense knowledge graph using conceptualization.",
        "Our method generates new triples with conceptualization and examines them using a discriminator transferred from pretrained language models.",
        "Future studies will focus on strategies of conceptualization and its role in natural languages and commonsense through deep learning approaches.",
        "We introduce the use of a discriminator transferred from pretrained language models to examine the generated triples and evaluate their quality.",
        "Our approach leverages the power of deep learning techniques to improve the construction of commonsense knowledge graphs."
    ],
    "7035": [
        "SMDN is a simple yet effective post-processing method for detecting unknown intent in dialogue systems.",
        "The proposed SofterMax calibrates the confidence of softmax output with temperature scaling to reduce the open space risk in probability space.",
        "The method combines traditional novelty detection algorithm, LOF, with feature representations learned by the deep neural network.",
        "The method can yield significant improvements compared with state-of-the-art baselines on three benchmark datasets.",
        "The method applies to images and can be used to distinguish unknown intent from known intents even if their semantics meanings are highly similar.",
        "The method can be easily applied to a pre-trained deep neural network classifier without any changes in the model architecture."
    ],
    "7041": [
        "ESBM overcomes the limitations of available benchmarks discussed in Section 2.",
        "It contains 175 entities which is 2-3 times as large as available benchmarks [22, 7, 8] .",
        "In ESBM, property values are not filtered as in [7, 8] but can be any entity, class, or literal.",
        "Different from the task-specific nature of [22] , ESBM provides general-purpose ground-truth summaries for evaluating general-purpose entity summarizers.",
        "Besides, ESBM meets the seven desiderata proposed in [18] as follows.",
        "However, ESBM has its own limitations, which we will discuss in Section 6."
    ],
    "7042": [
        "DeepLENS achieves new state-of-the-art results on the ESBM benchmark, significantly outperforming existing methods.",
        "Entity summarization becomes another research field where a combination of deep learning and knowledge graph is likely to shine.",
        "The current model only exploits textual semantics, leaving room for improvement by incorporating ontological semantics in future work.",
        "Revisiting the usefulness of structural semantics may also be beneficial for improving the model's performance."
    ],
    "7043": [
        "Existing sentence embedding models exhibit regularities with regard to sentence analogies, but are not good at capturing semantic regularities.",
        "BERT-style contextual embeddings are successful in recognizing syntactic analogies based on lexical ones, but do not always translate into better regularities in the vector space of fixed-length sentence embeddings.",
        "More training data and model parameters do not necessarily yield better results for sentence embedding models.",
        "Word vector averages or a Discrete Cosine Transform of word embeddings can outperform more complex sentence embedding models."
    ],
    "7045": [
        "We introduce a novel framework to characterize machine reading comprehension gold standards.",
        "Our framework has potential applications when comparing different gold standards, considering the design choices for a new gold standard, and performing qualitative error analyses for a proposed approach.",
        "We reveal issues with their factual correctness, show the presence of lexical cues, and observe that semantics-altering grammatical modifiers are missing in all of the investigated gold standards.",
        "Studying how to introduce those modifiers into gold standards and observing whether state-of-the-art MRC models are capable of performing reading comprehension on text containing them is a future research goal.",
        "A future line of research is to extend the framework to be able to identify the different types of exploitable cues such as question or entity typing and concrete overlap patterns.",
        "Investigating gold standards under this framework where MRC models outperform the human baseline (e.g. SQUAD) will contribute to a deeper understanding of the seemingly superb performance of deep learning approaches on them."
    ],
    "7048": [
        "model robustness to undersensitivity attacks can be drastically improved using appropriate defences",
        "data augmentation and adversarial training can improve model robustness without sacrificing in-distribution test set performance",
        "in more detail, the causes and better defences to model undersensitivity",
        "a model's RC capabilities can be improved by addressing the causes of undersensitivity",
        "using appropriate defences, we can improve a model's ability to generalize to semantically altered questions"
    ],
    "7049": [
        "We constructed a new video caption dataset for describing human actions in Japanese.",
        "The advantage of this dataset is that the captions are written in Japanese and specify 'who does what and where'.",
        "Our dataset, consisting of 79,822 videos and 399,233 captions, is the first Japanese caption dataset, and the largest video caption dataset in any language with respect to the number of captions.",
        "We evaluated two approaches based on a multi-modality fusion caption generation method on our dataset: Sentencewise and phrase-wise approaches.",
        "Experiments showed that the phrase-wise approach outperformed the sentencewise approach with respect to BLEU and CIDEr.",
        "In addition, we evaluated phrase generation quality using our dataset, employing phrase generation tasks to ascertain whether the generation methods specified 'who does what and where'.",
        "We observed that the image and motion modalities were useful in explaining PERSON and ACTION, while image modality alone was sufficient for PLACE."
    ],
    "7051": [
        "ReZero outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Applying ReZero to various residual architectures -fully connected networks, Transformers and ResNets -we observed significantly improved convergence speeds.",
        "We were able to efficiently train Transformers with hundreds of layers, which has been difficult with the original architecture.",
        "Patterns of residual weights can be crucial to understand the training dynamics of such deeper networks and might be important to model performance, which we will explore in future work."
    ],
    "7053": [
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Inductive transfer learning methods are very useful for social media flood detection data with minimal labeled data.",
        "We used Queensland Twitter data as one of the flood locations and used the pretrained model ULMFiT to successfully classify with accuracy 95% the flood-related tweets with only 5% of labeled target samples under 10 seconds.",
        "The usage of pre-trained models with minimal space and time complexity can be a huge advantage to the time-sensitive application where we need to process millions of tweets efficiently and classify them accordingly with high performance without compromising on the accuracy."
    ],
    "7054": [
        "The beam problem can largely be explained by the brevity problem.",
        "We have explored simple and effective ways to alleviate or eliminate the beam problem.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The response concepts learnt by the model are indeed quite meaningful.",
        "Mask & Focus is able to capture important concepts and generate a topically coherent response.",
        "The proposed Mask & Focus model discovers interesting concepts from challenging datasets that fail to be discovered by manual annotation.",
        "Mask & Focus achieves significant improvement in performance over existing baselines for conversation modelling with respect to several metrics."
    ],
    "7056": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset using inductive unsupervised domain adaption.",
        "The proposed Cosine Annealing Strategy combines Similarity Entropy Minimization and Adversarial Distribution Alignment to promote clustering and align similar class distribution across domains.",
        "The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset."
    ],
    "7058": [
        "The state-of-the-art model, BERT, is sensitive to keyboard typos.",
        "Different types of words have different sensitivities to keyboard typos.",
        "Considering the robustness of neural models is necessary for NLP tasks.",
        "The authors will release their code to reproduce their results.",
        "Future work may involve making subword-based models more robust to human typos in NLP tasks."
    ],
    "7059": [
        "The proposed method for scoping relevant information within emails and the impact of the model on a suite of tasks is effective.",
        "The model outperforms existing publicly available baselines and generalizes better on real-world use-cases for the task of Signature Detection.",
        "The model works well with emails and has potential for other tasks that process large textual inputs, such as document classification and sentiment analysis.",
        "Using BERT for inference poses latency challenges in a production system, and leveraging distilled versions of BERT may be a promising direction for future work.",
        "The proposed method for scoping relevant information within emails and the impact of the model on a suite of tasks is effective. [Paragraph 1]",
        "The model outperforms existing publicly available baselines and generalizes better on real-world use-cases for the task of Signature Detection. [Paragraph 2]",
        "The model works well with emails and has potential for other tasks that process large textual inputs, such as document classification and sentiment analysis. [Paragraph 3]",
        "Using BERT for inference poses latency challenges in a production system, and leveraging distilled versions of BERT may be a promising direction for future work. [Paragraph 4]"
    ],
    "7062": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to substantial BLEU gains.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "The proposed multi-task masking strategy significantly contributes to all downstream tasks without discrimination to different encoders."
    ],
    "7064": [
        "The proposed attentive dual encoder achieves better dialogue context-response prediction results compared to existing methods.",
        "The attention mechanism integrated into the model improves the performance and shows the word importance during the prediction.",
        "The use of mutual information to constrain unimportant words in the dialogue context improves the word-level interpretability.",
        "The addition of a residual layer between encoded sentence features and raw word embeddings provides more fine-grained information on the word level, with little effect on the prediction."
    ],
    "7065": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our evaluation verifies the effectiveness of our method, while also indicating a scope for further study, enhancement, and extensions in the future.",
        "Our dataset creation methodology is a three-step procedure as shown in Figure 9.",
        "The annotators also convert a subset of our dataset into complete sentence descriptions."
    ],
    "7066": [
        "The proposed approach incorporates document context inside a trained sentence-level neural machine translation model using self-training.",
        "Self-training improves sentence-level baselines by up to 0.93 BLEU.",
        "The proposed approach demonstrates strong preference in human evaluation study.",
        "Self-training achieves higher improvement on longer documents.",
        "Using better sentence-level models can improve the results.",
        "The approach only uses self-training on source-to-target NMT models to capture the target side document context.",
        "Investigating the application of self-training on both target-to-source and source-to-target sentence-level models could incorporate both source and target document context into generated translations.",
        "The work motivates novel approaches for making trained sentence-level models better suited for document translation at decoding time."
    ],
    "7068": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "We showed that the method works for the static GloVe embeddings.",
        "Our approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The brevity problem leads to significant BLEU gains.",
        "Solving label bias in general may lead to more improvement.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The model is the first that performs entity linking without any pipeline or any heuristics, compared to all prior approaches.",
        "With our approach, we can learn additional entity knowledge in BERT that helps in entity linking.",
        "Almost none of the downstream tasks really required entity knowledge, which is an interesting observation and an open question for future research."
    ],
    "7070": [
        "Incorporating the heterogeneous context and the local neighborhood information results in a better performance for the query answering task.",
        "The type information is important for faster convergence and finding more diverse paths.",
        "The neighborhood information improves the performance on unseen queries.",
        "We plan to explore more efficient strategies for action-space pruning to improve the scalability of existing RL solutions.",
        "We plan to develop more effective type embeddings considering the hierarchical structure of the type information."
    ],
    "7073": [
        "The existing corpora for detecting abusive messages in Wikipedia talk pages have weaknesses such as lack of conversational structure and inaccurate annotations.",
        "Combining WCC and WikiConv can compensate for their individual drawbacks and provide a more comprehensive dataset for abuse detection.",
        "There is no standard protocol for evaluating the performances of abuse detection tools, which hinders comparative overviews of proposed approaches and progress in the research area.",
        "None of the listed works provide an open-source version of their code, making it difficult to compare and evaluate different approaches."
    ],
    "7076": [
        "The proposed semantic query expansion framework outperforms a widely used baseline model - Lucene's practical scoring function model.",
        "The proposed approach enriches a user's query from both geospatial and thematic perspectives.",
        "The proposed approach improves the effectiveness of geographic information retrieval systems.",
        "There is room for improvement in the efficiency of the presented semantic query expansion framework.",
        "Other ways to measure spatial similarity, such as Space2Vec (Mai et al. 2020 ), may be investigated in future research.",
        "The added geospatial aspect of GIR may affect the way how we evaluate the system."
    ],
    "7078": [
        "The proposed Cross-modal Memory Network (CMN) can tackle the challenging task of visual dialogue navigation by exploring cross-modal memory of the agent.",
        "The language memory in CMN can help the agent better understand the responses from the oracle based on the communication context.",
        "The visual memory in CMN aims to explore visually grounded information on the previous navigation path, providing temporal correlations for the views.",
        "Benefiting from the collaboration of both visual and language memory, CMN achieves constant improvement over popular benchmarks on visual dialogue navigation, especially when generalizing to unseen environments."
    ],
    "7080": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "The solution to the brevity problem is a very limited form of globally-normalized models for NMT.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The ConMask model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Applying a filter to modify the list of predicted target entities can improve the overall performance of the model."
    ],
    "7086": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The OGTD dataset is facilitating in offensive language detection with a significant success for Greek, taking into consideration its size and label distribution.",
        "The linear SVM model achieved the best results among classical machine learning approaches."
    ],
    "7087": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The Cosine Annealing Strategy is proposed to combine the two methods.",
        "The approach uses pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "Experimental results demonstrate that the approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The approach can be improved by reducing the noise of pseudo labels to improve domain adaptation performance.",
        "The study also explores cross-lingual and multilingual approaches in end-to-end personalized dialogue modeling.",
        "The XPersona dataset is presented as a multilingual extension of Persona-Chat for evaluating multilingual personalized chatbots.",
        "Cross-lingual and multilingual baselines are provided and compared with monolingual and two-stage translation approaches.",
        "Extensive automatic evaluation and human evaluation were conducted to examine the models' performance."
    ],
    "7090": [
        "The use of different modalities in computer vision is still an underrepresented topic and deserves more attention.",
        "The audio and speech modalities are important for dense video captioning tasks.",
        "A multi-modal dense video captioning module (MDVC) based on the transformer architecture outperforms visual-only models in the existing literature.",
        "Extensive ablation study verifies the superior performance of a captioning module using multiple modalities.",
        "Future works in video captioning should utilize a multi-modal input."
    ],
    "7091": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?"
    ],
    "7092": [
        "Posterior calibration is one lens to understand the trustworthiness of model confidence scores.",
        "Pre-trained Transformers, such as BERT and RoBERTa, can achieve low ECEs in-domain with temperature scaling.",
        "When trained with label smoothing, these models are also competitive out-of-domain.",
        "In-domain and out-of-domain settings can be used to examine the calibration of pre-trained Transformers."
    ],
    "7096": [
        "parity is globally achieved on the whole, but interactions with other corpus characteristics reveal that gender misrepresentation needs more than just a number of speakers to be identified.",
        "In non-elicited data (meaning type of speech that would have existed without the creation of the corpus, such as TEDTalks or radio broadcast), we found that, except in Librispeech where gender balance is controlled, men are more represented than women.",
        "It also seems that most of the corpora aimed at developing TTS systems contain mostly female voices, maybe due to the stereotype associating female voice with caring activities.",
        "We also observe that gender description of data has been taken into account by the community, with an increased number of corpora provided with gender meta-data in the last two years.",
        "Our sample containing only 66 corpora, we acknowledge that our results cannot necessarily be extended to all language resources, however it allows us to open discussion about general corpus description practices, pointing out a lack of meta-data and to actualise the discourse around the social implications of NLP systems."
    ],
    "7101": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Providing lexical information to parsing improves the accuracy of tagging and dependency parsing.",
        "The performance of SCITE is limited by the lack of high-quality annotated data."
    ],
    "7104": [
        "The proposed improvements to the self-attention mechanism (NSA and GSA) provide solid improvements over strong baselines on various NLP tasks, including image captioning, video captioning, machine translation, and visual question answering.",
        "The Normalized Self-Attention (NSA) mechanism reduces internal covariate shift problem inside the self-attention module.",
        "The Geometry-aware Self-Attention (GSA) explicitly and dynamically computes the geometric bias between objects to benefit image understanding.",
        "The proposed methods have been extensively experimented on MS-COCO image captioning dataset, validating their effectiveness.",
        "The proposed methods are general and significant improvements over vanilla self-attention module on various NLP tasks."
    ],
    "7110": [
        "Our neural compound splitter outperforms two well-known methods, CharSplit, and SECOS.",
        "We present a two-stage approach to compound splitting and idiomatic compound detection in German.",
        "Our approach exploits a common technique for compositionality detection to detect idiomatic compounds.",
        "The suggested approach to idiomatic compound detection has proof of concept nature and can be improved with other word embedding models that encode not only distributional but also structural features.",
        "Using other word embedding models, such as Poincare embeddings or contextual embedding models like ELMo or BERT, could improve the approach to idiomatic compound detection."
    ],
    "7111": [
        "Our approach constrains the probability of the output conditional on the rationales being the same across multiple environments.",
        "The framework consists of three players, which competitively rule out spurious words with strong correlations to the output.",
        "We theoretically demonstrate the proposed game-theoretic framework drives the solution towards better generalization to test scenarios that have different distributions from the training.",
        "Extensive objective and subjective evaluations on both synthetic and multi-aspect sentiment classification datasets demonstrate that INVRAT performs favorably against existing algorithms in rationale generation."
    ],
    "7113": [
        "The proposed method, Sparse Adaptive Connection (SAC), is competitive with state-of-the-art models on various tasks such as neural machine translation, language modeling, graph classification, and image classification, while reducing memory costs.",
        "The SAC method adapts the self-attention mechanism to downstream tasks by using an LSTM edge predictor to construct edges for self-attention operations, allowing for control of how sparse the self-attention is through the sparse coefficient \u03b1.",
        "The SAC method reduces memory costs compared to state-of-the-art models."
    ],
    "7119": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; more general globally-normalized models can be trained in a similarly inexpensive way.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "It is possible to produce low latency online recognition results on the Switchboard task without a significant decrease in performance using S2S attention-based models."
    ],
    "7121": [
        "Our proposed end-to-end system for entity typing is competitive with state-of-the-art mention-level entity typing models.",
        "Our mention-level model outperforms state-of-the-art mention-level entity typing models.",
        "Our end-to-end model (E2EET) effectively determines the type(s) of all tokens in a sentence.",
        "Our end-to-end model performs well on clean datasets and is capable of outperforming the mention-level model despite not knowing which tokens in each sentence are entities.",
        "In future, we plan to run ten-fold cross validation to ensure statistical significance in the experiments.",
        "It would be interesting to investigate the effectiveness of other recent context-dependent embeddings such as XLNet [17] , transformers to replace Bi-GRU for context representation learning, and to incorporate hierarchical encoding techniques to improve prediction accuracy."
    ],
    "7122": [
        "The Edit Transformer can function cross-domain, applying edits learned on a source domain with plentiful data to a data-constrained target domain.",
        "The Edit Transformer is faster to train and use than the Neural Editor that precedes it.",
        "The synthetic data generated by the Edit Transformer helps in downstream tasks, such as training a subjectivity classifier.",
        "The results on several well-known data augmentation tasks show that the Edit Transformer generates more effective synthetic data than previous methods.",
        "The Edit Transformer is an important element in the text Data Augmentation toolbox."
    ],
    "7127": [
        "We analyze the problem of VOG which aims to localize the referred objects in a video given a language query.",
        "Semantic-role labeling systems can be used to sample contrastive examples.",
        "The model views the contrastive samples as a whole video so that the model explicitly learns object relations.",
        "We further propose an additional self-attention layer to capture language dependent object relations along with a relative position encoding.",
        "Our proposed model VOGNet on our dataset ActivityNet-SRL which emphasizes the role of object interactions."
    ],
    "7130": [
        "The proposed method, rAIRL, addresses the reward ambiguity problem in image captioning by disentangling reward for each word in a sentence.",
        "The rAIRL method achieves stable adversarial training by refining the loss function to shift the generator towards Nash equilibrium.",
        "The incorporation of mode control technique mitigates mode collapse.",
        "The proposed method can learn compact reward through extensive experiments on MS COCO and Flickr30K."
    ],
    "7141": [
        "The beam problem in neural machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset, and we hope to see it included in stronger baseline NMT systems.",
        "Solving the brevity problem leads to significant BLEU gains, but there is still room for improvement by solving label bias in general.",
        "Our method for attenuating biases in word representations using a projection-based approach works for static GloVe embeddings and can effectively attenuate bias in contextualized embeddings without loss of entailment accuracy.",
        "The introduction of video-and-language inference (VIOLIN) as a new task provides an opportunity for developing stronger models to push the state of the art on multimodal inference.",
        "There is a significant gap between baseline models and human performance in the VIOLIN task, providing a challenge for future research.",
        "Localizing key frames and better utilizing the alignment between video and subtitles could improve reasoning ability in multimodal inference."
    ],
    "7143": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We introduce a new user embedding framework, Author2Vec, based on BERT, which was pretrained on an novel unsupervised objective: 'Authorship' classification.",
        "The clear clusters shown in visualization and the classification F1-score of 93.3% suggested that the Author2Vec embedding successfully encodes features of social media users.",
        "Pre-trained Author2Vec embedding outperformed all the baseline embedding methods including LSI, LDA, and Word2Vec."
    ],
    "7144": [
        "The attention mechanism does not have internal states to keep track of attentions previously paid, which can lead to repeated similar attentions and increase the chance of repetition.",
        "Using pre-attention as part of the input to RNNs enables the model to keep track of previous attention status.",
        "Highway connections can be used to more effectively combine context and step-wise input than simple concatenation and linear projection.",
        "The transformer model (Vaswani et al., 2017) already has an analogous architecture as our pre-attention, but applying highway connections to the transformer attention outputs has not been successful in reducing repetitive generations.",
        "The lack of temporal inner states in transformers may be a drawback in reducing repetition, and more work needs to be done to mitigate this issue.",
        "There are more fundamental reasons for the repetition problem."
    ],
    "7145": [
        "CORD-NER will be constantly updated based on the incremental updates of the CORD-19 corpus and the improvement of our system.",
        "We will also build text mining systems based on the CORD-NER dataset with richer functionalities.",
        "We hope this dataset can help the text mining community build downstream applications for the COVID-19 related tasks.",
        "We also hope this dataset can bring insights for the COVID-19 studies on making scientific discoveries."
    ],
    "7149": [
        "We have designed an elaborate two-stage learning algorithm over the interaction-augmented KG.",
        "The update in both stages is directed.",
        "The first stage propagates entity semantics to user nodes, which aims to learn entity-oriented user preference; the second stage collects the learned user preference at the target entity, which aims to learn preference-enhanced entity representations.",
        "When the involved weight parameters are fixed, it can be proved that p h is indeed a linear combination of user preference representations (learned in the first stage), given the fact that we aggregate the information by layer and start from the first layer of user nodes.",
        "It can be formally given as: EQUATION ) where \u1e7du is the user embeddings learned in the first stage (Eq. 2), and w h,u (set to zero for unactivated users) can be computed according to the accumulative attention coefficients along the paths from user u to target entity h.",
        "Besides the learned semantic representation v h , we enhance the entity representation using the entity-level preference of the users with high-order connectivity."
    ],
    "7151": [
        "The Variational Transformer (VT) outperforms baselines in terms of diversity, semantic relevance, and human judgment.",
        "The Global Variational Transformer (GVT) incorporates a global latent variable as additional input to the transformer decoder.",
        "The Sequential Variational Transformer (SVT) generates latent variables for each position during the decoding process.",
        "The pre-training language models (Radford et al., 2019) can be used as the backbone to strengthen the language model of the VT for better generation."
    ],
    "7152": [
        "Conditioning both top-down and bottom-up visual processing on language is beneficial for grounding language to vision.",
        "Our proposed generic architecture with explicit bottom-up and top-down visual branches for vision-language problems involving dense prediction demonstrates the best results.",
        "Conditioning the bottom-up branch on language plays a vital role in processing color-dependent input language.",
        "The language-guided image colorization experiments demonstrated similar conclusions, with the bottom-up baseline failing to colorize target objects due to the absence of color information in the input images.",
        "Our model's performance decreases in the presence of transparent and/or reflective objects.",
        "Our current model is limited to integrated vision and language."
    ],
    "7156": [
        "The abstractive text summarization model incorporates convolutional self-attention in BERT, leading to improved performance compared to state-of-the-art systems using ROUGE scores.",
        "The model is suitable for languages other than English and domains other than the CNN/Daily Mail data set, as demonstrated by its application to the German SwissText data set.",
        "The model can handle texts longer than BERT's window size (limited to 512 WordPiece tokens), using a cascading approach that is effective for longer input texts.",
        "The source code of the system is publicly available, and it is being integrated as a summarization service in platforms such as Lynx, QURATOR, and European Language Grid."
    ],
    "7157": [
        "The authors have developed a new corpus for biomedical natural language processing, which focuses on five distinct entity types and includes relationships between entities.",
        "The corpus is the first of its kind and poses new challenges for NLP methods due to its discontinuous nature and high number of nested and multi-label entities.",
        "The corpus can serve as a valuable source for insights into what entities \"look like in the wild\" and can be used as a playground for new modelling techniques such as the resolution of discontinuous entities and multi-task learning.",
        "The authors have evaluated four distinct NER systems on the corpus, which serve as robust baselines for future work but are unable to solve all the complex challenges posed by the dataset.",
        "The authors are currently integrating a functional service based on the presented corpus into the QURA-TOR platform."
    ],
    "7158": [
        "We introduced a novel method to define and extract topological features from word embedding representations of corpus and used them for text classification.",
        "Our defined topological features can outperform conventional text mining features, especially when the textual documents are long.",
        "However, in TIES, we are analyzing different embedding dimensions as time series. Thus, to achieve reasonable results, a large number of tokens in each textual document is required.",
        "It is not easy to measure and/or interpret the impact of different parts of the text input on the output in TIES."
    ],
    "7164": [
        "The proposed end-to-end training framework improves disaster prediction by learning from social media and environmental data.",
        "The approach uses joint training models, such as BiL-STM+CNN, to extract adaptive features from tweets and environmental data.",
        "Applying semantically-enriched data representation improves the performance of the system.",
        "The approach achieves significantly improved accuracies compared to state-of-the-art baselines.",
        "Feeding the proposed joint models with the semantics representation of entities improves F1-score further.",
        "In future work, the authors aim to construct a domain-specific knowledge graph for disaster relief tasks and compare generic word embedding models with embedding from domain-specific corpus, knowledge graphs, and conceptualized embedding."
    ],
    "7165": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.'",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We proposed a novel family of algorithms for discontinuous constituency parsing achieving state-of-the-art results."
    ],
    "7167": [
        "We introduce an end-to-end model that benefits from both entity-aware representations and attribute-aware representations to jointly predict attributes values and their transitions related to an entity.",
        "Our model achieves the state of the art results on various tasks over the PROPARA dataset and the NPN-COOKING dataset.",
        "Future work involves extending our method to automatically identifying entities and their attribute types and adapting to other domains.",
        "We present a general formalism to model procedural texts and introduce a model to translate procedural text into that formalism.",
        "Entity-aware and temporal-aware construction of the input helps to achieve better entity-aware and attribute-aware representations of the procedural context.",
        "Our model can achieve inferences about state transitions by tracking transition in attribute values."
    ],
    "7176": [
        "The resulting fully-supervised parser outperforms the state-of-theart baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The approach is competitive with state-of-the-art Transformers, yet being significantly faster and inherently scalable.",
        "The unification-based mechanism supports the construction of complex and many hops explanations.",
        "The constructed explanations improves the accuracy of BERT for question answering by up to 10% overall."
    ],
    "7184": [
        "Proposed method improves the existing RawNet system by using FMS-based methods to scale filters in feature maps, resulting in better speaker recognition performance.",
        "The proposed method achieves an EER of 2.46% on the original VoxCeleb1 protocol, outperforming the original RawNet's EER of 4.80%.",
        "The proposed method marginally outperforms current state-of-the-art methods in an evaluation using recently expanded evaluation protocols."
    ],
    "7190": [
        "We investigate calibration as a technique for improving the trustworthiness of link prediction with KGE.",
        "We contribute both closed-world and open-world evaluations, the latter is rarely studied for KGE, even though it is more faithful to how practitioners would use KGE for completion.",
        "There is significant room for improvement in calibrating KGE under the OWA.",
        "Our work is a first step toward making KGE predictions more trustworthy."
    ],
    "7194": [
        "The proposed loss function, Aligned Cross Entropy (AXE), outperforms traditional cross entropy training methods for non-autoregressive models in machine translation tasks.",
        "The AXE loss function focuses on relative order and lexical matching instead of relying on absolute positions, leading to better performance.",
        "The use of the AXE loss function in combination with a conditional masked language model (CMLM) achieves a new state-of-the-art for non-autoregressive models in machine translation tasks.",
        "The CMLM trained with AXE significantly outperforms cross entropy trained models, demonstrating the effectiveness of the proposed loss function."
    ],
    "7196": [
        "Our proposed sequence-to-sequence pretraining objectives achieve comparable or better performance compared to models pre-trained on larger corpora.",
        "The proposed method uses sentence reordering, next sentence generation, and masked document generation to improve abstractive summarization tasks.",
        "The SEQ2SEQ model for abstractive document summarization can be pre-trained using the proposed objectives and fine-tuned on the summarization dataset.",
        "The use of smaller corpora (19GB) for pre-training results in comparable or better performance compared to models pre-trained on larger corpora.",
        "Future work includes investigating other objectives for pre-training SEQ2SEQ models for abstractive summarization."
    ],
    "7197": [
        "Our method is highly effective, giving the best performance on a standard benchmark.",
        "We are the first to explicitly model both events and noise over a fundamental stock value state for news-driven stock movement prediction.",
        "Our approach uses an efficient sentence compression model.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks."
    ],
    "7200": [
        "The proposed fine-tuned T5-base model achieves state-of-the-art performance in conversational question reformulation (CQR) tasks, with performance on par with humans (at least measured by BLEU).",
        "The fine-tuned T5-base model can be directly used in a transfer setting and achieves better performance than other neural sequence-to-sequence (S2S) models by a large margin.",
        "The original implementation of the model does not support beam search, but the proposed fine-tuned model can be used with random sampling for inference.",
        "The average performance of the model over 10 repetitions is reported as a measure of its performance."
    ],
    "7201": [
        "Using NOTA options in the training data can result in more false-positive isNOTA predictions at inference time, leading to poorer performance on borderline predictions.",
        "Replacing ground truths and strong distractors with NOTA can result in fewer samples for the model to learn from, leading to poorer performance on borderline predictions.",
        "The LogReg approach does not require data- or model-dependent input like embedding vectors or hidden layer output, making it insensitive to the underlying architecture.",
        "The LogReg approach can be used with any model, regardless of its architecture."
    ],
    "7202": [
        "The proposed models achieved high rankings in various subtasks of Track 2 of DSTC 8, specifically ranking fourth in subtask 1, third in subtask 2, and first in subtask 3 and subtask 4 respectively.",
        "Pre-trained attention-based networks were designed for each subtask according to different evaluation dimensions.",
        "Investigating other strategies for better employing pre-trained language models for multi-turn dialogue will be a part of the authors' future work."
    ],
    "7206": [
        "The authors propose a novel neural architecture called graph sequential network (GSN) to facilitate reasoning over graphs with sequential data on the nodes.",
        "The GSN model avoids the information loss inherent in existing GNN-based models and improves the reasoning ability on sentence level.",
        "The authors achieve better performance than existing GNNs on different types of tasks using their proposed GSN model, as shown through experiments on HotpotQA and FEVER.",
        "The authors plan to apply GSN to other applications in NLP that require complex reasoning in future work."
    ],
    "7207": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Future work include (1) expanding the database to more papers (2) improving the QA model using the collected data to better handle question answering in the context of research domain."
    ],
    "7208": [
        "HMNet achieves state-of-the-art performance in both automatic metrics and human evaluation.",
        "The two-level hierarchical structure and the role vector contribute to the model's performance.",
        "Pretraining on news summarization data alleviates the data scarcity problem.",
        "The ablation study shows that the role vector, hierarchical architecture, and pretraining all contribute to the model's performance.",
        "Future work includes utilizing organizational chart, knowledge graph, and topic modeling to generate better meeting summaries."
    ],
    "7209": [
        "We proposed the novel task of generating rationales as a measure of model understanding for Visual Question Answering tasks.",
        "A well-reasoned explanation implies a thorough understanding of all components of the task: the image, the question and the answer.",
        "We further proposed an end-to-end training method to improve the model's commonsense understanding.",
        "We demonstrated the effectiveness of our proposed method through quantitative and qualitative results."
    ],
    "7212": [
        "The authors have created a resource of 198 complete inflectional paradigms in San Juan Quiahije Chatino, which will facilitate research in computational morphological analysis and inflection for low-resource tonal languages and languages of Mesoamerica.",
        "The authors provide strong baseline results on computational morphological analysis, lemmatization, and inflection realization using character-level neural encoder-decoder systems.",
        "The authors will continue to expand their resource to include more paradigms, and will also follow the community guidelines in extending their resource to include morphological analysis and inflection examples in context."
    ],
    "7214": [
        "We capitalize on the supervised NMT and UNMT use of the pivot language in pivot translation.",
        "We propose the reference language-based UNMT framework, in which a reference agreement mechanism is introduced in several implementations to better leverage the reference agreement in parallel data brought by the reference language to reduce the uncontrollable intermediate quality problem in back-translation.",
        "The experimental results show that we achieved an improvement over our strong baseline, and our proposed RUNMT framework is compatible with and exceeds the traditional pivot translation framework."
    ],
    "7215": [
        "The proposed generator with a well-trained discriminator achieves better sample quality and diversity compared to previous autoaggressive neural language models.",
        "The new generator rejects generated samples with large discrepancies predicted by the discriminator, leading to improved distribution matching.",
        "The generator has plenty of scope for future work, particularly in improving the text classifier and exploring a GAN framework for training the filter and discriminator.",
        "Improving conditional text generation is an important consideration for future work."
    ],
    "7216": [
        "The authors introduced a new task called \"multi-hop question generation\" that extends natural language question generation to multiple document QA.",
        "They proposed a novel reward formulation for improving multi-hop question generation using reinforcement and multi-task learning frameworks.",
        "Their proposed method outperforms state-of-the-art question generation systems on the HotPotQA dataset.",
        "The authors introduced a new evaluation metric called SF Coverage to compare the performance of question generation systems based on their ability to accumulate information from various documents.",
        "The proposed method has practical applications and the authors plan to improve its performance without any strong supporting facts supervision in the future."
    ],
    "7217": [
        "FastBERT achieves the goal of gaining more efficiency with less accuracy loss.",
        "Self-distillation and adaptive inference are first introduced to NLP models in this paper.",
        "FastBERT has a practical feature in industrial scenarios, i.e., its inference speed is tunable.",
        "FastBERT can be 2 to 3 times faster than BERT without performance degradation.",
        "If we slack the tolerated loss in accuracy, the model is free to tune its speedup between 1 and 12 times.",
        "FastBERT remains compatible to the parameter settings of other BERT-like models (e.g., BERT-WWM, ERNIE, and RoBERTa), which means these public available models can be readily loaded for FastBERT initialization."
    ],
    "7218": [
        "Our approach helps generate better text when fine-tuned with EOP and EOS information.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Learning to end paragraphs can still benefit the story generation in terms of automatic metrics and human evaluation results."
    ],
    "7220": [
        "Extensive experiments demonstrate that learning-based evaluation methods are much better than traditional metrics.",
        "The proposed metric significantly outperforms the state-of-the-art learning-based evaluation method.",
        "We conduct a comprehensive analysis of the existing evaluation methods of open-domain dialogue systems from fluency, coherence, engagement."
    ],
    "7221": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "CLR helps to improve the generalization capability in terms of test set results, and it also allows using larger batch size for training without adversely affecting the generalization capability.",
        "Choosing a useful optimizer and an associated learning rate policy is important for NMT."
    ],
    "7224": [
        "We introduced the first Norwegian lexical resource of categorized medical entities.",
        "The resource unites information from medical databases as well as entries automatically mapped from a medical lexicon.",
        "A manual evaluation of a subset of the mapped terms confirmed that the automatic mappings were of a suitable quality to be used as additional supervision signal with machine learning based NER approaches.",
        "In future work, we plan to apply the resource in medical entity recognition for Norwegian, using it to provide initial categories for distant supervision.",
        "We also plan to perform annotations with multiple raters and measure inter-annotator agreement for the proposed categories."
    ],
    "7225": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to improvement in BLEU scores.",
        "Our attention mechanism not only considers the interaction between fact description and law articles but also the differences among similar law articles.",
        "The experimental results on real-world datasets show that our LADAN raises the F1-score of state-of-the-art by up to 5.79%.",
        "In the future, we plan to study complicated situations such as a law case with multiple defendants and charges."
    ],
    "7226": [
        "We propose a dictionary-based data augmentation (DDA) method for cross-domain NMT.",
        "The generated data are used to enhance OOD trained baseline NMT models.",
        "Extensive experiments are performed to various NMT models on four translation directions covering English \u2194 French and English \u2194 German language pairs across a general domain and the medical domain.",
        "The results demonstrate consistent significant improvements in BLEU scores over the baseline models.",
        "When combined with back-translation, the proposed method can improve the cross-domain translation performance.",
        "DDA is confirmed to be a viable complement to BT-based methods in data augmentation for cross-domain NMT.",
        "A further analysis unveils that the improvement is associated with the gain of enhanced domain coverage produced by DDA."
    ],
    "7228": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The Cosine Annealing Strategy is proposed to combine the two methods.",
        "The approach uses pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "Experimental results demonstrate the effectiveness of the approach on the FewRel 2.0 dataset.",
        "The quality analysis shows that the passage-to-summary dataset, PSG2SUM, is capable of being a training and evaluation corpus despite its imperfections.",
        "Extractive models tend to select longer sentences and achieve higher recall scores compared to abstractive and mixed models."
    ],
    "7229": [
        "The proposed learnable data manipulation model can boost the performance of existing dialogue systems.",
        "The resulting data manipulation model is fully end-to-end and can be trained jointly with the dialogue generation model.",
        "The proposed framework is able to improve the performance of neural dialogue generation systems.",
        "The learning-to-manipulate framework for neural dialogue generation is not limited to the elaborately designed manipulation skills in this paper.",
        "Future work will investigate other data manipulation techniques (e.g., data synthesis), which can be further integrated to improve the performance."
    ],
    "7231": [
        "'Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.'",
        "'We first train a representation extractor with the Clustering Promotion Mechanism.'",
        "'Two methods are combined by the proposed Cosine Annealing Strategy.'",
        "'The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.'",
        "'Sub-sentential unit extraction performs competitively compared to the full-sentence-extraction systems.'",
        "'Our experiments and analyses on revisiting the basic extraction unit could provide some hints for future research on this direction.'"
    ],
    "7232": [
        "META-MT outperforms alternative domain adaptation approaches in most domains using only a small fraction of fine-tuning data.",
        "The proposed meta-learning approach for few shot NMT adaptation learns better parameters for NMT systems that can be easily adapted to new domains.",
        "META-MT validates the superiority of the proposed approach compared to alternative domain adaptation strategies.",
        "The META-MT approach can adapt NMT systems to new domains using only a small fraction of fine-tuning data, making it more efficient and effective than previous methods."
    ],
    "7233": [
        "The approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The Cosine Annealing Strategy combines the two methods.",
        "The pseudo-labeled target-domain data is used to train the few-shot classifier.",
        "The approach can reduce the noise of pseudo labels to improve domain adaption performance in future work.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our method is conceptually simple, over two orders of magnitude faster than competing models, and can be applied in tandem with more general NER models.",
        "The TOR method achieves performance comparable to a CNN when more data is added.",
        "Our method can also be leveraged as a preprocessing step, consistently resulting in improved accuracy.",
        "Future work will explore the application of our method in different fields and more advanced ways to leverage the title tree used in this paper."
    ],
    "7234": [
        "We explore the problem of following navigation instructions in continuous environments with low-level actions, lifting many of the unrealistic assumptions in prior nav-graph-based settings.",
        "In models presented here, we took an approach where observations were mapped directly to low-level control in an end-to-end manner; however, exploring modular approaches is exciting future work.",
        "Crucially, setting our VLN-CE task in continuous environments (rather than a nav-graph) provides the community with a testbed where these sort of integrative experiments studying the interface of high-and low-level control are possible."
    ],
    "7235": [
        "taking speaker information into consideration was beneficial to the task of DA classification",
        "our CRF layer outperforms vanilla CRF",
        "brings greater gains than previous attempts at taking speaker information into account",
        "our improved CRF was able to learn complex speaker-change aware DA transition patterns in an end-to-end way"
    ],
    "7238": [
        "MobileBERT is comparable with BERT BASE while being much smaller and faster.",
        "MobileBERT can enable various NLP applications to be easily deployed on mobile devices.",
        "It is crucial to keep MobileBERT deep and thin.",
        "Bottleneck/inverted bottleneck structures enable effective layer-wise knowledge transfer.",
        "Progressive knowledge transfer can efficiently train MobileBERT."
    ],
    "7240": [
        "'Our approach achieves new state-of-the-art on FewRel 2.0 dataset.'",
        "'We first train a representation extractor with the Clustering Promotion Mechanism...'",
        "'The resulting fully-supervised parser outperforms the state-of-theart baseline parser...'",
        "'Domain-specific commonsense can noticeably improve multiple review comprehension tasks that conventional commonsense knowledge bases cannot.'",
        "'We develop XSENSE, a system that can exploit relatively small domain-specific knowledge bases on top of transformer-based language models for review comprehension...'",
        "'We publicly release three domain-specific knowledge bases...'",
        "'Our approach uses Similarity Entropy Minimization to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.'",
        "'The proposed Cosine Annealing Strategy combines two methods...'",
        "'We utilize pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.'"
    ],
    "7243": [
        "Incorporating context information gives significant improvement in predicting argument impact.",
        "Flat representation of the context gives the best improvement in performance.",
        "Contextual models perform better even for claims with limited context.",
        "Our study investigates the role of argumentative discourse context in argument impact classification.",
        "We present a dataset of claims with their corresponding impact votes."
    ],
    "7246": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed dataset contains both real and fake candidate sentences for filling the clozes, which requires the machine to choose the correct sentence and distinguishes the real sentence from fake sentences.",
        "The state-of-the-art models still underperform the human performance, especially on PAC evaluation metric.",
        "The release of this dataset could bring language diversity in machine reading comprehension tasks and accelerate further investigation on solving the questions that need comprehensive reasoning over multiple clues."
    ],
    "7250": [
        "We have shown that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Including length constraints to the decoder is important for achieving translations fulfilling the constraints.",
        "Modeling the task in an end-to-end fashion improves over splitting the task into different sub-tasks, even in zero-shot conditions."
    ],
    "7251": [
        "Our approach significantly outperforms previous methods in situations where only a few input tokens are available to begin translation.",
        "Visual information is important for simultaneous translation, especially in low latency setups and for language pairs with word-order differences.",
        "We hope that our proposed method can be explored even further for various tasks and datasets.",
        "A separate model was created for each value of wait-k, but a single model for all k values could be experimented with in future work.",
        "Investigating MSNMT effects on more realistic data (e.g. TED) is important to ensure the method's robustness."
    ],
    "7257": [
        "The corpus of German Twitter, news, and traffic report texts has been annotated with fine-grained geo-entities and a set of mobility- and industry-related events.",
        "Many of the event types annotated in the corpus are not available in standard knowledge bases, such as accidents, traffic jams, and strike events.",
        "The dataset is distributed in an AVRO-based compact binary format, along with the corresponding schema and reader tools.",
        "The corpus and guidelines are made available to the community at https://dfki-lt-re-group. bitbucket.io/smartdata-corpus."
    ],
    "7258": [
        "Our proposed approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks, as shown by a human evaluation.",
        "Our datasets and baselines will facilitate future research on not only improving Korean NLU systems but also increasing language diversity in NLU research."
    ],
    "7259": [
        "The current neural summarization models fix the length of the source texts in training, which can lead to truncation of longer documents at inference.",
        "Our proposed windowing summarization models allow for processing arbitrarily long documents at inference, taking into account full source text.",
        "The proposed models are effective in summarizing long texts with evenly distributed summary-relevant content."
    ],
    "7261": [
        "We propose an inexpensive, CPU-only method for domain-adapting pretrained language models.",
        "Our approach can cover over 60% of the BioBERT - BERT F1 delta at 5% of BioBERT's domain adaptation CO2 footprint and 2% of its cloud compute cost.",
        "We have also shown how to rapidly adapt an existing BERT QA model to an emerging domain - the Covid-19 pandemic - without the need for target-domain language model pretraining or finetuning.",
        "Our approach can benefit practitioners with limited time or resources and encourage environmentally friendlier NLP."
    ],
    "7262": [],
    "7263": [
        "We operationalize a 'legal approach to hate speech' by translating the requirements of the EU Framework Decision into a series of annotation steps that can be reliably performed by laypersons.",
        "However, we show that learning a model of whether a post is punishable remains challenging.",
        "We thus propose to tackle two subtasks instead: group detection and conduct detection.",
        "Depending on the applicable legal framework, a final decision on the legal status of a comment can then be derived from the combination of detected group and conduct.",
        "Relying on subtasks comes with the added benefit of increased transparency and explainability compared to black-box models.",
        "This is crucial for systems that potentially interfere with human rights, such as the balance between freedom of expression and the prevention of discrimination.",
        "Hence, we recommend this modular approach as the preferred way of composing systems for legal decision-making."
    ],
    "7265": [
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\")",
        "the model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing",
        "late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits",
        "our approach allows lexicality and syntax to interact with each other in the joint search process, it improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages",
        "we can label users with only few topical tweets with high accuracy, often above 95% when we can obtain their timeline tweets",
        "the training data was obtained using unsupervised user classification, we can automatically label the most active users with nearly perfect accuracy",
        "we plan to explore the effectiveness of cross topic classification, where training and testing are done on different topics"
    ],
    "7270": [
        "The proposed neural network model based on SpanBERT and a graph convolutional network achieves state-of-the-art results on a large-scale TACRED dataset.",
        "The model is particularly effective at capturing long-distance relations compared to other models.",
        "The model achieves state-of-the-art results on a large-scale TACRED dataset.",
        "The proposed model is effective at capturing long-distance relationships."
    ],
    "7272": [
        "The brevity problem in natural machine translation (NMT) can largely be explained by the locally-normalized structure of the model.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "The current dataset used in the study is limited and does not include human-generated questions or charts extracted from real-world documents.",
        "Future CQA datasets should include human-generated question-answer pairs and document-level CQA.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments."
    ],
    "7274": [
        "The choice of input encoding affects the performance of pre-trained language models on end tasks.",
        "Tokenization encodes a surprising amount of inductive bias.",
        "Unigram LM tokenization may be the better choice for developing future pre-trained models."
    ],
    "7276": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We propose a novel framework towards dealing with disentanglement, which introduces both local and global semantics to disentangle conversations.",
        "Our model reaches state-of-the-art results on the newly published disentanglement dataset with a substantial margin in comparison to other baseline models."
    ],
    "7279": [
        "We demonstrated the existence of catastrophic forgetting in large language model pre-training.",
        "Our approach using constraint and replay based mitigation techniques closes the performance gap between general and domain specific natural language tasks.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The ConMask model was able to capture the correct relationship even when the name of the screenwriter did not appear in the film's description.",
        "There is room for improvement in the model, as it still has some limitations and can be affected by entities with similar names to the given relationships."
    ],
    "7280": [
        "The proposed algorithm, MADPL, trains both user and system policies simultaneously using an actor-critic framework for multi-domain task-oriented dialog.",
        "MADPL integrates task knowledge into the algorithm through role-aware reward decomposition.",
        "MADPL enables developers to set up a dialog system rapidly from scratch without needing to build a user simulator explicitly.",
        "Extensive experiments demonstrate the effectiveness, reasonableness, and scalability of MADPL.",
        "Future work includes applying MADPL in more complex dialogs and verifying the role-aware reward decomposition in other dialog scenarios."
    ],
    "7281": [
        "The beam problem in NMT can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "SIA is very lightweight and can be deployed into Cloud Foundry, a platform as a service provider.",
        "Our system is capable of sustaining a high number of daily requests and the request handling time is dominated by individual corpus downloading times.",
        "The message bus overhead is about 10%, stemming from individual message serialization and persistence compared to running the annotators stand alone."
    ],
    "7283": [
        "Our approach improves the performance of multilingual models over 4 tasks on 25 datasets.",
        "The experimental results show that our approach has stronger zero-shot transfer ability on unseen languages on the NER and POS tagging task.",
        "We propose two structure-level methods to distill the knowledge of monolingual models to a single multilingual model in sequence labeling.",
        "Our method uses Top-K knowledge distillation and posterior distillation to improve the performance of the multilingual model."
    ],
    "7284": [
        "Our system performs well on the DM and PSD frameworks and achieves the best scores on the in-framework metrics.",
        "The result shows that our system performs well on all frameworks.",
        "We will improve our system to achieve better performance on all these frameworks.",
        "In future work, we will explore cross-framework multi-task learning."
    ],
    "7285": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "We propose a keyphrase-aware news multi-headline generation model that contains a multi-source Transformer decoder with three variants of attention-based fusing mechanisms.",
        "Our approach can generate high-quality, keyphrase-relevant, and diverse news headlines, which outperforms many strong baselines."
    ],
    "7288": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "providing lexical information to parsing ('tag \u2192 parse') leads to more benefits than providing syntactic information to tagging ('tag \u2190 parse').",
        "inplace distillation has higher average accuracy on MRPC and RTE in training DynaBERT W , but performs worse on three data sets after training DynaBERT.",
        "using progressive rewiring (PR) or universally slimmable training (US) [36] has no significant difference from using the method in Section 2.1.",
        "under the same efficiency constraint, sub-networks extracted from the proposed DynaBERT consistently achieve better performance than the other BERT compression methods."
    ],
    "7289": [
        "Evaluation results show improvement compared to competitive baselines and state-of-the-art models on multiple datasets.",
        "Gaz-SelfAttn improves performance on non-English language datasets.",
        "Improving multi-token gazetteer matching with fuzzy string matching.",
        "Exploring transfer learning of gazetteer embeddings from high-resource to low-resource setting."
    ],
    "7290": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Non-isomorphism is not primarily a result of typological differences between languages, but in large part due to degenerate vector spaces and discrepancies between monolingual training regimes and data availability.",
        "Improving monolingual word vector spaces and monolingual training conditions can unlock the true potential of cross-lingual learning."
    ],
    "7292": [
        "Model compression is a common way to deal with latency-critical or memory-intensive scenarios.",
        "Existing model compression methods for BERT are expensive as they require re-training on a large corpus to reserve the original performance.",
        "Our proposed LadaBERT model compression pipeline generates an adaptive BERT model efficiently based on a given task and specific constraint.",
        "LadaBERT achieves comparable performance with other state-of-the-art solutions using much less training data and computation budget.",
        "LadaBERT can be easily plugged into various applications to achieve competitive performances with little training overheads.",
        "In the future, we would like to apply LadaBERT to large-scale industrial applications, such as search relevance and query recommendation."
    ],
    "7293": [
        "The authors introduced a ground truth dataset of emotional responses in the UK to the Corona pandemic.",
        "The authors used topic modeling to understand what people in the UK are concerned about.",
        "The authors ran prediction experiments to infer emotional states from text using machine learning.",
        "Some emotional states correlated with word lists made to measure these constructs.",
        "Longer texts were more useful to identify patterns in language that relate to emotions than shorter texts.",
        "Tweet-sized texts served as a means to call for solidarity during lockdown measures while longer texts gave insights to peoples' worries.",
        "Preliminary regression experiments indicate that the authors can infer from the texts the emotional responses with an absolute error of 1.26 on a 9-point scale (14%)."
    ],
    "7294": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "QAGS correlates with human judgments of factuality significantly better than standard automatic evaluation metrics for summarization, and outperforms related NLI-based approaches to factual consistency checking.",
        "QAGS is naturally interpretable, and the questions and answers produced in computing QAGS indicate which tokens in a generated summary are inconsistent and why.",
        "Future work should explore improved QA models.",
        "Our approach can also be applied to diverse modalities, such as translation and image captioning.",
        "QAGS is useful in quantifying and incentivizing factually consistent text generation."
    ],
    "7296": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Our proposed method, SCITE, can extract causality in natural language text based on our causality tagging scheme.",
        "The APR task has the potential to improve readability in speech transcription systems.",
        "Fine-tuned models improved the readability of ASR output significantly, hinting at potential benefits of the APR task."
    ],
    "7297": [
        "PANDORA dataset comprises 17M comments, personality, and demographic labels for over 10k Reddit users, including 1.6k users with Big 5 labels.",
        "To our knowledge, this is the first Reddit dataset with Big 5 traits, and also the first covering multiple personality models (Big 5, MBTI, Enneagram).",
        "We showcased the usefulness of PANDORA with three experiments, showing (1) how more readily available MBTI/Enneagram labels can be used to estimate Big 5 traits, (2) that a gender classifier trained on Reddit exhibits bias on users of certain personality traits, and (3) that certain psycho-demographic variables are good predictors of propensity for philosophy of Reddit users.",
        "The poor performance of deep learning baseline models, the rich set of labels, and the large number of comments per user in PANDORA suggest that further efforts should be directed toward efficient user representations and more advanced deep learning architectures."
    ],
    "7300": [
        "The proposed approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "There is a large gap between the performance of state-of-the-art models and human performance in MuTual.",
        "The manually annotated multi-turn dialogue reasoning dataset (MuTual) contains 8,860 dialogues and can be used to test reasoning ability of dialogue models.",
        "The process for generating MuTual is described and analyzed in detail."
    ],
    "7304": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "In this work, we demonstrated that dense retrieval can outperform and potentially replace the traditional sparse retrieval component in open-domain question answering.",
        "While a simple dual-encoder approach can be made to work surprisingly well, we showed that there are some critical ingredients to training a dense retriever successfully.",
        "Moreover, our empirical analysis and ablation studies indicate that more complex model frameworks or similarity functions do not necessarily provide additional values.",
        "As a result of improved retrieval performance, we obtained new state-of-the-art results on multiple open-domain question answering benchmarks."
    ],
    "7306": [
        "CLUSTER is a straightforward and effective method for identifying distributional differences in cross-group perspectives.",
        "CLUSTER generalizes well to out-of-domain scenarios, even when trained on Wikipedia and tested on claims collected from Reddit.",
        "The proposed method learns the task, not the data.",
        "The general model of perspective difference identification can be useful in many NLP tasks such as fact checking, sentiment analysis, and cross-cultural studies.",
        "Our work still has much room for improvement, with potential directions including more complicated ways of composing negative samples, more well-crafted models, and extending the pipeline to fine-grained subgroups speaking the same language."
    ],
    "7310": [
        "We take the first step towards question generation in the long-answer setting, where answers can contain 4 or more sentences on average.",
        "We benchmark newly released Natural Questions corpus for question generation with both transformer networks (used for the first time in context of QG) and previously used LSTM networks.",
        "We show transformer models outperform LSTM-based models in terms of automatically computed metrics like BLEU as well as human evaluation.",
        "Our work can be directly applied to different domains such as improve the quality of question long-answer pairs in educational testing.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits."
    ],
    "7311": [
        "The proposed method of transferring pre-trained sentence embedding models can improve the classification performance of pre-trained models like BERT on small datasets.",
        "Concatenating pre-trained and domain specific sentence embeddings with or without fine-tuning can improve the classification performance.",
        "The residues r 1s and r 2s contain information important for prediction.",
        "The method is successful under certain conditions, as explained by the theoretical analysis.",
        "The classifier on the concatenation of r 1s and r 2s achieves accuracy 96.4%, which is better than 81.3% on the combined embeddings via CCA."
    ],
    "7312": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "MARDI can work with heterogeneous NER resources in the form of either a dataset or a model, making it a flexible framework to consolidate NER systems across different domains.",
        "Despite the unavailability of annotations, MARDI performs on par with the state-of-the-art (SOAT) method on joint training of NER models with either disjoint tag sets or heterogeneous tag sets.",
        "It significantly outperforms the SOAT model on the progressive NER task.",
        "We are also interested in applying the CRF distillation technique to other NLP problems such as part-of-speech tagging and chunking."
    ],
    "7314": [
        "It is expected that the multiple classifications can likely have an outstanding growth in bigger training data.",
        "Both binary and multiple classifications are varied in terms of word embedding, our research also shows that we can not be cognizant of using an absolute word embedding method and vouch for its superiority.",
        "The main achievement is our proposed deep learning stacks which both obtained appropriate f1-scores.",
        "B-LSTM works better for sentiment classification due to its internal memory units that are able to handle long dependencies.",
        "CNN's backbone is appropriate for the scope of image processing rather than text classification.",
        "More future researches can be conducted in bidirectional LSTM regarding its connectivity with sentiment analysis."
    ],
    "7319": [
        "LAReQA is a challenging new benchmark for testing answer retrieval from a multilingual candidate pool.",
        "The current state-of-the-art baseline for LAReQA is to simply translate all test data to English, which sacrifices performance on both retrieval from a monolingual pool and retrieval of same-language candidates.",
        "Our strongest baseline (\"X-Y\") actively removes language bias by augmenting training data to include machine-translated cross-lingual examples, but this comes at the cost of sacrificing performance on both retrieval from a monolingual pool and retrieval of same-language candidates.",
        "There is significant headroom for models to improve on LAReQA, and it is an interesting question for future work whether strong alignment always comes at a cost or if better training techniques will lead to models that can improve on all these measures simultaneously."
    ],
    "7320": [
        "The proposed LM-based model for MC-QA generates natural language text that can be used to understand the knowledge extracted from the LM, as an intermediate step.",
        "The performance of our model is on par with end-to-end models, while providing an inspectable layer for practitioners and users.",
        "Our approach is supervised from downstream application signal only, and thus can be generalized to any scenario where we would like to train a LM to generate text that is useful for a downstream application."
    ],
    "7321": [
        "The proposed method, dual graph-sequence iterative inference, significantly advances the state-of-the-art results on two AMR corpora.",
        "The method constructs an AMR graph incrementally in a node-by-node fashion, with each spanning step explicitly characterized as answering two questions: which parts of the sequence to abstract, and where in the graph to construct.",
        "The model leverages mutual causalities between the two and designs an iterative inference algorithm.",
        "The idea proposed in this paper may be applied to a broad range of structured prediction tasks (not only restricted to other semantic parsing tasks) where the complex output space can be divided into two interdependent parts with a similar iterative inference process to achieve harmonious predictions and better performance.",
        "The number of inference steps could be made adaptive to input sentences, potentially leading to further improvements in performance."
    ],
    "7323": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Existing visual grounding based bias mitigation methods for VQA are not working as intended.",
        "We found that the accuracy improvements stem from a regularization effect rather than proper visual grounding.",
        "Future visual grounding methods should be tested with a more comprehensive experimental setup and datasets for proper evaluation."
    ],
    "7324": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "More general globally-normalized models can be trained in a similarly inexpensive way.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "The proposed system shows over 18% improvement on the erroneous sentence classification task and 0.5% improvement on the grammatically correct sentence classification task compared to the baseline system.",
        "The proposed algorithm can be applied to other types of sentence classification, such as sentiment analysis or comment categorization.",
        "The application of the proposed algorithm need not be restricted to the Korean text and can be applied to English text as well."
    ],
    "7326": [
        "We propose a new method which incorporates the superiority of the unified framework and the pre-trained BERT model.",
        "Augmented with bigram features and an auxiliary criterion classification task, we achieve the new state-of-the-art results for MCCWS.",
        "Our approach achieves the new state-of-the-art results for MCCWS.",
        "The proposed method incorporates the superiority of the unified framework and the pre-trained BERT model.",
        "The augmented features and auxiliary criterion classification task improve the performance of MCCWS."
    ],
    "7327": [
        "Existing dialogue agents are highly insensitive to contradiction, and introducing an orthogonally applicable method using the RSA framework (Frank and Goodman, 2012) can alleviate the issue.",
        "The proposed approach can be generalized to improve dialogue context-consistency.",
        "The self-conscious agents improved the base agents on the Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset, without consistency labels and NLI models.",
        "Generating distractors and learning the rationality coefficients are important future directions."
    ],
    "7328": [
        "Task-oriented dialogue systems aim to help users achieve a variety of tasks, and it is not unusual to have hundreds of different domains in modern task-oriented virtual assistants.",
        "To ensure the dialogue system is robust enough to scale to different tasks with limited amount of data, some approaches focus on domain expansion by training on several source domains and then adapting to the target domain. However, such methods can be successful in certain cases but hard to generalize to other completely different out-of-domain tasks.",
        "Machine reading comprehension provides a clear and general basis for understanding the context given a wide variety of questions, and by formulating the dialogue state tracking as reading comprehension, we can utilize recent advances in reading comprehension models.",
        "Reading comprehension datasets can be used to mitigate some of the resource issues in task-oriented dialogue systems, achieving much higher accuracy in dialogue state tracking across different domains given limited amount of data compared to existing methods.",
        "As the variety of tasks and functionalities in a dialogue system continues to grow, general methods for tracking dialogue state across all tasks will become increasingly necessary, and the developments suggested here will help address this need."
    ],
    "7329": [
        "The word frequency characteristics of adversarial word substitutions can be leveraged effectively to detect adversarial sequences for neural text classification.",
        "Our proposed approach outperforms existing detection methods despite representing a conceptually simpler approach to this task."
    ],
    "7337": [
        "Adversarial training can improve the robustness of reading comprehension models to adversarial attacks and also improve performance on source domain and generalization to out-of-domain and cross-lingual data.",
        "BayesAugment for policy search achieves results similar to computationally intensive AutoAugment method but with a fraction of computational resources.",
        "Models trained on SQuAD can be generalized to NewsQA and German, Russian, Turkish cross-lingual datasets without any training data from the target domain or language using a combination of policy search with rewards from the corresponding target development sets' performance."
    ],
    "7338": [
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Our approach uses a combination of Clustering Promotion Mechanism, Similarity Entropy Minimization, and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "We first train a representation extractor with the Clustering Promotion Mechanism and use pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset, demonstrating its effectiveness in inductive unsupervised domain adaption for few-shot classification.",
        "In the future, we will work on how to reduce the noise of pseudo labels to improve the domain adaption performance.",
        "Our extensive evaluation shows that while pretrained Transformers are moderately robust, there remains room for future research on robustness.",
        "We carefully restructured and matched previous datasets to induce numerous realistic distribution shifts, demonstrating the effectiveness of our approach in handling out-of-distribution examples."
    ],
    "7340": [
        "Oscar\" is a new pre-training schema that uses object tags as anchor points to align language and image modalities in a shared semantic space.",
        "The pre-trained Oscar models can successfully tackle a broad set of vision-and-language understanding and generation tasks, achieving new state-of-the-art results on six established tasks.",
        "Using object tags as anchor points improves the baseline performance, regardless of which set of object tags is used (VG or OI).",
        "The object detector trained on VG has a more diverse set of objects, although the object detector trained on OI has higher precision.",
        "The pre-trained Oscar models achieve state-of-the-art results on six established tasks, archiving new state-of-the-arts on these tasks."
    ],
    "7343": [
        "The proposed translation-based alternative for cross-lingual SRL is effective and can achieve significant improvements in performance.",
        "Using corpus translation from gold-standard SRL annotations of the source languages to construct high-quality datasets for the target languages can enhance cross-lingual SRL models.",
        "Combining the gold-standard source SRL corpora and pseudo translated target corpora together can further improve the cross-lingual SRL models.",
        "The PGN-BiLSTM encoder can better exploit the mixture corpora of different languages for cross-lingual SRL transferring.",
        "The translation-based method is a viable approach for cross-lingual SRL transferring, and it can achieve significant improvements for all selected languages, including both single-source and multi-source transfer."
    ],
    "7344": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed AdaBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The adaptive word convolution in the model helps to encode word-level representation dynamically and alleviate the segmentation error cascading trouble effectively.",
        "The model has an excellent ability to capture word-level semantics and can learn the candidate position information autonomously without relying on any word segmentation tool."
    ],
    "7345": [
        "The proposed method outperforms state-of-the-art models for the TBSA task.",
        "The syntactic dependency graph and dynamic heterogeneous graph improve the performance of the model.",
        "The model is effective for detecting connections among sentiments of different aspects and excluding entities without opinions.",
        "The proposed method outperforms state-of-the-art models for the TBSA task.",
        "The syntactic dependency graph and dynamic heterogeneous graph improve the performance of the model.",
        "The model is effective for detecting connections among sentiments of different aspects and excluding entities without opinions."
    ],
    "7347": [
        "We perform the first large scale analysis of multilingual hate speech.",
        "Using deep learning models, we develop classifiers for multilingual hate speech classification.",
        "We see that for low resource, LASER + LR is more effective, while for high resource BERT models are more effective.",
        "We suggest a catalogue which we believe will be beneficial for future research in multilingual hate speech detection.",
        "Our approach outperforms benchmark models across different datasets.",
        "We use deep learning models to develop classifiers for multilingual hate speech classification.",
        "We perform many experiments under various conditions - low and high resource, monolingual and multilingual settings - for a variety of languages."
    ],
    "7350": [
        "The potential for 'weight poisoning' attacks where pre-trained models are 'poisoned' such that they expose backdoors when fine-tuned.",
        "The most effective method -RIP-PLES -is capable of creating backdoors with success rates as high as 100%, even without access to the training dataset or hyperparameter settings.",
        "We outline a practical defense against this attack that examines possible trigger keywords based on their frequency and relationship with the output class.",
        "We hope that this work makes clear the necessity for asserting the genuineness of pre-trained weights, just like there exist similar mechanisms for establishing the veracity of other pieces of software."
    ],
    "7352": [
        "The concept of Interestingness in FR (formalized as an unknown set of implicit queries) is a generalization of Informativeness.",
        "Cosine similarity between word embedding vector representations outperforms discrete measures only on uni-grams.",
        "Discrete bi-gram and skip-gram LMs are a key point of Interestingness evaluation, significantly outperforming all experiments with word2vec models.",
        "An alternative word2vec bi-gram model learned on Wikipedia outperforms uni-gram word2vec models on nCG scores for small cut-off value.",
        "TC@INEX 2012 LogSim measure provides the best results for efficient Interestingness detection over this corpus, both on complete passages and on passages restricted to their anchor texts referring to entities."
    ],
    "7353": [
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We extend and improve over previous work on DDS (Wang et al., 2019b) , with a more efficient algorithmic instantiation tailored for the multilingual training problem and a stable reward to optimize multiple objectives.",
        "MultiDDS not only outperforms prior methods in terms of overall performance on all languages, but also provides a flexible framework to prioritize different multilingual objectives.",
        "MultiDDS is not limited to NMT, and future work may consider applications to other multilingual tasks.",
        "There are other conceivable multilingual optimization objectives than those we explored in \u00a7 6.4."
    ],
    "7354": [
        "Our approach can perform as well as or better than more complex solutions on HotpotQA tasks.",
        "Powerful pre-trained models allow us to score sentences one at a time without looking at other paragraphs.",
        "By operating jointly over these sentences chosen from multiple paragraphs, we arrive at answers and supporting sentences on par with state-of-the-art approaches.",
        "Retrieval in HotpotQA is not itself a multi-hop problem.",
        "Focusing on other multi-hop datasets can demonstrate the value of more complex techniques."
    ],
    "7360": [
        "The proposed transformer architecture, EAE, is effective at capturing declarative knowledge and can be used for a wide variety of tasks, including open-domain question answering, relation extraction, entity typing, and knowledge probing tasks.",
        "Our entity representations are of high quality compared to prior work such as KNOWBERT.",
        "The model learns representations for a pre-fixed vocabulary of entities and cannot handle unseen entities.",
        "Future work can explore representations for rare or unseen entities, as well as developing less memory-intensive ways to learn and integrate entity representations.",
        "Integrating information from knowledge-bases can further improve the quality of entity representation."
    ],
    "7361": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning.",
        "Our proposed recurrent chunking mechanisms outperform benchmark models across different datasets.",
        "We present HYBRIDQA, the first hybrid question answering dataset over both tabular and textual data.",
        "HYBRIDER is a strong baseline and offers interesting insights about the model.",
        "HYBRIDQA is an interesting yet challenging next problem for the community to solve."
    ],
    "7362": [
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Unlike many prior works, our models do not need to perform target length prediction, or re-scoring of candidates and our models use a simplified neural architecture without the need of cross-attention mechanism found in many prior encoder-decoder architectures.",
        "Applying these latent alignment models for parallel translation of long documents can be an interesting research direction."
    ],
    "7363": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Our method improves the speed/accuracy tradeoff for inference using pretrained language models.",
        "Our approach allows for controlling the speed/accuracy tradeoff using a single model, without retraining it for any point along the curve.",
        "Our approach requires neither additional training time nor significant number of additional parameters compared to the standard approach."
    ],
    "7364": [
        "It can act as a computer-aided system to automatically provide early warnings of online social users at risk and notify social workers to provide early intervention.",
        "It can also help the social workers and volunteers to identify the type of mental disorders, relieve online users\\' mental health issues through conversations, and suggest proper consultations or treatments.",
        "Our proposed model...proposes RNs with an attention mechanism for relational encoding.",
        "Experiments show the effectiveness of our proposed model.",
        "We argue that it is a significant step to combine canonical feature extraction with RNs for reasoning.\"']"
    ],
    "7365": [
        "We introduced five improved stack-augmented recurrent neural network (RNN) models and evaluated their ability to recognize complex context free grammars (CFGs) with recurrence on long strings.",
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "We demonstrated the value of memory structures and why they are important when learning from non-regular recursive language patterns.",
        "One interesting research direction would be to examine the effectiveness and stability of a discrete stack when the RNN that operates it is of first order.",
        "Another direction would be to explore data structures other than stacks and to develop the optimization algorithms needed to train networks to effectively operate these structures, especially on challenging real-world problems.\"']"
    ],
    "7366": [
        "Our procedure more than triples the velocity of annotation in comparison to previous methods, while ensuring a larger variety of different types of queries and covering a larger part of the underlying databases.",
        "Our method allows a fine-grained alignment of tokens to operations.",
        "Our statistical analysis showed that the corpus yields a higher coverage of attributes in the databases and more complex natural language questions than other existing methods.",
        "The inclusion of the token alignment results in an increase of precision of up to 7%.",
        "We plan to explore ways to leverage the token assignment to domain adaption and few-shot learning.",
        "We also plan to enhance the annotation process by automatically generating proposals for the NL questions and token assignments and letting the annotators only perform corrections.\"']"
    ],
    "7373": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The relevant contexts can be useful for improving the quality of multiturn dialogue generation, by using proper detection methods, such as self-attention.",
        "Our proposed ReCoSa model significantly outperforms existing HRED models and its attention variants.",
        "Introducing topical information can make the detected relevant contexts more accurate.",
        "Considering detailed content information in the relevant contexts can further improve the quality of generated response.\"]"
    ],
    "7374": [
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We have demonstrated its usefulness, in its current form, for a valuable task (zero-shot speech-to-phone recognition).",
        "As we develop the resource further, the accuracy of our recognizers will go up, our approximate search and forced alignment models will improve, and new avenues of research will be opened.",
        "Achieving this goal will require participation from more than just our research team: we invite linguists and language scientists who have special knowledge or a particular interest in a language to contribute their knowledge to AlloVera in the form of a simple allophone-to-phoneme mapping (preferably with natural language descriptions of the environments in which each allophone occurs).\"']"
    ],
    "7375": [
        "The proposed Transformer-based Text Autoencoder (T-TA) eliminates the computational overload of applying BERT for unsupervised applications.",
        "The T-TA model is significantly faster than the BERT-based approach.",
        "The T-TA model has competitive or even better encoding ability compared to BERT.",
        "The proposed T-TA achieves better performance in unsupervised tasks such as Nbest list reranking and semantic textual similarity.']"
    ],
    "7377": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Self-attentive encoders are well suited for RE on sentences of different complexity, but consistently perform lower on probing tasks.",
        "The bias induced by different architectures affects the learned properties, as suggested by probing task performance.",
        "Probing tasks can be used to study the linguistic features captured in sentence encoder representations trained on relation extraction.",
        "Future work could include extending probing tasks to cover specific linguistic patterns such as appositions, and investigating a model's ability to generalize to specific entity types.\"]"
    ],
    "7381": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach outperforms benchmark models across different datasets.",
        "Using contextual and KB embeddings and combining them in a new embedding, we explore a new approach at modelling KB and contextual information on the same level.",
        "With such a simple embedding model, we manage to obtain conclusive results for our typing task, demonstrating the interest of using KB and contextual information as input for embedding.",
        "There is still a lot to do in this field of trying to bridge the gap between knowledge and text processing.\"']"
    ],
    "7384": [
        "The authors introduced a new set of broad coverage rules called Syn-QG, which leverages event-based and sub-event based sentence views along with verb-specific argument descriptions to generate a large number of diverse and highly relevant questions with better fluency.",
        "The authors' work focuses on sentence-level question generation (QG), but they suggest that questions generated from VerbNet predicates could have an impact on multi-sentence or passage level QG, where the verb-agnostic states of the participants would change as a function of multiple verbs.",
        "The authors believe that with an extensible and transparent architecture, it is very much possible to keep improving the system continuously in order to achieve the larger goal of question generation.",
        "The authors' work on Syn-QG has led to automatic and manual evaluations showing that the system is able to generate a large number of diverse and highly relevant questions with better fluency.",
        "The authors suggest that understanding abstract representations, leveraging world knowledge, and reasoning about them is crucial for achieving the larger goal of QG.\"]"
    ],
    "7385": [
        "The source domain of pretraining datasets matters for visio-linguistic representations.",
        "Visio-linguistic pretraining works well only when the right choices are made.",
        "We can achieve near state-of-the-art performance without using any extra data.",
        "Unlabelled data can be synthetically labelled to scale the pretraining for superior performance.",
        "The bottleneck of labelled captioning data in visio-linguistic self-supervised pretraining can be relieved through this approach."
    ],
    "7386": [
        "Linear classifiers with traditional linguistic features can perform better than neural network-based models.",
        "Character-level features are more significant than word-level features.",
        "The use of punctuations in fake news is more frequent than authentic news.",
        "The least popular sites have more fake news.",
        "Incorporating character-level features in neural network models can improve the performance.",
        "Source can also play a key role in fake news detection.",
        "We will continue to expand our dataset and reach 50K mark.",
        "Our dataset will provide opportunities for other researchers to use computational approaches to fake news detection.\"']"
    ],
    "7387": [
        "Using a custom dataset, we showed that the automatically generated patterns perform slightly better than the manual ones and there is a 70% reduction in construction time.",
        "The distantly-supervised SVM with noisy labels is not far behind the pattern-based classification.",
        "The results reveals the applicability of this approach when the amount of available labels is limited.",
        "As our future work, we would like to increase the flexibility of our patterns by considering more complex terminal structures.",
        "Using techniques from entity learning, we would like to explore the automatic generation of our domain-specific gazetteers lists to increase coverage and the framework applicability in other domains.\"']"
    ],
    "7388": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "To understand charts in documents, information in the rest of the document may be necessary to answer questions about the chart.",
        "Creation of such a dataset would greatly increase the challenge for future algorithms and better match real-world usage.']"
    ],
    "7389": [
        "The proposed scene graph guided modular network (SGMN) outperforms existing state-of-the-art algorithms on a new real-world dataset for structured referring expression reasoning, demonstrating its effectiveness in grounding referring expressions.",
        "SGMN surpasses state-of-the-art structured methods on commonly used benchmark datasets, showcasing its versatility and superiority over existing approaches.",
        "The proposed method generates interpretable visual evidences of reasoning via a graph attention mechanism, allowing for a better understanding of the reasoning process.",
        "The large-scale real-world dataset (Ref-Reasoning) provides a new and challenging benchmark for evaluating the performance of referring expression reasoning systems.",
        "The locally-normalized structure of the model can largely explain the beam problem, highlighting the importance of addressing this issue in future research.']"
    ],
    "7390": [
        "We create a Chinese fine-grained entity typing dataset with each entity mention having an open number of entity types.",
        "The dataset contains a large distantly supervised dataset with 1.9M examples, and a smaller crowdsourced dataset containing 4,800 examples with 1,300 unique entity types.",
        "In total, our dataset contains 7,100 unique entity types.",
        "A mapping between fine-grained types and general types is established, creating a hierarchical relationship between the large number of types.",
        "We test the data on a number of models and show the usability of our dataset.\"']"
    ],
    "7391": [
        "Adversarial pre-training can significantly improve both generalization and robustness of large neural language models.",
        "ALUM substantially improved accuracy for BERT and RoBERTa in a wide range of NLP tasks.",
        "Adversarial fine-tuning can be combined with adversarial pre-training for further gains.",
        "Future directions include studying the role of adversarial pre-training in improving generalization and robustness, speeding up adversarial training, and applying ALUM to other domains."
    ],
    "7392": [
        "The proposed model framework with bi-attention mechanism, convolutions, and gated unit as relevance layer can well capture the topic words of prompts and key-phrase of responses.",
        "The use of residual connections with each major layer improves the performance of the model.",
        "Visualization analysis of the off-topic model can help study the essence of the model.",
        "A novel negative sampling augmentation method can be introduced to augment off-topic training data, achieving significant improvements on both seen and unseen test data.']"
    ],
    "7395": [
        "The emergence of generalization is strongly correlated with the variety of the input environment.",
        "Exposing neural networks to a richer input can help improve their generalization abilities.",
        "Studying the generalization abilities of neural networks in \"thought experiment\" setups can be misleading.",
        "The development of sophisticated generalization mechanisms in human language may have been driven by pressures from an increasingly complex environment.",
        "The origin of human language did not develop sophisticated generalization mechanisms until it was forced to evolve in a complex environment.']"
    ],
    "7398": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The cross-lingual ability of m-BERT has been learned from longer dependency (hundreds of tokens) instead of local co-occurrence information.",
        "A massive amount of data is necessary for the cross-lingual ability of m-BERT.",
        "We could make m-BERT align better between languages by just making a shift to its embeddings.']"
    ],
    "7399": [
        "The existing evaluation metrics for VisDial can suffer from issues such as a mismatch between the task and the evaluation method, which can lead to ranking-based scores not accurately reflecting the quality of answers.",
        "The proposed revised evaluation suite for VisDial uses answers directly generated by a model in concert with consensus-based metrics, which can provide a more accurate assessment of answer quality.",
        "The use of human annotations to verify the sets of relevant answers is not practical at scale, so a semi-supervised automated mechanism is developed to extract these sets from candidate sets using sparse human annotations and correlations through CCA.",
        "The revised evaluation scheme can be used to expand the VisDial dataset with reference set annotations and release this and the revised evaluation scheme as DenseVisDial for future evaluation and model development.",
        "The proposed revised evaluation scheme can improve the accuracy of answer quality assessment in the face of inherent constraints on the VisDial dataset, and it is hoped that the community will adopt this revised evaluation going forward.']"
    ],
    "7401": [
        "The current pretrained language models exhibit strong stereotypical biases.",
        "The GPT2 family of models exhibit relatively more idealistic behavior than other pretrained models like BERT, ROBERTA, and XLNET.",
        "The dataset of stereotypical biases in language models (Stere-oSet) is released to the public for further research and evaluation.",
        "The leaderboard with a hidden test set is presented to track the bias of future language models."
    ],
    "7402": [
        "We present an opposite agent-aware dialogue policy model which actively estimates the opposite agent instead of doing passive learning from experiences.",
        "We have shown that it is possible to harvest a reliable model of the opposite agent through more efficient dialogue interactions.",
        "By incorporating the estimated model output as part of dialogue state, the target agent shows significant improvement on both cooperative and competitive goal-oriented tasks.",
        "As future work, we will explore multi-party dialogue modeling in which multi-agent learning techniques can be applied.\"']"
    ],
    "7403": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Discrete variational attention models have advantages over continuous ones.",
        "The proposed approach can address issues of information underrepresentation and posterior collapse.",
        "The proposed approach is superior to other natural language processing approaches.",
        "The proposed approach has potential for future applications in natural language processing."
    ],
    "7405": [
        "We presented a method to make monolingual sentence embeddings multilingual with aligned vector spaces between the languages.",
        "This was achieved by using multilingual knowledge distillation.",
        "We demonstrated that this approach successfully transfers properties from the source language vector space (in our case English) to various target languages.",
        "Models can be extended to multiple languages in the same training process.",
        "This stepwise training approach has the advantage that an embedding model with desired properties, for example for clustering, can first be created for a high-resource language.",
        "Then, in an independent step, it can be extended to support further languages.",
        "This decoupling significantly simplifies the training procedure compared to previous approaches.",
        "Further, it minimizes the potential language bias of the resulting model.",
        "We extensively tested the approach for various languages from different language families.",
        "LASER and LaBSE work well for retrieving exact translations, however, they work less well assessing the similarity of sentence pairs that are not exact translations.",
        "LASER and LaBSE show a language bias, preferring some language combinations over other.",
        "mUSE and the proposed multilingual knowledge distillation approach show nearly no language bias.\"']"
    ],
    "7406": [
        "substantial performance gain in terms of distractor reliability and plausibility with less computational footprint.",
        "Using the proposed framework, we experimentally observe substantial performance gain.",
        "Depending on the characteristics (e.g. capacity, POS distribution) of different general-purpose knowledge bases, the generated distractors may vary.",
        "Importantly, as knowledge bases with larger coverage and more advanced ranker inevitably emerge, they can be expediently integrated into our framework for further performance gain."
    ],
    "7409": [
        "We introduced DIET, a flexible architecture for intent and entity modeling.",
        "Our study shows that DIET advances the state of the art on the challenging NLU-Benchmark dataset.",
        "Using embeddings from various pre-training methods, we find that there is no single set of embeddings which is always best across different datasets.",
        "Without using any pre-trained embeddings, DIET can still achieve competitive performance, outperforming state of the art on NLU-Benchmark.",
        "The best set of pre-trained embeddings for DIET on NLU-Benchmark outperforms fine-tuning BERT inside DIET and is six times faster to train.\"']"
    ],
    "7410": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The resulting fully-supervised parser outperforms the state-of-theart baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We proposed PReFIL, a new CQA system that improves the state-of-the-art and surpasses human accuracy on two datasets.\"']"
    ],
    "7412": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed method BERT-Attack achieves a high success rate while maintaining a minimum perturbation.",
        "Enhancing language models to generate more semantically related perturbations can be one possible solution to perfect BERT-Attack in the future.']"
    ],
    "7413": [
        "Our approach achieves comparable performance to previous state-of-the-art models while significantly improving efficiency.",
        "We proposed an adaptive interaction fusion network that captures conflicting semantics between posts and comments.",
        "Our method significantly outperformed previous state-of-the-art models on two real-world datasets.",
        "Considering the hierarchical interaction structure of comments can improve fake news detection."
    ],
    "7414": [
        "The proposed token-level adaptive graph-interactive framework can make a fine-grained intent information transfer for slot prediction, which is the first work to explore this area.",
        "The authors release two multi-intent datasets to push forward the research in this area.",
        "The proposed models achieve state-of-the-art performance on four datasets.",
        "The approach of modeling the interaction between multiple intents and slot at each token is effective in improving the performance of the few-shot learning.']"
    ],
    "7418": [
        "Our approach significantly improves the state-of-the-art models across three substantially different QA datasets.",
        "Our approach advances the state-of-the-art on QuaRel and WIQA, two standard benchmarks requiring rich logical and language understanding.",
        "Our approach can effectively learn from extremely limited training data."
    ],
    "7420": [
        "multitask learning is an effective mechanism to distill information from multiple clinical tasks into a single system.",
        "Our system directly increases the potential for the use of recent state-of-the-art NLP methods in clinical application.",
        "We contribute new state-of-the-art baselines for several clinical information extraction tasks.",
        "The data repositories and resources of the clinical NLP community have grown steadily over the past two decades.",
        "we make our implementation and pre-trained models publicly accessible.\"']"
    ],
    "7421": [
        "Our approach (ESPnet-ST) can be used for the fast development of end-to-end and cascaded ST systems.",
        "We provide various all-in-one example scripts containing corpus-dependent pre-processing, feature extraction, training, and inference.",
        "In the future, we will support more corpora and implement novel techniques to bridge the gap between end-to-end and cascaded approaches."
    ],
    "7423": [
        "We propose logical NLG to study the logical inference problem in generation.",
        "Existing NLG models are restricted by their monotonic nature.",
        "There are still some unsolved problems for Logical NLG, e.g., how to improve the quality of automatic metrics to better help human automatically judge models\\' performances.",
        "We host a LogicNLG challenge2 to help better benchmark the current progress."
    ],
    "7427": [
        "The proposed DB-VAE can provide the right balance between optimizing the inference network and the generative network.",
        "The proposed DB-VAE can interpret richer semantic information of discrete structured sequences.",
        "Extensive experiments demonstrate the improved effectiveness of the proposed approach compared to other methods.",
        "Future works include extending discrete bottleneck to large-scale NLP models such as the Transformer and BERT.']"
    ],
    "7428": [
        "proposed a contextualised graph attention network using edge features and operating on multiple sub-graphs for relation classification.",
        "learns rich vertex representations for relation classification.",
        "establishes a new state-of-the-art on the Se-mEval relation classification benchmark dataset.",
        "using multiple sub-graphs is better than using a single graph with graphical networks such as GCNs and GATs."
    ],
    "7430": [
        "We present VisualCOMET, a novel framework of visual commonsense reasoning tasks.",
        "To support research in this direction, we introduce the first large-scale dataset of Visual Commonsense Graphs consisting of 1.4 million textual descriptions of visual commonsense inferences carefully annotated over a diverse set of 59,000 images.",
        "We present experiments with comprehensive baselines on this task, evaluating on two settings: 1) Generating inferences with textual input (event and place) and images, and 2) Directly generating inferences from images.",
        "For both setups, we show that integration between visual and textual commonsense reasoning is crucial to achieve the best performance.\"']"
    ],
    "7431": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "With the improved beam search algorithm and addition speech enhancement, our system outperformed the hybrid baseline system by 2.7% WER abs.",
        "Further research might be the study on how, for the task under consideration, to achieve a recognition accuracy improvement when re-scoring hypotheses using external NNLM.",
        "It would be interesting to incorporate GSS-based enhancement in the end-to-end pipeline to train the system jointly.\"']"
    ],
    "7435": [
        "The proposed semi-supervised text classification system, Semi-Supervised Models via Data Augmentation (SMDA), achieved better performance compared to baseline models.",
        "SMDA utilized supervised learning over labeled data and self-training, entropy minimization, and consistency regularization over unlabeled data.",
        "The system outperformed baseline models significantly, as demonstrated by experimental results.",
        "The approach of using semi-supervised learning with data augmentation improved the performance of the text classification system.",
        "The use of unlabeled data in the training process improved the model's ability to generalize to new, unseen data.",
        "The proposed method achieved state-of-the-art results on a benchmark dataset.\"]"
    ],
    "7436": [
        "The proposed shared-private model is effective in investigating explicit modeling domain knowledge for multi-domain dialog.",
        "The dynamic fusion layer can dynamically capture the correlation between a target domain and all source domains.",
        "The proposed models can quickly adapt to a new domain with little annotated data.",
        "[The proposed shared-private model is effective in investigating explicit modeling domain knowledge for multi-domain dialog.] (Paragraph 3)",
        "[The dynamic fusion layer can dynamically capture the correlation between a target domain and all source domains.] (Paragraph 3)",
        "[The proposed models can quickly adapt to a new domain with little annotated data.] (Paragraph 4)']"
    ],
    "7439": [
        "The created Chinese dataset, DuReader robust, is used to evaluate the robustness and generalization of MRC (Machine Reading Comprehension) models.",
        "The questions and documents in the dataset are natural texts from Baidu search, presenting real-world application challenges.",
        "Pre-trained LMs (Language Models) based MRC models do not perform well on the DuReader robust challenge set.",
        "Extensive experiments are conducted to examine the behaviors of MRC models on the dataset and provide insights for future model development.']"
    ],
    "7440": [
        "We found that both according to automatic evaluation metrics and by human evaluation, finetuning of NMT models achieved comparable gains by learning from error corrections and markings.\" (emphasis on the effectiveness of using error corrections and markings for fine-tuning NMT models)",
        "However, error markings required several orders of magnitude less human annotation effort.\" (emphasis on the efficiency of using error markings compared to other forms of annotation)",
        "In future work we will investigate the integration of automatic markings into the learning process.\" (plans for further research on integrating automatic markings into the learning process)",
        "We will explore online adaptation possibilities.\" (plans for exploring the potential of using online adaptation techniques)']"
    ],
    "7441": [
        "The proposed model is designed for adaptively learning a forgetting curve for language learning using a modified HLR loss function and a neural network.",
        "The incorporation of linguistically and psychologically motivated features, such as word complexity, is shown to be important in predicting the probability of recall for a vocabulary item.",
        "Neural networks can capture the importance of word complexity while a simple HLR fails to take advantage of that signal.",
        "This work lays the foundation for future work in neural approaches to understanding language learning over time.",
        "Future work includes incorporating high-dimensional user embeddings to capture user specific signals that might influence the forgetting curve.']"
    ],
    "7446": [
        "'We introduced G-DAUG c, a novel data augmentation framework to generate synthetic training data, preserving quality and diversity.'",
        "'Our analysis shows that G-DAUG c tends to perform better in low-resource settings and that our data selection strategies are important for performance.'",
        "'G-DAUG c is effective on multiple commonsense reasoning benchmarks, with improvements on in-distribution performance, as well as robustness against perturbed evaluation sets and challenge sets.'",
        "'Future work might explore more sophisticated methods to enhance quality and diversity of generated training data, including having humans-in-the-loop for relabeling.'\"]"
    ],
    "7447": [
        "The proposed method, Graph-aware Co-Attention Networks (GCAN), achieves powerful effectiveness and reasonable explainability in fake news detection.",
        "GCAN can provide early detection of fake news with satisfying performance.",
        "GCAN can be used for not only fake news detection but also other short-text classification tasks on social media, such as sentiment detection, hate speech detection, and tweet popularity prediction.",
        "The proposed method can remove event-specific features to further boost performance and explainability.",
        "The model demonstrates new state-of-the-art results on the miniRCV1 and ODIC datasets.']"
    ],
    "7449": [
        "Our proposed cross-domain slot filling framework can handle the unseen slot type issue by learning to predict whether input tokens are slot entities or not, and then detecting concrete slot types based on slot type descriptions.",
        "Our model shares its parameters across all slot types, which improves adaptation robustness.",
        "Template regularization is proposed to further improve adaptation robustness.",
        "Our model significantly outperforms existing cross-domain slot filling approaches.",
        "Our approach also achieves better performance for the cross-domain NER task, where there is no unseen label type in the target domain.']"
    ],
    "7450": [
        "The proposed approach achieves state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The Cosine Annealing Strategy combines the two methods to improve domain adaptation performance.",
        "The approach utilizes pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "The proposed method outperforms benchmark models across different datasets in machine reading comprehension tasks.",
        "The multi-task style transfer (ST 2 ) framework applies meta-learning to small-data text style transfer, leading to improved language fluency and style transfer accuracy compared to state-of-the-art baselines.",
        "The approach does not rely on a large dataset for each style pair, but uses off-domain information to improve performance.",
        "The elimination of bias in the experiment by pretraining the base models using data from all tasks further enhances the performance of the proposed model.']"
    ],
    "7451": [
        "'Our proposed ESCA framework achieves statistically significant improvements over state-of-the-art baselines for document summarization.'",
        "'The novel hybrid approach of our ESCA model, combining a pair-wise ranking extractor with an abstractor armed with sentence-level attention, produces more desirable summary outputs.'",
        "'The explicit control mechanism in our framework allows the operator to selectively choose which sentences are extracted based on novelty and relevance scores.'",
        "'The abstractive generation process in our model attends to these metrics when inferring final summaries, leading to the most desirable results.'",
        "'Our ESCA model is designed to provide a clear explanation of why certain sentences are selected for extraction, allowing the operator to understand the reasoning behind the summary output.'\"]"
    ],
    "7455": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We analyze the effect of several components in our template-based approaches through ablation studies.",
        "We aim to experiment with other datasets and other domains, incorporate our synthetic data in a semi-supervised setting and test the feasibility of our framework in a multi-lingual setting.\"']"
    ],
    "7456": [
        "The ensemble model of BERT outperforms all other models in the task of Semantic Question Similarity in Arabic, achieving an F1-score of 95.924%.",
        "The ensemble model ranks first place among nine participating teams in the task.",
        "The BERT model shows better performance than other models, including RNN, CNN, and Multi-head models.",
        "The use of an ensemble model of BERT improves the performance of the system, achieving a high F1-score.']"
    ],
    "7460": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Integrating the neural model and the symbolic model with the graph network significantly improves performance.",
        "Our model performs better than strong baselines on PROPARA dataset.",
        "Identifying ways to improve the construction of the participant-specific structure in a dynamic environment.",
        "Developing a better knowledge-enhanced model to automatically retrieve the external/world knowledge and integrate the knowledge for making the prediction.",
        "Handling the error at the initial state with prior/commonsense knowledge.\"']"
    ],
    "7462": [
        "The proposed method, ALONE, reduces the number of parameters related to word embeddings.",
        "ALONE constructs embeddings for each word from one embedding, unlike the conventional method that prepares a large embedding matrix depending on the vocabulary size V.",
        "ALONE can represent each word while maintaining the similarity of pre-trained word embeddings.",
        "ALONE can be trained in an end-to-end manner on real NLP applications.",
        "The combination of ALONE with the strong neural encoder-decoder method, Transformer, achieves comparable scores on WMT 2014 English-to-German translation and DUC 2004 very short summarization with fewer parameters.']"
    ],
    "7464": [
        "There are many other tasks in LegalAI like legal text summarization and information extraction from legal contracts.",
        "Nevertheless, no matter what kind application is, we can apply embedding-based methods for better performance, together with symbol-based methods for more interpretability.",
        "Besides, the three main challenges of legal tasks remain to be solved. Knowledge modelling, legal reasoning, and interpretability are the foundations on which LegalAI can reliably serve the legal domain.",
        "Some existing methods are trying to solve these problems, but there is still a long way for researchers to go.",
        "In the future, for these existing tasks, researchers can focus on solving the three most pressing challenges of LegalAI combining embedding-based and symbol-based methods.",
        "For tasks that do not yet have a dataset or the datasets are not large enough, we can try to build a large-scale and highquality dataset or use few-shot or zero-shot methods to solve these problems.",
        "We need to take the ethical issues of LegalAI seriously. Applying the technology of LegalAI directly to the legal system will bring ethical issues like gender bias and racial discrimination.\"']"
    ],
    "7465": [
        "We have shown that the beam problem can largely be explained by the brevity problem.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our model outperforms multiple rule-based baselines and comment generation models.",
        "We have found that our approach can generate high-quality updates with respect to several automatic metrics and human evaluation."
    ],
    "7467": [
        "We propose a simple but effective semi-supervised learning method, Mix-Text, for text classification.",
        "Our proposed TMix technique and the Mixup model have better testing accuracy and more stable loss trend compared with current pre-training and fine-tuning models and other state-of-the-art semi-supervised learning methods.",
        "We plan to explore the effectiveness of MixText in other NLP tasks such as sequential labeling tasks and other real-world scenarios with limited labeled data.",
        "Our approach alleviates the dependencies of supervised models on labeled data.",
        "We introduce TMix, an interpolation-based augmentation and regularization technique.",
        "Through experiments on four benchmark text classification datasets, we demonstrated the effectiveness of our proposed TMix technique and the Mixup model.\"']"
    ],
    "7472": [
        "The proposed dataset (MATINF) has the potential to accelerate innovations in the three tasks (classification, question answering, and summarization) and multi-task learning.",
        "Existing methods and a straightforward baseline can be benchmarked on MATINF using a novel multi-task paradigm.",
        "The dataset is large-scale and jointly labeled for the three tasks.",
        "The dataset is noisier than a dataset annotated by hired annotators, but it can encourage more robust models and facilitate real-world applications.",
        "The proposed multi-task learning approach has not been explored in future work.']"
    ],
    "7473": [
        "We present a new task and a large-scale multidomain dataset, PEC, towards persona-based empathetic conversations.",
        "CoBERT is an effective and efficient model that obtains substantially better performance than competitive baselines on PEC, including the state-of-the-art Poly-encoder and several BERT-adapted models.",
        "CoBERT is free from hyper-parameter tuning and universally applicable to the task of response selection in any domain.",
        "The results reveal an empirical link between persona and empathy in human conversations and may suggest that persona has a greater impact on empathetic conversations than non-empathetic ones.\"']"
    ],
    "7474": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The connections between aspects and opinion words can be better established with our relational graph attention network (R-GAT) for sentiment classification.",
        "The performance of GAT and BERT is significantly improved as a result of the new tree structure and relational heads.",
        "An error analysis was performed on incorrectly-predicted examples, leading to some insights into this task.']"
    ],
    "7475": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR."
    ],
    "7476": [
        "The introduction of more fine-grained semantic units in the summarization graph helps our model to build more complex relationships between sentences.",
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "We plan to further investigate the proposed ReCoSa model, such as introducing topical information to make the detected relevant contexts more accurate.",
        "Our models have achieved the best results on CNN/DailyMail compared with non-BERT-based models, and we will take the pretrained language models into account for better encoding representations of nodes in the future.\"]"
    ],
    "7478": [
        "The proposed neural system ranks 3th/4th in the official evaluation of the shared task.",
        "It is worth nothing that the system is especially useful for estimating the models on relative sparse data (small treebanks), as it overcame other systems in terms of MLAS and BLEX.",
        "Our system is especially appropriate for processing Indo-European fusional languages.",
        "The self-training procedure significantly increases the performance of the system.",
        "The proposed loss function, in turn, has only a slight impact on the cycles reduction and UAS scores.",
        "The external word embeddings are crucial for our neural-based system.\"']"
    ],
    "7479": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptive search method incorporating two kinds of losses, task-useful knowledge distillation loss depending on the original BERT, and efficiency-aware loss based on the searched structure, can automatically and efficiently find task-suitable structures of compressed BERT.",
        "The proposed AdaBERT model is effective and efficient in compressing BERT for various downstream tasks.",
        "The adaptive study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data, especially when the relation requires more inference.",
        "The post-processing strategy presented can replace generated connectives in organic and fine-tuned language generation from GPT-2, such that they align better with human expectation.']"
    ],
    "7481": [
        "The proposed model, BMGF-RoBERTa, outperforms all previous state-of-the-art systems with substantial margins on the PDTB dataset and CoNLL datasets for implicit discourse relation classification.",
        "The \"previous-next\" context relationship provides much information for this difficult task of implicit discourse relation classification.",
        "The representation layer is important for the effectiveness of the well-designed modules in the model.",
        "Further analysis and ablation study reveal that the representation layer and the modules designed are effective in improving performance.",
        "There is room for improvement in finding better representations beyond the word-and sentence-levels."
    ],
    "7482": [
        "'Our approach significantly outperforms existing methods in transferring deep pretrained language models, as demonstrated by experimental results.'",
        "'We propose a Pretraining Simulation mechanism to learn the pretraining task without data, which helps to cope with the absence of pretraining data during joint learning.'",
        "'Our proposed Objective Shifting mechanism better balances the learning of the pretraining and downstream tasks, leading to improved performance.'",
        "'We provide an open-source RECADAM optimizer that integrates our proposed mechanisms into Adam optimizer, facilitating the better usage of deep pretrained language models.'\"]"
    ],
    "7483": [
        "Our non-autoregressive decoding approach integrates lexical constraints for NMT, and we have empirically validated its effectiveness.",
        "The approach demonstrates control over constraint terms in target translations while being able to decode as fast as a baseline Levenshtein Transformer model.",
        "Future work can potentially modify insertion or selection operations to handle target translations of multiple forms, which can potentially disambiguate the morphological variants of the lexical constraints."
    ],
    "7484": [
        "The absence of a reference-less evaluation metric for language quality has been an impediment to developing NLG models.",
        "BLEU Neighbors outperformed human annotators in predicting the quality of dialogue and open-ended generation data, on average.",
        "BLEU Neighbors is state-of-the-art on automatically grading essays, even beating out models that had access to a gold-standard reference essay.",
        "BLEU Neighbors is fast, data-efficient, and easy-to-use, with only two hyperparameters that work universally well across various tasks.",
        "BLEU Neighbors is intended to complement, not supplant, human evaluation, as it has speed, simplicity, and ease of use that make it ideal for rapidly iterating on NLG models long before any human evaluation is done.']"
    ],
    "7485": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism, which uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.",
        "We utilize pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "Our approach uses a KNN-based Information Fetching module that learns to identify relevant information from external knowledge sources by learning a mapping-based read operation.",
        "KIF modules benefit from the scalability and efficiency of K Nearest Neighbors search, enabling computation with large external memories.",
        "Our approach incorporates relevant knowledge to create more engaging, high quality dialog.']"
    ],
    "7492": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Human performance is found to exceed advanced contextual embedding and language models by a significant margin.",
        "With |B| = 5 blanks and |C| = 7 candidates, the size of answer space, |A|, is number of permutations |B| objects taken |C| at a time, i.e., P(7, 5) = 2520.",
        "If we have the same number of candidates as blanks, this is equivalent to |B|! -D |B|, where D |B| is the number of derangements 10 of |B| elements.",
        "There is a 48.2% probability of being entirely wrong with a randomly chosen set of answers to each blank in the passage.",
        "The expectation of number of distractors choosing by uniform model, it should be E [DE] , where DE denotes distractors errors.\"']"
    ],
    "7499": [
        "We studied out-of-sample representation learning for non-attributed multi-relational graphs - a problem that is surprisingly poorly studied.",
        "We created two benchmarks for this task and outlined the procedure we followed for creating these datasets to facilitate the creation of more datasets in the future.",
        "We also developed several baselines, a new training algorithm, and two aggregation models for out-of-sample representation learning.",
        "Future work includes developing new training strategies, testing other aggregation functions, combining the aggregation functions with other transductive models, extending out-of-sample reasoning to temporal KG completion and knowledge hypergraph completion.",
        "Transferring the knowledge learned over one graph to a new graph with new entities.",
        "Studying the similarities and differences between out-of-sample representation learning and out-of-vocabulary word embedding.",
        "Testing the proposed models on relational domains other than knowledge graphs.\"']"
    ],
    "7501": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our generation approach significantly outperforms a state-of-the-art model.",
        "Our model shows promise particularly for creativity, humor and sarcasticness, but less for grammaticality.",
        "Developing methods that are nuanced enough to recognize the difference between creativity and nonsense is key to future progress.\"']"
    ],
    "7504": [
        "Our model establishes new state-of-the-art performance in the discriminative setting and shows promising results in the generative setting on the visual dialog benchmarks.",
        "VD-BERT is capable of modeling all the interactions between an image and a multi-turn dialog within a single-stream Transformer encoder.",
        "It can either rank or generate answers seamlessly.",
        "Our model shows promising results in the generative setting on the visual dialog benchmarks.",
        "Without pretraining on external visionlanguage datasets, our model establishes new state-of-the-art performance in the discriminative setting.\"']"
    ],
    "7506": [
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\').",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing.",
        "We analyze state-of-the-art approaches for low-resource POS tagging of truly low-resource languages, and show that weakly supervised approaches only slightly outperform a state-of-the-art unsupervised baseline.\"']"
    ],
    "7509": [
        "The proposed approach of Scheduled DropHead can effectively improve the performance of a competitive transformer model, as demonstrated by experiments on machine translation and text classification benchmarks.",
        "The DropHead mechanism can reduce the influence of the dominant attention head and improve the model's robustness to attention head pruning.",
        "The proposed approach can improve the performance of a transformer model by motivating the training dynamic of the MHA mechanism.",
        "The specific dropout rate scheduler is effective in improving the performance of the vanilla DropHead.",
        "The proposed approach has the potential to be applied to pretraining language models such as BERT and other natural language generation tasks such as text summarization and dialogue systems.\"]"
    ],
    "7512": [
        "The proposed framework combining decision tree and neural attention networks is effective and explainable for discovering evidence for explainable claim verification.",
        "The framework constructed with a decision tree model to select comments with high credibility as evidence, and co-attention networks to make the evidence and claims interact with each other, demonstrated effectiveness on two public datasets.",
        "In future work, the proposed framework will be extended by considering more context (meta data) information, such as time, storylines, and comment sentiment, to further enrich explainability."
    ],
    "7516": [
        "Our framework outperforms prior works on the ACE 2005 benchmark.",
        "More natural questions lead to better performance.",
        "Our framework is capable of extracting event arguments of roles not seen at training time.",
        "Incorporating broader context (e.g., paragraph/document-level context) in our methods could improve the accuracy of the predictions."
    ],
    "7519": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The projection procedure described in this paper suggests a new approach to bootstrapping natural language applications.",
        "The declarative synthetic data generation procedure explicitly encodes key insights about the relationship between the world and language.",
        "Developers can hand-engineer synthetic grammars, use the generated data to train domain-specific machine learning models, and use projections to paraphrase test time natural language examples into their synthetic counterparts.']"
    ],
    "7520": [
        "The approach achieves state-of-the-art performance on a large-scale benchmark dataset for verifying textual statements over semi-structured tables.",
        "The system uses a sequence-to-action semantic parser for generating programs.",
        "The approach utilizes graph information with two mechanisms: a mechanism to learn graph-enhanced contextual representations of tokens with graph-based attention mask matrix, and a neural module network which learns semantic compositionality in a bottom-up manner with a fixed set of modules.",
        "Both graph-based mechanisms are beneficial to the performance.",
        "The sequenceto-action semantic parser is capable of generating semantic-consistent programs.']"
    ],
    "7526": [
        "CPE-PLM outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The proposed chart-based method and top-K ensemble improve performance in CPE-PLM.",
        "The paradigm of inducing parses directly from PLMs has the potential to be applied to different languages by using multilingual PLMs.",
        "The work provides a foundation for future research on unsupervised constituency parsing for under-represented languages and probing the inner workings of multilingual PLMs."
    ],
    "7529": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Biased representations lead to biased inferences, and our approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Our novel graph-based method for inductive text classification is effective in modelling local word-word relations and word significances in the text.']"
    ],
    "7531": [
        "It has been proven as a good approach to utilizing knowledge from formal data to informal data by exhaustive experiments.",
        "When data annealing is applied with BERT, it outperforms different state-of-the-art models on different informal language understanding tasks.",
        "Data annealing is also effective when there is limited labelled resources."
    ],
    "7532": [
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We plan to consider byte pair encoding (BPE) tokenization to account for Wolof\\'s morphological and compositional structure.",
        "We also plan to investigate the use of transformer models for the translation task as transformer have stated latest state of the art natural language processing performances.\"']"
    ],
    "7539": [
        "The proposed method can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The beam problem can largely be explained by the brevity problem.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "All models predict parody tweets consistently above random, even if tested on people unseen in training.",
        "The pre-trained contextual embedding based models perform best, with an average of around 10 F1 better than the linear methods.",
        "RoBERTa outperforms the other methods by a small, but consistent margin, similar to past research.",
        "The predictions are robust to any location or gender specific differences.",
        "Our models capture information beyond topics or features specific to any person, gender, or location and can potentially identify stylistic differences between parody and real tweets."
    ],
    "7541": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The proposed MacBERT could give significant gains in most of the tasks, and detailed ablations show that more focus should be made on the MLM task rather than the NSP task and its variants.",
        "With the release of the Chinese pre-trained language model series, we hope it will further accelerate the natural language processing in the Chinese research community.\"']"
    ],
    "7542": [
        "adopting a standard span-based QA framework, VSLBase, effectively addresses NLVL problem.",
        "there are two major differences between video and text.",
        "introduces a simple and effective strategy named query-guided highlighting, on top of VSLBase.",
        "With QGH, VSLNet is guided to search for answers within a predicted coarse region.",
        "the effectiveness of VSLNet (and even VSLBase) suggest that it is promising to explore span-based QA framework to address NLVL problems.\"']"
    ],
    "7545": [
        "We propose to learn a universal encoder that maps sequences of different linguistic granularities into the same vector space.",
        "Our proposed BURT outperforms all the baseline models on sentence and phrase level evaluations, and generates high-quality word vectors that are almost as good as pre-trained word embeddings.",
        "BURT is trained with three objectives on the NLI and PPDB datasets through a Siamese network.",
        "Our proposed BURT generates high-quality word vectors that are almost as good as pre-trained word embeddings.",
        "The proposed BURT is evaluated on a wide range of similarity tasks with regard to multiple levels of linguistic units (sentences, phrases and words).\"']"
    ],
    "7546": [
        "The proposed neural framework for conditional extractive news summarization reduces position bias in generated summaries while preserving comparable performance with other standard models.",
        "Conditional learning enables summaries to be more efficiently tailored to different user preferences and application needs.",
        "The use of subaspect functions of importance, diversity, and position in the framework helps to reduce position bias in generated summaries.']"
    ],
    "7547": [
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\')",
        "our approach outperforms benchmark models across different datasets",
        "the proposed scoring method estimates the quality of utterance pairs by focusing on the two crucial aspects of dialogue, namely, the connectivity and content relatedness of utterance pairs",
        "our method improves the performance of a response generation model by removing unacceptable utterance pairs from its training data",
        "we hope that this study will facilitate discussions in the dialogue response generation community regarding the issue of filtering noisy corpora.\"']"
    ],
    "7549": [
        "The proposed model (MVAN) outperforms existing state-of-the-art models for the visual dialog task.",
        "The proposed model (MVAN) has improved interpretability through detailed visualization of multi-level attention.",
        "The proposed model (MVAN) has the potential to be further improved by incorporating sequential information about the dialog history.",
        "The proposed model (MVAN) has the potential to be further improved by incorporating the latest pre-training methods."
    ],
    "7551": [
        "The primary contribution of our work is a novel, automatic method for identifying gender issues in machine translation.",
        "By performing BERT-based perturbations on naturally-occurring sentences, we are able to identify sentence pairs that behave differently upon translation to gender-marking languages.",
        "Our approach is naturally extensible to new language pairs, text genres, and different parts of speech.",
        "We demonstrate our technique over human reference forms and discover new sources of risk beyond the word lists used previously.",
        "Our dataset across four languages from three language families is publicly released, enabling the community to work towards solutions that are inclusive and equitable to all.']"
    ],
    "7555": [
        "We have introduced a novel approach to the analysis of lexical semantic change using neural contextualized word representations and no lexicographic supervision.",
        "Our method allows us to capture a variety of synchronic and diachronic linguistic phenomena, including semantic shifts.",
        "Our approach offers several advantages over previous methods, including the ability to capture morphosyntactic properties of word usage and the lack of reliance on a fixed number of word senses.",
        "We have shown that the representations and the detected semantic shifts are aligned to human interpretation.",
        "In recent work, we have experimented with alternative ways of obtaining usage representations and have obtained very promising results in detecting semantic change across four languages.",
        "In the future, we plan to investigate whether usage representations can provide an even finer grained account of lexical meaning and its dynamics.\"']"
    ],
    "7562": [
        "SE-KGE can handle different types of spatial relations such as point-wise metric spatial relations and partonomy relations.",
        "SE-KGE outperforms multiple baselines on the geographic logic query answering task.",
        "Considering the scale effect in location encoding is important for KG embedding models.",
        "SE-KGE ssl can significantly outperform the baseline SE-KGE 1 space on the spatial semantic lifting task.",
        "Existing KG embedding models cannot solve the spatial semantic lifting task except for SE-KGE."
    ],
    "7564": [
        "Our method dynamically associates an explanatory feature with a prediction if the feature explains the prediction well.",
        "The acoustic features are crucial for predicting negative sentiment or emotions, and the acoustic-visual interactions are crucial for predicting emotion angry.",
        "These observations align with prior work in psychological research.",
        "The advantage of both local and global interpretation is achieved without much loss of performance compared to the SOTA methods.",
        "We believe that this work sheds light on the advantages of understanding human behaviors from a multimodal perspective, and makes a step towards introducing more interpretable multimodal language models.\"']"
    ],
    "7565": [
        "We propose a fine-grained multitask learning approach that leverages declarative knowledge to detect propaganda techniques in news articles.",
        "The declarative knowledge is expressed in both first-order logic and natural language, which are used as regularizers to obtain better propaganda representations and improve logical consistency between coarse-and finegrained predictions, respectively.",
        "Our knowledge-augmented method achieves superior performance with more consistent between sentence-level and token-level predictions."
    ],
    "7566": [
        "Our proposed method can better preserve the cross-lingual ability of pre-trained language models during fine-tuning.",
        "Our method achieves better performance than fine-tuning baselines for strong multilingual models on zero-shot cross-lingual POS and NER tasks.",
        "We adopt a continual learning framework, GEM, to constrain the parameter learning in pre-trained multilingual models based on the MLM and XSR tasks when finetuning them to downstream tasks.",
        "Our methods can better preserve the cross-lingual ability of pretrained models compared to fine-tuning baselines."
    ],
    "7570": [
        "Our approach outperforms other state-of-the-art models on content preservation metrics while retaining (or in some cases improving) the transfer accuracies.",
        "We introduce a simple pipeline -tag & generate, which is an interpretable two-staged approach for content preserving style transfer.",
        "Our approach is robust in cases when the source is style neutral, like the \"non-polite\" class in the case of politeness transfer.",
        "We provide a dataset comprised of sentences curated from email exchanges present in the Enron corpus for the task of politeness transfer.",
        "Our approach outperforms benchmark models across different datasets.']"
    ],
    "7571": [
        "We release SUBJQA, a question-answering dataset which contains subjectivity labels for both questions and answers.",
        "The dataset allows i) evaluation and development of architectures for subjective content, and ii) investigation of subjectivity and its interactions in broad and diverse contexts.",
        "We further implement a subjectivity-aware model and evaluate it, along with 4 strong baseline models.",
        "We hope this dataset opens new avenues for research on querying subjective content, and into subjectivity in general."
    ],
    "7572": [
        "'We present HURRICANEEMO, an annotated dataset of perceived emotions spanning 15,000 tweets from multiple hurricanes.'",
        "'Tweets are annotated with fine-grained Plutchik-24 emotions, from which we analyze implicit and explicit emotions and construct Plutchik-8 binary classification tasks.'",
        "'Our dataset is a challenging benchmark, even for large-scale pre-trained language models.'",
        "'We release our code and datasets as a step towards facilitating research in disaster-centric domains.'"
    ],
    "7573": [
        "Evaluating response generation systems via response selection with well-chosen false candidates correlates more strongly with human evaluation compared to widely used metrics such as BLEU.",
        "The proposed method of constructing response selection test sets by filtering out unrelated and acceptable false candidates is effective in evaluating response generation systems.",
        "Providing labels that indicate \"Why this candidate is false\" for false candidates in the test set can be used for error analysis to detect weak points of systems.",
        "The proposed method of constructing response selection test sets with well-chosen false candidates can help improve the evaluation of response generation systems.']"
    ],
    "7575": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Unambiguous words provide an excellent bridge to reach a wider range of OOV senses, improving results for WSD.",
        "A single occurrence of OOV unambiguous words is enough to improve the performance of WSD models."
    ],
    "7576": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our approach to section title generation performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The UDapter model learns to adapt languagespecific parameters on the basis of adapter modules and contextual parameter generation.",
        "The CPG method enables language-specific adaptation, defined as a function of language embeddings projected from linguistically curated typological features.",
        "UDapter outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages.",
        "Typological features are crucial for zero-shot languages, where no significant difference between UDapter and multi-udify can be observed by significance testing.",
        "The analyses performed on the underlying characteristics of the model show that typological features are crucial for zero-shot languages.']"
    ],
    "7577": [
        "We introduce MultiATIS++, a multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We release our MultiATIS++ corpus to facilitate future research on cross-lingual NLU to bridge the gap between cross-lingual transfer and supervised methods."
    ],
    "7578": [
        "Informed Sampling leads to highly diverse output while minimising the cost to the quality of the text.",
        "We also show that Informed Sampling is better than previous work at determining the edge cases where it can still reliably generate diverse output even though the NLG model assigns a lower probability.",
        "Informed Sampling is agnostic to the underlying model; its input consists of hidden states/embeddings and a probability distribution that can be obtained from almost any language generation model.",
        "Our experimental results show that Informed Sampling leads to highly diverse output while minimising the cost to the quality of the text.",
        "We presented a thorough analysis of open-domain diversity methods applied to concept-to-text NLG.",
        "In future work, we aim to extend Informed Sampling to other language generation tasks, e.g. machine translation and open-domain NLG.",
        "It would be interesting to explore the application of Informed Sampling over the probability distribution of large pretrained models.\"']"
    ],
    "7580": [
        "We presented TOTTO, a table-to-text dataset that presents a controlled generation task and a data annotation process based on iterative sentence revision.",
        "We also provided several state-of-the-art baselines, and demonstrated TOTTO could serve as a useful research benchmark for model and metric development."
    ],
    "7581": [
        "There is a lack of large, labeled datasets of claims in the research community for claim detection tasks.",
        "Our dataset, ClaimBuster, provides a large labeled dataset of 23,533 sentences categorized into three categories (non-factual statements, unimportant factual statements, and check-worthy factual statements).",
        "One hundred one trained human coders labeled these claims over a two-year period.",
        "The ClaimBuster dataset is now publicly available to the research community.",
        "There are 23,533 sentences in our dataset, each categorized into one of three categories.",
        "Our dataset provides a large and diverse set of claims for researchers to use in claim detection tasks.\"']"
    ],
    "7583": [
        "We defined the task of Issue-Sensitive Image Captioning (ISIC) and developed a Bayesian pragmatic model that allows us to address this task successfully using existing datasets and pretrained image captioning systems.",
        "Our method can be used as a method for assessing the quality of the underlying caption model.",
        "Using a dataset with issue annotations, if the model trained over the plain captions is more issue-sensitive, then it is better at decomposing the content of an image by its objects and abstract concepts.",
        "One could extend our notion of issuesensitivity to other domains.",
        "Questions (as texts) naturally give rise to issues in our sense where the domain is sufficiently structured, so these ideas might find applicability in the context of question answering and other areas of controllable natural language generation.\"']"
    ],
    "7587": [
        "We study methods for neural zero-shot passage retrieval and find that domain targeted synthetic question generation coupled with hybrid termneural first-stage retrieval models consistently outperforms alternatives.",
        "Furthermore, for at least one domain, approaches supervised quality.",
        "While out of the scope of this study, future work includes further testing the efficacy of these first-stage models in a full end-to-end system.",
        "As well as for pre-training supervised models (Chang et al., 2020)",
        "To the extent that we pre-process the data, we will release relevant tools and data upon publication.\"']"
    ],
    "7588": [
        "The proposed model, COPT, is a model-agnostic approach that can be applied to any adversarial learning-based dialogue generation models.",
        "The COPT learns on counterfactual responses inferred from the structural causal model, which helps the model explore the high-reward area of the potential response space.",
        "The COPT significantly improves the quality of the generated responses, demonstrating its effectiveness.",
        "The COPT is a model-agnostic approach that can be applied to any adversarial learning-based dialogue generation models.",
        "The COPT learns on counterfactual responses inferred from the structural causal model, which helps the model explore the high-reward area of the potential response space.",
        "The COPT significantly improves the quality of the generated responses, demonstrating its effectiveness.']"
    ],
    "7589": [
        "The findings in NER are likely to be generalizable to other short-span classification tasks.",
        "The instance-based span model achieves competitive performance with classifier-based models.",
        "The method learns similarity between spans and has interpretable inference process.",
        "The models built by the method can be applied to downstream tasks requiring entity knowledge, such as entity linking and question answering.",
        "The network setup follows the encoder architecture proposed by Ma and Hovy (2016).",
        "The token-encoding layer encodes each token of the input sentence to a sequence of vector representations.",
        "The models using GloVe use the GloVe 100-dimensional embeddings and character-level CNN.']"
    ],
    "7590": [
        "The proposed sentence alignment method based on cross-language span prediction can achieve better performance than traditional methods.",
        "The method can be implemented using QANet or multilingual BERT, and future work includes investigating the best practice for combining manually and automatically aligned data.",
        "The amount of manually aligned data for training is usually limited, and there is a need to investigate the best practice for combining manually and automatically aligned data."
    ],
    "7591": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "using hierarchical attention-based models boosts performance.",
        "incorporating distinctions between different kinds of scene components directly into the model improves performance.",
        "our structure-aware methods are fundamentally generalizable and can be adapted to natural language understanding across virtually any domain in which structure can be extracted.\"']"
    ],
    "7593": [
        "We propose a Token-Aware Virtual Adversarial Training method to allow virtual adversarial training methods to construct fine-grained virtual adversarial samples.",
        "Our approach helps improve the performance of various tasks using pre-trained language models.",
        "In the future, we will further explore the potential of improving both generalization and robustness using token-aware virtual adversarial training methods."
    ],
    "7594": [
        "We propose a method named Filtering before Iteratively REferring (FIRE) for utilizing the background knowledge of dialogue agents in retrieval-based chatbots.",
        "Our approach achieves a new state-of-the-art performance on two datasets.",
        "We will explore better ways of integrating pre-trained language models into our proposed methods for knowledge-grounded response selection."
    ],
    "7596": [
        "The RikiNet outperforms single human performance on the Natural Questions dataset.",
        "The RikiNet ensemble achieves new state-of-the-art results on both long-answer and short-answer tasks, significantly outperforming all other models.",
        "The RikiNet uses a dynamic paragraph dual-attention reader to learn token-level, paragraph-level, and question representations.",
        "The RikiNet uses a multilevel cascaded answer predictor to jointly predict long and short answers in a cascade manner.",
        "The RikiNet achieves better results than all other models on both long-answer and short-answer tasks.']"
    ],
    "7599": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Loss truncation outperforms a range of baselines, including beam search, top-p, top-k, and full sampling, on distinguishability.",
        "Rejection sampling outperforms all baselines, including beam search, on generating factual summaries.",
        "Robust learning in the form of truncating the log loss can complement model-based approaches to faithful generation by ignoring invalid and undesired references.']"
    ],
    "7603": [
        "We formulate the insufficient knowledge problem in a real conversation where speakers cannot access needed knowledge to respond.",
        "To deal with this problem, we propose InjK that injects knowledge into a dialogue model by regularizing a latent variable being knowledgeable.",
        "Empirically, we analyze the impact of knowledge insufficiency in both training and testing phases.",
        "The results show that InjK outperforms state-of-the-art baselines when limited knowledge is obtainable in applications."
    ],
    "7607": [
        "The proposed capsule-Transformer achieves superiority over the strong Transformer baseline in the task of neural machine translation, indicating its potential for various NLP tasks.",
        "The capsule-Transformer is capable of obtaining a better attention distribution representation of the input sequence via information aggregation among different heads and words.",
        "The proposed model architecture design has potentially broad application prospects for various NLP tasks.",
        "The capsule-Transformer extends the linear transformation of self-attention in the vanilla Transformer into a more general capsule routing algorithm by taking SAN as a special case of capsule network.']"
    ],
    "7610": [
        "Our proposed method of linearizing constituent trees tied on spans tightly is more accurate than previous local normalization methods, and achieves competitive results with global models.",
        "Our method reserves the fast running speed due to the parallelizable linearization model, which allows for considering more span information.",
        "The experiments show that our model significantly outperforms existing local models and achieves competitive results with global models.",
        "Our new normalization method, which can add constraints on all the spans with the same right boundary, is effective in improving the accuracy of the model.']"
    ],
    "7611": [
        "The proposed approach is capable of boosting the performance of both NLU and NLG models.",
        "The duality between language understanding and generation provides flexibility of incorporating supervised and unsupervised learning algorithm to jointly train two models.",
        "Considering their data distribution, the proposed approach has the potential to unsupervise both language understanding and generation models.",
        "The experiments on benchmark dataset demonstrate that the proposed approach is effective in boosting the performance of NLU and NLG models.\"']"
    ],
    "7614": [
        "Our proposed conditional data augmentation approach for aspect term extraction is effective in generating qualified sentences, and allows more diversified new sentences.",
        "Unlike existing augmentation approaches, our method is controllable and can generate diverse sentences.",
        "Experimental results on two review datasets confirm the effectiveness of our proposed approach in a conditional augmentation scenario.",
        "Our qualitative studies show how the augmentation approach works, and we tested other language models to explain why our masked sequence-to-sequence generation framework is favored.",
        "The proposed augmentation method can be applied to other low-resource sequence labeling tasks such as chunking and named entity recognition.']"
    ],
    "7616": [
        "Our new annotation framework for reading comprehension supports a wide range of reading behavior analyses and has the potential to develop automated question validation tools for reading comprehension examinations for humans.",
        "Our methodology for automatic validation of annotations and detailed comparisons between human and machine reading comprehension demonstrates the promise of our annotation framework and dataset.",
        "The quality assurance issues with RACE (Reading Assessment Content Expert) are alleviated in our new dataset, indicating the feasibility of developing automated question validation tools for reading comprehension examinations.",
        "Our experiments demonstrate the potential of our annotation framework and dataset to support a wide range of reading behavior analyses.']"
    ],
    "7618": [
        "The performance of conditional language models is substantially better than widely-known baselines when initializing the decoder with an aggregate action feature vector or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "The proposed vocabulary adaptation method, which adapts the vocabulary of a pre-trained NMT model to a target domain, can improve the translation performance.",
        "The domain adaptation method with locally linear mapping (LLM) dramatically improved the translation performance.",
        "The vocabulary adaptation method is applicable to a wider range of neural network models and tasks and can be combined with existing fine-tuning-based domain adaptations.",
        "The code will be released to promote the reproducibility of the results.']"
    ],
    "7619": [
        "The proposed method SHIFT-ATT and SHIFT-AET can induce alignments at the step when the target token is the decoder input, improving the effectiveness of word alignment induction.",
        "Experimental results on three public alignment datasets and a downstream task prove the effectiveness of these two methods.",
        "The proposed method SHIFT-AET consistently outperforms prior neural aligners and GIZA++, without influencing the translation quality.",
        "The new state-of-the-art performance among all neural alignment induction methods is achieved by SHIFT-AET, as proven by experiments on three public alignment datasets and a downstream task.",
        "Future work can be left to extend the study to more downstream tasks and systems.']"
    ],
    "7620": [
        "The current DNN-based models do not systematically interpret monotonicity inference, but some models might have sufficient ability to memorize different types of reasoning.",
        "Our approach based on BERT trained with our synthetic dataset mixed with MultiNLI maintained performance on MultiNLI while improving the performance on monotonicity.",
        "The capability of three models to capture systematicity of predicate replacements was limited to cases where the positions of the constituents were similar between the training and test sets.",
        "No models consistently drew inferences involving embedded clauses whose depths were two levels deeper than those in the training set.",
        "Our work will be useful in future research for realizing more advanced models that are capable of appropriately performing arbitrary inferences.",
        "We hope that our work will be useful in future research for realizing more advanced models that are capable of appropriately performing arbitrary inferences.']"
    ],
    "7625": [
        "This work is important because algorithms such as GloVe and word2vec continue to be effective methods in a wide variety of scenarios, particularly in the computational humanities and languages where large corpora are not available.",
        "We studied the relationship between linguistic properties and stability, something that has been previously understudied.",
        "Languages with more affixing tend to have less stable embeddings, and languages with no gender systems tend to have more stable embeddings.",
        "These insights can be used in future work to inform the design of embeddings in many languages."
    ],
    "7629": [
        "Using a frozen (English-only) BART model with additional parameters at the source side improves performance over a randomly initialised baseline for a language with high quality parallel data but without a pre-trained model trained on monolingual data from that language.",
        "Freezing the pre-trained model is important for this approach.",
        "Using learned positional embeddings at the embedding layer and fixed sinusoidal positional embeddings at each layer of the input module improves performance.",
        "For a multilingual pre-trained model, there are performance improvements on some (mostly distantly related) languages for multilingual many-to-one fine-tuning.",
        "Bilingual En \u2192 Xx finetuning does not show any improvement, but with less memory at training time compared to fine-tuning.",
        "For Xx \u2192 En bilingual fine-tuning, it is important to unfreeze the encoder-decoder attention and keep the rest of the decoder frozen.",
        "Fine-tuning layer-norm parameters as a parameter-efficient complement to adapter layers can improve performance.",
        "It is necessary to fine-tune the token embeddings for mBART experiments, which correspond to a large number of parameters.']"
    ],
    "7631": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The approach of using multi-view language representations with SVCCA offers important advantages for evaluating projected languages with entries in only one of the views.",
        "The benefits of incorporating typologically-enriched embeddings in multilingual NMT are noticeable in multilingual tasks such as language clustering and ranking related languages for multilingual transfer.",
        "The study demonstrates the potential of using typological features for improving the performance of multilingual NMT.",
        "The use of a typological feature prediction task and the inference of phylogenetic trees in the combined representation preserves the knowledge and language relationship encoded in both sources.']"
    ],
    "7632": [
        "Document-level context can be leveraged to obtain domain signals.",
        "The proposed models benefit from large context and also obtain strong performance in multidomain scenarios.",
        "Our experimental results show the proposed models obtain improvements of up to 1.0 BLEU in this difficult zero-resource domain setup.",
        "Furthermore, they show that document-level context should be further explored in future work on domain adaptation.",
        "Larger context would be beneficial for other discourse phenomena such as coherence.\"']"
    ],
    "7633": [
        "'Handling mathematical symbols is crucial for solving the task of natural language premise selection.'",
        "'Specific embeddings and representation for mathematical formulas and discourse are needed to improve the prediction of the task.'",
        "'The task becomes more challenging when the premises are transitive, suggesting that graph-based representations could be beneficial.'",
        "'Our dataset can be used in a different set of natural mathematical reasoning tasks, aiding researchers in creating mechanisms for improving machine understanding of mathematical text.'"
    ],
    "7634": [
        "We propose a new task for generating stories from outlines, which we call outline-conditioned story generation.",
        "We facilitate training by altering three datasets to include plot outlines as input for long story generation.",
        "Our approach uses PLOTMACHINES, which generates paragraphs using a high-level discourse structure and a dynamic plot memory keeping track of both the outline and story.",
        "Quantitative analysis shows that PLOTMACHINES is effective in composing tighter narratives based on outlines compared to competitive baselines."
    ],
    "7636": [
        "We demonstrated internet-to-embodied transfer of visual concept grounding.",
        "Our path re-ranking setting improves over prior work.",
        "Each stage of our transfer curriculum contributes significantly.",
        "We leveraged large-scale image-text data from the web to improve a discriminative path-instruction alignment model for VLN.",
        "Our ablations show that each stage of our transfer curriculum contributes significantly.\"']"
    ],
    "7640": [
        "The proposed method, Dynamic Memory Induction Networks (DMIN), achieves new state-of-the-art results on few-shot text classification tasks.",
        "The model\\'s performance is better than previous state-of-the-art models, and it generalizes well to unseen classes.",
        "The use of dynamic memory as a learning mechanism can be more general than what has been used in this study for few-shot learning.",
        "The method learns to completely remove subsets of inputs or hidden states through masking, circumventing an intractable search.",
        "The model\\'s faithfulness is validated in a controlled experiment, pointing out flaws of other attribution methods.",
        "The method sheds light on what different layers \"know\" about the input and where information about the prediction is stored in different layers.",
        "The study used BERT-based models on sentiment classification and question answering tasks to test the effectiveness of the proposed method.']"
    ],
    "7641": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our proposed method works on both language models with relative position embeddings and pre-trained language models with absolute position embeddings.",
        "Our SegaBERT approach outperforms BERT on general language understanding, sentence representation learning, and machine reading comprehension tasks.",
        "Our SegaBERT-large model outperforms RoBERTa-large on zero-shot STS tasks.\"']"
    ],
    "7642": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The choice of linguistic formalism can have substantial, linguistically meaningful effects on role-semantic probing results.",
        "Linguistic formalism is an important factor to be accounted for in probing studies.",
        "Our refined implementation of the edge probing framework coupled with the anchor task methodology enabled new insights into the processing of predicate-semantic information within mBERT.",
        "The influence of linguistic formalism per se is likely to be present for any probing setup that builds upon linguistic material.",
        "An investigation of how, whether, and why formalisms affect probing results for tasks beyond role labeling and for frameworks beyond edge probing constitutes an exciting avenue for future research.']"
    ],
    "7644": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.']"
    ],
    "7645": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The proposed method for extracting causality is based on a self-attentive BiLSTM-CRF-based solution.",
        "The method introduces the multihead self-attention mechanism to learn the dependencies between cause and effect.",
        "The approach of transferring Flair embeddings trained from a large corpus into the task is used to alleviate the problem of data insufficiency.",
        "The proposed method is effective in extracting causality, but the performance is still limited by the insufficiency of high-quality annotated data.",
        "Future work can examine how best to incorporate properties unique to scientific papers, like sections, citation contexts or scientific discourse roles, to improve TLDR generation models.",
        "The idea of a TLDR can differ between academic disciplines, and the exploration of this is left open for future work.",
        "The use of longer input contexts in TLDR generation may lead to improved performance.']"
    ],
    "7646": [
        "Our results suggest that adding counterexamples in order to encourage a model to \\'unlearn\\' weak features is likely to have the immediately desired effect (the model will perform better on examples that look similar to the generated counterexamples), but the model is unlikely to shift toward relying on stronger features in general.",
        "Specifically, in our experiments, the models trained on data augmented with a small number of counterexamples (< 100K) still fail to correctly classify examples which contain only the strong feature.",
        "We see also that data augmentation may become less effective as the underlying strong features become more difficult to extract."
    ],
    "7647": [
        "Model stealing is a practical concern for production NLP systems, and it can degrade the performance of both the victim model and imitation models.",
        "Adversarial examples are another concern for NLP systems, and companies have been caught stealing models in NLP settings.",
        "The current defense against adversarial attacks in NLP is na\u00efve and can be easily bypassed by sophisticated attackers.",
        "The authors hope to improve and deploy defenses against adversarial attacks in NLP and make security and privacy a more prominent focus of NLP research.",
        "Model stealing is a practical concern for production NLP systems, and it can degrade the performance of both the victim model and imitation models. [claim 1, p. 3]",
        "Adversarial examples are another concern for NLP systems, and companies have been caught stealing models in NLP settings. [claim 2, p. 3]",
        "The current defense against adversarial attacks in NLP is na\u00efve and can be easily bypassed by sophisticated attackers. [claim 3, p. 4]",
        "The authors hope to improve and deploy defenses against adversarial attacks in NLP and make security and privacy a more prominent focus of NLP research. [claim 4, p. 4]']"
    ],
    "7649": [
        "The CxC dataset provides a more complete set of relationships between images and captions than the raw MS-COCO image-caption pairs, as evidenced by the strong performance of the dual encoder model.",
        "The annotations in the CxC dataset validate the strong semantic alignment between images and their original captions, with an average similarity of 4.85.",
        "Co-captions (captions for the same image) have a lower average score of just 3.0, questioning their usefulness in training and evaluating paraphrase generation models.",
        "Images are essential as context for human evaluation in paraphrasing, as suggested by the need for images in the CxC dataset.']"
    ],
    "7651": [
        "The proposed method for pretraining the dense corpus index can replace traditional IR methods in open-domain QA systems, achieving stronger QA performance with less computational resources.",
        "The approach is powered by a better data generation strategy and a simple yet effective data sampling protocol for pretraining.",
        "The method can encourage more energy-efficient pretraining methods in the dense retrieval paradigm, making it more widely used in different domains.",
        "The proposed method achieves stronger QA performance than ORQA, which uses much more computational resources.",
        "The method is efficient and can be used in different domains.']"
    ],
    "7653": [
        "We proposed a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our fine-tuned generation model is conditioned on topics, stances, and aspects and can reliably create arguments using these three control codes.",
        "Arguments generated with this approach are of high quality in general and can be used to improve the performance of stance detection models.",
        "Our approach can successfully generate counter-arguments in a transparent and interpretable way.",
        "We defined the method of argument aspect detection for controlled argument generation and introduced a novel annotation scheme to crowdsource argument aspect annotations, resulting in a high-quality dataset (\u03b1 u =.67).\"']"
    ],
    "7656": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We propose a general signaling game framework in which fewer a priori assumptions are imposed on the conversational situations.",
        "Under appropriate conditions, messages become discrete without the analyst having to force this property into the language.",
        "We find no evidence of compositional structure using vector analogies and a generalization thereof but do find sharp boundaries between the discrete message clusters.\"']"
    ],
    "7657": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "There is some gap when compared to supervised approaches.",
        "We propose two self-supervised tasks to tackle the training data bottleneck.",
        "Our detailed evaluation shows that our approaches achieve much better performance than simpler baselines.",
        "We study the results further to identify research challenges for future research.",
        "We will release all resources for use by the research community.']"
    ],
    "7658": [
        "We present an online algorithm for space efficient coreference resolution that incorporates contributions from recent neural end-to-end models.",
        "We show it is possible to transform a model which performs document-level inference into an incremental algorithm, reducing memory usage during inference at virtually no cost to performance.",
        "Our approach provides an option for researchers and practitioners interested in modern coreference resolution models for tasks constrained by memory, like the modeling of book-length texts."
    ],
    "7659": [
        "The proposed model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The model is effective in few-shot text classification, leveraging external working memory with dynamic routing to track previous learning experience and adapt to unseen classes.",
        "The use of dynamic memory as a learning mechanism can be more general than what has been used here for few-shot learning.",
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The use of HAN and rich context tags improves the prediction quality of scientific documents.",
        "A strong and significant correlation between accept/reject labels and number of citations was demonstrated, signaling the usefulness of the latter as a measure of scholarly document quality.",
        "With more training data, HAN with structure-tags outperforms other strong and recently proposed scholarly document quality prediction models.']"
    ],
    "7660": [
        "Our approach to creating interpretable entity representations achieves high performance on entity-related tasks out of the box.",
        "We can reduce the size of our type set in a learning-based way for particular domains.",
        "These embeddings can be post-hoc modified through simple rules to incorporate domain knowledge and improve performance."
    ],
    "7662": [
        "The brevity problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Our novel approach to augment standard summarization model with significant ontological terms within the source is effective in improving the quality of generated response.",
        "The intrinsic evaluations on two publicly available real-life clinical datasets show the efficacy of our model in terms of ROUGE metrics.",
        "The extrinsic evaluation by domain experts further reveals the qualities of our system-generated summaries in comparison with gold summaries.\"]"
    ],
    "7663": [
        "'JEANS significantly outperforms existing SOTA models on benchmarks without introducing additional labeled data.'",
        "'JEANS offers improved entity alignment performance by exploiting incidental supervision from free text.'",
        "'JEANS can learn associations on KGs with different specificity and in hyperbolic spaces.'",
        "'The representation scheme in JEANS can be extended to better capture the associations for hierarchical ontologies.'",
        "'Incorporating hyperbolic lexical embedding techniques can improve the performance of JEANS.'"
    ],
    "7664": [
        "Transformers perform well on an unreasonable range of problems in natural language processing.",
        "Crossattention over contextualized embeddings is too slow.",
        "Dual encoding into fixed-length vectors may be insufficiently expressive, sometimes failing even to match the performance of sparse bagof-words competitors.",
        "We have used both theoretical and empirical techniques to characterize the fidelity of fixed-length dual encoders, focusing on the role of document length.",
        "Based on these observations, we propose hybrid models that yield strong performance while maintaining scalability."
    ],
    "7666": [
        "We create high-quality human judgments on two GenQA datasets, MS-MARCO and AVSD, and show that previous evaluation metrics are poorly correlated with human judgments in terms of the correctness of an answer.",
        "Our approach has a dramatically higher correlation with human judgments than existing metrics, showing that our model-based importance weighting is critical to measure the correctness of a generated answer in GenQA.",
        "We propose KPQA-metric, which uses the pre-trained model that can predict the importance weights of words in answers to a given question to be integrated with existing metrics.",
        "Our model-based importance weighting is critical to measure the correctness of a generated answer in GenQA.",
        "Previous evaluation metrics are poorly correlated with human judgments in terms of the correctness of an answer.\"']"
    ],
    "7669": [
        "Transferability of linguistic knowledge to commonsense knowledge is exhibited through the extraction of a large amount of high-quality commonsense knowledge from linguistic graphs.",
        "A new resource of commonsense knowledge, TransOMCS, is presented, which is extracted from linguistic graphs and formatted in the OMCS subset of ConceptNet, with two orders of magnitude larger than the original OMCS.",
        "While TransOMCS is noisier than OMCS, it can still make significant contributions to downstream tasks due to its larger coverage, as evident by the extrinsic experiments.']"
    ],
    "7670": [
        "Our model achieves state-of-the-art performance on all datasets, improving previous scores up to 2.6%.",
        "The datasets used in our experiments are in English, but we expect our methodology to work in any language as long as there is a synonym dictionary for the language.",
        "An extrinsic evaluation of our methods is needed to prove the effectiveness of learned biomedical entity representations and the quality of the entity normalization in downstream tasks."
    ],
    "7671": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Tagging ambiguity is mostly local while dependency parsing heavily depends on POS tags to predict syntactic structures.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR), with performance not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "Existing works have studied very little about the temporal ordering of events in reading comprehension.",
        "TORQUE, a new English machine reading comprehension (MRC) dataset of temporal ordering questions, has 3.2k news snippets, 9.5k hard-coded questions, and 21.2k human-generated questions.",
        "An MRC setting allows for more convenient representation of these temporal phenomena than conventional formalisms.",
        "Even a state-of-the-art language model, RoBERTa-large, falls behind human performance by a large margin, necessitating more investigation on improving MRC on temporal relationships in the future.']"
    ],
    "7673": [
        "Our proposed method outperforms benchmark models across different datasets.",
        "We have developed a new task of transferring semantic role models from the labeled verbal domain to the unlabeled nominal domain.",
        "Our model realizes the intuition of exploiting the similarity in selectional preferences across domains and introduces a VAE model.",
        "We would like to explore alternative approaches to transfer learning, such as using paraphrase models or pivoting via parallel data.",
        "We plan to consider other languages and other linguistic constructions, such as prepositions and parallel data.",
        "Our method outperforms both supervised and unsupervised baselines.",
        "We will evaluate transfer quality using Sem-Link or AMR.",
        "We will obtain extra labeled verbal data by using a 0 A 1 A 2 A 3 A 4 A 5 A M -A D V A M -C A U A M -D I R A M -E X T A M -L O C A M -M N R A M -N E G A M -P R D A M.']"
    ],
    "7675": [
        "We propose a new framework of Knowledge Triplet Learning over knowledge graph entities and relations.",
        "Learning all three possible functions, f r , f h , and f t helps the model to perform zero-shot multiple-choice question answering.",
        "Our framework achieves state-of-the-art in the zero-shot question answering task, achieving performance like prior supervised work.",
        "Our framework sets a strong baseline in the few-shot question answering task."
    ],
    "7676": [
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We have shown that transfer learning approaches can be used successfully on Narabizi, even in zero-shot scenarios and unsupervised adaptation scenarios.",
        "Our results pave the way to using transfer learning approaches to build NLP tools for Narabizi and other vernacular varieties of Arabic written in Latin script, and more generally for any low resource language.",
        "Our paper sheds light on a way to initiate the development of NLP ecosystems for languages and language varieties that are increasingly used online, for which NLP is badly needed, but for which few resources, if any, are available to date.']"
    ],
    "7677": [
        "CDL utilizes two kinds of rewards to enhance emotion and content simultaneously via dual learning.",
        "Experimental results show that CDL can generate fluent, coherent, informative as well as emotional responses.",
        "The safe response problem deteriorates and hurts the content consistency when existing methods in this field only focus on the emotion expression of target label.",
        "CDL utilizes curriculum learning to enhance efficiency."
    ],
    "7682": [
        "The proposed BiFlaG model achieves new state-of-the-art results on nested NER tasks.",
        "The BiFlaG model consists of two interacting subgraph modules, allowing for bidirectional interaction between them.",
        "The flat module is responsible for outermost entities, while the graph module focuses on inner entities.",
        "The model can handle non-nested structures by simply removing the graph module.",
        "The BiFlaG model outperforms previous state-of-the-art models in the same strict setting.']"
    ],
    "7684": [
        "USR achieves statistically significant correlations with human judgement.",
        "The results hold across two datasets, Topical-Chat (Gopalakrishnan et al., 2019) and PersonaChat (Zhang et al., 2018).",
        "USR is configurable and composed of several specific dialog quality sub-metrics.",
        "USR can be used for model selection and hyperparameter tuning.",
        "USR should not be used to claim superior performance over another method.",
        "USR may not work with non-generative models, which were not addressed here.",
        "Responses produced by a model that is too similar to the evaluation models (e.g., to DR) are a particular concern.']"
    ],
    "7686": [
        "The novel task of answering regex queries over incomplete knowledge bases is presented.",
        "Two datasets and query workloads are provided for the task.",
        "A new model, RotatE-Box, is introduced, which models more relational inference patterns than other approaches.",
        "The baselines in this work are the first step in modeling regex operators, but there is a lot of scope for future research.",
        "Kleene Plus poses novel modeling challenges, such as idempotent unary operator and infinite union of path queries.",
        "In the future, the authors plan to work on constructing parameterized operator functions that honor these properties of the operator and can be applied compositionally and recursively, while being sufficiently expressive.']"
    ],
    "7689": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Creation of such a dataset would greatly increase the challenge for future algorithms and better match real-world usage.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments.\"']"
    ],
    "7690": [
        "We present GoEmotions, a large, manually annotated, carefully curated dataset for fine-grained emotion prediction.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We explore the use of a projection-based method for attenuating biases.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans\\' judgments.",
        "We are aware that the dataset contains biases and is not representative of global diversity.",
        "The emotion pilot model used for sentiment labeling, was trained on examples reviewed by the research team.",
        "Future work can explore the cross-cultural robustness of emotion ratings, and extend the taxonomy to other languages and domains.\"']"
    ],
    "7693": [
        "Clinical Reading Comprehension (CliniRC) tasks should not only be large-scale but also less noisy and more diverse.",
        "Future clinical QA datasets should include questions that involve complex relations and are across different domains.",
        "More advanced external knowledge incorporation methods as well as domain adaptation methods can be carefully designed and systematically evaluated."
    ],
    "7694": [
        "We proposed two additional measures for NMT robustness which can be applied when both original and noisy inputs are available.",
        "Our robustness metrics reveal a clear trend of subword regularization being much more robust to input perturbations than standard BPE.",
        "Furthermore, we identify a strong correlation between robustness and consistency in these models indicating that consistency can be used to estimate robustness on data sets or domains lacking reference translations."
    ],
    "7695": [
        "We propose three multi-scale transformer architectures for language modeling and show that they achieve competitive or better perplexities compared to vanilla transformer lan-guage models for the same model memory footprint across three different language modeling benchmarks.",
        "Our multi-scale architectures leverage the robustness of transformers to word ordering in distant context windows.",
        "The representation produced by coarser scales which operate on much shorter sequences, is combined with the representation of the finest scale, thereby reducing the overall computational and memory cost.",
        "Future work will explore how to combine our multi-scale architectures with adaptive attention head spans and how different types of information (e.g., topic, grammatical correctness) are differently affected by architecture scale choices.",
        "We experimented with adding this criterion of being able to predict the subsequent bag of words distribution in our top-down model at all scales, but didn\\'t observe any improvements.",
        "To ensure that representations at different scales learn meaningful representations, we qualitatively analyze them by looking at nearest neighbors computed using them in Table 6 . We found that nearest neighbors computed on Wikitext-103 typically returned segments within highly related topics.\"']"
    ],
    "7696": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits."
    ],
    "7697": [
        "Massively multilingual transformers (MMTs) have recently been praised for their zero-shot transfer capabilities that mitigate the data scarcity issue.",
        "Similar to earlier language transfer paradigms, MMTs perform poorly in zero-shot transfer to distant target languages, and for languages with smaller monolingual data for pretraining.",
        "Our results have revealed that structural language similarity determines the transfer success for lower-level tasks like POS-tagging and parsing; on the other hand, the pretraining corpora size of the target language is crucial for explaining transfer results for higher-level language understanding tasks.",
        "The MMT potential on distant and lower-resource target languages can be quickly unlocked if they are offered a handful of annotated target-language instances.",
        "This finding provides strong evidence towards intensifying future research efforts focused on the more effective few-shot learning setups.\"']"
    ],
    "7699": [
        "We provide recommendations for future evaluation protocols",
        "In the absence of multiple held-out samples, using biased splits better approximates real-world performance",
        "Evaluating on new samples is superior and also enables significance testing across datasets",
        "Several benchmarks already provide multiple, diverse test sets",
        "What explains the high variance across samples in NLP?",
        "One reason is the dimensionality of language",
        "but in \u00a7A.5 we also show significant impact of temporal drift."
    ],
    "7700": [
        "The ever-expanding number of entities in knowledge graphs warrants the exploration of knowledge graph completion methods that can be applied to emerging entities without retraining the model.",
        "Our method provides explainable reasoning paths for the inferred links as support evidence.",
        "Through experiments, we demonstrate that our model significantly outperforms the baselines across the new inductive benchmark datasets introduced in this paper.",
        "Prior approaches assume a static snapshot of the knowledge graph, while our joint framework for inductive representation learning predicts missing links in a dynamic knowledge graph with many emerging entities.']"
    ],
    "7701": [
        "The proposed model, SPADE, can extract highly structured information from documents with complex layouts.",
        "The model formulates document IE as a spatial dependency graph construction problem, providing a powerful unified framework for extracting hierarchical information without feature engineering.",
        "The model demonstrates effectiveness in extracting structured information from real-world documents such as receipts, name cards, and invoices.",
        "The model achieves popular form understanding tasks with high accuracy.']"
    ],
    "7702": [
        "Our retrieval-based model using query-to-query similarity can achieve high performance in WikiSQL semantic parsing task even when labeled data is scarce.",
        "Pre-training using natural language paraphrasing data can help the generation of logical forms in our query-similarity based retrieval approach.",
        "Our retrieval-based semantic parser can generate unseen logical forms during the training stage.",
        "Careful design of data distribution is necessary for optimal performance of the model under a data-scarce environment.']"
    ],
    "7703": [
        "The proposed method, MHGRN, can leverage general knowledge via multi-hop reasoning over interpretable structures.",
        "The method merges relations that are close in semantics as well as in the general usage of triple instances in ConceptNet.",
        "The approach is principled and scalable.']"
    ],
    "7705": [
        "Tackling hallucination is a critical challenge for abstractive summarization, perhaps the most critical.",
        "NLU-driven pretraining in neural text generators is key to generating informative, coherent, faithful, and factual abstracts, but it is still far from solving the problem.",
        "Measures such as ROUGE or BERTScore will not be sufficient when studying the problem; semantic inference-based automatic measures are better representations of true summarization quality."
    ],
    "7707": [
        "The proposed generator of multi-hop knowledge paths provides structured evidence for answering commonsense questions.",
        "The generator, learned by fine-tuning GPT-2 on random walks sampled from ConceptNet, produces a path between each pair of question and answer entities.",
        "All generated paths are aggregated into a knowledge embedding and fused with a context embedding given by a text encoder for classification.",
        "The QA framework enhanced with this generator outperforms both pre-trained language models and prior KG-augmented methods on two commonsense QA benchmarks.",
        "The accuracy gain increases with less training data.",
        "Automatic-and human-based evaluations of the generated paths yield high scores for their validity, novelty, and relevance.",
        "Future research should investigate how to optimally fuse the knowledge and the context embeddings.",
        "It should also address the ambiguity of the entity mentions in the questions, the answers, and the lexical nodes in ConceptNet.']"
    ],
    "7708": [
        "the inherent lack of sufficient inter-lingual supervision signals\" is a key shortcoming of current LRL XEL techniques.",
        "Proposed method leverages query logs to address this challenge, leading to a 25% increase in candidate generation.",
        "Future research direction: improving candidate ranking in LRL by incorporating coherence statistics and entity types.",
        "Query logs can be applied to other cross-lingual tasks like relation extraction and Knowledge Base completion."
    ],
    "7709": [
        "The dataset of emojis and emotions is a new resource for researchers.",
        "There is a strong association between emojis and emotions, as revealed by a study of 9 human raters.",
        "The associations between emojis and emotions are language-independent.",
        "Different cultures may have different associations with the same emojis.",
        "Automated methods can accurately estimate emotional association ratings for emojis.",
        "High-quality affective intensity information at the lexical level is important for predicting emotions.",
        "The success of the methods suggests potential downstream applications such as consumer sentiment and emotion analytics, context-sensitive emoji recommendation, and computational social science.']"
    ],
    "7711": [
        "The ability of transformers to incorporate large contexts effectively in multiple layers has improved the effectiveness of NLP tools, but this comes at a significant complexity cost.",
        "Modeling large contexts may not always be necessary, as we have shown that substantial improvements in inference speed and memory reduction can be achieved while retaining most of the original model's accuracy.",
        "The decomposition model provides a simple yet strong starting point for efficient QA models as NLP moves towards increasingly larger models handling wider contexts.",
        "The distillation techniques used in the model reduce the performance gap with respect to the original model.",
        "The architecture of the decomposition model remains largely the same as the original model, allowing us to avoid repeating pretraining and use the original model weights for finetuning.\"]"
    ],
    "7712": [
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\')",
        "the alignment target for generating multilingual word embeddings also affects such bias",
        "gender bias commonly exists across different languages",
        "choosing the embeddings aligned to a gender-rich language can reduce the bias",
        "this study is limited to European languages due to resource limitations",
        "future research should focus on analyzing and mitigating bias in multilingual embeddings, particularly for languages with different grammatical genders (such as Czech and Slovak)\".']"
    ],
    "7713": [
        "The state-of-the-art authorship obfuscation methods are not stealthy, as they cause degradation in text smoothness that can be detected by a detector.",
        "Our proposed obfuscation detectors were effective at classifying obfuscated and evaded documents with high F1 scores (as high as 0.92 and 0.95, respectively).",
        "The degradation in text smoothness caused by authorship obfuscators provides an opportunity for future research on building stealthy authorship obfuscation methods.",
        "Obfuscation methods should strive to preserve text smoothness in addition to semantics.']"
    ],
    "7714": [
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the wellcharacterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "We presented AVA, an automatic evaluator method for QA systems.",
        "Our data collection strategy and model design to enable AVA development.",
        "We collected seven different datasets, classified into three different types, which we used to develop AVA in different stages.",
        "We proposed different Transformer-based modeling designs of AVA to exploit the feature signals relevant to address the problem.",
        "Our extensive experimentation has shown the effectiveness of AVA for different types of evaluation: point-wise and system-wise over Accuracy, MAP and MRR.\"']"
    ],
    "7717": [
        "We introduce the concept of module-wise faithfulness, a systematic evaluation of faithfulness in neural module networks (NMNs) for visual and textual reasoning.",
        "Na\u00efve training of NMNs does not produce faithful modules.",
        "Our approach leads to much higher module-wise faithfulness at a low cost to performance.",
        "We encourage future work to judge model interpretability using the proposed evaluation and publicly published annotations, and explore techniques for improving faithfulness and interpretability in compositional models."
    ],
    "7720": [
        "The proposed \"hard-coded\" Gaussian attention can rival multi-headed attention for neural machine translation, despite lacking any learned parameters.",
        "Encoder and decoder self-attention is not crucial for translation quality compared to cross attention.",
        "A model with hard-coded self-attention and just a single cross attention head can perform slightly worse than a baseline Transformer.",
        "The proposed approach provides a foundation for future work into simpler and more computationally efficient neural machine translation.",
        "A mixed position for hard-coded self-attention works best for the task, with the best results achieved using a (l, l) configuration for the encoder and decoder."
    ],
    "7721": [
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Our approach employs Synthetic Attention, which is a new Transformer model that demonstrates competitive performance compared to vanilla self-attention on multiple tasks.",
        "We found that we are not able to replace cross-attention with simpler variants in most cases.",
        "Our Synthesizers can outperform or match Dynamic Convolutions and Factorized Synthesizers can outperform other low rank Linformer models.\"']"
    ],
    "7722": [
        "BERT-kNN sets a state-of-the-art on LAMA without any further training.",
        "BERT-kNN can be easily enhanced with knowledge about new events that are not covered in the training text used for pretraining BERT.",
        "The kNN component of BERT-kNN provides utility for explainability, as kNN predictions are based on retrieved contexts that can be shown to users to justify an answer."
    ],
    "7723": [
        "the benefits of transfer learning are more pronounced than previously thought",
        "especially when target training data is limited",
        "we develop methods that learn vector representations of tasks that can be used to reason about the relationships between them",
        "these task embeddings allow us to predict source tasks that will likely improve target task performance",
        "data size, the similarity between the source and target tasks and domains, and task complexity are crucial for effective transfer, particularly in data-constrained regimes"
    ],
    "7724": [
        "The authors have presented a new common sense dataset with many novel features.",
        "The dataset includes a large set of raw answer strings and clustering of these strings for generative evaluation.",
        "Existing fine-tuned state-of-the-art models have a way to go before modeling the distribution of this common sense data.",
        "The task of modeling common sense is appealing from a practical perspective, as millions of people have played phone-based games based on this premise.",
        "Prior works have obtained valuable annotations from trivia game participants.",
        "The dataset lays the foundation for larger-scale data collection which leverages people's natural interest to encourage high-quality answers to more common sense questions.",
        "The authors have defined a similarity function based on WordNet, which allows for the calculation of synset similarity.\"]"
    ],
    "7725": [
        "We propose VisCOLL, a novel continual learning setup for visually grounded language acquisition.",
        "VisCOLL presents two main challenges: continual learning and compositionality generalization.",
        "Continual learning algorithms struggle at composing phrases which have a very large search space, and show very limited generalization to novel compositions.",
        "Future works include looking into models and continual learning algorithms to better address the challenge."
    ],
    "7727": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The ConMask model can capture the correct relationship even when the name of the entity does not appear in the description.",
        "The model has some limitations and room for improvement, such as the nature of the relationship-dependent content masking.",
        "Applying a filter to modify the list of predicted target entities may be easy and effective.",
        "The ConMask model exceeds the performance of previously proposed methods for the same task.",
        "External knowledge can be useful for the task of cross-domain sentiment analysis.']"
    ],
    "7728": [
        "We explored a simple approach to task-oriented dialogue (SimpleTOD) that uses a single, causal language model.",
        "Our proposed approach outperformed all prior methods in dialogue state tracking as well as in action and response generation in the end-to-end setting.",
        "The pre-trained weights were essential, but to leverage these weights fully we had to guide the system with special tokens that mark user and system responses as well as different portions of the sequence related to different sub-tasks.",
        "We found that SimpleTOD was effective at tracking dialogue state over long context with many turns and required no more than greedy decoding to achieve new state-of-the-art results despite noisy annotations.",
        "We hope that these results and the code, models, and discovered noisy annotations will encourage further exploration of simple, unified approaches for dialogue systems.\"']"
    ],
    "7732": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "DQI is effective in reducing spurious biases during the data creation process.",
        "Retraining BERT and ROBERTa on the higher quality, renovated SNLI dataset has resulted in an increase in their generalization capability on out-of-distribution datasets.",
        "DQI takes the process of dynamic dataset creation forward, and serves as a means of benchmarking the true progress of AI.\"']"
    ],
    "7733": [
        "Polyglot NER models do not improve over their monolingual counterparts, despite using more labeled data.",
        "The polyglot models learn more from observing multiple languages and this information can transfer to each language.",
        "The ideal optima for a monolingual model may not be achievable using standard training objectives without observing other languages.",
        "Jointly optimizing all languages naively may provide too challenging an optimization landscape to obtain that optima for each language simultaneously.",
        "The transferability of the polyglot parameters to unseen languages depends on a variety of factors, including relatedness to languages in the original polyglot training set.']"
    ],
    "7734": [
        "We proposed a new method to train nonautoregressive neural machine translation systems via minimizing pretrained energy functions with inference networks.",
        "Our method seeks to expand upon energy-based translation using our proposed approach.",
        "In the future, we seek to expand upon energy-based translation using our method.",
        "Our method can potentially improve the performance of nonautoregressive neural machine translation systems.",
        "Our proposed approach has the potential to expand upon energy-based translation and improve the performance of neural machine translation systems."
    ],
    "7735": [
        "We proposed a method to automatically select the output based on a language score when decoding with a multilingual model.",
        "The system does not need a language identification module, so the latency of the system will be improved but the accuracy is still similar to the case of input language is specified.",
        "We presented an approach to develop a multilingual model for English and Vietnamese using a universal phone set and a single DNN architecture.",
        "A multilingual approach is a simple but effective solution to enhance a speech recognition model not only for under-resourced domains but also for foreign words.",
        "The experimental results showed that a multilingual approach can reduce WER by 2.7% and improve the performance on foreign words by 32.4%.",
        "In subsequent studies, we will improve the method and evaluate in more detail for cases of cross-lingual speaking happens at the word level.\"']"
    ],
    "7737": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The proposed method outperformed single models in both text classification and sequence labeling tasks.",
        "The proposed method with TFM:BERT surpassed the normal ensemble on the IMDB and Rotten datasets, while its parameter size was 1/K-times smaller.",
        "Explicitly creating virtual models within a single model improves performance.",
        "The proposed method is not limited to the two aforementioned tasks, but can be applied to any NLP as well as other tasks such as machine translation and image recognition.']"
    ],
    "7738": [
        "Our framework is inspired by existing cognitive theories on human memory recall and recognition, and can be easily understood by users as well as provide reasonable explanations to justify its prediction.\" (related to the interpretability of the model)",
        "Essentially, it leverages corpus-level statistics to recall associative contexts and recognizes their relational connections as model rationales.\" (related to the methodology of the framework)",
        "Compared with a comprehensive list of baseline models, our model obtains competitive predictive performances.\" (related to the performance of the model)",
        "Moreover, we demonstrate its interpretability via expert evaluation and case studies.\" (related to the interpretability of the model)']"
    ],
    "7739": [
        "We propose a method to synthesize dialogues for a new domain using an abstract dialogue model, combined with a small number of domain templates derived from observing a small dataset.",
        "For transaction dialogues, our technique can bootstrap new domains with less than 100 templates per domain, which can be built in a few person-hours.",
        "With this little effort, it is already possible to achieve about 2/3 of the accuracy obtained with a large-scale human annotated dataset.",
        "Furthermore, this method is general and can be extended to dialogue state tracking beyond transactions, by building new dialogue models.",
        "We show improvements in joint accuracy in zero-shot and few-shot transfer learning for both the TRADE and BERT-based SUMBT models.",
        "Our technique using the SUMBT model improves the zero-shot state of the art by 21% on average across the different domains.",
        "This suggests that pretraining complements the use of synthesized data to learn the domain, and can be a general technique to bootstrap new dialogue systems.\"']"
    ],
    "7741": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Monolingual data augmentation reduces overfitting and improves the translation quality of NAR-MT models.",
        "The benefits of monolingual data are orthogonal to other techniques like iterative refinement.",
        "The observed improvements may be language-dependent."
    ],
    "7743": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "We have shown that complex features of polysynthetic morphology, such as reduplication and distributional morphotactic information, can be simulated in the dataset and used to train a robust neural morphological analyzer for a polysynthetic language.",
        "The concept of bootstrapping a model implies an iterative development story where much of the scaffolding used in early efforts will eventually fall away.",
        "Once the bootstrapped model has been used to tag verbs containing reduplication, we can confirm the model's high-confidence predictions and retrain.",
        "The pattern of training robust systems on data that has been augmented by the knowledge captured in symbolic systems could be applied to areas outside of morphological analysis, and is a promising avenue of future exploration.\"]"
    ],
    "7746": [
        "Our results show that BERT-fuse GED was one of the most effective techniques when it was fine-tuned with GEC corpora.",
        "In future work, we will investigate whether BERT-init can be used effectively by using methods to deal with catastrophic forgetting.",
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The experimental results show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our core idea is to utilize the self-attention mechanism to effectively capture the long distant dependency relations.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.\"']"
    ],
    "7747": [
        "The proposed model for Aspect Based Sentiment Analysis relies on relative positions of words with respect to aspect terms, and uses parameter-less decay functions to weight words based on their distance from aspect terms.",
        "The use of decay functions in the proposed model is effective in capturing the relationship between words and aspect terms, and does not require any explicit positional information about aspect terms.",
        "The proposed model achieves better results compared to other recent architectures that do not use positional information of aspect terms, demonstrating the strength of the decay idea in the proposed model.']"
    ],
    "7748": [
        "The proposed model significantly alleviates the information hallucination, repetition, and missing problems in data-to-text generation without sacrificing fluency and diversity.",
        "The model is end-to-end trainable, domain-independent, and allows explicit control over the structure of generated text.",
        "The model is interpretable in the correspondence between segments and input records, allowing for easy combination with hand-engineered heuristics or user-specific requirements to further improve performance.",
        "The model can be used to generate text that is both fluent and diverse, while also controlling the structure of the generated text.']"
    ],
    "7749": [
        "'We associated font with written text and tackled the problem of font recommendation from the input text.'",
        "'We collected more than 1,300 short written texts and annotated them with ten fonts.'",
        "'We formulated this task as a ranking problem and compared different models based on emotional and contextual representations that exploit label distribution learning to predict fonts.'",
        "'The current approach covers a fixed number of fonts, but it can be extended to support a larger set of fonts.'",
        "'We can use font similarity techniques and enable users to pick a group of fonts.'\"]"
    ],
    "7751": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our models capture both local characteristics and global interactions of entities from the input, thus generating summaries of higher quality.",
        "In tandem with the graph representation, our cloze reward further improves summary content.",
        "Human evaluation confirms that our graph-augmented models trained with the cloze reward produce more informative summaries and significantly reduce unfaithful errors.",
        "We use the base version of BERT model (Devlin et al., 2019) to select candidate answers and we finetune the base version of RoBERTa model (Liu et al., 2019) to build our QA model.\"']"
    ],
    "7752": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR).",
        "Different architectures often have similar internal representations, but differ at the level of individual neurons.",
        "Higher layers are more localized than lower ones.",
        "Comparing finetuned and pre-trained models, we found that higher layers are more affected by fine-tuning in their representations and attention weights, and become less localized.",
        "Our approach is complementary to the linguistic analysis of models via probing classifiers.",
        "The similarity analysis may also help improve model efficiency, for instance by pointing to components that do not change much during fine-tuning and can thus be pruned.",
        "One could also study whether a high similarity entail that two models converged to a similar solution.']"
    ],
    "7754": [
        "The existing semantically-motivated metrics for reference-free evaluation of MT systems have poor correlation with human estimates of translation quality.",
        "The authors investigate cutting-edge models for inducing cross-lingual semantic representations, such as cross-lingual (contextualized) word embeddings and cross-lingual sentence embeddings.",
        "The use of reference-free metrics based on these cutting-edge models can surpass the reference-based BLEU in segment-level MT evaluation.",
        "The reference-free coupling of cross-lingual similarity scores with the target-side language model can be used to mitigate the undesired phenomenon of literal word-by-word translations.",
        "The use of an additional (weakly-supervised) cross-lingual alignment step can reduce the mismatch between representations of mutual translations.",
        "The authors believe that their results have two relevant implications: (1) they portray the viability of reference-free MT evaluation and (2) they indicate that reference-free MT evaluation may be the most challenging (\"adversarial\") evaluation task for multilingual text encoders.",
        "The inability to capture semantically non-sensical word-by-word translations or paraphrases remains hidden in their common evaluation scenarios.']"
    ],
    "7755": [
        "Our approach outperforms all previous methods (including supervised ones) on the evidence selection task on two datasets: MultiRC and QASC.",
        "When these evidence sentences are fed into a RoBERTa answer classification component, we achieve the best QA performance on these two datasets.",
        "Considerable improvements can be obtained by aggregating knowledge from parallel evidence chains retrieved by our method.",
        "Our approach improves QA performance.",
        "Simple unsupervised components of AIR will benefit future work on supervised neural iterative retrieval approaches by improving their query reformulation algorithms and termination criteria.']"
    ],
    "7764": [
        "The first system that can reliably explain the effects of a perturbation.",
        "QUARTET not only predicts meaningful explanations but also achieves a new state-of-the-art on the end-task itself.",
        "Models can make better predictions when forced to explain.",
        "Additional background context from the Web may improve explainable reasoning.",
        "Structured explanations can be applied to other NLP tasks."
    ],
    "7766": [
        "The proposed approach of representing summaries as a set of key points scored according to their relative salience is effective and feasible.",
        "A domain expert can quickly come up with a short list of pro and con key points per topic, even without being exposed to the arguments themselves.",
        "Automatically matching arguments to key points is a challenging task that cannot be effectively solved using unsupervised methods based on word or sentence-level embedding.",
        "By using state of the art supervised learning methods for match scoring, together with an appropriate key point selection policy for match classification, promising results can be achieved on this task.",
        "The natural next step for this work is the challenging task of automatic key point generation.",
        "Applying the methods presented in this work to automatically-mined arguments is another potential direction for future work.",
        "Detecting the more implicit relations between the argument and the key point, as seen in the error analysis, is another intriguing direction for future work.']"
    ],
    "7767": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Our benchmark system represents a reasonable approach to solving the problem based on past work and highlights many directions for improvement, e.g. joint modeling and making better use of distributional semantic information."
    ],
    "7768": [
        "Our new NER corpus is an ideal benchmark dataset for contextual word representations.",
        "Our proposed attention-based model, Soft-NER, outperforms state-of-the-art NER models on this dataset.",
        "Our code recognition model captures additional spelling information beyond the contextual word representations and consistently helps to improve the NER performance.",
        "Our corpus, Stack Overflow-specific BERT embeddings, and named entity tagger will be useful for various language-and-code tasks, such as code retrieval, software knowledge base extraction, and automated question-answering.",
        "The regular expressions are developed to recognize specific categories of coderelated entities.\"']"
    ],
    "7769": [
        "The proposed Dynamic Memory Induction Networks (DMIN) achieve new state-of-the-art results on few-shot text classification tasks.",
        "The use of dynamic memory as a learning mechanism can be more general than what has been used in previous few-shot learning models.",
        "The model can adapt and generalize better to support sets and unseen classes.",
        "The approach can be applied to other clustering problems beyond typeface clustering.",
        "The developed model can be incorporated into OCR models for historical text.']"
    ],
    "7770": [
        "The current datasets and models for visual referring expressions fail to make effective use of linguistic structure.",
        "Existing HRED models and their attention variants can be significantly outperformed by our proposed model.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "There is still significant scope for improvement in the current models and datasets for visual referring expressions."
    ],
    "7774": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Our method for solving the brevity problem is helpful and easy, and we hope to see it included in make stronger baseline NMT systems.",
        "Solving label bias in general may lead to more improvement.",
        "The brevity problem can be seen as the simplest case of the more general problem of label bias and the more general solution of globally-normalized models for NMT.']"
    ],
    "7775": [
        "This paper represents the first attempt at generating full-length SOAP notes by summarizing transcripts of doctor-patient conversations.",
        "We proposed a spectrum of extractive-abstractive summarization methods that leverage: (i) section-structured form of the SOAP notes and (ii) linked conversation utterances associated with every SOAP note sentence.",
        "The proposed methods perform better than a fully abstractive approach and standard extractive-abstractive approaches that do not take advantage of these annotations.",
        "We demonstrate the wider applicability of proposed approaches by showing similar results on the public AMI corpus which has similar annotations and structure.",
        "Our work demonstrates the benefits of creating section-structured summaries (when feasible) and collecting evidence for each summary sentence when creating any new summarization dataset.\"']"
    ],
    "7776": [
        "Using a variety of IR techniques, we are able to obtain a 68% improvement in accuracy over a baseline system.\" This claim highlights the effectiveness of the Named Entity Linking approach and the potential for improving the accuracy of the system.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.\" This claim emphasizes the superior performance of the ConMask model compared to other KGC models, indicating its potential as a useful tool for Named Entity Linking tasks.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.\" This claim highlights the importance of considering late gaze features when performing Named Entity Linking, as they can provide valuable information for disambiguating categories.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.\" This claim suggests that the Named Entity Linking approach using gaze features and part-of-speech information can be effective even without relying on text processing, which could be beneficial for real-world applications.",
        "Future work includes exploring the possibility of leveraging neural models to improve semantic matching capabilities of the system.\" This claim suggests that there is potential for further improvement in the Named Entity Linking approach by incorporating neural models to enhance the semantic matching capabilities of the system.']"
    ],
    "7779": [
        "The proposed techniques for data augmentation have the potential to generate almost limitless synthetic data for the task of hypernymy detection.",
        "The proposed techniques perform better than extending the training set with additional nonsynthetic data drawn from an external knowledge source in most cases.",
        "The approach is effective even when the word vector space model has already been specialized for hypernymy detection.",
        "The use of data augmentation leads to increasing performance with a more complex classifier.",
        "Future work includes leveraging data augmentation for more complex models and extending the approach to a multilingual setup as well as domains with a more specialized vocabulary such as Healthcare or Fashion.']"
    ],
    "7780": [
        "using a fusion module can increase the precision of knowledge extraction when it works above all individual extractor components.",
        "the acquired results show that using a fusion module can increase the precision of knowledge extraction.",
        "future work will address the low recall and precision of some of our extractor components.",
        "we plan to add some new supervised and unsupervised modules to increase the recall and precision of the system.",
        "we will provide some new supervised and unsupervised modules to increase the recall and precision of the system.",
        "we plan to add some extractors which will work on other types of resources, such as tabular data.",
        "one of the potential solutions is to utilize crowdsourcing to accelerate the triple verification process.",
        "the output of current universal dependency parser for the Persian language has many errors, especially for long sentences.",
        "we can create another UD corpus by converting Dadegan dependency parser dataset and implement a better UD parser in the future works.\"']"
    ],
    "7783": [
        "We propose a two-step framework for paraphrase generation that involves constructing diverse syntactic guides followed by actual paraphrase generation.",
        "Our approach can be used to produce paraphrases that achieve a better quality-diversity trade-off compared to previous methods and strong baselines.",
        "Our experiments show that this approach can be used to produce paraphrases with improved quality and diversity compared to previous methods and strong baselines.",
        "The proposed two-step framework for paraphrase generation can be used to generate high-quality paraphrases that are diverse and contextually appropriate.",
        "Our method is able to produce paraphrases that are not only of high quality but also diverse, which is important for maintaining the readability and accessibility of the content.\"']"
    ],
    "7784": [
        "The proposed attentional Seq2seq model with a neural style component achieves superior performance in unsupervised style transfer tasks.",
        "The model is trained to reconstruct input sentences and predict word-level style relevance simultaneously, leveraging the predicted style relevance for better style transfer.",
        "The use of a pre-trained style classifier provides quantified style relevance supervision information for the model's training.",
        "The model can exploit the word-level predicted style relevance for better style transfer, leading to improved performance in unsupervised style transfer tasks.",
        "The authors plan to adapt variational neural network to refine their style transfer model, which has shown effectiveness in other conditional text generation tasks, such as machine translation.\"]"
    ],
    "7787": [
        "We identify how switching patterns can be effective in improving three different NLP applications.",
        "We present a set of nine features that improve upon the state-of-the-art baselines.",
        "We exploit modern deep learning machinery to improve performance further.",
        "This model can be improved further by pumping the switching features in the final layer of the deep network.",
        "In future, we would like to extend this work for other language pairs.",
        "We have seen examples of such switching in English-Spanish 10 and English-Telugu 11 pairs also.",
        "Further, we plan to investigate other NLP applications that can benefit from the simple linguistic features introduced here.\"']"
    ],
    "7788": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "We proposed a new multi-turn dialogue generation model, namely ReCoSa.",
        "Our core idea is to utilize the self-attention mechanism to effectively capture the long distant dependency relations.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "We created two high-quality manually annotated datasets (NEWSELA-MANUAL and WIKI-MANUAL) for training and evaluation.",
        "Using the neural CRF sentence aligner, we constructed two largest sentence-aligned datasets to date (NEWSELA-AUTO and WIKI-AUTO) for text simplification.",
        "We trained each model using Adam optimizer with a learning rate of 0.0001, linear learning rate warmup of 40k steps and 200k training steps.\"']"
    ],
    "7790": [
        "We convert eight existing QA tasks from the MRQA shared task (Fisch et al., 2019b) into retrieval versions.",
        "We establish baselines using unsupervised term-based information retrieval methods (the BM25 ranking function), as well as two supervised neural models built on pre-trained USE-QA and BERT models.",
        "A classical term-based retrieval approach, BM25, is a strong baseline, and could likely be improved further using additional information retrieval techniques such as normalization and synonym handling.",
        "The neural models perform particularly well on tasks with a low degree of question/answer token overlap, or in situations where the context length is limited.",
        "The neural model performance can also be improved through the addition of indomain training data.",
        "However, we find that QA tasks are not all alike and having training data in the precise target domain is important.\"']"
    ],
    "7792": [],
    "7796": [
        "We propose a top-down neural architecture for text-level discourse parsing.",
        "We cast the discourse parsing task as an EDU split point ranking task.",
        "Our approach can determine the complete discourse rhetorical structure as a hierarchical tree structure.",
        "Experimentation on the English RST-DT corpus and the Chinese CDTB corpus shows the great effectiveness of our proposed approach.",
        "In future work, we will focus on more effective discourse parsing with additional carefully designed features and joint learning with EDU segmentation.\"']"
    ],
    "7797": [
        "'The use of synthetic data in English surface realization gives a significant performance boost on the shallow task, from 72.7 BLEU up to 80.1.'",
        "'Assuming the use of synthetic data, more needs to be investigated in order to fully maximize its benefit on performance.'",
        "'Future work will look more closely at the choice of corpus, construction details of the synthetic dataset, as well as the tradeoff between training time and accuracy that comes with larger vocabularies.'",
        "'The work described in this paper has focused on English. Another avenue of research would be to investigate the role of synthetic data in surface realization in other languages.'\"]"
    ],
    "7799": [
        "The proposed Type Auxiliary Guiding encoder-decoder framework takes full advantage of the type information associated with the code for code comment generation tasks, leading to significant improvements over state-of-the-art approaches.",
        "The experimental results demonstrate the effectiveness and applicable potential of the proposed framework in software development.",
        "The type information in the code translation related tasks is verified as necessary by the practical framework and good results.",
        "The proposed framework has the potential to be extended to more complex contexts by devising efficient learning algorithms for future work.']"
    ],
    "7800": [
        "The use of entity embeddings for entity retrieval leads to increased effectiveness of query results on DBpedia-Entity V2.",
        "Including a representation of the graph structure in the entity embeddings leads to better clusters and higher effectiveness of retrieval results.",
        "Queries which get linked to relevant entities or pages neighboring to relevant entities are helped the most, while queries with wrongly linked entities are helped the least.",
        "Enriching entity retrieval methods with entity embeddings leads to improved effectiveness, but there are limitations to this study, such as not all query categories lead to improvements on NDCG and the use of Tagme to identify entities in queries may lead to lower performance.",
        "Replacing the entity linking component with a state-of-the-art approach may improve results further.",
        "Using alternative entity embedding methods like TransE may lead to better results.']"
    ],
    "7803": [
        "Our proposed neural response selection method can select the best response from an ensemble of dialogue agents using contextualised word embeddings as input features.",
        "We have developed a curriculum training mechanism to make our model robust to real-world settings, moving from Utterance, Conversation to Scheduled Sampling phases.",
        "Our model outperforms cosine-similarity inspired baselines in selecting relevant responses.",
        "The use of contextualised word embeddings as input features allows the model to learn response selection independently of the dialogue system.']"
    ],
    "7804": [
        "The performance gap between seen and unseen environments is a widely observed phenomenon in vision-and-language navigation (VLN) tasks.",
        "The environment bias exists in low-level visual appearance.",
        "Semantic features can decrease the performance gaps in VLN datasets and achieve state-of-the-art results.",
        "Re-splitting and feature replacement are possible initial solutions to address the environment bias.",
        "The proposed method achieves competitive performance compared to previous systems using less than 1% of the training data.']"
    ],
    "7805": [
        "We have presented a novel approach: pseudo visual pivoting for unsupervised multimodal MT.",
        "Our model utilizes the visual space as the approximate pivot for aligning the multilingual multimodal embedding space.",
        "Besides, it synthesizes image-pivoted pseudo sentences in two languages and pairs them to translate by reconstruction without parallel corpora.",
        "The experiments on Multi30K show that the proposed model generalizes well and yields new state-of-the-art performance.\"']"
    ],
    "7806": [
        "'We proposed and evaluated a CNN-based architecture for end-to-end speech recognition.'",
        "'Our model achieves a better accuracy on the LibriSpeech benchmark with much fewer parameters compared to previously published CNN models.'",
        "'The proposed architecture can easily be used to search for small ASR models by limiting the width of the network.'",
        "'Initial study on a much larger and more challenging dataset also confirms our findings.'"
    ],
    "7810": [
        "The proposed DramaQA dataset has cognitive-based difficulty levels for QA as a hierarchical evaluation metric, providing coreference resolved script and rich visual metadata for character-centered video.",
        "The Multilevel Context Matching model is useful for verifying the usefulness of multi-level modeling and character-centered annotation in video story understanding.",
        "The proposed DramaQA dataset can be utilized as a good resource for video-related researches, including emotion or behavior analysis of characters, automatic coreference identification from scripts, and coreference resolution for visual-linguistic domain.",
        "The model efficiently learns underlying correlations between the video clips, QAs, and characters using both low-level and high-level representations.",
        "The application area of the proposed DramaQA dataset is not limited to QA based video story understanding, and it can be utilized for other video-related researches.",
        "The integrated multimodal data analysis in the video story understanding can be resolved using the proposed model and dataset.",
        "The future work includes extending the two criteria of hierarchical QA to deal with longer and more complex video stories and expanding the coverage of evaluation metric.']"
    ],
    "7815": [
        "The widely used ROUGE evaluation metric has limitations when adopting Korean summarization due to the agglutinative language structure, leading to inaccurate evaluation results.",
        "Proposed RDASS (Reference and Document Aware Semantic Score) evaluation metric can reflect deep semantic relationships of a generated, reference summary, and document.",
        "The correlation with human judgment is higher for the proposed evaluation metric (RDASS) than for ROUGE scores.",
        "The proposed method will be effective in English summarization datasets in future work.']"
    ],
    "7818": [
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\').",
        "Interrogation of control-and dementia-based LMs using synthetic transcripts and interpolation of parameters reveals inconsistencies harmful to model performance that can be remediated by incorporating interpolated models and pre-trained embeddings, with significant performance improvements."
    ],
    "7820": [
        "We propose a new CQA system that improves the state-of-the-art and surpasses human accuracy on two datasets.",
        "Our approach outperforms benchmark models across different datasets.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We find that unsupervised action segmentation in naturalistic instructional videos is greatly aided by the inductive bias given by typical step orderings within a task, and narrative language describing the actions being done.",
        "Structured generative models provide a strong benchmark for the task due to their abilities to capture the full diversity of action types (by directly modeling distributions over action occurrences).",
        "Future work might explore methods for incorporating richer learned representations both of the diverse visual observations in videos, and their narrative descriptions, into such models.\"']"
    ],
    "7821": [
        "We investigate the faithfulness problem in neural abstractive summarization.",
        "Current models suffer from an inherent trade-off between abstractiveness and faithfulness.",
        "The death of a man whose body was found in a river in Cumbria has been identified as murder."
    ],
    "7822": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Our methods for calculating a well-motivated measure of phonotactic complexity can be used to compare cross-linguistic.",
        "There is a strong negative correlation between average word length and phonotactic complexity.",
        "This trade-off with word length can be seen as an example of complexity compensation or perhaps related to communicative capacity.\"']"
    ],
    "7823": [
        "Our work provides an important first step on the challenging problem of grounding natural language instructions to mobile UI actions.",
        "Our decomposition of the problem means that progress on either can improve full task performance.",
        "Action span extraction is related to both semantic role labeling (He et al., 2018) and extraction of multiple facts from text (Jiang et al., 2019).",
        "Reinforcement learning that has been applied in previous grounding work may help improve out-of-sample prediction for grounding in UIs and improve direct grounding from hidden state representations.",
        "Our work provides a technical foundation for investigating user experiences in language-based human computer interaction.\"']"
    ],
    "7826": [
        "The brevity problem in neural machine translation can largely be explained by the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The complexity in DuRecDial makes it a great testbed for more tasks such as knowledge grounded conversation, domain transfer for dialog modeling, target-guided conversation, and multi-type dialog modeling.",
        "We generate the first-name Chinese character(or last-name Chinese character) by randomly sampling Chinese characters from a set of candidate characters used as first name (or last name) for the gender of the seeker.",
        "The study of these tasks will be left as future work.']"
    ],
    "7827": [
        "vocabulary reliance in scene text recognition methods\" is an important but long-neglected problem.",
        "A comprehensive framework is built for comparing and analyzing individual text recognition modules and their combinations.",
        "Based on this framework, valuable observations and findings have been acquired, as well as recommendations for future research.",
        "Current contextual and prediction modules can be improved through a mutual learning strategy that enhances their vocabulary learning ability or generalization ability to words out of vocabulary.",
        "The proposed approach can improve the recognition of scene text by leveraging the mutual relationship between vocabulary and context.']"
    ],
    "7828": [
        "The proposed system, CAiRE-COVID, is efficient in mining scientific literature given a query, as it has shown in the Kaggle CORD-19 Challenge.",
        "The system is effective in multi-document summarization and open-domain QA.",
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "Disabling interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "The system is easy to generalize to general domain-agnostic literature information mining, especially for possible future pandemics.",
        "The system has been launched on website 2 for real-time interactions and the code has been released for broader use.']"
    ],
    "7829": [
        "The knowledge-enhanced CNN outperforms the model without knowledge information.",
        "The KMCNN and MCNN models outperform the plain CNN adopted by Lee et al.",
        "The information brought by multichannel and knowledge embeddings could enhance the classification performance.",
        "The relative importance of multiple channels and knowledge embeddings is not fixed.",
        "The performance of different models on GWAS Catalog 2019(b) is much lower than that on GWAS Catalog 2019(a).",
        "The negative samples in the former are more confusing, and the classifiers tend to be confused when it comes to publications closely related to genes and diseases simultaneously.']"
    ],
    "7830": [
        "The proposed architecture, SentiBERT, is effective in capturing compositional sentiment semantics.",
        "SentiBERT considers contextual information and explicit syntactic guidelines for modeling semantic composition.",
        "Experimental results show the effectiveness and transferability of SentiBERT.",
        "Further analysis demonstrates the interpretability and potential of SentiBERT with less supervision.",
        "The proposed architecture can be extended to other applications involving phrase-level annotations in future work.']"
    ],
    "7834": [
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "The proposed sequence modeling framework improves over BERT in terms of handling reporting bias, taking into account the ordinal relations and exploiting interactions among multiple dimensions of time.",
        "The success of this model is confirmed by intrinsic evaluations on RealNews and UDS-T (where we see a 19% improvement), as well as extrinsic evaluations on TimeBank, HiEVE and MCTACO.\"']"
    ],
    "7835": [
        "The need for a centralized platform to perform evaluation of technology for codeswitching data across multiple tasks and language pairs.",
        "The introduction of the Linguistic Code-switching Evaluation (LinCE) benchmark using ten publicly available datasets.",
        "The identification of important issues that undermine the evaluation process, such as labels not appearing in the test set or substantially different distributions among splits.",
        "The proposal of new splits using a new stratification technique with up to three criteria (e.g., LID labels, task-specific labels, and sentence lengths).",
        "The preservation of the distribution of the full corpus in the proposed splits used in LinCE.",
        "The provision of strong baselines using state-of-the-art models on monolingual datasets, including BERT and ELMo.",
        "The expectation that LinCE will be well-received by the NLP community and the intention to keep the platform evolving with the incorporation of more tasks and language pairs in the near future.']"
    ],
    "7837": [
        "We investigated how we can incorporate user feedback into existing active learning approaches without hurting the user\\'s actual needs.",
        "We formalize both system (active learning) and user objectives and propose two novel sampling strategies which aim to maximize both objectives jointly.",
        "Our findings open up new opportunities for training models on low-resource scenarios with implicitly collected user feedback while jointly serving the user\\'s actual needs.",
        "Additional use cases like the training of personalized recommendation models as well as the use of reinforcement learning to find a good trade-off between system and user objective remain to be investigated in future work.\"']"
    ],
    "7840": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "We have the following recommendations: \u2022 Charts in the wild: The charts in FigureQA and DVQA were methodologically generated, but humangenerated charts in real-world business and scientific documents can contain variations that these datasets omit. Additional text in the chart or human annotations would likely cause the dynamic encoding method used by PReFIL to fail. \u2022 Human generated questions: The questions in both FigureQA and DVQA were created with templates, which do not capture all the nuances of natural language.",
        "Document-level CQA: FigureQA and DVQA have well-defined image regions and all information needed to answer a question is contained in that image. To understand charts in documents, information in the rest of the document may be necessary to answer questions about the chart."
    ],
    "7842": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The gap between short and long tasks is still large and needs to be further reduced.",
        "Developing agents that are robust to variations in both visual appearance and instruction descriptions is an important next step."
    ],
    "7843": [
        "The proposed non-autoregressive image captioning model can achieve a performance comparable to state-of-the-art autoregressive counterparts, while enjoying 13.9\u00d7 inference speedup.",
        "The decoding inconsistency problem in non-autoregressive models is well addressed by the combined effect of cooperative agents, sentence-level team-reward, and agent-specific counterfactual baseline.",
        "Using unlabeled images can further boost caption quality.']"
    ],
    "7844": [
        "We proposed CTC-synchronous training (CTC-ST) to provide hard monotonic attention in MoChA with reliable alignments extracted from CTC.",
        "By jointly training both sub-networks with the shared encoder and generating CTC alignments simultaneously, we enabled effective interaction between MoChA and CTC.",
        "Experimental evaluations revealed that CTC-ST significantly improved the performance of MoChA and greatly reduced the gap from the offline models.",
        "Further gains with SpecAugment were obtained when CTC-ST was applied, thus verifying its robustness to noisy alignments.\"']"
    ],
    "7846": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but the results for DVQA were more nuanced due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The current state-of-the-art multimodal models perform relatively poorly on the dataset, with a large gap to human performance.",
        "The challenge of detecting hate speech in memes requires improvements in the capabilities of modern machine learning systems.",
        "Detecting hate speech in memes requires reasoning about subtle cues and the task was constructed such that unimodal models find it difficult, by including \"benign confounders\" that flip the label of a multimodal hateful meme.']"
    ],
    "7852": [
        "The proposed commonsense evidence generation and injection model enhances the model's ability to answer questions that require commonsense reasoning.",
        "The model uses both textual and factual evidence generators to generate evidence for the reading comprehension task.",
        "The model utilizes attention mechanism to find the relation between paragraph, question, option, and evidence.",
        "The model captures diverse features using a capsule network to predict the answer.",
        "The proposed method outperformed the current state-of-the-art approach K-Adapter with a 2% increase in terms of accuracy on the AI2 Leaderboard of CosmosQA task.",
        "The generated evidence is human-readable and helpful for the reasoning task.\"]"
    ],
    "7855": [
        "SOLOIST is a method of building task bots at scale with transfer learning and machine teaching.",
        "Unlike GPT-2, SOLOIST is pre-trained in a task-grounded manner, allowing it to generate responses grounded in user goals and real-world knowledge for task completion.",
        "Experiments show that SOLOIST creates new SoTA on two popular task-oriented dialog benchmarks, outperforming existing methods by a large margin in few-shot fine-tuning settings.",
        "The few-shot fine-tuning settings involve only a limited amount of task labels being available for finetuning.",
        "The recipe presented in the paper, which involves formulating task-oriented dialog as a single auto-regressive language model and pre-training a task-grounded response generation model on heterogeneous dialog corpora, can be improved through exploration and refinement of the new paradigm for building task bots based on task-grounded pre-training and finetuning via machine teaching.']"
    ],
    "7856": [
        "We presented a simple strategy for the task of infilling which leverages language models.",
        "Our approach is capable of infilling sentences which humans have difficulty recognizing as machinegenerated.",
        "Furthermore, we demonstrated that our infilling framework is effective when starting from large-scale pre-trained LMs, which may be useful in limited data settings.",
        "In future work, we plan to incorporate these features into co-creation systems which assist humans in the writing process.",
        "We hope that our work encourages more investigation of infilling, which may be a key missing element of current writing assistance tools.\"']"
    ],
    "7857": [
        "The proposed approach, Memory-Augmented Recurrent Transformer (MART), outperforms baseline methods in terms of overall performance for video paragraph captioning.",
        "MART is able to generate more coherent and less redundant paragraphs without any degradation in relevance.",
        "The auxiliary memory module in MART enables recurrence in transformers, which improves the performance of the model.",
        "The experimental results on two standard datasets show that MART has better overall performance than the baseline methods.",
        "(Page 2) The proposed approach, Memory-Augmented Recurrent Transformer (MART), outperforms baseline methods in terms of overall performance for video paragraph captioning.",
        "(Page 3) MART is able to generate more coherent and less redundant paragraphs without any degradation in relevance.",
        "(Page 4) The auxiliary memory module in MART enables recurrence in transformers, which improves the performance of the model.",
        "(Page 5) The experimental results on two standard datasets show that MART has better overall performance than the baseline methods.']"
    ],
    "7865": [
        "The proposed two-stage speech recognition model, double visual awareness multi-modality speech recognition (AE-MSR) network, leads to a significant performance gain on MSR especially in noisy environments.",
        "The use of double visual awareness for MSR is necessary and effective.",
        "The AE-MSR model outperforms state-of-the-art models on the LRS3-TED and LRW datasets by a significant margin.']"
    ],
    "7866": [
        "We have presented NMN, a novel embedded-based framework for entity alignment.",
        "NMN tackles the ubiquitous neighborhood heterogeneity in KGs.",
        "We achieve this by using a new sampling-based approach to choose the most informative neighbors for each entity.",
        "As a departure from prior works, NMN simultaneously estimates the similarity of two entities, by considering both topological structure and neighborhood similarity.",
        "We perform extensive experiments on real-world datasets and compare NMN against 12 recent embedded-based methods.",
        "Experimental results show that NMN achieves the best and more robust performance, consistently outperforming competitive methods across datasets and evaluation metrics.\"']"
    ],
    "7869": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We have explored simple and effective ways to alleviate or eliminate the beam problem.",
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our study is \\'large-scale\\' in many respects, with 91 languages and up to 2 million entries each.",
        "The predicted ratings showed consistently high correlation with human judgment, compared favorably with state-of-the-art monolingual approaches to lexicon expansion.",
        "Our data suggests that embedding-based word emotion models can be used as a repair mechanism, mitigating poor target-language emotion estimates acquired by simple word-to-word translation.",
        "Future work will have to deepen the way we deal with word sense ambiguity by exchanging the simplifying type-level approach our current work is based on with a semantically more informed sense-level approach.\"']"
    ],
    "7870": [
        "The proposed method achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size. (Claim 1)",
        "AdaBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks. (Claim 2)",
        "The proposed method sheds light on understanding the behaviors of language encoders against grammatical errors and encourages future work to enhance the robustness of these models. (Claim 3)",
        "ELMo, BERT, and RoBERTa embeddings all encode gender, religion, and nationality biases. (Claim 4)",
        "A projection-based method can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy. (Claim 5)",
        "The proposed method concentrates on three aspects of their abilities against grammatical errors: performance on downstream tasks when confronted with noised texts, ability in identifying errors, and capturing the interaction between tokens in the presence of errors. (Claim 6)']"
    ],
    "7871": [
        "DMIN achieves new state-of-the-art results on the miniRCV1 and ODIC datasets for few-shot text classification.",
        "Dynamic Memory Induction Networks (DMIN) can generalize better to support sets and unseen classes compared to other few-shot learning methods.",
        "Dynamic memory can be a more general learning mechanism than what has been used in this study for few-shot learning."
    ],
    "7873": [
        "The improvement achieved by fine-tuning over a similar or same dataset might come from better dataset fitting rather than better commonsense reasoning.",
        "The unsupervised setting might be the more reasonable evaluation setting for evaluating current AI systems' understanding of commonsense knowledge.",
        "Current commonsense reasoning models have strengths and limitations, and there are specific kinds of commonsense knowledge required to be acquired for better commonsense reasoning.",
        "The new task called WinoWhy, which requires models to select the plausible reasons for answering WSC questions, is a more challenging task than the original WSC task.",
        "Current models have gained significant improvement over the original WSC task but still cannot fully understand the reasons behind.\"]"
    ],
    "7875": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed CMR framework exceeds the state-of-the-art on NLVR 2 and VQA v2.0 datasets.",
        "The model trained on NLVR 2 boosts the training of VQA v2.0 dataset.",
        "The experiments and empirical analysis demonstrate CMR\\'s capability of modeling relational relevance for reasoning and consequently its better generalizability to unobserved data.\"']"
    ],
    "7876": [
        "The developed models outperform the only available baseline, namely, the SNR at which an ASR is able to correctly recognize each consonant. Variance of the ASR-based estimate of SNR90 is 15.",
        "The main advantage of using the models developed here is to estimate intelligibility of speech syllables in background speech-weighted noise, without the need of running expensive and time-consuming experiments with human subjects in controlled conditions.",
        "Our results show that the developed models outperform the only available baseline.",
        "The speech augmentation methods introduced in the current study help to increase the size of the training database adequately to train deep learning models for speech processing.']"
    ],
    "7878": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We expect that the dataset can serve as a testbed for developing new kinds of models and representations that can handle semistructured information as first class citizens."
    ],
    "7879": [
        "We propose a novel response-anticipated document memory to exploit and memorize the document information that is important in response generation.",
        "Our approach outperforms benchmark models across different datasets.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Our model obtains the state-of-the-art performance on the CbR task."
    ],
    "7881": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model of part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "The approach of attenuating biases in word representations using a projection-based method can effectively reduce gender, religion, and nationality biases in contextualized embeddings without loss of entailment accuracy.",
        "The novel latent structure refinement (LSR) model for better reasoning in the document-level relation extraction task dynamically learns a document-level structure and makes predictions in an end-to-end fashion, potentially improving the accuracy of the model."
    ],
    "7884": [
        "The proposed weakly supervised approach for summary quality evaluation achieves better performance than baselines, especially on linguistic aspects.",
        "The use of massive, precious human written summaries in summarization datasets is effective in improving the quality of generated summaries.",
        "The approach is able to outperform existing methods, especially when it comes to linguistic aspects.",
        "The study inspires more reference-free summary evaluation.",
        "The use of deep bidirectional transformers for language understanding is effective in improving the quality of generated summaries."
    ],
    "7885": [
        "We propose the training of an algorithm to find patterns that distinguish lobbied bills from non-lobbied ones.",
        "Our linear models perform well, it has the appeal to make it interpretable, which is important for social science applications.",
        "We would also like to experiment with transfer learning and examine if our model, together with a small sample of labelled data from another English-speaking jurisdiction, can be used to predict lobbying activity in countries where such data is less easily available than in the US.",
        "Even humans with the highest level of domain specific expertise on lobbying, legislation, and rent seeking would struggle to mark out those bills that had been targeted by lobbying.",
        "We believe our paper belongs in a different group.",
        "The legal field can learn from this exercise, our future work will focus on a more detailed analysis of what factors are important in the distinction between the two types of bills.",
        "Our results change with time, with subject area, or with the identity of the lobbying organisation -all of which is available in our dataset -affects our results.\"']"
    ],
    "7888": [
        "MAE outperforms the transformer baselines on machine translation and language modeling benchmarks.",
        "Using a block coordinate descent algorithm to train MAE improves its performance.",
        "Using a learned gating function in MAE improves its performance.",
        "Avoiding using momentum-based optimizing algorithms (e.g., Adam) for the gating functions helps alleviate the \"rich gets richer\" degeneracy."
    ],
    "7890": [
        "The proposed entity-enriched QA models trained with an auxiliary task improve over the state-of-the-art models by about 3 - 6% across the large-scale clinical QA dataset, emrQA (Pampari et al., 2018) (as well as MADE (Jagannatha et al., 2019))",
        "We also show that multitask learning for logical forms along with the answer results in better generalizing over unseen paraphrases for EMR QA",
        "The predicted logical forms also serve as an accompanying justification to the answer and help in adding credibility to the predicted answer for the physician"
    ],
    "7893": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The dataset of dangerous speech is rare online, making it difficult to find data for training machine learning classifiers.",
        "The authors plan to explore other deep learning methods on the task and further explore contexts of use of dangerous language in social media for future work."
    ],
    "7895": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We contrast the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "The latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "CrisisBERT improves state of the art for both detection and recognition class, and further demonstrates robustness by extending from 6 classes to 36 classes, with only 51.4% additioanl data points.",
        "Our experiments with two classification models show that Crisis2Vec enhances classification performance as compared to Word2Vec embeddings, which is commonly used in prior works.\"']"
    ],
    "7899": [
        "We investigated the representation of structural information for discourse representation tree structure parsing.",
        "A graph neural network can bring significant improvements in representing syntax and structure.",
        "Our method uses GAT for representing syntax in encoding, and a structural backbone for decoding.",
        "Experiments on the standard GMB dataset show that our method is high effective, achieving the best results in the literature."
    ],
    "7902": [
        "The proposed framework learns the speaker-independent emotional expression pattern for both spectrum and prosody across speakers.",
        "Speaker-independent training outperforms speaker-dependent training, while successfully preserving speaker identity of the source speaker.",
        "The proposed CWT-based F0 conditioning improves spectrum conversion.",
        "The proposed framework is capable of converting anyone's emotion.",
        "This paper provides a speaker-independent perspective to emotion conversion, which has not been explored before.\"]"
    ],
    "7903": [
        "'Our approach outperformed the baseline fixed-topology architecture significantly under both monolingual and multilingual ASR settings.'",
        "'The experiment results show that our approach outperformed the baseline fixed-topology architecture significantly under both monolingual and multilingual ASR settings.'",
        "'Our approach can be incorporated with other ASR or meta-learning approaches for further improvement.'",
        "'We applied our approach not only on many languages to perform monolingual ASR, but also on a multilingual ASR setting.'",
        "'The searched architectures by DARTS-ASR were analyzed in our study.'\"]"
    ],
    "7905": [
        "We have introduced a zero-shot method for learning a model for relation extraction from semistructured documents that generalizes beyond a single document template.",
        "Moreover, this approach enables OpenIE extraction from entirely new subject verticals where no prior knowledge is available.",
        "By representing a webpage as a graph defined by layout relationship between text fields, with text fields associated with both visual and textual features, we attain a 31% improvement over the baseline for new-vertical OpenIE extraction.",
        "Future extensions of this work involve a more general pre-training objective allowing for the learned representations to be useful in many tasks as well as distantly or semi-supervised approaches to benefit from more data.\"']"
    ],
    "7906": [
        "Our system achieves SoTA on all of the eight corpora.\" - This claim states that the system performs well on a variety of named entity corpora, demonstrating its effectiveness in NER tasks.",
        "Advanced structured prediction techniques lead to substantial improvements for both nested and flat NER.\" - This claim suggests that the use of advanced techniques in the system leads to improved performance in both nested and flat NER tasks.",
        "We employ a biaffine model to assign scores for all spans in a sentence.\" - This claim highlights the specific approach used in the system for assigning scores to different spans in a sentence.",
        "Our system uses contextual embeddings as input to a multilayer BiLSTM.\" - This claim indicates that the system uses a specific type of embedding (contextual embeddings) and a particular architecture (multilayer BiLSTM) to process the input data.",
        "We reformulate NER as a structured prediction task and adopted a SoTA dependency parsing approach for nested and flat NER.\" - This claim states that the system uses a specific approach (structured prediction) and a particular technique (dependency parsing) to perform NER tasks.']"
    ],
    "7908": [
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "data and model uncertainties can be included as part of the evaluation of any deep learning model without harming its performance.",
        "even though data uncertainty estimation changes the loss function of a model, it often leads to improvements (Kendall and Gal, 2017).",
        "supervised rejection leverages all forms of uncertainty together and dictates the number of instances to remove.",
        "in rumour verification linguistic markers of user uncertainty (words like \"may\", \"suggest\", \"possible\") are associated with rumours.\"']"
    ],
    "7911": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Soft-Masked BERT significantly outperforms the state-of-art method of solely utilizing BERT.",
        "Experimental results on two datasets show that Soft-Masked BERT significantly outperforms the state-of-art method of solely utilizing BERT.",
        "The technique of soft-masking is general and potentially useful in other detection-correction tasks.",
        "As future work, we plan to extend Soft-Masked BERT to other problems like grammatical error correction and explore other possibilities of implementing the detection network.\"']"
    ],
    "7914": [
        "We propose novel data augmentation methods for formality style transfer.",
        "Our proposed data augmentation methods can effectively generate diverse augmented data with various formality style transfer knowledge.",
        "The augmented data can significantly help improve the performance when it is used for pre-training the model and leads to the state-of-the-art results in the formality style transfer benchmark dataset."
    ],
    "7916": [
        "We consider the case of pruning of pretrained models for task-specific fine-tuning and compare zeroth-and first-order pruning methods.",
        "We show that a simple method for weight pruning based on straight-through gradients is effective for this task and that it adapts using a first-order importance score.",
        "Our method consistently yields strong improvements over existing methods in high-sparsity regimes.",
        "The analysis demonstrates how this approach adapts to the fine-tuning regime in a way that magnitude pruning cannot.",
        "In future work, it would also be interesting to leverage group-sparsity inducing penalties [Bach et al., 2011] to remove entire columns or filters.\"']"
    ],
    "7920": [
        "The proposed model, Jointly trained Duration Informed Transformer (JDI-T), is effective for Text-to-Speech (TTS) synthesis.",
        "The model is trained jointly without explicit alignments.",
        "The proposed model can synthesize high-quality speech.",
        "The model achieves state-of-the-art performance in the internal studioquality dataset compared to other popular TTS models implemented from ESPnet-TTS.",
        "The proposed model is effective for TTS synthesis.",
        "The model is trained jointly without explicit alignments.",
        "The proposed model can synthesize high-quality speech.",
        "The model achieves state-of-the-art performance in the internal studioquality dataset compared to other popular TTS models implemented from ESPnet-TTS."
    ],
    "7923": [
        "The proposed method TPC-GCN integrates information from graph structure and content of topics, posts, and comments for post-level controversy detection on social media.",
        "Unlike existing works, the proposed method exploits information from related posts in the same topic and the reply structure for more effective detection.",
        "The proposed model DTPC-GCN disentangles topic-related and topic-unrelated features and dynamically fuses them for improved performance in inter-topic detection.",
        "The proposed models outperform compared methods and demonstrate significant generalizability in integrating both semantic and structural information.",
        "Extensive experiments conducted on two datasets prove the effectiveness of the proposed models.']"
    ],
    "7924": [
        "The proposed technique for automatic detection of speech polarity using oscillating moments can outperform existing approaches with an average error rate of 0.15% compared to 0.64% for the best state-of-the-art technique.",
        "The introduction of polarity-dependency through non-linearity or higher-order statistics improves the accuracy of speech polarity detection.",
        "The proposed method only requires a rough estimate of the mean pitch period for the considered voice, making it more practical and efficient.",
        "The use of oscillating moments for speech polarity detection has the potential to improve the accuracy of speech recognition systems.",
        "The proposed technique is based on the observation that the moments of the speech signal oscillate at the local fundamental frequency and have a phase shift which is dependent upon the speech polarity.']"
    ],
    "7932": [
        "The performance of the baseline BERT-Large model can be improved by using recurrent chunking mechanisms.",
        "The proposed chunking policy network for machine reading comprehension enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Adding a recurrent mechanism allows the information to flow across segments, so that the model could have knowledge beyond the current segment when selecting answers.",
        "The approach outperforms benchmark models across different datasets.']"
    ],
    "7934": [
        "We have explored simple and effective ways to alleviate or eliminate the beam problem.",
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "We have argued that the brevity problem is an example of label bias, and that the solution is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our proposed WLO model, which treats each word as a linear transformation operator, achieves the best performance and lends itself to analysis.\"']"
    ],
    "7935": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism, which uses Similarity Entropy Minimization to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "Our approach can be used to generate section titles that lead to strong improvements in reading comprehension tasks.",
        "RPD can offer us a new perspective to understand and compare word embeddings.",
        "We propose RPD, a metric to quantify the distance between embedding spaces (i.e different sets of word embeddings).",
        "With the help of RPD and its properties, we verify some intuitions and answer some questions.\"']"
    ],
    "7940": [
        "'We propose to address the task of cross-lingual set-to-description retrieval in e-commerce, which involves automatically retrieving a persuasive product description in the target language given a product attribute set in the source language.'",
        "'We collected a high-quality heuristic dataset with 13.5K pairs created by experts and two large monolingual auxiliary corpora for sharing.'",
        "'We further propose a novel cross-lingual matching network (CLMN) aligned with a refined context-dependent cross-lingual mapping upon pretrained BERT to address this low-resource cross-lingual retrieval task directly.'",
        "'Experimental results indicate that our proposed CLMN is effective for solving this challenging task.'",
        "'In our future work, we will evaluate our model on other open datasets for cross-lingual information retrieval.'\"]"
    ],
    "7941": [
        "We propose the Decay RNN, a bioinspired recurrent network that emulates the decaying nature of neuronal activations after receiving excitatory and inhibitory impulses from upstream neurons.",
        "The balance between the free term (h(t)) and the coupled term (Wh(t)) enabled the model to capture syntax-level dependencies.",
        "Explicitly modeling hierarchical structure helps to discover non-local structural dependencies.",
        "Recently, Maheswaranathan and Sussillo (2020) showed the existence of a line attractor in the dynamics of the hidden states for sentiment classification.",
        "Our results here do at least indicate that the complex gating mechanisms of LSTMs may not be essential to their performance on many linguistic tasks, and that simpler and perhaps more cognitively plausible RNN architectures are worth exploring further as psycholinguistic models."
    ],
    "7942": [
        "'We present the first available SRL resource in Hebrew, which is constructed through annotation projection from aligned English sentences.'",
        "'The dataset is derived from the OpenSubtitles collection of movie subtitles. The genre is of informal spoken language.'",
        "'We designed a full pipeline to map a noisy collection of aligned sentences in English and Hebrew into an annotated SRL dataset in both FrameNet and PropBank styles in CoNLL 2009 format.'",
        "'In order to control the quality of the generated data, we introduce a set of filters, following the approach of filtered projection of Akbik et al. (2015) .'",
        "'We specifically verified the impact of the Hebrew rich morphology and complex word formation rules on the process.'",
        "'We finally trained a neural SRL system in Hebrew as a baseline model, building on the BERT multi-lingual model.'",
        "'In future work, we intend to manually curate the generated data while enriching the Hebrew FrameNet database with annotated exemplar sentences.'\"]"
    ],
    "7945": [
        "The handling of multiple modalities is under-explored in the computer vision community.",
        "A novel bi-modal transformer with a bi-modal multiheaded proposal generation module can facilitate the performance of dense video captioning.",
        "The proposed model achieves state-of-the-art results on F1 and BLEU metrics on the ActivityNet Captions dataset.",
        "The ablation study results show that the proposed model provides an effective and elegant way of fusing audio and visual features, outperforming uni-modal configurations in all settings.']"
    ],
    "7949": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed attention-based transducer with chunk size w = 1 is conventional RNN-T.",
        "When w is equal to the length of encoder output, attention-based transducer is quite similar to AED.",
        "Our experiments show superior performance of the proposed attention-based transducer over conventional RNN-T and Kaldi\\'s TDNNf models.\"']"
    ],
    "7950": [
        "performing well across all the eight tasks is more challenging than existing tasks\" - This claim suggests that the dataset compiled by the authors is more difficult to solve than previous datasets, and that their baseline performance is a significant improvement over existing tasks.",
        "the effect of our baseline performance on providing external knowledge to a language model promises the benefit of using cheat sheet for those tasks which need knowledge\" - This claim suggests that the authors\\' baseline performance has a positive impact on providing external knowledge to a language model, and that this could be useful for tasks that require specific knowledge.",
        "We expect this dataset and the task to motivate researchers to analyze the generalization capability of neuro symbolic models\" - This claim suggests that the authors\\' dataset and task will serve as a motivation for researchers to analyze the generalization capability of neuro symbolic models, and that this could lead to new insights and discoveries.",
        "Our future work will explore novel ways to do transfer learning in this multi-task setting\" - This claim suggests that the authors plan to explore new methods for transfer learning in the multi-task setting, and that this could lead to new techniques and strategies for improving the performance of neuro symbolic models.']"
    ],
    "7952": [
        "Unidirectional encoders achieve better translation qualities than bidirectional ones in online machine translation.",
        "Joint training for wait-k decoders can address the need to train a different model for each lagging value, and our models can operate across the full spectrum of lagging with the quality increasing with the value of k.",
        "Pervasive Attention models are competitive with Transformers for online translation in low-resource settings.",
        "Our wait-k models are state-of-the-art among deterministic online translation strategies, and provide a strong baseline for simultaneous translation with dynamic decoding.']"
    ],
    "7955": [
        "The approach presented enhances existing multi-label classification techniques by employing soft n-gram interaction matching.",
        "The approach is effective at identifying labels in the long tail, which are underrepresented with current state-of-the-art classification approaches.",
        "The approach can effectively label items that do not appear at all in the training data.']"
    ],
    "7959": [
        "We propose a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "We evaluate the proposed AdaBERT on six datasets involving three kinds of NLP tasks and achieve comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "We hope this work inspires future research on better understanding the differences between embedding methods, and on designing simpler and more efficient models.\"']"
    ],
    "7961": [
        "The authors have presented models that can be used to generate the turn switch offset distributions of SDS system responses.",
        "Humans are sensitive to the timings of SDS system responses and these timings can impact how responses are perceived by a listener.",
        "The timings of SDS system responses are an important element of producing naturalistic interactions that is often overlooked.",
        "Generating realistic response behaviors is a potentially desirable addition to the overall experience of SDS systems."
    ],
    "7965": [
        "We explored a novel approach of scope localization and classification with a single end-to-end CNN model.",
        "We demonstrated good performance and thereby make a case for using multi-label clinical text that is often found in real world.",
        "For future work, we would like to explore the usage of inception layers; different sets of kernel sizes in each layer.",
        "The output layer will then have varying receptive fields i.e scope lengths in our problem.",
        "This increases the generalization of the model to scope lengths that are unseen in the training data.\"']"
    ],
    "7967": [
        "The proposed models achieve the best accuracy overall, and perform particularly well on LQAs.",
        "Using two attention mechanisms in a mutual way and the definition of history are effective in the model decisions.",
        "More discourse relations in dialogues can be explored in future work.",
        "Utilizing out-of-domain knowledge is another research direction for utterances matching task, especially for health-related dialogues.']"
    ],
    "7970": [
        "It is hard to predict the best auxiliary language based on language distances.",
        "Meta-embeddings can leverage multiple embeddings effectively for those tasks and set the new state of the art on part-of-speech tagging across different languages.",
        "We proposed a guide on how to decide which method of including auxiliary languages one should use."
    ],
    "7971": [
        "The multilingual neural model with FastText or BERT embeddings can effectively extract temporal expressions from text, outperforming a state-of-the-art model by a large margin.",
        "Adversarial training for creating multilingual embedding spaces can be used to transfer the model to unseen languages in a cross-lingual setting.",
        "The model can be transferred to unseen languages and outperform a state-of-the-art model by a large margin.",
        "The use of FastText or BERT embeddings in the model improves its performance in extracting temporal expressions from text.",
        "The model's ability to extract temporal expressions from text is effective, as evidenced by its outperformance of a state-of-the-art model.\"]"
    ],
    "7972": [
        "We tackled the alignment issue in monotonic multihead attention (MMA) for online streaming ASR with HeadDrop regularization and head pruning in lower decoder layers.",
        "We also stabilized streamable inference by head-synchronous decoding.",
        "Our future work includes investigation of adaptive policies for head pruning and regularization methods to make the most of the MA heads instead of discarding them.",
        "Minimum latency training as done in MoChA [48] is another interesting direction."
    ],
    "7973": [
        "We investigate the effects of de-identification on concept extraction and show that it positively influences the concept extraction performance.",
        "We propose two models to learn both tasks jointly, a multitask model and a stacked model, and set the new state of the art on medical concept extraction benchmark datasets for English and Spanish.",
        "De-identification positively influences concept extraction performance.",
        "Our proposed models learn both tasks jointly and achieve state-of-the-art performance on medical concept extraction benchmark datasets for English and Spanish.\"']"
    ],
    "7975": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Our novel fine-grained content-based unsupervised stance detection method can detect fine-grained stances of users towards different topics with high accuracy.",
        "Our method can be used to cluster user stances towards various polarizing issues in Turkish society, noting correlations between positions across topics.",
        "The resultant clusters can be used to quantify polarization on topics of interest.']"
    ],
    "7979": [
        "Pre-training data with matching speaking style was found to be more useful on downstream recognition tasks.",
        "Using MPC directly on streaming models helps, but combining MPC with APC brings further improvements on streaming models.",
        "The combination of target data adaption and layer-wise discriminative training provides consistent gains on knowledge transfer to downstream tasks."
    ],
    "7981": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets using Dynamic Memory Induction Networks (DMIN) for few-shot text classification.",
        "Adapting the relative position encoding scheme to speech Transformers for end-to-end speech translation and audio segmentation provides consistent and significant improvement across different tasks and data conditions.",
        "Inevitably, audio segmentation remains a barrier to end-to-end speech translation; future neural solutions are needed.']"
    ],
    "7982": [
        "The importance of graph representations in MDS can be leveraged to improve the performance of neural abstractive MDS.",
        "Our proposed model can incorporate explicit graph representations into the document encoding process to capture richer relations within long inputs.",
        "Our proposed method can utilize explicit graph structure to guide the summary decoding process and generate more informative, fluent, and concise summaries.",
        "Our model outperforms several strong baselines by a wide margin.",
        "We propose an effective method to combine our model with pre-trained LMs, which further improves the performance of MDS significantly.",
        "In the future, we would like to explore other more informative graph representations such as knowledge graphs and apply them to further improve the summary quality.']"
    ],
    "7984": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Including additional text only data into the training of attention based implicit alignment models can improve over the cross entropy baseline with shallow fusion at decoding time.",
        "The MMI criterion leads to the largest improvement of 18% relative on test-other, but at the cost of a five-fold increase in training time.",
        "The local fusion criterion improves over the baseline by 10% rel. on test-other without requiring additional effort or resources.\"']"
    ],
    "7985": [
        "We present a new large-scale MDS dataset for the news domain, consisting of large clusters of news articles, associated with short summaries about news events.",
        "We hope this dataset will facilitate the creation of real-world MDS systems for use cases such as summarizing news clusters or search results.",
        "We conducted extensive experiments to establish baseline results, and we hope that future work on MDS will use this dataset as a benchmark.",
        "Important challenges for future work include how to scale deep learning methods to such large amounts of source documents and how to close the gap to the oracle methods."
    ],
    "7987": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our proposed recurrent chunking mechanisms outperform benchmark models across different datasets.",
        "By exploiting temporal expressions, we have improved the date-wise approach and yielded new state-of-the-art results on all tested datasets.",
        "An expensive combinatorial search over all sentences in a document collection is not necessary to achieve good results for news TLS.",
        "A more robust and diverse evaluation can be constructed by constructing a new TLS dataset with a much larger number of topics and with longer time-spans than in previous datasets.",
        "Potential future directions include a more principled use of our proposed heuristic for detecting content relevant to specific dates, the use of abstractive techniques, a more effective treatment of the redundancy challenge, and extending the new dataset with multiple sources.']"
    ],
    "7989": [
        "Using a large batch size and feature invariant input allows the transformer to achieve strong performance on character-level tasks.",
        "However, it is unclear what linguistic errors the transformer makes compared to recurrent models on these tasks.",
        "Future work should analyze the errors in detail as Gorman et al. (2019) does for recurrent models.",
        "While Wu and Cotterell shows that the monotonicity bias benefits character-level tasks, it is not evident how to enforce monotonicity on multi-headed self-attention.",
        "Future work should consider how to best incorporate monotonicity into the model, either by enforcing it strictly (Wu and Cotterell, 2019) or by pretraining the model to copy (Anastasopoulos and Neubig, 2019).']"
    ],
    "7991": [
        "Our approach achieves a new state-of-the-art on the DiscoEval benchmark, outperforming BERT-Large with the same number of parameters as BERT-Base.",
        "Our model benefits most on ordering tasks rather than discourse relation classification tasks on the DiscoEval benchmark.",
        "The key architectural aspects of our model are cross attention, an auxiliary MLM objective, and a window size that is two or greater.",
        "Future work should explore the extent to which our model could further benefit from initializing with stronger models and what computational challenges may arise.",
        "Our ablation analysis shows that the key architectural aspects of our model are cross attention, an auxiliary MLM objective, and a window size that is two or greater.']"
    ],
    "7993": [
        "pre-training in the form of T5 leads to state-of-the-art results, while greatly improving robustness to out-of-domain inputs.",
        "We hope to design unsupervised pre-training objectives that are specifically tailored for the data-to-text task.",
        "We also hope to extend this work to multiple languages, especially low resource ones."
    ],
    "7999": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets using Dynamic Memory Induction Networks (DMIN).",
        "Dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning, and is worth investigating in other learning problems.']"
    ],
    "8001": [
        "The proposed method achieves new state-of-the-art results on the miniRCV1 and ODIC datasets for few-shot text classification.",
        "Dynamic Memory Induction Networks (DMIN) can be a learning mechanism more general than what has been used here for few-shot learning.",
        "The model achieves better results by leveraging dynamic routing to track previous learning experience and adapt to unseen classes.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The approach of joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "The use of graph neural networks can be effective in incorporating high bandwidth feedback via diagnostic feedback for learning to edit based on written feedback.']"
    ],
    "8003": [
        "The ultimate chatbot evaluation metric should be user-centric, as chatbots are there to provide human with an enjoyable experiences.",
        "Previously Likertscore based self-reported rating is the de-facto standard for current dialog evaluation .",
        "However, our analysis indicates that self-reported dialog ratings are skewed (J-shape), noisy and insensitive due to bias and variance among different users.",
        "We propose a three-stage denoising pipeline CMADE to reduce self-reported ratings and, at the same time, build an automatic comparison-based automatic dialog quality predictor.",
        "CMADE\\'s results highly correlate with expert judgments on pair-wise dialog comparison ratings (89.2% agreement, 0.787\").\"']"
    ],
    "8007": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Label bias arises when different output states have very different numbers of outgoing transitions.",
        "In linear-chain CRF, partition function can be accurately and efficiently computed with the Viterbi algorithm based on dynamic programming.",
        "Our proposed model still favors generic texts to a large degree.",
        "In token-level MLE training, each update requires one forward pass and one backward pass.",
        "In sequence-level training, an extra decoding step is required.",
        "Auto-regressive decoding is a sequential process, and therefore is pretty slow.\"']"
    ],
    "8009": [
        "The use of mathematical encodings did not significantly improve classification accuracy, possibly due to a low inter-class variance of the math encodings.",
        "The identifier x occurs very often in many subject classes, but with different meanings, leading to a large overlap of the formula identifier namespaces.",
        "Disambiguating the identifier semantics by annotation would increase the vector distance between subject classes and possibly increase classification accuracy.",
        "There are two ways to tackle the identifier disambiguation: supervised with human quality control or unsupervised without human intervention.",
        "The authors have presented their results of unsupervised semantification using three different sources, and are planning to additionally perform supervised annotation in the future work.']"
    ],
    "8010": [
        "We introduce and address an important problem towards a better understanding of support tickets - segmentation of various nonnatural language segments.",
        "We create an annotated dataset for the task, on questions from the publicly available website, Ask Ubuntu.",
        "We also study the performance of the most recent Recurrent Neural Network-based approaches to sequence labelling, on this task.",
        "We propose the novel idea of combining pre-trained embeddings from language models trained on different data sources, which substantially improves performance.",
        "We also demonstrate the usefulness of the task with improvements in retrieval of the correct answer.",
        "Our future research direction includes a thorough study of differences in this dataset with actual tickets, and potential for transfer.\"']"
    ],
    "8011": [
        "The proposed framework combines dictionary-based labeling with syntactically-informed label expansion to efficiently enrich seed dictionaries, leading to effective named entity recognition in a low-resource setting.",
        "Experimental results on a dataset of manually annotated e-commerce product descriptions demonstrate the effectiveness of the proposed framework.",
        "The proposed approach integrates a PU learning algorithm for recognizing named entities, which improves the performance of the model.",
        "The use of syntactically-informed label expansion enhances the accuracy of the model.",
        "The proposed method is effective in low-resource settings where manual annotation is not feasible.']"
    ],
    "8014": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The automatic hypernym candidate generation from an existing taxonomy is a feasible task, so it can be used to assist manual taxonomy enrichment.",
        "The obtained levels of quality will allow direct use of some of the best-performing methods in the further development of lexical resources, such as thesauri, taxonomies, and ontologies.']"
    ],
    "8015": [
        "GeoCoV19 outperforms previous datasets in terms of geographical coverage and scale.",
        "The dataset consists of over 524 million multilingual tweets, spanning 218 countries and 47K cities.",
        "The gazetteer-based approach used to extract toponyms from user location and tweet content derives geolocation information with high accuracy.",
        "The dataset has been collected over a period of 90 days from February 1 to May 1, 2020.",
        "The tweets in the dataset are from more than 43 million Twitter users, including around 209K verified accounts.",
        "The dataset can be used to develop computational models for understanding how societies are coping with the COVID-19 pandemic.",
        "The dataset can inform the development of AI-based systems to forecast disease outbreaks and provide surveillance for authorities.",
        "The dataset can help identify knowledge gaps, urgent needs, and unanticipated issues related to the pandemic.",
        "The dataset can be used to tackle misinformation and fake news related to the pandemic.\"']"
    ],
    "8017": [
        "We studied the accuracy-latency trade-off under chunk-based incremental inference.",
        "By selecting a subset of chunk-level outputs, we achieved large error reduction with minimal delay.",
        "The most efficient trade-off was achieved by examining the local agreement of hypotheses.",
        "With a simple yet effective adaptation procedure on partial inputs, the unidirectional model performed onpar with its bidirectional counterpart on ASR.",
        "Our approach was also effective on speech translation.",
        "Besides ASR, our approach was also effective on speech translation.",
        "A future direction is to incorporate adaptive chunk sizes.\"']"
    ],
    "8018": [
        "We introduced a framework for generating a large-scale parallel corpus for sentence simplification.",
        "Our method is general and can be used to automatically generate simplification parallel corpora and thus data-driven simplification models using state-of-the-art architectures for any given language.",
        "Our results suggest that easier a language pair to translate, the better the simplification model that will result.",
        "Our work merges two important sub-fields of NLP (machine translation and sentence simplification) and paves the path for future research in both of these fields.",
        "We perform thorough empirical analysis to give insights into language pairs to select for simplifying a given language.",
        "Our method accommodates collecting multiple reference simplifications for a given source sentence by leveraging opensource multilingual corpora.\"']"
    ],
    "8024": [
        "Average F1 score improvement of 3.9\" - This claim suggests that the proposed approach (ADA) improves the performance of supervised models on out-of-domain data by an average of 3.9 points.",
        "Best performing model (BERT-A) was able to reach 44-49 F1 across both domains using no labeled target domain data\" - This claim highlights the superior performance of the BERT-A model on both domains without any labeled target domain data.",
        "Preliminary experiments showed that finetuning BERT-A on 1% labeled data, followed by selftraining led to substantial improvement, reaching 51.5 and 67.2 F1 on literature and news respectively\" - This claim suggests that further fine-tuning and self-training of the BERT-A model can lead to even better performance, as shown in the preliminary experiments.",
        "While these results are encouraging, we are yet to match supervised in-domain model performance\" - This claim highlights the current limitation of the proposed approach (ADA) in achieving the same level of performance as supervised in-domain models.",
        "Future directions to explore include incorporating noise-robust training procedures (Goldberger and Ben-Reuven, 2017) and example weighting (Dehghani et al., 2018) during self-training\" - This claim suggests that the authors plan to explore additional techniques to improve the performance of their approach in future work.']"
    ],
    "8026": [
        "Our RAG models obtain state of the art results on open-domain QA.",
        "People prefer RAG\\'s generation over purely parametric BART, finding RAG more factual and specific.",
        "The learned retrieval component is effective and can be hot-swapped to update the model without requiring any retraining.",
        "Investigating if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective, may be fruitful in future work.",
        "Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\"']"
    ],
    "8027": [
        "The brevity problem in NMT can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Laymen language concept normalization is much more difficult compared to medical technical expressions.",
        "Existing and freely available structured resources are less abundant for German compared to English.",
        "Crosslingual methods such as in Roller et al. (2018) might help increase the coverage of technical terms.",
        "The new corpus based upon a patient forum for kidney disease and stomach-intestines could be a valuable resource for mapping and translating between laymen and technical language styles in the medical domain.",
        "Two resources can support this translation process, and a simple baseline was tested on the corpus.']"
    ],
    "8028": [
        "Our proposed question type module and copy loss mechanism improve the performance over the base-line model, achieving the state-of-the-art.",
        "Our model has the ability and flexibility to generate multiple questions for one source sentence.",
        "The idea of question type module and copy loss mechanism can also be used to do answer-aware QG task or other similar text generation tasks."
    ],
    "8029": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning.",
        "The approach of debiasing the first (non-contextual) layer alone can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The impact of different factors on the performance of MAML in NLP applications, such as the amount of data and the similarity between tasks.",
        "It is unnecessary to customize the fine-tuning epoch number for each task according to the task profile or data quantity information.']"
    ],
    "8030": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Our evidence-searching approach is helpful to commonsense explanation task."
    ],
    "8031": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Developing annotated datasets from multiple sources based on existing datasets and our causality tagging scheme can improve the performance of SCITE.",
        "Combining our method with distant supervision and reinforcement learning can achieve better performance without having to build a high-quality annotated corpus for causality extraction.",
        "Incorporating visual information into Speech Enhancement (SE) can provide significant complementary information for the task.",
        "Using the EncoderAE and Qualatent modules together can address privacy problems.",
        "Time-domain compression can further enhance the online computation efficiency of the proposed LAVSE system.']"
    ],
    "8033": [
        "'Our approach achieves new state-of-the-art on FewRel 2.0 dataset.'",
        "'We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.'",
        "'We demonstrated the effectiveness of these models in language understanding settings that require precisely the type of knowledge that one finds in ConceptNet and OMCS.'",
        "'Our findings stress the importance of having detailed analyses that compare (a) the types of knowledge found in external resources being injected against (b) the types of knowledge that a concrete downstream reasoning tasks requires.'",
        "'We hope this work motivates further research effort in the direction of fine-grained knowledge typing, both of explicit knowledge in external resources and the implicit knowledge stored in pretrained transformers.'\"]"
    ],
    "8035": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "BART is a simple baseline model that is superior to Kaneko et al. (2020) in PUNCT and DET errors.",
        "The fine-tuned BART achieved remarkable results, which were comparable to the current strong results in English GEC.",
        "The monolingual BART seems to be more effective for GEC than the model with a multilingual setting.",
        "ConMask still has some limitations and room for improvement, such as the ability to apply a filter to modify the list of predicted target entities.']"
    ],
    "8039": [
        "The proposed approach achieves state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Clustering Promotion Mechanism and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The Cosine Annealing Strategy is used to combine the two methods.",
        "The approach can achieve competitive performance with less than 1% of the training data.",
        "The method significantly outperforms previous systems, reducing the error by 21% on English Switchboard.",
        "The parser architecture is effective for the type of dependency graphs exhibited by ED.",
        "With more resources for BERT fine-tuning, hyperparameter tuning, languagespecific pre-trained representations, and careful preand post-processing, the parser will be a competitive system in this task.",
        "The contribution is a transition system that can directly handle ED in a unified transition-based parsing system.']"
    ],
    "8041": [
        "The task of compressing ASR outputs to enhance subtitle readability has unique properties, such as not being solely deletion-based and needing to limit unnecessary paraphrasing to maintain a consistent user experience between hearing and reading.",
        "The compression model is trained in an unsupervised fashion due to the absence of supervised corpora.",
        "Adapting an end-to-end ASR model on a small corpus with compressed transcriptions can improve transcription quality while shortening outputs.",
        "Injecting length constraints via reverse positional encoding can further improve performance while adhering to length constraints.",
        "Measuring length by the number of BPE-tokens is not always directly related to the output length perceived visually, and a more direct metric would be the number of characters.",
        "Output complexity, such as the proportion of long words, is also important for readability and therefore worth exploring.",
        "Augmenting the training data with synthesized utterances from summarization corpora can alleviate resource scarcity for end-to-end ASR compression.']"
    ],
    "8042": [
        "The proposed approach can rate the quality of AMR graphs in the absence of costly gold data, using a human judge-like model that is efficient and accurate.",
        "The method utilizes convolutions to process AMR in its native multi-line Penman format, allowing for efficient and accurate AMR processing.",
        "The proposed approach rates AMR quality more accurately and efficiently than previous work."
    ],
    "8043": [
        "The performance of widely-known baselines is substantially worse than a much more sophisticated GRU-based sequence-to-sequence baseline when using conditional language models.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "The approach of generating semantically and syntactically correct variations of standard test sentences without introducing noise can effectively identify surprising cases where the translations of extremely similar sentences are different.",
        "Both RNN and Transformer models exhibit volatile behavior with changes in translation quality for a significant percentage of sentence variations, highlighting the problem of generalizability of current NMT models.",
        "The insights from this study can be useful for developing more robust NMT models.']"
    ],
    "8044": [
        "'Our final system performed 4 steps: edit tree retrieval, additional lemma retrieval, paradigm size discovery, and inflection generation.' (related to the methodology)",
        "'The last component was either an LSTM sequence-to-sequence model with attention (IMS-CUB1) or a pointergenerator network (IMS-CUB2).' (related to the methodology)",
        "'Although our systems could not outperform the official baselines on average, IMS-CUB2 was the best submitted system.' (related to the results)",
        "'It further obtained the overall highest performance for Bulgarian and Kannada.' (related to the results)"
    ],
    "8052": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Creation of such a dataset would greatly increase the challenge for future algorithms and better match real-world usage.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments.\"']"
    ],
    "8054": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our method for jointly learning interpretable alignments as part of the downstream prediction shows superiority in text matching applications.",
        "Our method is very general in nature and can be used as a differentiable hard-alignment module in larger NLP models.",
        "Our method is agnostic to the underlying nature of the two objects being aligned and can therefore align disparate objects such as images and captions, enabling a wide range of future applications within NLP and beyond.']"
    ],
    "8055": [
        "The state-of-the-art dialogue systems can be manipulated.",
        "Current neural dialogue models can be manipulated using reinforcement learning-based sentence generation frameworks.",
        "Our proposed method can manipulate neural dialogue models and is likely to be applied on black-box dialogue systems based on other methods or models.",
        "The proposed method has the potential to be applied on other natural language generation tasks, such as text summarization, machine translation, etc.",
        "The current state-of-the-art dialogue systems are not robust against manipulation.']"
    ],
    "8056": [
        "Our new version of the named entity annotation layer of the French TreeBank (FTB-NE) achieves a state-of-the-art performance for French NER using state-of-the-art neural techniques and recently produced neural language models.",
        "The FTB-NE is a good approximation of a real use case, as its chronological partition increases the number of unseen entities, allowing for a better estimation of the generalization capacities of machine learning models.",
        "Integrating the NER annotations in the UD version of FTB would allow for training more refined models, either by using more information or through multitask learning by learning POS and NER at the same time.",
        "Using dependency relationships to provide additional information to a NE linking algorithm is an interesting point to investigate in future research."
    ],
    "8060": [
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our self-trained models strongly outperform comparable baselines and advance the state of the art on PTB by 1.6% F1.",
        "Our approach yields most gains for longer sentences.",
        "Our architecture can also leverage limited amounts of parsing supervision when available.",
        "It is beneficial to develop better UP models for semi-supervised settings.\"']"
    ],
    "8061": [
        "The proposed Neural Temporal Opinion Model (NTOM) effectively addresses users' changing interest and dynamic social context.",
        "The model captures the dynamics in the focused topics and contextual attention, as verified by experimental results.",
        "The temporal point process-based approach can jointly predict the posting time and stance label of the next tweet.",
        "Visualization of topics and attention signals shows the effectiveness of NTOM in capturing the dynamics of the focused topics and contextual attention.\"]"
    ],
    "8067": [
        "Replacing the upper self-attention layers with feed forward layers will not harm the model\\'s performance.",
        "Our experiments on WSJ and SWBD confirm that replacing the upper self-attention layers with feed forward networks even gives small error rate reductions.",
        "Future work includes investigating this idea on other domains (e.g., natural language processing) and developing novel network architectures based on this observation."
    ],
    "8068": [
        "Variational autoencoders have proven their benefit in many tasks, while providing an attractive machine learning framework that combines many strengths of neural-network training with the uncertainty metrics of statistical models.",
        "They can learn a low-dimensional manifold to summarize the most salient characteristics of data and come with a natural, statistical interpretation, both in the latent as well as in the observation space.",
        "Incorporating an assumption of Student-t distributed data into the joint learning mechanism for the latent manifold and its statistical distribution can improve modeling capacity.",
        "Simultaneously solving both tasks (jointly learning a nonlinear mapping to transform a given dataset of interest onto a lower-dimensional manifold in latent space and grouping the latent representations into meaningful categories) can be beneficial for learning latent representations.",
        "The proposed VAE with an embedded Student-t mixture model has proven its capability to obtain better results than both SVM-based classifiers as well as the standard Gaussian VAE on the real-world task of authorship attribution.\"']"
    ],
    "8071": [
        "The proposed language model achieves strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings.",
        "The model nearly matches the performance of state-of-the-art fine-tuned systems in some cases.",
        "The model generates high-quality samples and exhibits strong qualitative performance at tasks defined on-the-fly.",
        "Scaling in performance is roughly predictable without using fine-tuning.",
        "Very large language models may be an important ingredient in the development of adaptable, general language systems.",
        "The results suggest that very large language models have the potential to be a game-changer in the field of NLP.']"
    ],
    "8072": [
        "We propose Hardware-Aware Transformers (HAT) framework to solve the challenge of efficient deployments of Transformer models on various hardware platforms.",
        "We conduct hardware-aware neural architecture search in an ample design space with an efficient weight-shared SuperTransformer, consuming four orders of magnitude less cost than the prior Evolved Transformer.",
        "We hope HAT can open up an avenue towards efficient Transformer deployments for real-world applications.",
        "The larger the inherited val loss, the lower the trained from-scratch BLEU.",
        "SubTransformer trained from-scratch BLEU (Target) A Appendix for \\'HAT: Hardware-Aware Transformers for Efficient Natural Language Processing\\' A.1 SubTransformer Performance Proxy In Figure 11, we show the relationship between the validation loss of SubTransformers directly inherited from the SuperTransformer, and the BLEU score of the SubTransformers trained from-scratch.",
        "Therefore the validation loss can be a good performance proxy.",
        "The searched model for Raspberry Pi is deep and thin, while that for GPU is shallow and wide.",
        "The BLEU scores of the two models are similar: 28.10 for Raspberry Pi CPU, and 28.15 for Nvidia GPU.\"']"
    ],
    "8073": [
        "The proposed method achieves state-of-the-art performance on both CoNLL and TAC-KBP 2010 with a four-layer Transformer-based model.",
        "The creation of new datasets and more difficult entity linking settings, such as zero-shot and low resource domains, are crucial areas for future work.",
        "The limited headroom remaining in the current datasets and the strong impact of alias tables in simplifying the problem.",
        "The importance of creating new datasets and more challenging entity linking settings to advance the field.",
        "The potential for zero-shot and low resource domains to provide a challenge for future work.",
        "The current datasets (CoNLL and TAC-KBP 2010) being limited in terms of headroom, and the need for more challenging entity linking settings.",
        "The impact of alias tables in simplifying the entity linking problem.']"
    ],
    "8074": [
        "Incorporating structural information into the standard semantic+sequential model can lead to performance boosts for background-aware dialogue response generation.",
        "Different linguistic priors and ways of combining sequential and structural information can be studied to further improve performance.",
        "Explicit incorporation of structural information helps richer deep contextualized representation-based architectures.",
        "The analysis presented in this work provides a blueprint for analyzing future work on GCNs, ensuring that the gains reported are robust and evaluated across different configurations.']"
    ],
    "8075": [
        "Our framework is robust to speech transcription and user errors that occur frequently in spoken dialog systems.",
        "We have shown that featurizing the output of NEU and feeding these features into other language understanding tasks substantially improves the accuracy of these models.",
        "Our approach jointly identifies and resolves entities present in an utterance when a user interacts with a voice assistant.",
        "We have leveraged rich signals from entity links in the knowledge base while simultaneously linking these entities to the knowledge base.",
        "Our system targeted towards noisy natural language utterances, we have shown that our framework is robust to speech transcription and user errors that occur frequently in spoken dialog systems.\"']"
    ],
    "8076": [
        "Our proposed method guarantees robustness against all possible perturbations, and is structure-free, making it applicable to any pre-trained models like BERT or character-level models like Char-CNN.",
        "The construction of the perturbation set is crucial to our method, and we used a heuristic approach based on the synonym network, but there may be more efficient ways to construct the perturbation set.",
        "Our approach can be generalized to achieve certified robustness against other types of adversarial attacks in NLP, such as the out-of-list attack.",
        "Adding the \"OOV\" token into the synonyms set of every word may be a na\u00efve way to generalize our approach, but there may be better procedures to explore.']"
    ],
    "8077": [
        "We propose a sub-band knowledge distillation framework for single-channel speech enhancement.",
        "Compared to the full-band model, although the sub-band enhancement model loses the ability to capture global cross-band information, the performance does not show a significant drop as expected.",
        "The teacher models further improve the performance of the sub-band enhancement model.",
        "The performance of the student model exceeds the corresponding full-band model."
    ],
    "8080": [
        "We proposed a novel neural model architecture for simultaneous MT that incorporates a component for splitting the incoming source stream into translatable chunks.",
        "The proposed learned source chunking outperforms a fixed wait-k strategy by a large margin.",
        "We also investigated the value of backwards source encoding in the context of simultaneous MT by comparing uni-and bidirectional versions of our architecture."
    ],
    "8082": [
        "The proposed method achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The approach uses a representation extractor with the Clustering Promotion Mechanism to align similar class distributions across domains.",
        "The use of pseudo labels improves the domain adaptation performance.",
        "The proposed Cosine Annealing Strategy combines two methods to improve the domain adaption performance.",
        "Experimental results demonstrate the effectiveness of the approach on the FewRel 2.0 dataset.",
        "The method can reduce the noise of pseudo labels to improve the domain adaption performance in the future.",
        "The approach focuses on optimizing the interdependence between context and word interpretation.",
        "The SCAIN algorithm achieves online optimization by dynamically estimating the context and interpretation of polysemous words.",
        "The SCAIN algorithm can obtain new interpretations of words during sequential input of sentences in conversation.",
        "The approach achieved accuracy exceeding that of humans in a conversation dataset.']"
    ],
    "8084": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR), with performance not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "The simple unsupervised approaches based on large pre-trained neural language models yield results comparable to sophisticated traditional supervised baseline approaches.",
        "Integration of the information about the target substantially boosts the quality of lexical substitution and shall be used whenever possible.",
        "Depending on the type of semantic relations required in an NLP application, one or another type of neural LM shall be used.\"']"
    ],
    "8085": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "The model can improve BERT base performance with significantly fewer parameters.",
        "We also explain the contribution of essential phrases of perspectives in detecting their stance using maxpooling operation.\"']"
    ],
    "8087": [
        "The proposed user memory graph has the benefits of accumulating knowledge for a user to reason dialog policy.",
        "We propose a baseline model called UMGR that performs reasoning over such a user memory graph in open space policy.",
        "UMGR is structure-preserving for policy generation and provides zero-shot reasoning capability for user memory graphs that have never been seen before.",
        "Experimental results demonstrate the effectiveness of UMGR over a wide spectrum of metrics."
    ],
    "8088": [
        "The Malayalam-English corpus is a code-mixed corpus of YouTube comments annotated for sentiment analysis, which enables research on code-mixed sentiment analysis and provides useful data for codemixed research.",
        "The annotation project aims to provide an inter-annotator agreement score in terms of Kripendorff's alpha and baseline results.",
        "The corpus is available to the research community."
    ],
    "8097": [
        "Our new lexicon for connotation aspects aligns well with human judgments and confirms hypotheses about semantic divergences between synonyms.",
        "Our unified connotation representation for words from all parts of speech captures more connotative information than pre-trained word embeddings.",
        "Models using our connotation representations are well suited for few-shot stance detection and may also generalize well to zero-shot settings.",
        "Our method can be adapted to learn multi-lingual connotation representations that accurately capture cultural and linguistic variations in future work.']"
    ],
    "8099": [
        "The embeddings of GloVe, ELMo, and BERT contain gender, religion, and nationality biases.",
        "A projection-based method can be used to attenuate these biases.",
        "The method works for static GloVe embeddiments.",
        "Debiasing the first non-contextual layer of ELMo and BERT embeddings can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The proposed constituent hierarchy predictor based on recurrent neural networks can capture global sentential information.",
        "The fully-supervised parser outperforms the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "The combined text and acoustic embeddings can boost the accuracy of language identification systems.",
        "Semisupervised learning using text-based LID as a teacher model can reduce the accuracy gap between acoustic-only and combined models.']"
    ],
    "8105": [
        "Using teacher-student distillation for UD parsing is an effective means of increasing parsing efficiency.",
        "The baseline parser used for our experiments was not only accurate but already fast, meaning it was a strong baseline from which to see improvements.",
        "We obtained parsing speeds 2.30x (1.19x) faster on CPU (GPU) while only losing \u223c1 point for both UAS and LAS when compared to the original sized model.",
        "The smallest model which obtains these results only has 20% of the original model's trainable parameters, vastly reducing its environmental impact.\"]"
    ],
    "8106": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The addition of contextual information to target response improves the performance of the fine-tuned contextual word embeddings for detecting sarcasm.",
        "The addition of a separation token between context and target response also performs competitively, markedly showing an improvement of 5.13% in the F1-score of Reddit dataset.",
        "Future works include exploring different contextual cues, including user-specific attribute information and extending this hypothesis to other figurative speeches like irony detection and humor detection.\"]"
    ],
    "8107": [],
    "8110": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Document-level CQA requires document question answering, page segmentation, and more.",
        "Creation of a dataset with human-generated question-answer pairs would greatly increase the challenge for future algorithms and better match real-world usage.']"
    ],
    "8112": [
        "The first Speech Translation systems specifically tailored for subtitling have been presented, which allow for the creation of satisfactory subtitles both in terms of translation quality and conformity to the subtitling constraints.",
        "The two systems (ASR-MT cascade and direct, end-to-end ST system) have similar translation quality performance, but the E2E system models the subtitle constraints better.",
        "The E2E system's better modelling of subtitle constraints is attributed to acoustic features such as natural pauses becoming available through audio input.",
        "The segmentation closer to the speech rhythm leads to a pleasant user experience.",
        "The work takes into account the intersemiotic nature of subtitling by avoiding conditioning the translation on the textual source language length.",
        "Good automatic subtitling is achieved through prosodic elements such as intonation, speech tempo, and natural pauses, rather than the length of the textual source language.\"]"
    ],
    "8113": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach to joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Using less than 1% (1000 sentences) of the training data, our approach can achieve competitive performance compared to the previous systems (trained using the full dataset).",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Normalizing the input before POS tagging results in significantly higher POS accuracies for code-switching data.",
        "The proposed model does not have to be trained on intrasentential CS data and can be trained on a mix of two monolingual datasets, thereby handling many more language pairs.",
        "The LAI scores show that most of the normalization replacements are necessary for ID and DE.",
        "The performance of the last two models is highest on respectively EN and DE, which is probably due to the original model being developed mostly with a focus European languages.']"
    ],
    "8114": [
        "The approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The proposed Cosine Annealing Strategy combines the two methods.",
        "The representation extractor encodes unlabeled target-domain data into features, which are then passed to a k-means cluster miner to generate pseudo labels.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The approach improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Ensemble learning can reduce the variance of predictions and improve prediction performance."
    ],
    "8116": [
        "The proposed BERT-based predictive ensemble model achieves competitive results for label prediction on the dataset Get it #OffMyChest.",
        "The dataset exhibits highly imbalanced distributions of the given labels, and high variations in some features like score, word count, comments per parent post, and comments per user.",
        "Day of the week has no significant impact on the frequency of Disclosure and Support based comments on Reddit.",
        "Future work may involve exploring more ensembling techniques and sophisticated architectures to predict the impact of a comment."
    ],
    "8118": [
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection, which outperforms the simple projection baseline using fast-align on most languages and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We demonstrate the utility of augmenting training data for offensive language detection in Arabic, and show that finetuning affective models is useful, especially in the case of offensive language detection.",
        "In the future, we will investigate other methods for improving our automatic offensive and hateful language acquisition methods, such as semi-supervised methods.",
        "We plan to explore other machine learning methods on the tasks, such as investigating the utility of semi-supervised methods as a vehicle of improving our models.']"
    ],
    "8119": [
        "The proposed method improves the performance of fine-grained NER, especially for instances with low-frequency labels.",
        "The method shares and learns the embeddings of label components.",
        "Future work may apply the method to other tasks and datasets and investigate its effectiveness.",
        "The method may be extended to more sophisticated label embedding calculation methods.']"
    ],
    "8120": [
        "Our system is a parser with strong contextual embeddings and second-order inference.",
        "For the lowresource language, we propose to train the model with a mixture of datasets.",
        "Empirical results show that the second-order inference is stronger than the first-order one.",
        "Mixing data improves the performance of parser significantly for the lowresource language.",
        "After we fix the graph connectivity issue, our system outperforms the system ranked 1st by 0.56 ELAS in the official results.",
        "We also show that the non-connected graphs are practically useful for its higher performance and faster speed.\"']"
    ],
    "8122": [
        "We presented Situated Interactive Multi-Modal Conversations (SIMMC), an important new direction towards building next generation virtual assistants with evolving multimodal inputs.",
        "We collected two new datasets using the SIMMC platform, and provided the contextual NLU and coreference annotations on these datasets, creating a new SIMMC task for the community to study.",
        "We established several strong baselines for some of the tasks enabled by the datasets, showcasing various uses of the datasets in real-world applications.",
        "The fine-grained annotations we collected open the door for studying several different tasks in addition to the ones highlighted in this work, which we leave as future work for the community to tackle.\"']"
    ],
    "8123": [
        "The proposed question answering system, JarvisQA, can answer several types of questions on table content representing scholarly data.",
        "The ORKG-QA benchmark is a starting point for collaborating on adding more data to better train, evaluate, and test QA systems designed for tabular views of scholarly knowledge.",
        "JarvisQA addresses several open questions in current information retrieval in the scholarly communication domain.",
        "JarvisQA can help researchers, librarians, and ordinary users to inquire for answers with higher accuracy than traditional information retrieval methods.",
        "The system is designed to improve information retrieval on scholarly knowledge.']"
    ],
    "8125": [
        "Our method can reveal more inconspicuous but perfectly sensible markers for particular categories compared to a priori approaches to discourse marker categorization.",
        "The resulting associations can be used to guide corpus analyses, such as defining an empirically grounded typology of marker use.",
        "Our approach can help elucidate subtleties in the most unexpected results through more qualitative analyses.",
        "We plan to use the associations we found as a heuristic to choose discourse markers whose prediction is the most helpful for transferable sentence representation learning.']"
    ],
    "8128": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our method not only mitigates direct bias of a word but also reduces its associations with other words that arise from gender-based predilections.",
        "We proposed a word classification method, called KBC, for identifying the set of words to be debiased.",
        "RAN-Debias significantly outperformed other debiasing methods on a suite of evaluation metrics, along with the downstream application task of coreference resolution while introducing minimal semantic disturbance.",
        "We would like to enhance KBC by utilizing machine learning methods to account for the words which are absent in the knowledge base.\"']"
    ],
    "8131": [
        "The proposed norm-based curriculum learning method for NMT improves the representation learning and generalizability of the model.",
        "The novel sentence difficulty criterion, consisting of linguistically motivated features and learning-dependent features, is effective in improving the performance of the model.",
        "The novel model competence criterion enables a fully automatic learning framework without the need for a task-dependent setting of a feature.",
        "The novel sentence weight alleviates any bias in the objective function and further improves the representation learning.",
        "The proposed method provides a significant performance boost and training speedup for NMT.']"
    ],
    "8133": [
        "\"We have proposed cross-model back-translated distillation (CBD), a method that works outside the three existing principles for unsupervised MT and is applicable to any UMT methods.",
        "\"CBD establishes the state of the art in the unsupervised WMT\\'14 English-French, WMT\\'16 English-German and English-Romanian translation tasks.",
        "\"It also outperforms the baselines in the IWSLT\\'14 German-English and IWSLT\\'13 English-French tasks by up to 3.0 BLEU.",
        "\"Our analysis shows that CBD embraces data diversity and extracts more model-specific intrinsic information than what an ensemble of models would do.\"']"
    ],
    "8136": [
        "The use of pseudo-labels for end-to-end ST is effective in low- and high-resource data conditions, across two domains and three language pairs.",
        "Fine-tuning and larger architectures are critical for obtaining improvements over the baseline in high-resource settings.",
        "Larger amounts of pseudo-labels allow for increasing the model size further, leading to state-of-the-art results on the MuST-C English-French and English-German datasets.",
        "Pre-training the decoder is more effective than encoder pre-training.",
        "End-to-end ST systems may simplify pseudo-labeling by utilizing a single system instead of a cascade system.']"
    ],
    "8138": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses a combination of Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The use of pseudo labels improves the domain adaptation performance.",
        "The approach achieves better performance than traditional deep reinforcement learning models without domain knowledge sharing.",
        "The method outperforms other approaches in terms of both success rate and length of dialogue.",
        "The proposed approach can be generalized to more meta-RL applications in multi-domain and few-shot learning scenarios."
    ],
    "8139": [
        "\"ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "\"Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "\"Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "\"We have presented in this paper a new pre-trained model M 3 P which combines Multilingual Pre-training and Multimodal Pre-training into a unified framework via Multitask Pre-training for multilingual multimodal scenarios.",
        "\"We proposed Multimodal Code-switched Training to further alleviate the issue of lacking enough labeled data for non-English multimodal tasks and avoid the tendency to model the relationship between vision and English text.\"']"
    ],
    "8140": [
        "Our joint end-to-end system for source number counting and multi-talker speech recognition achieves a new state-of-the-art WER of 7.5% on the WJS0-2mix database.",
        "Our specialized model for two talkers outperforms the TasNet-based model on three speakers and generalizes well to an unknown number of speakers, including four speakers.",
        "The OR-PIT-based system for source counting shows promising performance."
    ],
    "8144": [
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our models show solid performance and thus prove that this type of data modeling is feasible and can lead to systems that are applicable in production settings.",
        "Our findings generalize well by applying model architectures developed on our corpus to another dataset.",
        "A natural next step is to combine the datasets in a multi-task setting to investigate to what extent models can profit from combining the information annotated in the respective datasets.",
        "Further research will investigate the joint modeling of entity extraction, typing and experiment frame recognition.",
        "There are also further natural language processing tasks that can be researched using our dataset.\"']"
    ],
    "8145": [
        "Our main contribution is a dataset that captures these complex biases by labeling each sentence with its news source origin.",
        "Substantial insights on the political viewpoints of media sources can be identified by models trained on this data, without the use of manually annotated labels required by previous datasets.",
        "A model trained on NewB could potentially be used as an internet browser extension to better inform readers of the biases present in online newspaper articles.",
        "While NewB sets up the foundation for more complex political bias analysis, our current work has not used comprehensive pre-training strategies such as BERT and only identifies some of the political tendencies encompassed by this large text corpus.",
        "Important future work remains in developing new methods to translate the learned features of classifiers into well-defined political ideologies represented by news organizations.\"']"
    ],
    "8147": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "With larger sample sizes and when training on a variety of points during the outbreak, we can obtain stronger correlations to other countries.",
        "Our analysis can lead to future integration of social media in epidemiological prediction across countries, enhancing outbreak detection systems."
    ],
    "8148": [
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\")",
        "our model significantly outperforms existing HRED models and its attention variants",
        "the relevant contexts detected by our model are significantly coherent with humans\\' judgements",
        "the excellent learnability and strong noise resistance of the SOTA approach",
        "WLCS-l exhibits a higher correlation with human score than \u03c4",
        "improving common sequence instead of concentrating on the order between sequence pairs",
        "Flexible decoder generator may be a direction with greater value\"']"
    ],
    "8150": [
        "Our three base-models provide valuable insights regarding inappropriate tweets.",
        "The logistic regression model trained on LIWC features provides insights into psycholinguistic patterns that are emergent in such tweets.",
        "The n-gram based classifier provides word embeddings that are tuned with respect to abusive and hateful behavior.",
        "The attention-based BiLSTM highlights some of the important words that influence its predictions.",
        "Our stacked ensemble provides classification accuracy that is comparable to the state-of-the-art by only using textual properties.",
        "Future research on the topic may benefit from exploring the subtle linguistic differences in abusive and hateful tweets.\"']"
    ],
    "8152": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The component that constrains the offline SLT from real-time processing is the ASR, not the MT.",
        "We observe a significant qualitative difference between the end-to-end offline ASR methods and hybrid online methods.",
        "Submitting all our unselected candidates for contrastive evaluation on the test set."
    ],
    "8154": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks."
    ],
    "8158": [
        "The proposed LID architecture outperforms the baseline significantly (Table 4), indicating the importance of a domain-specific LID.",
        "The addition of a second filter can reduce the number of GSW false positives significantly, depending on the complexity of the target dataset.",
        "The multi-filter architecture is effective in filtering out non-GSW tweets during the inference phase.",
        "The F1-score of the proposed model on the GSW LID shared task test-set is 0.982.",
        "Other useful features, such as orthographic conventions in GSW writing, can be used during training to improve LID classification for dialects.",
        "Tweets metadata can improve LID classification for dialects considerably, but were not used in this paper.",
        "There are future works that need to be studied to determine the usefulness of certain features for low-resource language identification.']"
    ],
    "8162": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Tagging ambiguity is mostly local while dependency parsing heavily depends on POS tags to predict syntactic structures.",
        "Enabling \"tag \u2192 parse\" only also improves the tagging accuracy itself.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, leading to improved performance.",
        "Our method improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.']"
    ],
    "8163": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improves the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Our method outperforms popular compression methods, such as knowledge distillation, and results in large CPU speedups compared to BERT and DistilBERT."
    ],
    "8164": [
        "Our model has a substantial improvement on the two benchmark datasets where most models compete for improvement on the decimal point.",
        "We also conducted analyses to reflect on our model\\'s performance on learning multi-relation data and the proximity of distributions of correlation of the gold and predicted relations.",
        "Our model uses a GNN and a matrix transformer to capture the RoR in data.",
        "In this paper, we proposed a new paradigm of RE, which is capable of modeling the interdependency among multiple relations in the same text."
    ],
    "8168": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "By learning target concept representations along with input concept mention representations, our approach a) exploits valuable target concept information unlike existing text classification approaches and b) eliminates the need to separately generate target concept embeddings unlike existing text matching approach.",
        "Our model surpasses all the existing methods across three standard datasets by improving accuracy up to 2.31%.",
        "In future, we would like to explore other possible options to include target concept information which may further improve the results.\"']"
    ],
    "8171": [
        "'PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.'",
        "'While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.'",
        "'All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.'",
        "'Two-sided pragmatics models outperform one-sided ones by finding ways to exploit the potential of the language to a greater extent.'",
        "'Empirically speaking, IBR achieves significant communication accuracy with a practically viable procedure for humans, where typically two-level reasoning is sufficient.'",
        "'Our advanced game-theoretic model provides new upper limits for communication accuracy, outperforming the classic model.'",
        "'The model can be applied to communication between StarCraft II allies, allowing them to communicate more efficiently and thereby mitigating the effects of message drop.'\"]"
    ],
    "8172": [
        "The proposed approach achieves new state-of-the-art performance on FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The Cosine Annealing Strategy combines the two methods to improve domain adaptation performance.",
        "The approach uses pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Hyperarticulated speech is characterized by a larger vocalic space, higher fundamental frequency, and increased glottal formant frequency.",
        "Hypoarticulated speech is characterized by a lower number of glottal stops, breaks, and syllables, and significant phone variations (especially deletions).",
        "The synthesized hypo and hyperarticulated speech was performed using HTS, based on modifications of the phonetic transcriptions at the input of the synthesizer, and of the characteristics of the excitation modeling.']"
    ],
    "8174": [
        "PABEE can simultaneously improve the efficiency, accuracy, and adversarial robustness upon a competitive ALBERT model.",
        "However, a limitation is that PABEE currently only works on models with a single branch (e.g., ResNet, Transformer).",
        "For future work, we would like to explore our method on more tasks and settings.",
        "Also, since PABEE is orthogonal to prediction distribution based early exit approaches, it would be interesting to see if we can combine them with PABEE for better performance."
    ],
    "8176": [
        "The proposed counterfactual inference framework, CF-VQA, can reduce language bias in VQA.",
        "The reduction of language bias is achieved by subtracting the direct linguistic effect from the total causal effect.",
        "Experimental results demonstrate the effectiveness and generalizability of CF-VQA.",
        "Recent debiasing studies [11, 14] can be unified into the proposed counterfactual inference framework.",
        "The proposed framework can improve RUBi [11] by simply changing a few lines of code and including one more learnable parameter.",
        "In the future, the authors will consider how to balance robustness and debiasing ability.']"
    ],
    "8177": [
        "\"providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "\"ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "\"correcting spelling errors in the data set with the pre-trained Word2Vec model improved our model performance by 0.2% on average.",
        "\"using global max-pooling, min-pooling and max-pooling on chunks to extract important features from each convolutional layer proved to be very effective.",
        "\"our competitive results show the potential of CNNs for the detection of semantically equivalent questions, a task that is typically solved by using RNNs and / or attention mechanisms.\"']"
    ],
    "8180": [
        "The proposed novel generative model, WaveNODE, leverages a CNF framework for speech synthesis, which has not been previously applied to this task.",
        "The CNF framework is effective in handling large-dimensional data (e.g., audio) without any additional loss term.",
        "WaveN-ODE demonstrates comparable performance with fewer flow steps compared to conventional flow-based models.",
        "WaveNODE can synthesize audio samples in real-time due to the parallel sampling process.",
        "Applying CNF to speech synthesis has the potential to produce more realistic waveforms, but further development and refinement are needed to achieve this goal.']"
    ],
    "8181": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "Our proposed method, ColdGAN s, can improve performance on both conditional and unconditional text generation compared to MLE-based training.",
        "Our proposed IS method makes it compatible with advanced sampling methods, such as nucleus, or other future decoding methods.",
        "In the future, we plan to combine ColdGAN s with orthogonal approaches proposed by previous works, such as denser rewards.']"
    ],
    "8185": [
        "The proposed task is to generate full-length natural answers from spoken questions and factoid answers.",
        "The authors have generated a dataset consisting of triples (spoken question, factoid answer, full length answer) and extracted confusion network from the questions.",
        "The authors have used the pointer-network over ASR graphs (confusion network) and shown that it gives comparable results to the model trained on the best hypothesis.",
        "The system achieves a BLEU score of 55.92% and ROGUE-L score of 76.78% on SQuAD/HarvestingQA dataset.",
        "The authors have performed cross-dataset evaluation to obtain a BLEU score of 42.89% and ROGUE-L score of 66.39% on Freebase, and a BLEU score of 56.86% and ROGUE-L score of 73.12% on NewsQA dataset."
    ],
    "8187": [
        "The experimental results show that GRAT outperforms existing state-of-the-art models in graph property prediction tasks.",
        "GRAT applies the graph-to-graph translation approach to the task of predicting resulting products in organic chemical reactions for the first time, and shows competitive performance over existing methods.",
        "GRAT has the potential to be applied to tasks in other domains beyond chemistry, such as physics or knowledge graphs, to prove its generality.",
        "Incorporating domain-specific knowledge into the model without detriment to the general Transformer architecture is a future research direction.']"
    ],
    "8189": [
        "We demonstrate how a combination of RL learned policies for choosing attribute-based clarification and active learning queries can be used to improve an interactive system that needs to retrieve images based on a natural language description, while encountering novel attributes at test time not seen during training.",
        "Our experiments show that in challenging datasets where it is difficult to obtain an accurate attribute classifier, learned policies for choosing clarification and active learning queries outperform strong static baselines.",
        "We further show that in this challenging setup, a combination of learned clarification and active learning policies is necessary to obtain improvement over directly performing retrieval without interaction."
    ],
    "8191": [
        "The simple baselines for predicting the chance of a bill being enacted have achieved good performance, with F1 scores of 0.86 and 0.41 for bills not enacted and bills enacted respectively.",
        "The current work has limited scope, as it only considers bills introduced in recent years and does not explore other metadata like parliamentary debates.",
        "There is room for improvement by gathering more bills from earlier years and exploring other metadata to identify dynamic structural factors in the behavior of the Kenya legislature.",
        "The current approach can only predict with a certain degree of accuracy whether a bill will be enacted or not, but does not provide information about the reasons for the prediction.']"
    ],
    "8193": [
        "'We presented a first effort at annotating points of correspondence between disparate sentences.'",
        "'Our approach fills a notable gap in coreference resolution and summarization research by presenting a benchmark dataset comprised of documents, source and fusion sentences, and human annotations of points of correspondence between sentences.'",
        "'Our findings shed light on the importance of modeling points of correspondence, suggesting important future directions for sentence fusion.'"
    ],
    "8195": [
        "We have shown that masking positions as well as tokens leads to both a convergence time advantage as well as steady state accuracy improvement.",
        "In the future, we plan to map this to other architectures to determine if the performance advantage scales.",
        "Masking the position opens up a new dimension for optimizing transformer networks that hopefully can open up new ideas which greatly improve performance."
    ],
    "8198": [
        "The direct approach can actually exploit audio information to better handle speaker-dependent gender phenomena, out of reach for cascade solutions.",
        "In spite of lower overall performance, the direct approach can handle speaker-dependent gender phenomena that are out of reach for cascade solutions.",
        "The use of parallel texts during training can help MT models develop skills related to the proper treatment of gender.",
        "ST technology has a potential advantage in inferring speakers\\' gender from input audio signals, which is not accessible to cascade solutions.",
        "Our evaluation shows that the direct approach can better handle speaker-dependent gender phenomena than cascade solutions, even though it has lower overall performance.",
        "The use of audio information in ST technology can help improve the handling of speaker-dependent gender phenomena.\"']"
    ],
    "8203": [
        "VILLA achieves consistent performance boost on all the benchmarks evaluated.",
        "By performing AT in both pre-training and finetuning stages, and by adding adversarial perturbations to the embedding space, VILLA achieves consistent performance boost.",
        "As AT is time-consuming, for future work, we plan to study how to accelerate AT so that it can be more feasible for large-scale pre-training in practice.",
        "VILLA performs better than previous methods by adding adversarial perturbations to the embedding space.",
        "VILLA is an advanced adversarial training (AT) framework for better visionand-language representation learning.\"']"
    ],
    "8205": [
        "We have presented experiments comparing the discrete representations learned by a Categorical VAE, a VQ-VAE, and Hard EM in terms of their ability to improve a low-resource text classification system.",
        "Our best classification models are able to outperform previous work, and this remains so even when we reembed discrete latents from scratch in the learned classifier.",
        "We find that amortized Hard EM is particularly effective in low-resource regimes when reembedding from scratch, and that VQ-VAE struggles in these settings."
    ],
    "8206": [
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\')",
        "Our approach outperforms benchmark models across different datasets.",
        "We proposed a new data augmentation technique, CRA (Contextual Response Augmentation), that utilizes the conversational context of the unlabeled data to generate meaningful training samples.",
        "The employment of both augmentations with labeled and unlabeled data enables the system to achieve the best F1 scores to win the FigLang2020 sarcasm challenge on both datasets of Twitter and Reddit.\"']"
    ],
    "8210": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Learning visual representations using textual annotations can be competitive to methods based on supervised classification and self-supervised learning on ImageNet.",
        "The usage of captions opens a clear pathway to scaling our approach to web-scale image-text pairs, that are orders of magnitude larger, albeit more noisy than COCO Captions.",
        "Larger backbones generally improve downstream performance.\"']"
    ],
    "8211": [
        "The proposed method, called SCITE, extracts causality in natural language text using a self-attentive BiLSTM-CRF-based solution.",
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The model based on gaze features and part-of-speech information achieves accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features are the most discriminative ones for disambiguating categories of it.",
        "The decoupled non-local block design facilitates learning for both terms in the non-local block, leading to improved performance on vision tasks such as semantic segmentation, object detection, and action recognition.",
        "Increasing the number of attention blocks in the backbone can further enhance the performance of the proposed DNL method over the NL method.']"
    ],
    "8213": [
        "The proposed benchmark is in a different vein than previous ones, with a much more complex search space and a non-skewed distribution of performance metrics.",
        "The benchmark will bring diversity and new challenges to the neural architecture search community, particularly for larger architectures that are more realistic and pose a challenge on feature engineering.",
        "The complexity of recurrent cells in the benchmark opens up opportunities for experimenting with sophisticated feature engineering methods.",
        "The performance of architectures highly correlates on different datasets, and the results extend the findings of previous works that GRU and LSTM architectures generally have the top performance.",
        "The benchmark will bring new insights regarding the performance of various recurrent architectures and better NAS methods.']"
    ],
    "8215": [
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\')",
        "Our most effective models are DistilBERT with PAR+INV and SVM+CRF with PAR SPLT.",
        "Both models perform similarly for the AD classification task, but the deep learning approach can also output MMSE score predictions.",
        "The DL methods will likely generalise better as the majority of the modelling is accomplished by the embedding layer.",
        "Both models could be deployed to mobile devices for a potentially ubiquitous early diagnostic tool.",
        "In a potential diagnostic scenario, models would seek to balance recall and precision.",
        "A true-positive label of AD would prompt the user to seek a formal evaluation, under medical supervision, potentially leading to an earlier diagnosis allowing for slower progression of the disorder.",
        "However, ensuring the false-positive rate is low would minimise unnecessary anxiety during the following formal clinical evaluation.\"']"
    ],
    "8216": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "We observed that skilled forecasters are more open-minded and exhibit a higher level of uncertainty about future events.",
        "It is possible to identify skilled forecasters and accurate predictions based solely on language.\"']"
    ],
    "8217": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Meronymic relations are one of the most important relationships between entities.",
        "The approach has yielded the first resource that is all three of: accurate (\u224890%), salient (covers relationships a person may mention), and has high coverage of common terms.",
        "It contains information about quantifiers, argument modifiers, and links the entities to appropriate concepts in Wikipedia and WordNet.",
        "The KB is available for the community at https://allenai.org/data/haspartkb.\"']"
    ],
    "8220": [
        "transferring an English XLNet model to a new target language, Tigrinya, we achieve comparable performance to a monolingual XLNet model (TigXLNet) pre-trained on Tigrinya text corpus.",
        "Computational saving of performing transfer learning only is enormous.",
        "The proposed method also has comparable results to mBERT on CLS dataset, especially on Japanese and German languages.",
        "PLM based XLNet has better performance in the case of unseen languages when compared to MLM based BERT and mBERT.",
        "training multilingual transformer models using PLM could achieve better performance boost across a range of downstream NLP tasks.",
        "This is due to the advantages of PLM over other language models like MLM to discover more insights about languages that are not even in the pre-training corpus.\"']"
    ],
    "8223": [
        "The proposed approach of using XL-VAE with UWSpeech achieves significant improvements in translation accuracy compared to direct translation and VQ-VAE baseline.",
        "The enhanced version of VQ-VAE, XL-VAE, can improve the translation accuracy of unwritten speech.",
        "The proposed approach of using UWSpeech with XL-VAE has the potential to be applied to truly unwritten languages for speech-to-speech translation."
    ],
    "8224": [
        "The pre-trained financial-task oriented BERT model, FinBERT, outperforms generic BERT models on three financial sentiment classification tasks.",
        "FinBERT is trained on a large financial corpora that are representative of English financial communications.",
        "The FinBERT model can be used for a wider range of applications beyond sentiment prediction, such as financial-related outcomes including stock returns, stock volatilities, corporate fraud, etc.",
        "The use of a specific domain-oriented BERT model (FinBERT) leads to better performance compared to generic BERT models on financial tasks.",
        "The pre-training of FinBERT on a large financial corpus improves its performance on financial sentiment classification tasks.']"
    ],
    "8225": [
        "\\'providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\")\\'",
        "\\'Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.\\'",
        "\\'Our approach selectively uses evidence to generate different inferential texts from multiple perspectives.\\'"
    ],
    "8226": [
        "The Transformer model with subword acoustic units achieved the best WER result on YouTube and books validations.",
        "The hybrid model still performs better in the case of a small amount of training data.",
        "The use of external NNLM for hypotheses rescoring of the hybrid system provides a WER reduction for all validation sets.",
        "NNLM using for end-to-end rescoring delivers ambiguous results.",
        "We observed performance degradation in almost all cases (for RNN-transducer in all) except the books validation.",
        "We think that this may be due to a sub-optimal ESPnet default hypotheses rescoring algorithm.",
        "As future work, we are going to study using NNLM for improving end-to-end decoding results.']"
    ],
    "8232": [
        "'Our proposed method outperforms two retriever-reader baselines by a large margin that is much greater than the difference between the two baselines.'",
        "'The reader-retriever structure of our proposed method is different from the design of any existing work.'",
        "'Our method has the potential to get further improved if solutions can be proposed in future work to handle less typical question types better or generate more realistic questions.'"
    ],
    "8233": [
        "The proposed approach significantly outperformed the benchmark baseline and vanilla DNN models.",
        "While requiring no feature engineering nor post-processing, it achieved comparable performance to the state-of-the-art rule-based machine learning system.",
        "The introduced residual facilitates to train a deeper network, and confirmed the domain-specific contextual word embeddings make significant contributions to the performance gain.",
        "The significant reduction of reliance on domain-specific knowledge would play a crucial role in certain expert-costly fields.",
        "Our investigation on key components may also shed light upon other deep low-resource NER applications.\"']"
    ],
    "8234": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses a combination of Similarity Entropy Minimization and Adversarial Distribution Alignment to promote clustering and align similar class distributions across domains.",
        "The use of pseudo-labeled target-domain data and labeled source-domain data improves the domain adaptation performance.",
        "The approach can be used to reduce the noise of pseudo labels to improve the domain adaption performance in future work.",
        "The experiment reveals that GloVe, ELMo, and BERT embeddings all encode gender, religion, and nationality biases.",
        "A projection-based method can be used to attenuate biases in word representations.",
        "The approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The simple approach can be extended to debias the first (non-contextual) layer alone for the well-characterized gender direction.']"
    ],
    "8239": [
        "We have presented DYNE, a simple approach to ensembling for multi-input problems, allowing SDS models to be directly leveraged to achieve state-of-the-art results on the MDS task.",
        "The ensembling approach presented here is well-suited to problems like multi-document summarization, where the information overlap between individual inputs is presumed to be high; however, for problems where the inputs contain disjoint information, a more sophisticated decoding controller with global awareness of all inputs may be needed.",
        "A weakness of the method proposed here is that document length is not a factor in the relative contribution of input documents to summaries.",
        "For news data in particular, it is plausible that a short document such as a tweet could add relevant novel information to that provided by the longer documents in the cluster.",
        "Another weakness is that the decoder memory usage increases linearly with the number of inputs, despite the decoded prefix being the same for all inputs.",
        "However, we note that DYNE allows parallel decoding of the documents in a cluster, in contrast to other methods which necessarily combine all documents in a cluster into a single input.\"']"
    ],
    "8240": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Correlations can be efficiently introduced into the bits using the distribution of Boltzmann machine as the posterior.",
        "An asymptoticallyexact lower bound of ELBO is developed to tackle the tricky normalization term in Boltzmann machines.",
        "Significant performance gains are observed in the experiments after introducing correlations into the bits of hash codes.\"']"
    ],
    "8241": [
        "The proposed cross-lingual pivoting technique can generate large volumes of labeled data for pronoun classification without explicit human annotation.",
        "The BERT-based classifier fine-tuned on the generated data achieves F1 score improvements of 92% for feminine pronouns compared to neural machine translation models and non-fine-tuned BERT.",
        "The classifier can be incorporated into a standard sentence-level neural machine translation system to yield an 8.8% F1 score improvement in feminine pronoun generation.",
        "The data creation method is largely language-agnostic and may be extended to new language pairs or different grammatical features in future work.",
        "The technique has the potential for zero-shot pronoun gender classification, particularly in low-resource languages.']"
    ],
    "8242": [
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR), with performance not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "Mood induction can be done within a concise time online, making it a method usable in larger scale settings.",
        "Frequently used emotion measurements only capture a small portion of the happiness and sadness of the authors.",
        "Pretrained models perform poorly on predicting ground truth emotion data.",
        "Linguistic measurements only partially explain the true emotions.",
        "Understanding the nexus from cognition to language is one of the fundamental challenges for computational research about human behavior.\"']"
    ],
    "8243": [
        "PReFIL surpasses prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL has the potential to improve retrieval of information from charts.",
        "Better OCR methods can improve PReFIL further.",
        "Future developments in OCR technology would likely improve PReFIL.",
        "The results suggest that the community is ready for more difficult CQA datasets."
    ],
    "8246": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "The solution to the brevity problem is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.",
        "The AVLnet model learns directly from raw video, reducing the need for spoken or text annotations.",
        "AVLnet can learn audio-visual concepts by relating speech and sound to visual objects.",
        "The tri-modal model, AVLnet-Text, that additionally learns from the text narration which already exists in many instructional video datasets, results in a multi-modal embedding space useful for text to video retrieval."
    ],
    "8247": [
        "Models trained on standard natural language inference datasets generalize poorly to other distributions.",
        "Achieving high accuracy on out-of-domain data may not be possible if the test data requires abilities that are not learnable from the training data.",
        "Adversarially chosen ungrammatical text can cause catastrophic errors in NLP systems.",
        "Traditional NLU systems typically have a natural ability to abstain.",
        "Our work provides a framework to study how models can more judiciously abstain in challenging environments."
    ],
    "8248": [
        "The approach used in this paper is semi-automatic, which allows for the creation of a new dataset of labeled potentially idiomatic expressions with high accuracy.",
        "The dataset is segregated into two categories, formal and static, based on the difference in the potentially idiomatic span detection mechanisms.",
        "The approach uses sentence extraction from the BNC corpus to create a large dataset of labeled expressions.",
        "The resulting dataset has high accuracy, with a significant improvement over previous methods.",
        "The distinction between formal and static categories is important for accurately detecting potentially idiomatic spans.']"
    ],
    "8249": [
        "The proposed self-supervised training approach achieves state-of-the-art performances in unsupervised machine translation and sentence retrieval.",
        "The approach is able to achieve these results even though the amount of unlabeled data used was artificially limited.",
        "Future work should explore a thorough analysis and theoretical understanding of how the language agnostic representation arises from denoising pretraining.",
        "Whether the same approach can be extended to pretrain models for non-seq2seq applications, such as unsupervised structural discovery and alignment.",
        "Whether the learned cross-lingual representation can be applied to other NLP and non-NLP tasks, and how.']"
    ],
    "8250": [
        "We introduced the SuspectGuilt corpus, which provides a basis for a quantitative study of how readers arrive at judgments of Reader perception and Author belief.",
        "We also showed that SuspectGuilt can be used to train predictive models on top of BERT parameters, and that these models are improved by genre-specific pretraining and supervision derived from token-level highlighting.",
        "Understanding how news reporting affects reader judgments is a difficult task.",
        "The span-level highlighting in SuspectGuilt provides some insight into the factors at work here.",
        "We sought to match this with an introspective analysis of our predictive models using the gradient-based token importance method of Sundararajan et al. (2017) .",
        "This yielded a very different picture from what we see in SuspectGuilt.",
        "Ultimately, this combination of annotations and model introspection might lead to new insights concerning how our models make decisions in this and other domains.",
        "We also hope that this work paves the way to large-scale studies of how readers formulate judgments of guilt in crime reporting and encourages the development of systems that provide guidance on the presentation of these reports.\"']"
    ],
    "8251": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "CO-Search ranks best amongst automated systems, and near the best amongst all systems, as judged using various metrics including nDCG@10, P@5, P@10, MAP, and Bpref.",
        "The system uses a combination semantic and keyword retriever that combines SBERT-derived embeddings with TFIDF & BM25 vectors to retrieve documents.",
        "It leverages a Wikipedia & PubMed pre-trained multi-hop question answering system, together with an abstractive summarizer, to modulate retrieval scores.",
        "This structure allows CO-Search to disambiguate between subtle word orderings that, in biological contexts, result in critically different meanings (e.g. \"What regulates expression of the ACE2 protein?\" vs. \"What does the ACE2 protein regulate?\").\"']"
    ],
    "8252": [
        "Using nonspeaker annotations as an alternative to crosslingual methods for building low-resource NER models can be effective.",
        "The resulting data provides insights about what makes NS annotators successful.",
        "Active learning can help in low-resource situations.",
        "Optimal ways to combine NS annotators with FS annotators should be explored.",
        "Using nonspeaker annotations as an alternative to crosslingual methods for building low-resource NER models can be effective.",
        "The resulting data provides insights about what makes NS annotators successful.",
        "Active learning can help in low-resource situations.",
        "Optimal ways to combine NS annotators with FS annotators should be explored."
    ],
    "8253": [
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\')",
        "Our approach works in an unsupervised manner that does not require a parallel corpus for training.",
        "In future work, we plan to add paraphrase generation to generate diverse simple sentences.\"']"
    ],
    "8254": [
        "The major contribution of this paper is to propose DE-VAE, a carefully designed hierarchical architecture that maintains both disentangled feature representation and entangled sentence representation.",
        "The invertible normalizing flow in DE-VAE enables learning of complex interdependency between disentangled features and entangled sentence representation without losing information.",
        "The design choice of DE-VAE is key to achieving accurate fine-tuning of sentiment while keeping the content intact, which is a key achievement considering the difficulty of the problem and modest performance of state-of-the-art techniques.",
        "Extensive experiments on real-world datasets emphatically establish the well-rounded performance of DE-VAE and its superiority over baselines.']"
    ],
    "8257": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "The use of a projection-based method can effectively attenuate biases in both contextualized embeddings without loss of entailment accuracy.",
        "The study's dialogues show that some users provided means of sensing the goal rather than the cause of the goal.",
        "CORGI's ability to successfully reason about an if-then-because statement was heavily dependent on whether the user knew how to give the system what it needed, and not necessarily what it asked.\"]"
    ],
    "8258": [
        "We propose an experimental protocol called Modifying Count Distribution (MCD) to penalize open-ended counting models that over-rely on statistical shortcuts.",
        "Our approach generates various modified dataset versions where the distributions of even and odd count labels are different between the training and testing sets, while keeping similar distributions of words and images.",
        "We introduce a model called Spatial Counting Network (SCN) which encompasses important design choices that help to overcome statistical shortcuts.",
        "Our model models region relationships and associates a score to each region before summing them to the final predicted count.",
        "We evaluate SCN against state-of-the-art models and report more robustness to distribution changes.",
        "Our entropy-based regularization strategy has a beneficial impact on grounding ability.",
        "We plan to extend our experimental protocol to more general machine learning problems.",
        "Our framework aims at reducing the undesired learning of statistical shortcuts, or unwanted biases, from the training data.",
        "Reducing the learning of shortcuts is essential if we aim to use those models in the real world, where the data distribution does not necessarily follow the training data distribution.",
        "Our work is a step towards better interpretability for counting models.\"']"
    ],
    "8259": [
        "We address the problem of extending event detection to unseen event types through fewshot learning.",
        "Our method increases the efficiency of using training data, resulting in better classification performance.",
        "Our ablation study shows that both intra-cluster matching and inter-cluster matching contributes to the improvement.",
        "We investigate four metric-based few-shot learning models with different encoder types (CNN, LSTM, and GCN).",
        "We propose two novel loss functions to provide more training signals to the model exploiting domain-matching information in the support set.\"']"
    ],
    "8267": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We have introduced sentence-level topical discourse and document-level topic association to retain sentence-level topical discourse.",
        "Experimental results on several language understanding tasks have supported our multi-fold contributions.\"']"
    ],
    "8269": [
        "ZSL-KG improves over existing WordNet-based methods on OntoNotes, BBN, SNIPS-NLU, aPY, and ImageNet (all test classes).",
        "DGP does slightly better on AWA2, but it performs relatively poorly on aPY and ImageNet.",
        "ZSL-KG achieves the highest performance on larger test sets of ImageNet and achieves a new state-of-the-art for aPY, while maintaining competitive performance on AWA2.",
        "ZSL-KG outperforms the best performing WordNet-based method (GCNZ) by an average of 3.5 accuracy points and all the WordNet-based methods by an average of 2.4 accuracy points.",
        "Our framework introduces a novel transformer graph convolutional network to learn rich representations from common sense knowledge graphs.",
        "Our work demonstrates that common sense knowledge graphs are a source of high-level knowledge that can benefit many tasks.\"']"
    ],
    "8272": [
        "Most text summarization methods work well for Russian without any special modifications.",
        "mBART performs exceptionally well even though it was not initially designed for text summarization in the Russian language.",
        "The authors were unable to extend the dataset using data from other sources due to legal issues.",
        "The authors plan to pre-train BART themselves on standard Russian text collections and open news datasets.",
        "The authors believe that headline generation as a pretraining task will increase the performance of the models.']"
    ],
    "8275": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The experimental results show that our model achieves a new state of the art on the full Librispeech benchmark for noisy speech.",
        "Our model achieves results which achieve a WER of 4.8/8.2 on test-clean/other of Librispeech.",
        "Our approach is effective when large amounts of labeled data are available.",
        "We expect performance gains by switching to a seq2seq architecture and a word piece vocabulary.\"']"
    ],
    "8278": [
        "We proposed a novel end-to-end Arabic text classification framework called AraDIC.",
        "Our image-based character embedding strategy eliminated the need for complicated preprocessing, segmentation and morphological analysis, and achieved much better performance than conventional deep and classical text classification techniques.",
        "We have shown that class-balanced loss is useful for text classification tasks with long-tailed distribution datasets.",
        "AraDIC achieved better performance than conventional deep and classical text classification techniques.",
        "Our approach eliminated the need for preprocessing, segmentation, and morphological analysis.\"']"
    ],
    "8279": [
        "We develop a novel defense algorithm to NLP models to substantially improve the robust accuracy without sacrificing their performance too much on clean data.",
        "Our method is broadly applicable, generic, scalable, and can be incorporated with negligible effort in any neural network.",
        "A novel adversarial training algorithm is also proposed, which enables NLP models to defense against the strong attacks that search for the worst-case over all combinations of word substitutions.",
        "Our adversarially trained smooth classifiers consistently outperform all existing empirical and certified defenses by a significant margin on IMDB, AGNEWS and SNLI across different network architectures.",
        "We establish state-of-the-art for the defenses against text adversarial attacks.\"']"
    ],
    "8280": [
        "The present work addresses the issue of gendered terms being clustered in debiased word embeddings, introduced in prior work [5].",
        "Our pipeline combines a post-processing step (MDR [7]) and a debiasing method (Cluster Debias) to outperform state-of-art debias methods on mitigating bias.",
        "The success of our pipeline validates our proposed reasons behind the observations made by Gonen and Goldberg [5].",
        "Our model outperforms existing approaches on coreference resolution tasks in terms of mitigating gender bias, but there are still other ways in which word embeddings encode gender bias.",
        "To avoid a \"whack-a-mole\" approach for mitigating bias, we encourage a focus on the development of more downstream tasks relative to further upstream analysis.']"
    ],
    "8283": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Creation of such a dataset would greatly increase the challenge for future algorithms and better match real-world usage.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments.\"']"
    ],
    "8290": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "We will attempt to solve this problem by developing annotated datasets from multiple sources based on existing datasets and our causality tagging scheme.",
        "We will combine our method with distant supervision [55] and reinforcement learning [56] to achieve better performance without having to build a high-quality annotated corpus for causality extraction.",
        "Our approach uses the Clustering Promotion Mechanism, which combines Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "We utilize pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.\"']"
    ],
    "8292": [
        "The use of gaze data can be effective in disambiguating categories.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of linguistic-based models and state-of-the-art systems.",
        "Late gaze features are the most discriminative ones for disambiguation.",
        "Disambiguation effort indicators can be as late as third pass revisits."
    ],
    "8293": [
        "Our approach reduces error rates by almost one half compared to non-ensemble models.",
        "We successfully surpassed all organizer-provided baselines on the task and compared favorably to several other submitted models.",
        "Our future work includes scaling up our self-training with larger Wikipedia data and choosing fully-trained models.",
        "Our multilingual approach exploiting Transformers in a fully supervised setting reduces error rates by almost one half.",
        "We leveraged multilingual Wikipedia data via a self-training strategy, but due to time constraints, we were not able to incorporate enough silver labeled data into training.\"']"
    ],
    "8294": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Annotation inconsistencies are prevalent in online misbehavior datasets, illustrating the challenges in online misbehavior data collection.",
        "The proposed framework for examining annotation consistency in online misbehavior datasets can be applied to analyze three popular online misbehavior datasets.']"
    ],
    "8298": [
        "The proposed Differential Window method for dynamic window selection improves the standard attention modules by enabling more focused attentions.",
        "The Trainable Soft Masking and Segmentbased Masking methods proposed in the paper can be applied to encoder/decoder self-attentions and cross attention, leading to significant performance improvements across all NLP tasks.",
        "The incorporation of the differentiable window in the attention mechanism benefits the models, as shown by the experimental results on four NLP tasks.",
        "The proposed methods outperform the baselines significantly, demonstrating the effectiveness of the novel Differential Window method.",
        "The future work of extending the work to make a syntactically-aware window that can automatically learn tree (or phrase) structures has the potential to further improve the performance of the models.']"
    ],
    "8302": [
        "Our proposed model requires no training and can be readily applied to any knowledge graphs.",
        "It achieves new state-of-the-art performance in NELL-995 and FB122 datasets.",
        "Our approach is robust in low-data settings.",
        "Overall, our nonparametric approach is capable of deriving crisp logical rules for each query by extracting reasoning patterns from other entities and entirely removes the burden of storing logical rules in the model parameters.\"']"
    ],
    "8303": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We demonstrate the effectiveness of our model NER and IC tasks, showing substantial improvement over the baseline.",
        "Our approach allows us to reduce the amount of needed manual annotations, and make it easier for developers to create high-quality skill grammars.\"']"
    ],
    "8308": [
        "We introduced a new approach to pre-training models for natural language understanding and generation, by using retrieved documents to reconstruct the original.",
        "MARGE exhibits strong performance on a range of discriminative and generative tasks in many languages, both with and without fine-tuning.",
        "These results establish MARGE as a viable alternative to masked language modeling and provide a step towards pre-trained models that can perform any task with little or no fine-tuning.",
        "Future work should scale MARGE to more domains and languages, and study how to more closely align pre-training objectives with different end tasks.\"']"
    ],
    "8312": [
        "We propose to use reinforcement learning with a bilingual semantic similarity metric as rewards for cross-lingual document summarization.",
        "We demonstrate the effectiveness of the proposed approach in a resource-deficient setting, where target language gold summaries are not available.",
        "We also propose simple strategies to better initialize the model towards reinforcement learning by leveraging machine translation and monolingual summarization.",
        "In future work, we plan to explore methods for stabilizing reinforcement learning as well to extend our methods to other datasets and tasks.",
        "We propose using the bilingual similarity metric as a reward to improve the quality of machine translation.\"']"
    ],
    "8315": [
        "Our approach for domainspecific generation of long text passages is simple and efficient.",
        "We demonstrate that our method outperforms a wide range of large pretrained LMs with single-stage generation or prior planning-then-generation strategies in terms of quality and coherence of the produced samples.",
        "The multi-stage generation opens up new opportunities to enhance controllability of text generation.",
        "Our method is effective in generating high-quality, coherent text passages that are comparable to those generated by large pretrained LMs with single-stage generation or prior planning-then-generation strategies.",
        "The multi-stage generation strategy allows for greater controllability of text generation, which we plan to explore further in the future.']"
    ],
    "8316": [
        "The proposed method achieves better performance compared with previous methods.",
        "The proposed method provides insight into which words in the text contribute to the classification decision.",
        "The method combines a convolutional neural network and a bi-directional RNN to process text and obtain the word's contextual representation.",
        "The weight matrix and the word's contextual representation are combined to get the text representation.",
        "The proposed method can be used for text classification.",
        "The method has the potential to explore features extracted from the method to learning methods, such as model space.",
        "[The proposed method achieves better performance compared with previous methods.]",
        "[The proposed method provides insight into which words in the text contribute to the classification decision.]",
        "[The method combines a convolutional neural network and a bi-directional RNN to process text and obtain the word's contextual representation.]",
        "[The weight matrix and the word's contextual representation are combined to get the text representation.]",
        "[The proposed method can be used for text classification.]",
        "[The method has the potential to explore features extracted from the method to learning methods, such as model space.]\"]"
    ],
    "8318": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\').",
        "introducing word sense advice can elicit greater improvements in the quality of semantic parsing, but relying too heavily on WSD output results in an increase in parse fragmentation."
    ],
    "8319": [
        "The use of RoBERTa is effective in identifying, extracting, and normalizing ADR mentions in tweets.",
        "The proposed models achieved promising results with significant improvements over average scores in tasks 2 and 3.",
        "The effectiveness of the proposed models is evident in both task 2 and task 3.",
        "The use of RoBERTa improves the accuracy of ADR mention identification, extraction, and normalization.",
        "The proposed models provide significant improvements over average scores, indicating their effectiveness in identifying and extracting ADR mentions.']"
    ],
    "8321": [
        "Our proposed novel generative model discovers a sparse prototype set automatically by optimizing a variational lower bound of the log marginal data likelihood.",
        "We demonstrate the effectiveness of our proposed framework on language modeling and its efficiency advantages over previous prototype-driven generative models.",
        "The framework proposed here might be generalized to automatically discover salient prototypes from a large corpus.",
        "New kinds of prototype structure in text might be discovered through either injecting different biases into the model or incorporating prior knowledge into the prototype library before training.",
        "Our approach can be easily extended to conditional generation (e.g. with the edit vectors depending on other data input), and we envision that inducing a sparse prototype set in this case may potentially facilitate controlling text generation through prototypes.",
        "We leave exploration in this direction as our future work.']"
    ],
    "8323": [
        "The proposed EM approach to nonautoregressive conditional sequence generation effectively addresses the multi-modality issue in NAR training by iteratively optimizing both the teacher AR model and the student NAR model.",
        "The developed principled plug-and-play decoding method for efficiently removing word duplication in the model's output is effective.",
        "Experimental results on three tasks prove the effectiveness of the approach.",
        "The method is planned to be examined for its effectiveness in a broader range of applications, such as text summarization.",
        "Non-autoregressive transformers are universal approximators of sequence-to-sequence functions.\"]"
    ],
    "8324": [
        "The proposed UNION model achieves state-of-the-art results on the ComVE dataset, as evidenced by high human evaluation scores.",
        "The UNION model is effective in multi-tasking on couple of commonsense related datasets.",
        "The proposed auxiliary metrics provide a better way to evaluate different models for commonsense response generation tasks without human evaluations.",
        "The future exploration of other possible areas of commonsense reasoning, such as quantitative reasoning, logic puzzles, and visual common sense reasoning, is promising.']"
    ],
    "8327": [
        "We proposed ERNIE-ViL to learn the joint representations of vision and language.",
        "In addition to conventional MLM for cross-modal pre-training, we introduce Scene graph Prediction tasks to characterize the cross-modal detailed semantic alignments.",
        "Experiment results on various downstream tasks demonstrate the improvements of incorporating structured knowledge obtained from scene graphs during cross-modal pre-training.",
        "For future work, scene graphs extracted from images could also be incorporated into cross-modal pretraining.",
        "Moreover, Graph Neural Networks that integrate more structured knowledge could be considered as well.\"']"
    ],
    "8329": [
        "We introduced Adversarial Mutual Information (AMI), a novel text generation framework that addresses a minimax game to iteratively learn and optimize the mutual information between the source and target text.",
        "Our objective can be trained more stably by being transformed to the problem of Wasserstein distance minimization.",
        "The experimental results on two popular text generative tasks demonstrated the effectiveness of our framework, and we show our method has the potential to lead a tighter lower bound of the MMI problem.",
        "We will attempt to explore a lower variance and more unbiased gradient estimator for the text generator in this framework and apply the AMI in multi-modality situations.",
        "We train for 340K steps; after 170K steps, we start halving the learning rate every 17K step.",
        "During training, we iteratively update the noise \u03b4, forward network and backward network for 5k steps, 35 steps, 10k steps respectively.",
        "Our batch size is set as 128, and the dropout rate is 0.2.",
        "We adopt the default setting of the base Transformer for NMT in the OpenNMT framework.",
        "The \u03bb in the latent noise sampling (LNS) is set as 0.1.\"']"
    ],
    "8330": [
        "The use of self-attention mechanisms in dialogue generation improves the performance of the model.",
        "The widely used HRED based models do not capture the important characteristic of multi-turn dialogue generation, and this can be improved by using proper detection methods.",
        "The proposed ReCoSa model outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by the ReCoSa model are significantly coherent with humans' judgements.",
        "The use of self-attention mechanisms can improve the quality of multiturn dialogue generation.",
        "Introducing topical information or content information can further improve the quality of generated responses.",
        "The multi-view FLSTM architectures outperform the baseline AM and single-view FLSTM model across all subsets.",
        "The mvFLSTM model achieves an additional 3% relative WERR over the best single view FLSTM, and a relative WERR of 7% on the non-native speakers test set.\"]"
    ],
    "8333": [
        "Training BioBERT to classify relationships between sentence pairs improved its performance in biomedical QA.",
        "Fine-tuning BioBERT on the NLI dataset improved its performance on the BioASQ dataset from the BioASQ Challenge.",
        "We unified the distributions of context length to mitigate the discrepancy between NLI and biomedical QA.",
        "The order of sequential transfer learning is important when fine-tuning BioBERT.",
        "When converting the format of the BioASQ dataset to the SQuAD format, we measured the unanswerable rate of the extractive QA setting where an answer does not exactly match the human annotated corpus.\"']"
    ],
    "8334": [
        "'Our MTST approach is model-agnostic and outperforms the baseline models on the outdoor VLN task.'",
        "'We believe our study provides a possible solution to mitigate the data scarcity issue in the outdoor VLN task.'",
        "'Experimental results show that our MTST approach outperforms the baseline models on the outdoor VLN task.'",
        "'Our learning framework allows us to utilize out-of-domain navigation samples in outdoor environments and enrich the original navigation reasoning training process.'\"]"
    ],
    "8340": [
        "The beam problem in neural machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "We demonstrated the effectiveness of interfacing a neural language model with an interpretable symbolically bound memory by modifying only the non-parametric memories and without any additional training.",
        "Our method allows for modifying facts, such that they contradict the initial pre-training text, and our model is still largely able to answer these questions correctly.",
        "The model is exposed to full knowledge in the pretraining data and KB in the Full setting, while in the Filter setting, the models have access to no direct knowledge about question answer entity pairs from either the pretraining corpus or KB. In the Inject Facts setting, new facts are injected into the models memory allowing it to recover most of the drop from the Full setting.']"
    ],
    "8341": [
        "We have defined a new task referred to as fact-based text editing and made two contributions to research on the problem.",
        "We have proposed a data construction method for fact-based text editing and created two datasets.",
        "We have proposed a model for fact-based text editing, named FACTEDITOR, which performs the task by generating a sequence of actions.",
        "Experimental results show that the proposed model FACTEDI-TOR performs better and faster than the baselines, including an encoder-decoder model."
    ],
    "8342": [
        "The use of a projection-based method can effectively attenuate biases in contextualized embeddings without loss of entailment accuracy.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods can improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "The use of human-generated questions and document-level CQA can better match real-world usage.",
        "The potential applications of improving retrieval of information from charts include automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments.']"
    ],
    "8343": [
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We release our MultiATIS++ corpus to facilitate future research on cross-lingual NLU to bridge the gap between cross-lingual transfer and supervised methods.",
        "We highlight the educational aspects, e.g. introducing a wider audience to AI concepts and methodologies during our annothathons.",
        "The scarcity of non-English data motivates us to gather native QA samples for the French language using a participatory approach.\"']"
    ],
    "8345": [
        "We study a simple approach to open domain question answering that relies on retrieving support passages before processing them with a generative model.",
        "Our approach is competitive with existing methods, and it scales well with the number of retrieved passages.",
        "In future work, we plan to make this model more efficient, in particular when scaling to large number of support passages.",
        "We also plan to integrate the retrieval in our model, and to learn the whole system end-to-end.\"']"
    ],
    "8347": [
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\')",
        "our approach achieves new state-of-the-art on FewRel 2.0 dataset",
        "the inclusion of the other features (W , Sp, T , P , D, E) are not useful for the OED task once the contextual embeddings are included",
        "in an OED task trained on a small labeled datasets, the use of pre-trained features is crucial",
        "the choice of features, and in particular the use of transfer learning, is more important than having a large or complex model\"']"
    ],
    "8349": [
        "We trained several baseline BERT-like models using translated data, but most importantly we evaluated a cross-lingual transfer model trained on English and then evaluated directly on Czech.",
        "The performance of this model is exceptionally good, despite the fact that no Czech training data nor Czech translation system was needed to train it.",
        "We demonstrate that our cross-lingual transfer model achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Our model outperforms the simple projection baseline using fast-align on most languages.",
        "We release our MultiATIS++ corpus to facilitate future research on cross-lingual NLU to bridge the gap between cross-lingual transfer and supervised methods.\"']"
    ],
    "8351": [
        "LaBSE achieves state-of-the-art performance on various bi-text retrieval/mining tasks compare to the previous state-of-the-art.",
        "The model performs strongly even on those languages where LaBSE doesn\\'t have any explicit training data, likely due to language similarity and the massively multilingual nature of the model.",
        "Additive margin softmax is a key factor for training the model.",
        "Parallel data quality matters, but the effect of increased amounts of parallel data diminishes when a pre-trained language model is used.",
        "The pretrained model is released at https://tfhub.dev/google/LaBSE.\"']"
    ],
    "8353": [
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\')",
        "Our approach outperforms benchmark models across different datasets.",
        "By leveraging the surprise scores from different language models, the satirical news was differentiated from true news articles effectively.",
        "This method is not only free from extracting numerous linguistic features as previous works did, but also it does not require any sophisticated neural network structures or advanced embeddings.",
        "More importantly, this proposed method proves the value of the selected statistical features from a language model output, and shows the effectiveness of these features in depicting the characteristics of the corresponding document category.\"']"
    ],
    "8354": [
        "The proposed system architecture combining preprocessing, model framework, and ensemble models achieves comparable performance while significantly improving efficiency.",
        "Ensemble models with power weighted sum outperform any single model with same parameters.",
        "There is an imbalance between categories in the EmotionGIF task, but the present work does not address this issue.",
        "Replacing multi-label classification with ranking classification may improve performance due to the dependency between categories.",
        "The proposed method can significantly improve efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.']"
    ],
    "8358": [
        "The proposed method transfers binary word attributes using reflection-based mappings and keeps non-attribute words unchanged, without attribute knowledge in inference time.",
        "The experimental results showed that the proposed method outperforms analogy-based and MLP baselines in transfer accuracy for attribute words and stability for non-attribute words.",
        "The proposed method can transfer binary word attributes using reflection-based mappings without requiring explicit knowledge of whether the input word has the attribute or not.",
        "The method keeps non-attribute words unchanged, without attribute knowledge in inference time.",
        "The proposed method outperforms analogy-based and MLP baselines in transfer accuracy for attribute words and stability for non-attribute words.\"']"
    ],
    "8360": [
        "The proposed spoken language representation learning framework learns contextualized representations of lattices, as shown by the experiments that provide high-quality representations of lattices and yield consistent improvement on SLU tasks.",
        "The lattice language modeling objective and two-stage pre-training method efficiently train a neural lattice language model to provide downstream tasks with contextualized lattice representations.",
        "The proposed framework is capable of providing high-quality representations of lattices, as evidenced by the improvement on SLU tasks."
    ],
    "8361": [
        "Our proposed approach effectively reduces the gap between the source language space and the target language space, leading to significant improvement of translation quality.",
        "Experiments show that, on both close language pairs and distant language pairs, our proposed approach leads to significant improvement of translation quality over the MT approaches that do not use the dictionary and the approaches that use the dictionary to supervise the cross-lingual word embedding transformation.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, which improves over previous work on joint POS tagging and dependency parsing.",
        "Injecting the bilingual dictionary into MT via anchored training can drive both language spaces closer so that the translation becomes easier.\"']"
    ],
    "8363": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Tagging ambiguity is mostly local while dependency parsing heavily depends on POS tags to predict syntactic structures.",
        "Enabling \"tag \u2192 parse\" only also improves the tagging accuracy itself.",
        "The proposed model demonstrated its capabilities to capture tones and expressions of the spoken words.",
        "This is the first work that tries to model variable-length spoken words using convolutional autoencoders.']"
    ],
    "8365": [
        "The proposed decoder, DAM, improves the generation performance of existing models and achieves new state-of-the-art results on a popular benchmark dataset.",
        "The novel decoder adopts a compositive decoding mode to model information from both response-level and word-level, discouraging repetition in the generated responses.",
        "DAM is a universal decoding architecture that can be incorporated with existing visual dialogue encoders to improve their performance.",
        "The extensive experiments of combining DAM with LF, MN, and DualVD encoders verify that our proposed DAM can effectively improve the generation performance of existing models.']"
    ],
    "8366": [
        "The proposed CoDiR model outperforms its baselines in a classification task.",
        "The CoDiR representations contain rich contextual information and have a high rank, enabling them to classify unseen labels with a simple logistic regression.",
        "The CoDiR representations are continuous and structured, allowing for a semantic interpretation of the content.",
        "The CoDiR method can decompose, modify, and recompose the representations to reflect modified information while conserving existing information.",
        "The CoDiR approach opens up new possibilities for deep learning applications, similar to how structured matrices played a central role in language processing approaches in the past.",
        "The method can be applied to other tasks and modalities with alternative building blocks for the environments.",
        "The use of Wasserstein-based distance is one possible option, but other distance or similarity metrics could be examined in future work.']"
    ],
    "8370": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-theart baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "It is the largest place-related dataset available.",
        "There still remains lots of challenges in place recognition."
    ],
    "8379": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "We have described the rationale and design principles for using coherence in mixed-initiative referential communication dialogues.",
        "We showed that we could engineer significant coverage of natural human-human dialogue utterances and strategies, drawing systematically on a range of machine learning methods.",
        "The system's representations and architecture promise better coverage of anaphorically-mediated real-world reference and dialogues that mix agreement, partial agreement, and explicit disagreement.",
        "Some gaps issues remain to offer a complete and convincing demonstration of our framework.",
        "We need better performance evaluations, along with closer links between the reinforcement learning models and the system's interaction with people.",
        "We need to substantiate the capacity of the architecture to support mixed initiative by learning strategies where the system acts as the director, not just as the matcher.",
        "We need to explore more systematic processing of discourse in context, including a robust and general treatment of anaphora and attachment.",
        "We need to start explicitly drawing on general methods for parsing and DRS construction.\"]"
    ],
    "8380": [
        "incorporating conformal predictors as a rejection model helps filter unreliable reviews better than a baseline approach.",
        "Our paper demonstrates a novel and effective application of conformal predictors to a retrieval task.",
        "We test 3 state-of-the-art PQA models, moqa, fltr and bertqa, and found that incorporating conformal predictors as the rejection model helps filter unreliable reviews better than a baseline approach.",
        "Our approach demonstrates a novel and effective application of conformal predictors to a retrieval task.",
        "We propose incorporating conformal predictors as a rejection model to a PQA model to reject unreliable reviews.\"']"
    ],
    "8383": [],
    "8386": [
        "The proposed model outperforms existing HRED models and their attention variants.",
        "The self-attention mechanism is effective in capturing long-distance dependency relations.",
        "The relevant contexts detected by the model are significantly coherent with human judgments.",
        "The proposed ReCoSa model can improve the quality of multi-turn dialogue generation.",
        "Introducing topical information or considering detailed content information can further improve the quality of generated responses.",
        "The created corpus of Twitter posts related to eating is useful for various research areas, including linguistic, sociological, behavioral, and other research.",
        "The data can be used to inspire related future research.']"
    ],
    "8387": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our method is related to multi-task learning, transfer learning, and curriculum transfer learning.",
        "Our method can be regarded as a special kind of transfer learning where the auxiliary tasks are temporally correlated with the main task.",
        "Our learning process is dynamically controlled by a scheduler rather than some pre-defined rules.",
        "Our method is also related to Different Data Selection (DDS).",
        "Training with temporally correlated auxiliary tasks helps the main task for two reasons: (1) shared representation learning and (2) noisy augmentation of the training data.']"
    ],
    "8389": [
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension that enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Our approach uses extensive experiments on three public datasets of machine reading comprehension: CoQA, QuAC, and TriviaQA.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We believe that other advanced pre-training methods, such as XLNET, and ELECTRA, may hold the key to better performance on this task.']"
    ],
    "8391": [
        "The proposed method, Grid-wise Decomposable Hyper Projections (HYPERGRID), achieves competitive results to the state-of-the-art model that is specially and individually fine-tuned on each and every tasks.",
        "The efficient multi-tasking method using HYPERGRID results in 16 times parameter savings compared to the state-of-the-art model.",
        "All GLUE and SuperGLUE tasks are learned and fit within the same set of model parameters, leading to efficient multi-tasking.",
        "The proposed method achieves competitive results on GLUE/SuperGLUE with 16 times parameter savings.",
        "The method is able to learn and fit all GLUE and SuperGLUE tasks within the same set of model parameters, leading to efficient multi-tasking.']"
    ],
    "8394": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Additional text in the chart or human annotations would likely cause the dynamic encoding method used by PReFIL to fail.",
        "Next generation datasets should contain charts extracted from real-world documents.",
        "Human-generated questions are not captured well by current CQA systems.",
        "Document-level CQA is necessary for understanding charts in documents.\"']"
    ],
    "8395": [
        "Current VQA models rely heavily on the language priors that exists in the train set, without considering the image.",
        "We propose VGQE, a novel question-encoder that utilizes both the modalities equally and generates visually-grounded question representations.",
        "The VGQE is model agnostic and can easily incorporate into existing VQA models without the need for additional manually annotated data and training.",
        "We did extensive experiments on the bias sensitive VQA-CPv2 dataset and achieved a new state-of-the-art.",
        "Unlike existing bias reduction techniques, VGQE does not sacrifice the original model\\'s performance on the standard VQAv2 benchmark.\"']"
    ],
    "8396": [
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\')",
        "our approach using depth-wise LSTM can bring about significant BLEU improvements in both WMT 14 English-German and English-French tasks over the standard Transformer with residual connections",
        "our depth-wise LSTM approach also has the ability to ensure deep Transformers with up to 24 layers",
        "the 12-layer Transformer using depth-wise LSTM already performs comparably to the 24-layer Transformer with residual connections",
        "our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages",
        "the use of depth-wise LSTM can alleviate some drawbacks of residual connections while ensuring the convergence",
        "our deep Transformer experiment demonstrates that our approach using depth-wise LSTM can bring about significant BLEU improvements in both WMT 14 English-German and English-French tasks over the standard Transformer with residual connections\"']"
    ],
    "8397": [],
    "8400": [
        "The proposed metric CIDErBtw can evaluate distinctiveness of image captions quickly and easily.",
        "Human annotations for each image vary in distinctiveness based on CIDErBtw.",
        "A novel training strategy that assigns weights to human ground-truth annotations based on their distinctiveness improves the distinctiveness of generated captions.",
        "Using CIDErBtw directly as part of the reward in RL improves the performance of the model.",
        "The proposed method is widely applicable to many captioning models and achieves state-of-the-art performance on CIDEr, CIDErBtw, and retrieval metrics (R@K).']"
    ],
    "8403": [
        "We have presented a voting mechanism for system combination in machine translation.",
        "Our approach combines the advantages of statistical and neural methods by taking the relations between hypotheses into account and training models in an end-to-end manner.",
        "The voting mechanism allows words in hypotheses to vote on words that should be included in the output.",
        "Experiments show our approach achieves significant improvements over state-of-the-art baselines on Chinese-English and English-German translation tasks."
    ],
    "8409": [
        "The proposed approach, UniTrans, achieves new state-of-the-art performance for all target languages on benchmark datasets.",
        "The use of enhanced knowledge distillation with supervision from both hard and soft labels improves the performance of UniTrans.",
        "The voting scheme to generate pseudo hard labels for part of words in the unlabeled target-language data enhances the performance of UniTrans.",
        "The extension of UniTrans with teacher ensembling leads to further performance gains.']"
    ],
    "8410": [
        "We presented HPM as a computationally efficient hard negative mining strategy to fine-tune a speaker embedding extractor towards out-of-domain Farsi data.",
        "A correct configuration of s-normalization has proved to be crucial to handle the cross-lingual trials presented in the SdSV Challenge 2020.",
        "A fusion of five systems based on our ECAPA-TDNN architecture in conjunction with the proposed techniques resulted in a final top-scoring submission on Task 2 of the SdSVC with an EER of 1.45% and a MinDCF of 0.065."
    ],
    "8411": [
        "Sequence-to-sequence based neural machine translation models always suffer from the under-and over-translation problem.",
        "Our proposed method provides a more direct and contextual-rich supervision signal for the translated and un-translated words.",
        "Our method outperforms the previous adequacy-based methods and achieves significant improvement in mitigating over-and under-translation problem."
    ],
    "8414": [
        "Standard summarization algorithms often return summaries that are dialect biased.",
        "The proposed framework can improve the diversity of generated summaries by using a small set of dialect diverse posts.",
        "The framework is independent of dialect classification tools.",
        "Using standard summarization algorithms as blackbox, we seek to exploit their utility.",
        "By using sets of diverse examples, we ensure that the framework is independent of dialect classification tools.']"
    ],
    "8417": [
        "Our proposed approach (SENT-DEBIAS) accurately captures the bias subspace of sentence representations by using a diverse set of templates from naturally occurring text corpora.",
        "We can remove biases that occur in BERT and ELMo while preserving performance on downstream tasks.",
        "Using a large number of diverse sentence templates is important when estimating bias subspaces.",
        "Our approach allows for the removal of social biases from sentence representations, leading to fairer NLP."
    ],
    "8418": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The state-of-the-art machine readers still fall far behind human performance.",
        "LogiQA can serve as a benchmark for re-examining the long pursued research of logical AI in the deep learning NLP era."
    ],
    "8419": [
        "Our framework significantly outperforms other proposed methods, achieving the state-of-the-art result on all five datasets across different domains.",
        "We investigate an automatic distant annotator to build the labeled target domain dataset, effectively addressing the OOV issue.",
        "An adversarial training procedure is designed to capture information from both the source and target domains.",
        "Our method achieves the state-of-the-art result on all five datasets across different domains.",
        "The proposed framework significantly outperforms other methods in cross-domain CWS tasks.\"']"
    ],
    "8422": [
        "The proposed model, HIN-SR, outperforms other models in document-level sentiment analysis tasks, demonstrating its effectiveness in tackling long documents with vague semantic links and abundant sentiments.",
        "The use of a hierarchical interaction network (HIN) to learn a subject-oriented document representation for sentiment classification is effective in alleviating the negative impact of noisy data.",
        "The incorporation of a sentiment-based rethinking mechanism (SR) refines the weights of document features to learn a more sentiment-aware document representation, leading to improved performance in document-level sentiment analysis tasks.",
        "The proposed model is significant for document-level sentiment analysis and related applications.",
        "Experimental results on three widely public datasets have demonstrated the effectiveness of the proposed model in tackling long documents with vague semantic links and abundant sentiments.']"
    ],
    "8423": [
        "The proposed model, TGLS, significantly outperforms existing HRED models and their attention variants in unsupervised text generation tasks.",
        "The use of simulated annealing search in TGLS provides high-quality examples for the conditional text generator to learn from, leading to improved performance.",
        "The alternation of search and learning in TGLS can boost the performance of the model on two unsupervised text generation tasks, paraphrase generation and text formalization.",
        "The proposed model is computationally more efficient compared to search-based generation methods.",
        "The learning-from-search framework has the potential to be applied to other sequential prediction tasks in NLP.",
        "The use of a better combination of search and learning may lead to further improvements in the performance of TGLS.",
        "The model can be fine-tuned using hyperparameters for language models, with a maximum input length of 35 and optimization using Adam with specific beta values.']"
    ],
    "8426": [
        "We have proposed a novel graph-based multi-modal fusion encoder for NMT.",
        "Experiment results and analysis on the Multi30K dataset demonstrate the effectiveness of our model.",
        "In the future, we plan to incorporate attributes of visual objects and dependency trees to enrich the multi-modal graphs.",
        "Besides, how to introduce scene graphs into multi-modal NMT is a worthy problem to explore.",
        "We will apply our model into other multi-modal tasks such as multimodal sentiment analysis.\"']"
    ],
    "8428": [
        "We proposed a novel task-level curriculum learning method to improve the accuracy of non-autoregressive neural machine translation.",
        "Experiments on several benchmark translation datasets demonstrate the effectiveness of our method for NAT.",
        "In the future, we will extend the task-level curriculum learning method to other sequence generation tasks such as non-autoregressive speech synthesis, automatic speech recognition and image captioning.",
        "We expect task-level curriculum learning could become a general training paradigm for a broader range of tasks."
    ],
    "8429": [
        "The proposed method, SummPip, is a novel unsupervised method for multi-document summarization.",
        "The main novelties of SummPip are the combinations of sentence graphs and sentence compression, which have not been leveraged before in the literature of unsupervised multi-document summarization.",
        "Experiments on Multi-News and DUC-2004 show that our method is effective and comparable to strong supervised neural approaches.",
        "Human evaluation indicates the great potential of our approach in producing high-quality summaries.",
        "In the future, we plan to add a ranking mechanism to the sentence graph construction stage and apply autoencoders to get more abstractive summaries during the compression step.']"
    ],
    "8430": [
        "We developed personalized online language learning (POLL) as a new setting for continual learning.",
        "Our experiments indicate that there is significant room for progress in continual learning with real web-scale data.",
        "We benchmarked continual learning algorithms on this data and contributed an effective algorithm for continual gradient descent (CONGRAD).",
        "There is significant room for progress in continual learning with real web-scale data.",
        "We collected a massive web-scale dataset that comprises 100 million tweets posted over six years (FIREHOSE).\"']"
    ],
    "8431": [],
    "8432": [
        "The proposed spatial representation language can compactly represent the spatial aspect of complex sentences that were challenging for existing schemes.",
        "The integration of the spatial representation scheme with AMR achieves a richer meaning representation language.",
        "The annotated sentences from the Minecraft corpus and trained automatic parser can convert sentences into the extended AMR.",
        "The spatial configuration schema provides a succinct way to represent the spatial aspects of the full AMR annotation, as demonstrated through examples.",
        "The adoption of the spatial representation scheme and the extended AMR as general-purpose tools by the research community can be beneficial particularly in spatially involved domains.']"
    ],
    "8433": [
        "We propose to use a length level embedding for length-controllable image captioning.",
        "By simply adding our length level embedding on the word embeddings of input tokens, we endow existing image captioning methods with the ability to control the length of their predictions.",
        "To improve the decoding efficiency of long captions, we further propose a non-autoregressive image captioning model, LaBERT, that generates image captions in a length-irrelevant complexity.",
        "The experiments demonstrate the effectiveness of the proposed method.\"']"
    ],
    "8434": [
        "framing can serve to shape the public discourse on an issue.",
        "politically active supporters of the Democratic and Republican parties distinctly frame the COVID-19 pandemic as a political issue, rather than solely a public health issue.",
        "we use unsupervised stance detection to effectively identify the supporters of each party and analyze their most distinctive hashtags, retweeted accounts, URLs, and rhetorical devices.",
        "supporters of both parties crafted two primary frames, namely: a blame frame and a support frame.",
        "the blame frame is where GOP supporters blame China and conspiracies, while DNC supporters blame Trump and the GOP.",
        "the support frame is where each supports the candidate(s) of their respective parties.",
        "these frames dominate the discourse of both groups, thus revealing that support and blame prominently influence the political sentiments attached to COVID-19.\"']"
    ],
    "8435": [
        "The proposed language models (BertPT and AlbertPT) achieve good results in natural language understanding tasks.",
        "The pre-training procedure and resources used for the models are described.",
        "The models outperform existing baselines for each task.",
        "A multilingual model based on Albert would be a useful resource.",
        "The authors plan to pretrain a multilingual model using languages with good availability of training data similar to Portuguese (e.g., Spanish, French, Italian).']"
    ],
    "8436": [
        "The proposed approach improves upon the work of DSTC7.",
        "Increasing the dataset by a factor of 2 using shuffling dialog history, combined with teacher forcing, max pooling, and GRU encoders, produces the best results within the scope of the tests.",
        "Teacher forcing and shuffling dialog histories result in a substantial improvement over the baseline model and the own tests which do not use these methods.",
        "The order of dialog histories is less important than the raw information present within for QA problems.",
        "Individual tokens within a sentence have a very important temporal dependence, which is critical for generating accurate natural language.",
        "The incorporation of teacher forcing suggests that individual tokens within a sentence do have a very important temporal dependence.",
        "Further investigation into the fusion of visual and textual data may produce more informative responses with specific details extracted from videos.']"
    ],
    "8438": [
        "We introduced CoVoST 2, the largest speech-to-text translation corpus to date for language coverage and total volume.",
        "CoVoST 2 is free to use under CC0 license and enables the research community to develop methods including, but not limited to, massive multilingual modeling, ST modeling for low resource languages, self-supervision for multilingual ST, semi-supervised modeling for multilingual ST.",
        "We provided extensive monolingual, bilingual and multi-lingual baselines for ASR, MT and ST.",
        "CoVoST 2 is the largest speech-to-text translation corpus to date for language coverage and total volume.",
        "Our corpus is free to use under CC0 license, enabling the research community to develop methods for multilingual ST, self-supervision, and semi-supervised modeling.\"']"
    ],
    "8441": [
        "Our proposed method outperforms previous methods including scheduled sampling in neural machine translation tasks.",
        "The novel error correction mechanism effectively alleviates the problem of error propagation in sequence generation.",
        "Our method has the potential to be applied on other sequence generation tasks such as text summarization, unsupervised neural machine translation.",
        "Incorporating our error correction mechanism into other advanced structures may further improve the performance of neural machine translation.']"
    ],
    "8443": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Our main approach used Transformer-based models like BERT, RoBERTa, XLNet, and their ensembles.",
        "Future work includes making an application that automates the process of poster or advertisement making.\"']"
    ],
    "8444": [
        "Our proposed models for detecting propaganda fragments in articles and classifying the propaganda technique used in a given propaganda fragment show promise.",
        "State-of-the-art language models can be useful for both subtasks of detecting and classifying propaganda fragments.",
        "BIOE tagging scheme can help detect spans better.",
        "Including context surrounding a propaganda fragment in the classification task does not significantly improve predictions.",
        "The distribution of span lengths is different for different categories, and modeling this fact by doing hierarchical classification is something that can be explored in the future.']"
    ],
    "8445": [
        "The authors used Transformer-based approaches for all sub-tasks and a soft label LSTM-based approach for sub-task C.",
        "The authors plan to use data augmentation techniques to improve the F1 score on the gold labels for each sub-task.",
        "The authors achieved competitive performance compared to previous systems using less than 1% of the training data.",
        "The authors significantly outperformed previous methods by reducing the error by 21% on English Switchboard.']"
    ],
    "8446": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The ensemble model with higher dropout shows significant improvements on accuracy, up to 8.6%, on the dev set than baseline models.",
        "However, the ensemble model performs worse than the baseline model B-FT and original ensemble model E on the test set, which has a 92.153% accuracy.",
        "Model overfitting and data imbalance may be caused by the problems that need to be taken into consideration in future experiments.']"
    ],
    "8447": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Developing annotated datasets from multiple sources based on existing datasets and the proposed causality tagging scheme can improve the performance of SCITE.",
        "Combining SCITE with distant supervision and reinforcement learning can achieve better performance without requiring a large amount of high-quality annotated data.",
        "The proposed constituent hierarchy predictor based on recurrent neural networks can capture global sentential information and outperform state-of-the-art baseline parsers.",
        "The proposed architecture for rumor detection is expandable to accommodate several feature classes, including those yet to be discovered.",
        "Using attention layers at two levels (intra-feature level and inter-feature-class level) can provide a granularity of explanations for the model's decisions.",
        "The intra-feature level attention weights capture the relative importance of individual features within each category, while the inter-feature attention weights reveal the relative importance of the features across categories.",
        "Providing average case analysis of the importance of features can help a model developer trim the model according to needs.",
        "The proposed architectures perform the best among eleven benchmark models while providing meaningful interpretations of the decisions.\"]"
    ],
    "8448": [
        "The proposed Convolutional Neural Network (CNN) and readability scores achieve better performance in book success prediction compared to strong baseline methods, without using any feature engineering.",
        "The success prediction accuracy using a portion of the book is better than it is when using the whole book.",
        "Book embeddings based on genre visualization leads to better results in book success prediction compared to other embeddings.",
        "More readability does not always correspond to more success, as shown by the inconsistent performance of different readability indices such as Coleman-Liau Index (CLI) and Flesch Kincaid Grade (FKG).",
        "It is better for a book to have high words-per-sentences ratio and low sentences-per-words ratio, as suggested by CLI and ARI.",
        "Employing more pretrained language models for sentence embedding, such as BERT and GPT2, is worthy of exploring and would likely give better results in the future.",
        "Investigating the connection between readability and success with a more detailed empirical analysis is a potential future work.']"
    ],
    "8449": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach using static Word2vec and dynamic BERT embeddings under the top classification layers consisting of simple classifiers is effective for hypernym detection in Financial texts.",
        "BERT embeddings are equally accurate for terms containing the one hypernym within them, but they lag behind for the other subset of terms.",
        "With higher computational resources, BERT could be pre-trained on the whole corpus, and the performance may improve.",
        "Unsupervised metrics are efficient and independent of data size, but they lag behind supervised classifiers for terms exclusive of class label.",
        "The task of hypernym detection in Financial texts advances the NLP community towards the broad area of Financial Document Processing and encourages collaboration between the fields of Finance and NLP.']"
    ],
    "8450": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL exceeded the human baseline for FigureQA, but results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Developing annotated datasets from multiple sources based on existing datasets and the proposed causality tagging scheme.",
        "Combining the method with distant supervision and reinforcement learning to achieve better performance without needing a high-quality annotated corpus for causality extraction.']"
    ],
    "8455": [
        "The problem of generating succinct, grammatically correct voice titles for products in a large e-commerce catalog with limited labels is studied.",
        "BERT summarization can generate good titles through ROUGE metrics and human evaluation, even when there is extremely limited data.",
        "Generating personalized titles for different user segments based on rich user metadata and incorporating web data with additional product attributes that may be product dependent are some directions to extend this work.",
        "The use of limited labels does not limit the performance of BERT summarization in generating good titles, as demonstrated by the high ROUGE scores and human evaluation.']"
    ],
    "8459": [
        "Our models can effectively preserve salient source relations in summaries.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "FiSSA is a good alternative to a standard model, especially when the amount of labeled training data is limited.",
        "Fine-tuning a pre-trained language model can achieve a weighted F1-score of 0.537 on development data and 0.739 on test data.",
        "The effect of tokenization and language-specific performance of each model can be evaluated to better understand the overall results.\"']"
    ],
    "8463": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Employing machine learning entity detection models to extract medical entities and relationships.",
        "Using CKG relations to obtain paper embeddings capturing topological isomorphic and semantic information for the application of similar paper retrieval.",
        "Future work may include further enhancements to CKG information retrieval capabilities such as expanding biomedical entity extraction using biomedical concept annotators like PubTator 2, re-training RGCN models with additional entity and relation attributes, and incorporating additional KGs into the CKG e.g. COVID-19 drug repurposing graphs.\"']"
    ],
    "8466": [
        "Combining BERT with CNN yields better results than using BERT on its own.",
        "The proposed model with minimum text pre-processing was able to achieve very good results on average.",
        "Our team was ranked among the highest four participating teams for all languages in the scope of the OffensEval2020.",
        "The pre-training process of ArabicBERT was explained.",
        "The proposed model with minimum text pre-processing achieved very good results on average.\"']"
    ],
    "8468": [
        "We found large discrepancies when it comes to select a particular model highlighting the limits of these intrinsic quality metrics.",
        "Intrinsic quality metrics only show partial views of the overall reliability of a model.",
        "Variations on intrinsic quality metrics should not be accounted for certain progress on downstream tasks.",
        "More attention should be brought on downstream tasks that have the credit to answer practical problems."
    ],
    "8469": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism, which uses Similarity Entropy Minimization to promote clustering and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.",
        "We utilize pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "BIGBIRD is a sparse attention mechanism that is linear in the number of tokens, and it satisfies a number of theoretical results, such as being a universal approximator of sequence-to-sequence functions and being Turing complete.",
        "Theoretically, we use the power of extra global tokens to preserve the expressive powers of the model.",
        "Empirically, BIGBIRD gives state-of-the-art performance on a number of NLP tasks such as question answering and long document classification.\"']"
    ],
    "8472": [
        "The proposed modular system can POS tag English-Bengali code-mixed sentences.",
        "The system uses sub-modules that can be trained for any given language, making it versatile and applicable to a variety of code-mixed data involving different language pairs.",
        "The system can be enhanced further by training the sub-modules using more annotated data, leading to improved accuracy and robustness.",
        "The current system has the potential to be developed into an end-to-end system that minimizes the propagation of errors from one submodule to another.",
        "The proposed approach can solve the problem of tagging untrained tokens with 'UN' tags and correctly distinguish between NOUN, ADJ, VERB, and PRON.",
        "The transliteration module can also be improved by developing an end-to-end system that minimizes errors.\"]"
    ],
    "8473": [
        "The proposed object-and-action aware model for robust visual and language navigation demonstrates superiority compared to several state-of-the-art navigating methods, especially on unseen test scenarios.",
        "The model uses separate highlighting of object and action information in instructions, which are then matched to visual perceptions and orientation embeddings, respectively.",
        "To encourage the robot agent to stay on the path, a path loss based on the distance to nearest ground-truth viewpoint is proposed.",
        "The model demonstrates the ability to predict correct actions even when the OA or AA modules give wrong predictions, such as at step 2 and step 3.",
        "All the OA and AA modules as well as the adaptive combination module are crucial to the final success.']"
    ],
    "8477": [
        "The proposed modular pipeline of cascading classifiers for document stance classification towards claims achieves state-of-the-art performance.",
        "The pipeline model improves the performance of the important-for fake news detection disagree class by 28% (F1 score) without significantly affecting the performance of the agree class.",
        "There is still room for further improvements, mainly due to the lack of semantic understanding of the language used to express agreement or disagreement.",
        "The proposed model can be further optimized by reducing the number of misclassified agree and disagree documents in each stage."
    ],
    "8478": [
        "Our approach outperforms methods specially designed for either grammar formalism alone.",
        "Importantly, our work also adds novel insights for the unsupervised grammar induction literature by probing the role that factorizations and initialization have on model performance.",
        "Different factorizations of the same probability distribution can lead to dramatically different performance and should be viewed as playing an important role in the inductive bias of learning syntax.",
        "Additionally, where others have used pretrained word vectors before, we show that they too contain abstract syntactic information which can bias learning.",
        "Modeling these dependencies would require marginalizing over all possible dependents for each span-head pair.",
        "There are several potential methods to side-step this problem, including the use of sampling in lieu of dynamic programming, using heuristic methods to prune the grammar, and designing acceleration methods on GPU.\"']"
    ],
    "8483": [
        "'Our approach achieves new state-of-the-art on FewRel 2.0 dataset.'",
        "'ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.'",
        "'Morpheme-based models better recognize OOVs that result from morphological composition.'",
        "'Our approach uses a Hybrid architecture that combines NER with morphological decomposition to improve performance.'",
        "'Our experiments show that while NER benefits from morphological decomposition, downstream results are sensitive to segmentation errors.'",
        "'We deliver new state-of-the-art results for Hebrew NER and MD, along with a novel benchmark, to encourage further investigation into the interaction between NER and morphology.'\"]"
    ],
    "8492": [
        "The proposed relational teacher-student learning framework with neural label embedding can resolve the device mismatch issue in acoustic scene classification.",
        "The structural relationship between pairs of classes is learned and encoded into NLE, and then transferred from the source device domain to the target device domain via the relational teacher-student approach.",
        "Our proposed framework can achieve a significant improvement in classification accuracy on the target device data.",
        "The visual analysis provided sheds light on the key characteristics of the proposed neural label embedding concept."
    ],
    "8493": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The beam problem in NMT can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains, but there is still room for improvement by solving label bias in general.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset, and more general globally-normalized models can be trained in a similarly inexpensive way.",
        "TROJAN LM is a new attack that trojans LMs and activates malicious functions in downstream tasks via logical combinations of trigger words.",
        "The current practice of re-using pre-trained LMs raises concerns about the security of NLP tasks.",
        "There are potential countermeasures and technical challenges to mitigate the vulnerabilities of LMs to trojaning attacks.",
        "Further investigation is needed to understand the vulnerabilities of LMs to instance-level attacks, and to study whether other existing mitigation strategies are effective in the context of LMs.']"
    ],
    "8494": [
        "The proposed method achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The model based on gaze features and part-of-speech information achieves accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerge as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Multilingual neural machine translation models can be created from pretrained models such as mBART, and extending these models to double the number of original languages is possible without loss of performance on the original languages.",
        "The proposed multilingual finetuning strategy outperforms all baselines, achieving strong improvements of over 2 BLEU points in the Many-to-one setting.",
        "Average across the Many-to-one and one-to-Many directions, the proposed multilingual finetuning strategy outperforms all baselines.']"
    ],
    "8498": [
        "AVTacotron2 outperforms the modular approach in all aspects.",
        "The additional task of facial control estimation in the end-to-end approach can sometimes lead to a mismatch between the prosody of the synthesized acoustic speech and the original recordings.",
        "AV-Tacotron2 produces synthesized talking faces that are rated almost as natural as the ground truth.",
        "Our on-going work involves the task of estimating head pose to enhance the experience of the talking faces.",
        "We are investigating video-based and audiovisual-based emotion embeddings compared to the audio-only one used here.\"']"
    ],
    "8500": [
        "The new grapheme-based model using metalearning significantly outperforms multiple strong baselines on two tasks: data-stress training and code-switching.",
        "Our model was favored in both voice fluency as well as pronunciation accuracy.",
        "The model's attention module can be improved further for future work.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.\"]"
    ],
    "8501": [
        "We find that even though COVID-19 is a recent event, misinformation related to it has created a set of polarized communities with high echo-chamberness.",
        "Mis-informed communities are observed to be denser than informed communities, which is in line with previous studies such as [MTMC20] .",
        "We find that bots exist in both the informed and misinformed groups, but the percentage of bots in misinformed users is significantly higher suggesting the prevalence of disinformation campaigns.",
        "Our sociolinguistic analysis suggests that both the target communities depict negative emotional tone in their posts, with signals that informed users use many more narratives than misinformed users.",
        "Many misinformed users may be anti-vaxxers.",
        "Our analyses suggest that misinformation communities are much more complex as they are highly organized, and tend to be highly analytical.",
        "Unlike previous suggestions [SOC19] , they may not be responsive to narrative correctives, and hence, a \\'one size fits all\\' generic messaging intervention for debunking misinformation may not be a feasible solution.\"']"
    ],
    "8503": [
        "GloVe, ELMo, and BERT embeddings all encode gender, religion, and nationality biases.",
        "A projection-based method can be used to attenuate biases in word representations.",
        "The method works for static GloVe embeddings.",
        "Debiasing the first non-contextual layer alone can effectively attenuate bias in contextualized embeddings without loss of entailment accuracy.",
        "NLPDove's approach to SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media yielded significant results in the competition for multiple languages.",
        "Large-scale pre-trained language models and various preprocessing schemes and model ensembling are important for offensive language identification.",
        "Data selection with TED increased performance in cross-lingual transferability.",
        "The system has limitations, such as failing to detect the speaker's intention expressed subtly in the context.",
        "Future work in the field may involve addressing these limitations and improving the system's performance.\"]"
    ],
    "8504": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our PAES method is the first to use a neural-network combined with non prompt-specific features in a single-stage approach for cross-prompt AES.",
        "We utilize POS embeddings to achieve syntactic representations, avoiding the issue of overfitting to the training data, as with the prompt-specific approaches.",
        "Our single-stage approach avoids the need to use unlabelled target-prompt essays in the training phase, thus eliminating the issues of insufficient unlabelled target-prompt essays and target-prompt essay quality distribution that exist in TDNN [11].",
        "Our model is robust and is able to achieve state-of-the-art performance on the widely-used ASAP dataset.']"
    ],
    "8506": [
        "The proposed semi-autoregressive generator for multi-turn incomplete utterance restoration is significantly superior to other state-of-the-art methods.",
        "The proposed model takes into account the high efficiency of inference time from sequence labeling and the flexibility of generation from autoregressive modeling.",
        "The proposed model is appropriate for utterance restoration in boosting multi-turn dialogue systems.",
        "The proposed method demonstrates improved performance on two benchmarks compared to other state-of-the-art methods.",
        "The use of sequence labeling and autoregressive modeling in the proposed method provides flexibility and high efficiency in inference time.']"
    ],
    "8507": [
        "Our system for worldwide COVID-19 information aggregation combines crowdsourcing, crawling, machine translation, and a topic classifier, providing reliable, comprehensive, and latest information from around the world.",
        "We proposed an effective approach to annotate large cross-lingual news topic datasets with high inner-annotation agreement, which could benefit the NLP community in enriching solutions for preventing COVID-19.",
        "The contextual BERT-based classification models achieve reasonable performance considering the imbalance of the topic labels.",
        "Our work could attract future research interests to COVID-19-related tasks.']"
    ],
    "8512": [
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The experimental results show that TensorCoder is competent in long-sequence tasks and helps to improve the efficiency of pre-trained language models in resource-limited environments.\"']"
    ],
    "8514": [
        "The proposed model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what has been used here for few-shot learning.",
        "The proposed AdaBERT model adaptively compresses BERT for various downstream tasks, achieving comparable performance with significant efficiency improvements.",
        "The proposed Ad-aBERT model can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "There is a lack of complete datasets for Urdu, making it difficult to properly define word boundaries and tokenize the data.",
        "Existing techniques for detecting compound words and outliers can be limited in their effectiveness.",
        "Ignoring spaces in tokenization of Urdu can help improve efficiency.']"
    ],
    "9": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "The context-theoretic framework provides a space of exploration for models of meaning in language.",
        "The model does not explicitly model syntax, but rather syntactic restrictions are encoded into the vector space and product itself.",
        "The component of meaning that square has in common with shape must be disjoint with the adjectival component of the meaning of square.",
        "Associativity is a very strong requirement to place; indeed Lambek (1961) introduced non-associativity into his calculus precisely to deal with examples that were not satisfactorily dealt with by his associative model (Lambek 1958).']"
    ],
    "10": [
        "The proposed novel constituent hierarchy predictor outperforms the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "The resulting fully-supervised parser leverages right-hand side syntax for local decisions, yet maintains the same model size and speed as the baseline parser.",
        "The causality extraction method proposed in the paper achieves effectiveness in extracting causality in natural language text, but is limited by the insufficiency of high-quality annotated data.",
        "The method uses a self-attentive BiLSTM-CRF-based solution for causality extraction and introduces the multihead self-attention mechanism to learn dependencies between cause and effect.",
        "The authors avoid using external labeled data and have been well-established in the NLP field, using benchmark systems that do not rely on external data.",
        "The best systems proposed have more engineered features relative to the best systems on simpler tasks, indicating that the more complex tasks require more sophisticated semantic understanding.",
        "The multilayer neural network architecture can handle a number of NLP tasks with both speed and accuracy, and relies on large unlabeled datasets to discover internal representations that prove useful for all the tasks of interest.']"
    ],
    "12": [
        "The proposed method is unsupervised and discovers root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of the rules allow for validating the pattern discovery method and root extraction method.",
        "The method is compared to a rule-based language-specific (in this case Arabic) root extractor, with performance not too far from the latter.",
        "Structured prediction algorithms take more effort on how to model the interdependence among the output variables, but less consideration is taken on the feature engineering which is a non-trivial and tedious task for general users.",
        "The proposed MTL struct framework learns both the weight of each template and the structured prediction model in the batch mode, simultaneously.",
        "Learning the weights of these groups is formulated as a Multiple Kernel Learning (MKL) problem.",
        "The MKL problem can be solved using an efficient cutting plane algorithm, and its convergence is presented.",
        "The proposed framework is more efficient and performs much better than OnlineMKL.",
        "The method can be extended for structured prediction with regression outputs in the future.']"
    ],
    "16": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our implementation of the categorical model of meaning combines the formal logical and empirical distributional frameworks into a unified semantic model.",
        "Our approach easily applies to compositions of adjectives, nouns, verbs, and adverbs, and can handle sentences containing combinations of these parts.",
        "Our model learns sentence/phrase meaning compositionally from the vectors of the compartments of the composites, whereas previous work learns matrices in a top-down fashion.",
        "Our approach is sensitive to grammatical structure and can differentiate itself from models with commutative composition operations.",
        "The high level categorical distributional model, uniting empirical data with logical form, can be implemented just like any other concrete model and shows better results in experiments involving higher syntactic complexity.']"
    ],
    "19": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our probabilistic approach outperforms all previously published PbA algorithms.",
        "Accuracy of a PbA algorithm can be improved by collation, using normalized frequencies, or using estimated probabilities as defined by Eqs. (1) and (9).",
        "Replacing the frequencies or estimated probabilities of the segment pronunciations by their cubic root can improve accuracy.",
        "The state of the art accuracy in other methods for text-to-speech conversion is better than in PbA, but these methods are more complicated and require more computational resources.']"
    ],
    "21": [
        "Our model can learn temporal relations from sentences where temporal information is made explicit via temporal markers, and this model can be used in cases where overt temporal markers are absent.",
        "We experimented with a variety of linguistically motivated features and showed that it is possible to extract semantic information from corpora even if they are not semantically annotated in any way.",
        "Our approach achieves an accuracy of 70.7% on the interpretation task and 97.4% on the fusion task, which is a significant improvement over the baseline and compares favourably with human performance on the same tasks.",
        "Previous work on temporal inference has focused on the automatic tagging of temporal expressions or learning the ordering of events from manually annotated data, but our approach can be used to support the \"annotate automatically, correct manually\" methodology used in large-scale annotation projects.",
        "Our model can be easily extended to include contextual features and richer temporal information such as tagged time expressions, and experimenting with models that do not assume main and subordinate clauses are conditionally independent may improve prediction accuracy.']"
    ],
    "40": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "Our algorithms do not need any pruning or threshold picking and give the exact k shortest paths.",
        "The experimental results on real-world input show that Algorithm 2 is highly efficient, adding very little overhead to the shortest distance pre-computation.']"
    ],
    "49": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach outperforms benchmark models across different datasets.",
        "It is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent).",
        "Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary.",
        "The word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts.",
        "Results from machine translation experiments also look very promising.",
        "Our comprehensive test set will help the research community to improve the existing techniques for estimating the word vectors.",
        "High quality word vectors will become an important building block for future NLP applications.\"']"
    ],
    "80": [
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "The strong correlation does not imply causality, and it is beyond the scope of this work to look for actual evidence of Ebonics in the tweets.",
        "People who are engaged in a conversation communicate using a shared context, which may utilize a more specialized lexicon (jargon or even coined words).",
        "The decrease in the frequency of words from 60.32% to 52.8% could mean that the use of jargon increased by about 60.32% - 52.8% = 7.52%.",
        "There is no obvious remaining factor that could bias the temporal analysis of utterance lengths after the shortening was shown to be robust.",
        "The geolocated tweets are relatively few and the tweets (users) were then aggregated by US state.",
        "A survey done by Smith and Brenner [29] showed that among the different races, Blacks significantly use Twitter more than other races.\"']"
    ],
    "81": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model for part-of-speech tagging and dependency parsing improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "The novel end-to-end model for joint slot label alignment and recognition does not require external label projection and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "The techniques introduced in this paper can be used also for training the continuous bag-of-words model.",
        "The choice of the training algorithm and the hyper-parameter selection is a task specific decision, as different problems have different optimal hyperparameter configurations.",
        "The word vectors can be combined using simple vector addition, and combining this approach with representing phrases with a single token gives a powerful yet simple way to represent longer pieces of text while having minimal computational complexity.']"
    ],
    "90": [
        "The choice of approximately one billion words might seem somewhat restrictive.",
        "It has been seen many times in the history of research that significant progress can be achieved when various approaches are measurable, reproducible, and the barrier to entry is low.",
        "The importance of such effort is unquestionable: it has been seen many times in the history of research that significant progress can be achieved when various approaches are measurable, reproducible, and the barrier to entry is low.",
        "The perfor-mance gain is very promising; the perplexity reduction of 35% is large enough to let us hope for significant improvements in various applications.",
        "In the future, we would like to encourage other researchers to participate in our efforts to make language modeling research more transparent.",
        "Ideally the benchmark would also contain ASR or SMT lattices/Nbest lists, such that one can evaluate application specific performance as well."
    ],
    "112": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The absence of differences due to the nature of the persuader (human or machine) can be interpreted as showing that judgments of moral acceptability address more persuasion acts than the actors performing them.",
        "The two latent dimensions underlying moral acceptability, in turn, suggest maximizing the impact of persuasion by targeting subjects who score high on them.",
        "Personalized persuasion could take advantage of studies addressing the dispositional nature (if any) of people\\'s attitude towards moral acceptability by, e.g., addressing the personality traits (if any) underlying it and their relationships to the two latent dimensions we found.\"']"
    ],
    "131": [
        "The proposed RNN Encoder-Decoder can learn the mapping from a sequence of arbitrary length to another sequence, possibly from a different set, of arbitrary length.",
        "The proposed model can capture linguistic regularities in phrase pairs well and propose well-formed target phrases.",
        "The use of the RNN Encoder-Decoder improves the overall translation performance in terms of BLEU scores.",
        "The contribution by the RNN Encoder-Decoder is rather orthogonal to the existing approach of using neural networks in the SMT system, suggesting potential for further improvement.",
        "The proposed architecture has large potential for further improvement and analysis, and can be applied to other applications such as speech transcription.']"
    ],
    "132": [
        "The proposed method achieves state-of-the-art results on miniRCV1 and ODIC datasets.",
        "The sparse coding method without a structured regularizer produces sparser representations.",
        "The average numbers of nonzero entries are 91% and 85% for M = 52 and M = 520, respectively.",
        "The dictionary learning step takes about 30 minutes and the overall learning procedure takes approximately 2 hours for M = 52 and M = 520.",
        "The SG model took about 1.5 hours and 5 hours for M = 52 and M = 520 using a highly optimized implementation from the author\\'s website.",
        "Forest with Algorithm 1 took about 2 hours for M = 520 on a large corpus with 6.8 billion words and vocabulary size of about 400,000.",
        "The larger magnitude of the vectors for more abstract concepts is suggestive of neural imaging studies that have found evidence of more global activation patterns for processing superordinate terms.",
        "Coefficients that differ in sign mostly correspond to leaf nodes, validating our motivation that top-level nodes should focus more on \"general\" contexts and leaf nodes focus on word-specific contexts.",
        "The method outperforms state-of-the-art methods on word similarity ranking, syntactic analogy, sentence completion, and sentiment analysis tasks.']"
    ],
    "149": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The use of a representation extractor with the Clustering Promotion Mechanism and Cosine Annealing Strategy improves the domain adaptation performance.",
        "The experimental results show that our model significantly outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "The choice of compositional operator is more important and task-specific, and the best choice may vary depending on the task.",
        "The use of a neural word embedding outperforms co-occurrence-based alternatives across the board.",
        "The approach lends itself better to composition using general mathematical operators.",
        "The baselines in isolation show that the best choice of vector representation might vary, but all methods give fairly competitive results for small-scale tasks.\"]"
    ],
    "163": [
        "The BilBOWA model achieves state-of-the-art results for English-German cross-lingual document classification while obtaining up to three orders of magnitude speedup compared to previous methods.",
        "The model is able to learn finer-grained cross-lingual relationships than traditional methods like translation matrix or multilingual CCA.",
        "The asynchronous implementation of the model significantly speeds up training without affecting the quality of the learned embeddings.",
        "Parallel subsampling improves the accuracy of the learned features, especially for frequent words.",
        "The use of a sampled L2 bag-of-words cross-lingual loss allows for more accurate feature learning and larger datasets.",
        "The model is computationally efficient and can be trained on large monolingual datasets.",
        "The hybrid model BilBOWA is a better choice than parallel-only techniques like BiCVM or BAEs for learning high-quality general purpose bilingual embeddings.']"
    ],
    "169": [
        "Majority voting performs poorly, and score combination is a better alternative because it takes into account the confidence level of output from different systems.",
        "Random forest combinations perform the best, but they require a supervised combination method.",
        "A small amount of labeled data is sufficient for random forest combinations to be successful.",
        "The size of the labeled data can potentially be reduced by choosing it carefully from the region that is expected to be most errorful.",
        "RULE+LM with random forest is a little better than RULE with random forest, with a gain of about 0.7% on F1-measure when evaluated at the ENTRY level using 10% data for training.",
        "The examination of examples that are marked as being errors in the ground truth but that were not detected by any of the systems suggests that some examples are decided on the basis of features not yet considered by any system.",
        "Automatic systems can detect errors, but they may not mark all errors as errors, and our ground truth may be wrong in some cases.",
        "Examination of false negatives and false positives can reveal useful information for refining how ground truth is generated from DML commands.']"
    ],
    "209": [
        "We contrast the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Normalisation is not necessary when neural models are used in addition to back-off n-gram models.",
        "The MT system learns a smaller weight for neural models and we believe their main use is to correct the inaccuracies of the n-gram models.",
        "When neural language models are used in isolation, we observe that normalisation does matter.",
        "The most effective normalization strategy in terms of translation quality is the class-based decomposition trick.",
        "Tree factored models are not a strong candidate for translation since they are outperformed by unnormalised models in every aspect.",
        "Noise contrastive estimation for class factored models can be used to train standard or class factored models in a little over 1 day.",
        "Diagonal context matrices are useful for speeding up training and do not require additional memory.",
        "Our work is important because it reviews the most important scaling techniques used in neural language modelling for MT.\"']"
    ],
    "214": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method outperforms previous methods by reducing the error by 21% on English Switchboard.",
        "We extended zero-shot image labeling beyond objects, showing that it is possible to tag images with attribute-denoting adjectives that were not seen during training.",
        "Performance was comparable to that of per-attribute supervised classifiers for some attributes.",
        "Attributes are implicitly induced when learning to map visual vectors of objects to their linguistic realizations as nouns.",
        "Improvements in both attribute and noun retrieval are attained by treating images as visual phrases.",
        "The resulting model outperformed a set of strong rivals.",
        "The zero-shot decompositional approach in the adjective-noun phrase labeling alone might still be low for practical applications, but the model can produce attribute-based representations that significantly improve performance in a supervised object recognition task.']"
    ],
    "225": [
        "The proposed probabilistic model is capable of generalization to any type of parts and can be learned using the framework that solves the three inference problems.",
        "For projective parsing, efficient algorithms exist to solve the three problems for certain factorizations with special structures.",
        "Non-projective parsing with high-order factorizations is known to be NPhard in computation.",
        "Our models capture multi-root trees, whose root symbols have one or more children.",
        "The parsers trained by online and off-line learning methods have distinctive error distributions despite having very similar parsing performance of UAS overall.",
        "By exploiting parallel computation techniques, our parsing models can be trained much faster than those parsers using online training methods.']"
    ],
    "241": [
        "The convolutional model outperforms the recursive model in translating long sentences.",
        "The fixed depth of the convolutional architecture limits its composition, but this limitation can be compensated with a network that takes a \"global\" synthesis on the learned sentence representation.",
        "The difficult curriculum does not contribute most to the improvement in translation performance.",
        "The \"negative\" examples may share the same semantic meaning with the positive one, leading to a wrong guide in supervised training.",
        "Our approach significantly improves the translation performance and obtains an improvement of 1.0 BLEU scores on the overall test data.",
        "Integrating deep architecture into context-dependent translation selection is a promising way to improve machine translation.']"
    ],
    "269": [
        "HDDCRP outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The traditional agglomerative clustering model (AGGLOMERATIVE) suffers from error propagation, while HDP-LEX often mistakes in grouping mentions together based on word co-occurrence statistics but not the apparent similarity features in the mentions.",
        "HDDCRP avoids errors by performing probabilistic modeling of clustering and making use of rich linguistic features trained on available annotated data.",
        "HDDCRP correctly groups the event mention \"unveiled\" in \"Apple\\'s Phil Schiller unveiled a revamped MacBook Pro today\" together with the event mention \"announced\" in \"this notebook isn\\'t the only laptop Apple announced for the MacBook Pro lineup today\", while both HDP-LEX and AGGLOMERATIVE models fail to make such connection.",
        "Many mistakes made by HDDCRP are due to errors in event extraction and pairwise linkage prediction, including false positive and false negative event mentions and event arguments, boundary errors for the extracted mentions, and argument association errors.",
        "The event extraction errors include false positive and false negative event mentions and event arguments, boundary errors for the extracted mentions, and argument association errors.",
        "Pairwise linking errors often come from the lack of semantic and world knowledge, especially for time and location arguments which are less likely to be repeatedly mentioned and in many cases require external knowledge to resolve their meanings.']"
    ],
    "291": [
        "The mQA model is able to provide answers to freestyle questions for images, achieving human-level performance with 64.7% accuracy.",
        "The Freestyle Multilingual Image Question Answering (FM-IQA) dataset contains over 310,000 question-answer pairs and can be used for other tasks such as visual machine translation.",
        "Modifying the LSTM in the first component to a multimodal LSTM allows the model to generate free-style questions and provide answers based on the content of an image.",
        "The model sometimes makes mistakes when the commonsense reasoning through background scenes is incorrect, such as in cases where the targeting object is too small or looks very similar to other objects.",
        "The model outputs a OOV sign when it meets a word it has not seen before, indicating that it needs more visual and linguistic information to answer certain questions.",
        "Incorporating more visual and linguistic information, such as object detection or attention models, may help address these issues in future work.']"
    ],
    "311": [
        "The proposed method outperforms previous systems by using less than 1% of the training data.",
        "The method significantly reduces the error compared to previous systems.",
        "The approach achieves competitive performance compared to the previous systems.",
        "The method is able to learn a mapping from action-oriented features to visual entities.",
        "The use of manual references, plot summaries, and synopses improves the performance of the summarization algorithm.",
        "Documentaries achieve higher scores than news articles or films when using original subtitles documents against manual plot summaries.",
        "Summaries for news articles preserve approximately 31% of the original articles.",
        "The best results for news articles and documentaries are achieved using LSA.",
        "LSA captures the relation between words in sentences, leading to high scores.",
        "The use of plot summaries, documentaries achieve higher results compared to films.']"
    ],
    "321": [
        "The proposed objective function in Eq. 1 outperforms the Socher et al. (2014) objective function in certain tasks.",
        "The use of word vectors and regularization terms in the objective function improves the performance of the model.",
        "Incorporating constraints during the pairing process can further improve the model's performance.",
        "The proposed method outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation."
    ],
    "322": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our system could also be used to enrich word representations, by highlighting aspects of concepts that are not salient in language but are probably learned by similarity-based generalization from the cross-modal mapping training examples.",
        "Inspired from recent work in caption generation that conditions word production on visual vectors, we plan to explore an end-to-end model that conditions the generation process on information encoded in the word embeddings of the word/phrase that we wish to produce an image for.",
        "We introduce the new task of generating pictures visualizing the semantic content of linguistic expressions as encoded in word embeddings, proposing more specifically a method we dubbed language-driven image generation.",
        "Our system completely ignores visual properties related to shape, but better visual representations or feature inversion methods might lead us in the future to associate, by means of images, typical shapes to shape-blind linguistic representations.\"']"
    ],
    "326": [
        "Linguistic resources like WordNet have found extensive applications in lexical semantics.",
        "Recently there has been interest in using linguistic resources to enrich word vector representations.",
        "Distributional representations have also been shown to improve by using experiential data in addition to distributional context.",
        "Simple vector concatenation can likewise be used to improve representations.",
        "Linguistic word vectors require no training as there are no parameters to be optimized, meaning they are computationally economical.",
        "Good quality linguistic word vectors may only be obtained for languages with rich linguistic resources.",
        "We have presented a novel method of constructing word vector representations solely using linguistic knowledge from pre-existing linguistic resources.",
        "Non-distributional, linguistic word vectors are competitive to the current models of distributional word vectors as evaluated on a battery of tasks.",
        "Linguistic vectors are fully interpretable as every dimension is a linguistic feature and are highly sparse, so they are computationally easy to work with.\"']"
    ],
    "331": [
        "The embeddings of GloVe, ELMo, and BERT encode gender, religion, and nationality biases.",
        "A projection-based method can be used to attenuate biases in these embeddings.",
        "The method works for static GloVe embeddings but not for contextualized embeddings (ELMo, BERT).",
        "Debiasing the first non-contextual layer alone can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The proposed self-supervised tasks can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The MT-based DCGM-II+CMM system generates responses that are shorter and more generic or commonplace than human responses, but are often reasonably plausible in the context.",
        "Longer generated responses are apt to degrade both syntactically and in terms of content, and may present information that conflicts with the context.",
        "The model lacks mechanisms for reflecting agent intent in the response and for maintaining consistency with respect to sentiment polarity.",
        "Larger datasets and incorporation of more extensive contexts into the model will help yield more coherent results.",
        "The neural network architecture for data-driven response generation trained from social media conversations consistently outperforms both context-independent and context-sensitive baselines by up to 11% relative improvement in BLEU in the MT setting and 24% in the IR setting.']"
    ],
    "337": [
        "The proposed approach achieves competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed AdaBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The use of unigram features together with a tensor kernel performs comparably to more complex systems on the WebQuestions dataset.",
        "Our system is simpler to implement and runs faster than more complex systems.",
        "There are spurious features such as the pair (live, birthplace) that are likely due to a large proportion of people who live in their birthplace.",
        "Ordering the training alphabetically by the text of the query leads to a large reduction in accuracy.",
        "The system has to learn that the term currency in the query maps to currency in the canonical utterance, which hints at ways of improving over the current system.",
        "Generating features for use in the tensor kernel directly from the logical form instead of via canonical utterances is a potential route for improvement.']"
    ],
    "339": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The chunking policy network for machine reading comprehension enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "The recurrent mechanism allows the information to flow across segments, enabling the model to have knowledge beyond the current segment when selecting answers.",
        "The approach outperforms benchmark models across different datasets.",
        "Incorporating syntactic information into neural language models can reduce the word error rate in a speech recognition task, but the approach did not consider neural language models and only extended to count-based ones.",
        "Other work on incorporating syntax into language modeling has been limited to count-based ones and did not extend to actual language models such as LBL and RNN.",
        "The dependency tree is supplied prior to running the RNN, limiting the scope of the Dependency RNN to the scoring of complete sentences, not to next word prediction.",
        "The approach proposed by Tai et al. (2015) is similar to ours, learning Long Short-Term Memory (LSTM) RNNs on dependency parse tree network topologies, but their architectures are not designed to predict next-word probability distributions as in a language model.",
        "The LSTMs are better than RNNs at storing long-term dependencies and thus do not benefit from the word ordering from dependency trees as much as RNNs.']"
    ],
    "345": [
        "The proposed methods have notable advantages over other techniques in literature.",
        "There is no need for building a separate dictionary for capturing noise characteristics.",
        "The methods operate on each speech frame independently, making them suitable for very short utterances.",
        "The methods are simple, easy to implement, and do not require the use of additional tools like speech/silence detectors or pre-built CDHMM models.",
        "The methods have a few disadvantages, such as requiring many iterations to converge and being computationally expensive for large databases.",
        "The use of building-blocks representation of speech is incorporated into the features, preserving the advantages of using MFCCs.",
        "The proposed MFCCs are able to achieve good results in all cases without any use of adaptation data, especially in unseen cases.",
        "The techniques in SPLICE framework operate on individual feature vectors and do not require estimation of parameters from test data, making them advantageous for shorter utterances and faster testing processes.']"
    ],
    "357": [
        "Our work increases the breadth of knowledge source and the depth of compositionality in semantic parsing.",
        "Different semantic parsing systems are designed to handle different sets of logical operations and degrees of compositionality.",
        "Our dataset contains a more diverse set of logical operations and is more compositional than other datasets.",
        "To parse a compositional utterance, many works rely on a lexicon that translates phrases to entities, relations, and logical operations.",
        "Our work takes a different approach by not anchoring relations and operations to the utterance.",
        "Recent works on semantic parsing for question answering operate on more open and diverse data domains.",
        "The increasing number of relations and entities motivates new resources and techniques for improving accuracy, including the use of ontology matching models, paraphrase models, and unlabeled sentences.",
        "Our work leverages open-ended data from the Web through semi-structured tables.",
        "There have been several studies on analyzing or inferring table schemas and answering search queries by joining tables on similar columns.",
        "While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences.']"
    ],
    "414": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The proposed document-specific language model improves results over a baseline classical Jelinek-Mercer language model.",
        "The document-specific model obtained better results than the generic one, validating the approach of document-dependent parameterization of the term representations.",
        "The use of a document-specific language model can improve the way a generic language model is modified using a small set of parameters that represent a textual object (document, paragraph, sentence, etc.).",
        "Future work will study different architectures of the neural language model and use a relevance language model based on pseudo-relevance feedback.']"
    ],
    "416": [
        "We have demonstrated that controlled experiments can be used to gain insight into a word embedding.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset; can more general globally-normalized models be trained in a similarly inexpensive way?",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR, but it still has some limitations and room for improvement.",
        "Introducing pseudowords into the corpus that mix, with varying proportions, the co-occurrence distributions of two words, can be used to study the path between the word vectors in the feature space.",
        "The word vectors obtained from the first synaptic layer have very different properties from those that could be obtained from the second layer.",
        "The co-occurrence distribution of VOID is the unconditional frequency distribution, and in this sense pure background noise.",
        "Whether improved performance on similarity tasks could be achieved by transforming the feature space or modifying the model such that the representation of pure noise, i.e., the vector for VOID, is at the origin of the transformed feature space.\"']"
    ],
    "417": [
        "The proposed graph-based representation (OmniGraph) with Weisfeiler-Lehman and node edge weighting graph kernel learning exhibits superior performance in text analysis tasks.",
        "OmniGraph's advantages stem from the use of semantic frames to generalize word meanings in a flexible and extensible graph structure, where rich relational linguistic information can be modeled and learned with graph kernels.",
        "The resulting graph features are able to reflect deeper semantic patterns beyond words and provide insights into the problem domain.",
        "OmniGraph can model complex intra-sentence semantic relations and has the potential to support a wide range of NLP classification problems.",
        "One future direction is to model inter-sentence relations through discourse structure to form a more linguistically informed document-level representation.\"]"
    ],
    "437": [
        "The method described in the paper can reason about the content of general images and answer a wide variety of questions about them.",
        "The method develops a structured representation of the content of the image and relevant information about the rest of the world based on a large external knowledge base.",
        "The method is capable of explaining its reasoning in terms of the entities in the knowledge base and the connections between them.",
        "The method can be applied to any knowledge base for which a SPARQL interface is available, including any of the over a thousand RDF datasets online.",
        "The method substantially outperforms the currently predominant visual question answering approach when tested.",
        "The method can use a knowledge base containing common sense to draw sensible general conclusions about the content of images.",
        "The dataset and methodology for testing the performance of general visual question answering techniques have been provided.",
        "The method has achieved a high overall accuracy for a given question type, with a large percentage of correct answers.",
        "The method has also demonstrated logical reasoning capabilities, with a high percentage of correct answers.']"
    ],
    "452": [
        "The use of clipart images can improve the performance of visual question answering tasks by introducing fine-grained semantic differences.",
        "The existing abstract binary VQA dataset can be balanced by augmenting the dataset with complementary scenes, which can help improve the performance of visual question answering models.",
        "Our approach outperforms the language prior baseline and a state-of-the-art VQA approach by a large margin on the balanced dataset.",
        "Our approach attends to relevant parts of the scene in order to answer the question, as shown by qualitative results.",
        "The model is able to achieve a VQA accuracy of 75.11%, which is substantially high as compared to the PRIOR (\"Yes\") baseline accuracy of 68.67%.",
        "The most frequently occurring n-gram in our balanced test set has an accuracy of 82.59% with the answer \"yes\".",
        "In some cases, the bias in the dataset is due to the limited clipart library, and the model can exploit this dataset bias by remembering the most common answer for such n-grams.",
        "Language only models, which include LSTMs, can learn to remember meaningful words in the question instead of the first n words, leading to higher accuracy.']"
    ],
    "470": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our method outperforms word2vec (w2v) baselines on all three tasks.",
        "Our method can be viewed as a modality to transfer knowledge from the abstract scenes domain to the real domain via text.\"']"
    ],
    "494": [
        "Our word vector averaging method along with tf-idf results in improvements of accuracy compared to existing state-of-the-art methods for sentiment analysis in Hindi.",
        "The size of the corpus is also small to learn paragraph vectors. Thus, our model overcomes these weaknesses with a better document representation.",
        "We observe that pruning high-frequency stop words improves the accuracy by around 0.45%. This is most likely because such words tend to occur in most of the documents and don\\'t contribute to sentiment.",
        "When the skipgrams are learned from the entire review corpus, it incorporates some knowledge of the test data.",
        "The earlier methods, which were all in some sense based on a sen-tiWordnet, and at that one that was initially translated from English, were essentially very weak.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our method is competitive with existing methods including state-of-the-art.",
        "The ensemble of RNNLM and Composite Document Vector has beaten state-of-the-art by a significant margin and has opened this area for future research.\"']"
    ],
    "511": [
        "Our approach outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Our approach provides an integrated approach for learning on several levels of the ontology.",
        "Our approach does not use syntactic analysis, like part of speech tags or dependency parsing, which makes our approach more language independent and useful for non-standard texts, where such analysis is not available.",
        "We are looking into integrating syntactic analysis for future work.",
        "One scenario is to automatically detect the property of the rule.",
        "Another idea for future work is to integrate some ideas from other grammar induction methods to detect meaningful patterns without relying on the annotation of text.\"']"
    ],
    "534": [
        "The proposed geolocation algorithm outperforms previous methods, with a median error of 54 km. (Claim 1)",
        "The algorithm returns a list of non-conflicting terms and the location chosen for each phrase. (Claim 2)",
        "The use of part-of-speech tagging and named entity recognition improves the accuracy of the geolocation algorithm. (Claim 3)",
        "The algorithm is designed to return multiple locations, including some very accurate results in most cases. (Claim 4)",
        "Future work on the final sorting of results may improve the median error of 54 km. (Claim 5)",
        "The geolocation algorithm could benefit from more use of the feedback loop between the extraction step and the disambiguation step. (Claim 6)",
        "Modifying the disambiguation algorithm to discard terms with low scores may improve accuracy. (Claim 7)",
        "Using part-of-speech taggers and named entity recognizers specifically trained for the target style of text may improve the geolocation algorithm. (Claim 8)']"
    ],
    "545": [
        "Swivel produces low-dimensional feature embeddings from a co-occurrence matrix.",
        "It optimizes an objective that is very similar to that of SGNS and GloVe: the dot product of a word embedding with a context embedding ought to approximate the observed PMI of the two words in the corpus.",
        "Swivel explicitly considers all the cooccurrence information -including unobserved cooccurrences -to produce embeddings.",
        "In the case of unobserved co-occurrences, a \\'soft hinge\\' loss prevents the model from over-estimating PMI.",
        "Swivel capitalizes on vectorized hardware, and uses block structure to amortize parameter transfer cost and avoid contention.",
        "We found that Swivel did, in fact, parallelize easily in our environment, and have been able to run experiments that use hundreds of concurrent worker machines.",
        "Swivel produces low-dimensional feature embeddings from a co-occurrence matrix, which can be applied to much larger corpora than other methods.",
        "Unlike SGNS, Swivel\\'s computational requirements depend on the size of the co-occurrence matrix, rather than the size of the corpus.",
        "Swivel explicitly considers all the cooccurrence information -including unobserved cooccurrences -to produce embeddings, which leads to demonstrably better embeddings for rare features without sacrificing quality for common ones.\"']"
    ],
    "616": [
        "Many metrics commonly used in the literature for evaluating unsupervised dialogue systems do not correlate strongly with human judgment.",
        "The BLEU metric may be a stronger correlator with human judgment than other metrics, especially when using a larger set of automatically retrieved plausible responses.",
        "The current evaluation methods for unsupervised dialogue systems are limited and do not provide good alternatives for evaluating the quality of generated responses.",
        "Metrics based on distributed sentence representations hold the most promise for the future, as they can capture the compositionality of dialogue responses.",
        "The skip-thought vectors of Kiros et al. (2015) could be considered as an example of such metrics.",
        "The current evaluation methods do not take into account the context of the conversation, and metrics that consider the context could provide more accurate evaluations.",
        "Learning a discriminative model from data to distinguish between model and human responses, or using data collected from human surveys to provide human-like scores to proposed responses, could be considered as alternative evaluation methods.']"
    ],
    "637": [
        "Punctuation marks are almost indistinguishable from other common words in terms of their statistical properties.",
        "Incorporating punctuation marks into an analysis extends its dimensionality and opens up more space for possible manifestations of previously unobserved effects.",
        "The inclusion of punctuation marks in word-occurrence and word-adjacency analysis can bring important results, as shown by the example of sentence length variability being multifractal for specific texts while monofractal for others.",
        "The study found that the punctuation marks are more important than typical nodes in word-adjacency networks, playing a role similar to that of hubs.",
        "The analysis showed that the punctuation marks locate themselves exactly on or in a close vicinity of the power-law Zipfian regime as if they were ordinary words.",
        "The inclusion of punctuation marks in an analysis can reveal systematic differences in network properties between different text samples, and further study is needed to explore these differences.']"
    ],
    "655": [
        "The CHAR+SAMPLE model performs well on both the test set and the development set, suggesting that it is effective in capturing certain types of orthographic errors.",
        "Adding additional correct source-target pairs to the training data may not significantly boost performance, as seen in the comparison between the CHAR+SAMPLE and WORD+SAMPLE models.",
        "Training at the lowest granularity of annotation is a more efficient use of data than training against the binary label.",
        "The encoder-decoder models can be used to generate corrections for end-users, but this capability comes at the expense of longer training and testing times compared to CNN classifiers.",
        "Post-hoc tuning provides a straightforward means of tuning the precision-recall tradeoff for these models.",
        "End-users may prefer greater emphasis placed on precision over recall.",
        "The encoder-decoder models can be used to identify locations of intra-sentence errors.",
        "Modeling at the sub-word level was beneficial for the encoder-decoder models, even though predictions were still made at the word level.",
        "Eliminating the need for an initial tokenization step and generalizing the approach to other languages, such as Chinese and Japanese, is an interesting direction to pursue.",
        "Inducing artificial errors to generate more incorrect source sentences is a potential direction for future work.",
        "The development of data-driven support tools for writers is an area that remains to be explored.']"
    ],
    "657": [
        "The proposed method is on-par with or surpasses state-of-the-art published systems.",
        "The number of involved languages is not limited in any way.",
        "The method is relatively simple and straightforward to implement.",
        "Semantic fingerprints approach boosts accuracy compared to direct \\'translation\\' of words using the same transformation matrix.",
        "Considering word tokens instead of word types leads to an accuracy boost.",
        "The difference between bag-of-words and semantic fingerprints is that vectors for frequent words become more important in determining the final average value.",
        "Semantic fingerprinting is significantly faster than matrix translation.",
        "The method can be applied to a wide range of problems, including cross-lingual translations and projecting texts into another style or genre.\"']"
    ],
    "689": [
        "The proposed approach achieves competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed approach uses less than 1% of the training data to achieve competitive performance.",
        "The method trained on the full dataset significantly outperforms previous methods.",
        "The approach achieves competitive performance compared to the previous systems using less than 1% of the training data.",
        "The proposed approach reduces the error by 21% on English Switchboard.",
        "The method is able to self-organize and build associations between embodied information in different modalities and the lexical structure information.",
        "The hierarchical RNNs, such as MTRNN, are shown to be particularly beneficial in building a neurorobotics cognitive architecture about language learning for robotic systems.",
        "The recurrent connections between the verbs and nouns are associated with different modalities of the training-data, which is strengthened during embodiment training by the sensorimotor interaction.",
        "The MTRNN showed how the embodied information about the verbs dominates a large portion of the network dynamics.']"
    ],
    "691": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Using less than 1% (1000 sentences) of the training data can achieve competitive performance compared to the previous systems.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The LS approaches still dominate COS and receive larger positive effects from the latency penalty despite being restricted to the first sentence.",
        "Having a model (beyond similarity) of what to select is helpful.",
        "We demonstrated the effectiveness of \"learning to search\" algorithms for this task.",
        "Improving the summary content selection, especially in article body, should be the focus of future work.",
        "Exploring deeper linguistic analysis (e.g. coreference and discourse structures) to identify places likely to contain content rather than processing whole documents is a potential area for future research.']"
    ],
    "692": [
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our approach, Visual-Labels, to automatic movie description trains visual classifiers and uses their scores as input to an LSTM.",
        "We expect new techniques, including relying on different modalities, to overcome the challenge of high-quality annotated data for large-scale movie description.",
        "Our dataset has already been used beyond description, e.g. for learning videosentence embeddings or for movie question answering.",
        "Beyond our current challenge on single sentences, the dataset opens new possibilities to understand stories and plots across multiple sentences in an open domain scenario on large scale.\"']"
    ],
    "700": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our results suggest that harder datasets are needed for CQA, and better OCR is also important for advancing the field.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments.",
        "Anchoring bias effect leads to an artificial boost of performance figures for the parsers in question and results in lower annotation quality as compared with human-based annotations.",
        "Our analysis demonstrates that despite the adverse effects of parser bias, predictions that are shared across different parsers do not significantly lower the quality of the annotations.",
        "A hybrid annotation strategy as a potential future alternative to human-based as well as parser-based annotation pipelines.\"']"
    ],
    "702": [
        "We have established a general Hypothesis Evaluation task with three datasets of various properties, and shown that neural models can exhibit strong performance.",
        "Our proposed evidence weighing model is never harmful and improves performance on some tasks.",
        "Simple models can outperform or closely match the performance of complex architectures.",
        "All the models we consider are task-independent and were successfully used in different contexts than Hypothesis Evaluation.",
        "Our results empirically show that a basic RNN text comprehension model well trained on a large dataset outperforms or matches more complex architectures trained only on the dataset of the task at hand.",
        "Our best proposed model is better or statistically indistinguishable from the best neural model reported so far, even though it has a simpler architecture and only a naive attention mechanism.",
        "Future research should include models pretrained on large data as baselines and validate complex architectures on tasks with large datasets if they cannot beat baselines on small datasets.",
        "Randomized machine comprehension models (e.g. neural networks with random weight initialization, batch shuffling or probabilistic dropout) should report expected test set performance based on multiple independent training runs.",
        "Simple averaging-based models are a great start as well, and preinitializing a model also helps against overfitting.\"']"
    ],
    "708": [
        "The Manhattan ConvNet performs better than PMI and orthographic models in terms of overall accuracy in all three language families.",
        "The Manhattan ConvNet shows mixed performance at the task of cross-family cognate identification.",
        "The Manhattan ConvNet does not turn out as the best system across all evaluation metrics in a single language family.",
        "The ConvNet performs better than PMI but is not as good as Orthographic measures at Indo-European language family.",
        "In terms of accuracies, the ConvNet comes closer to PMI than the orthographic system.",
        "ConvNets can compete with a classifier trained on different orthographic measures and different sound classes.",
        "ConvNets can also compete with a data-driven method like PMI which was trained in an EM-like fashion on millions of word pairs.",
        "ConvNets can certainly perform better than a classifier trained on word similarity scores at cross-concept experiments.",
        "The Orthographic system and PMI system show similar performance at the Austronesian cross-concept task.",
        "However, ConvNets do not perform as well as orthographic and PMI systems."
    ],
    "728": [
        "The proposed approach (Dynamic Memory Induction Networks) achieves new state-of-the-art results on few-shot text classification tasks.",
        "The use of dynamic memory as a learning mechanism is more general than what has been used in few-shot learning, and thus has the potential to improve performance in other learning problems.",
        "The edit selection method improves the accuracy of grammatical error correction by exploiting the n-best hypotheses of an SMT-based GEC system.",
        "The use of supervised learning to classify corrections as valid or invalid, and the integration of classifier scores into a reranking method, can significantly improve the performance of a baseline SMT system.",
        "The non-overlapping subset of valid edits selected by the classifier achieves the best score, indicating that the approach can select the most relevant edits to improve the system's performance.",
        "The hypothesis rank feature in the classifier is independent of the baseline SMT system, allowing for the combination of correction candidates from multiple GEC systems.\"]"
    ],
    "734": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "We have proposed the multiresolution recurrent neural network (MrRNN) for generatively modeling sequential data at multiple levels of abstraction.",
        "Our approach is able to generate more fluent, relevant and goal-oriented responses.",
        "The results suggest that the fine-grained abstraction (low-level) provides the architecture with increased fluency for predicting natural utterances, while the coarse-grained (high-level) abstraction gives it the semantic structure necessary to generate more coherent and relevant utterances.",
        "The results also imply that it is not simply a matter of adding additional features for prediction -MrRNN outperforms a competitive baseline augmented with the coarse-grained abstraction sequences as features -rather, it is the combination of representation and generation at multiple levels that yields the improvements.",
        "We leave this to future work.\"']"
    ],
    "742": [
        "DeepDive\\'s feature generator libraries let us easily create a large space of binary features and then let regularization address overfitting.",
        "In our extraction system, just using a context window of \u00b1 2 words and dictionaries representing the baseline memorization rules was enough to achieve median system performance.",
        "POS tag features had no statistically significant impact on performance in either EVENT/TIMEX3 extraction.",
        "A simple distant supervision rule that canonicalizes TIMEX3 timestamps and predicts nearby EVENT\\'s lead to a slight performance boost, suggesting that using a larger collection of unlabeled note data could lead to further increases.",
        "While our systems did not achieve current state-of-the-art performance, DeepDive matched last year\\'s top submission for TIMEX3 and EVENT tagging with very little upfront engineering - around a week of dedicated development time.",
        "One of the primary goals of this work was to avoid an over-engineered extraction pipeline, instead relying on feature generation libraries or deep learning approaches to model underlying structure.",
        "Both systems explored in this work were successful to some extent, though future work remains in order to close the performance gap between these approaches and current state-of-the-art systems.\"']"
    ],
    "750": [
        "The feedforward model outperforms the LSTM models, especially when the amount of training data is limited.",
        "Sequential and tree LSTM models may work better than the feedforward model when given more data.",
        "The contextual information encoded in the word vectors can compensate for the lack of structure in the model, leading to better performance.",
        "The use of linear inter-argument interaction and high-dimensional word vectors is crucial for building a competitive neural network model for classifying implicit discourse relations.",
        "Extending the results across label sets and languages can be done by using feature-based discourse parsing and selecting the best subset of each feature set.",
        "The number of dimensions in the word vectors can be tried to see what works best, with a range of 50, 100, 150, 200, 250, and 300 being tested.",
        "Inducing 1,000 and 3,000 Brown clusters on the Gigaword corpus can also be done to see what works best.']"
    ],
    "754": [
        "The model achieves state-of-the-art results on three datasets (miniRCV1, ODIC, and CNN).",
        "The brevity problem in neural machine translation can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "More general globally-normalized models can be trained in an inexpensive way.",
        "The model learns to discriminate the correct entity using only local context.",
        "The query attention wanders near or focuses on the placeholder location, attempting to discriminate its identity using only local context.",
        "The majority of questions can be answered after attending only to the words directly neighboring the placeholder.",
        "Using 8 timesteps works well consistently across the tested datasets.",
        "Dynamically selecting the number of inference steps conditioned on each example may benefit harder (easier) examples.",
        "Shifting towards stochastic attention may permit learning more interesting search policies.",
        "The model is fully general and can be applied in a straightforward way to other tasks such as information retrieval.']"
    ],
    "767": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The method manages to capture an intuitive relevance assessment for all three sentences, even though none of them contain meaningful keywords from the prompt.",
        "Automated assessment systems relying only on grammatical error detection would likely assign similar scores to all of them.",
        "The method maps sentences into the same vector space as individual words, therefore we are also able to display the most relevant words for each prompt, which could be useful as a writing guide for low-level students.",
        "The model has independently learned to disregard common stopwords, such as articles, conjunctions, and particles, as they rarely contribute to the general topic of a sentence.']"
    ],
    "778": [
        "The use of graph-community detection techniques for document relationship identification tasks is more suitable and leads to better results.",
        "The current state-of-the-art techniques for document relationship identification are not effective and rely on similarity spaces that are not appropriate for the task.",
        "The problem with clustering techniques is that they rely on a similarity space, which is not appropriate for the nature of this task.",
        "The document relationship identification task should be done by exploring network properties of a co-occurrence graph using a graph-community detection approach.",
        "The use of spectral clustering is more accurate than other techniques for document relationship identification.",
        "The problem with spectral clustering is that it merges the topic of \"Courses\" with \"Math\", which makes sense since math is a type of course and the same type of discussions might arise when talking about this topic.",
        "The remaining clusters are scattered across all topics, and the use of Louvain clusterings can lead to better capture of the \"Games\" topic.",
        "The 4S corpus is more heterogeneous than the AVL corpus, and the use of graph-community detection techniques is more suitable for this corpus.']"
    ],
    "811": [
        "The constructed DAGs suggest that the tags appearing in different news portals are organized to different degrees.",
        "The identification of frozen cliques can be applied by disambiguation techniques to identify cliques of equivalent semantic meaning.",
        "The number of connected components in the DAGs conveys information about the extent of organization in the data.",
        "The Spiegel and Guardian have an extra intermediate level of organization at certain locations.",
        "The New York Times has a few dozen components and breaks the world into independent pieces.",
        "The Australian has O(100) components, which are barely informative.",
        "The consistency of the results is encouraging, and suggests that the measures used are useful in the quantification and comparison of datasets from the aspect of hierarchical organization."
    ],
    "814": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The TransE model obtains a very competitive accuracy, particularly compared to the relation path model TransE-COMP.",
        "Our high results of the TransE model are probably due to a careful grid search and using the \"Bernoulli\" trick.",
        "The TransE results used for initializing TransR, TransD and TranSparse are not reported in Wang et al. (2014) , so it is difficult to determine exactly how much these models gain over TransE.",
        "For entity prediction, TransE-NMM with the filtering threshold \u03c4 = 10 only obtains better mean rank than TransE (about 15% relative improvement) but lower Hits@10 and mean reciprocal rank.",
        "Our neighborhood mixture model for knowledge base completion significantly improves TransE and obtains better results than the other state-of-the-art embedding models on triple classification, entity prediction and relation prediction tasks.']"
    ],
    "819": [
        "We present a segmental Bayesian model for speech recognition.",
        "Our system achieves WERs of around 84% on English and 76% on Xitsonga data.",
        "Despite much worse speaker-independent performance, we achieve improvements by incorporating frame-level features from an autoencoder-like neural network trained using weak top-down constraints.",
        "Our approach outperforms a purely bottom-up method that treats each syllable as a word candidate.",
        "The high WERs reported in this study show that there is still much work to be done in the area of zero-resource speech processing.",
        "Nevertheless, previous work shows that high-error rate unsupervised systems can still be useful in downstream tasks.",
        "Our own future work will consider better acoustic word embedding approaches, improving the recall of the syllabic presegmentation method, and improving the overall efficiency of the model.\"']"
    ],
    "889": [
        "The proposed generative neural networks for generating hypothesis using NLI dataset achieve high accuracy, with the best model achieving 78.5% accuracy, which is only 2.7% less than the accuracy of the classifier trained on the original human written dataset.",
        "The best dataset combined with the original dataset has achieved the highest accuracy, indicating that a good dataset should contain accurate, non-trivial, and comprehensible examples.",
        "The attention mechanism improves the model's performance.",
        "The discriminative evaluation shows that in 22.5% of cases, the human evaluator incorrectly distinguished between the original and the generated hypothesis.",
        "The standard text generation metrics ROUGE and METEOR do not indicate if a generated dataset is good for training a classifier, and the accuracy of classifier can be improved by incorporating the discriminative model into the generative model.",
        "Incorporating the discriminative model into the generative model can generate examples from a distribution that is more similar to the original training distribution.",
        "Constructing a dataset requires a lot of intensive manual work, mainly consisting of writing text with some creativity, and extending the original dataset by human users correcting or validating the generated examples can improve the dataset.\"]"
    ],
    "894": [
        "the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR",
        "students have a tendency to discuss positive aspects rather than negative aspects of their distance education programs",
        "the process of detecting a category first followed by the corresponding aspects is beneficial",
        "the results are satisfactory for frequent aspects",
        "the best precision is achieved by the flat model which makes decisions for all aspects independently of each other",
        "our method can support the processing of reviews in real-time to provide feedback about distance education programs",
        "interested students can select one of the aspects to rank programs and institutions based on that selection",
        "such an approach can help institutions to identify their specific weaknesses\"']"
    ],
    "895": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "word embeddings help us further our understanding of bias in language.",
        "the projection of gender neutral words on this direction enables us to quantify their degree of female-or male-bias.",
        "By equating grandmother and grandfather outside of gender, and since we\\'ve removed g from babysit, both grandmother and grandfather and equally close to babysit after debiasing.",
        "Our hard-debiasing algorithm significantly reduces both direct and indirect gender bias while preserving the utility of the embedding.",
        "By reducing the bias in today\\'s computer systems (or at least not amplifying the bias), which is increasingly reliant on word embeddings, in a small way debiased word embeddings can hopefully contribute to reducing gender bias in society.\"']"
    ],
    "915": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Network dynamics could be probed in a straightforward manner owing to the stationarity of the series obtained with the network metrics.",
        "The typical sizes of the networks were slightly more than 100 nodes, which are usually considered small.",
        "Our approach succeeds because it collects only global metrics, i.e. averages which are still reliable, in contrast to distributions over all nodes.",
        "When the typical network sizes are below a few hundreds of nodes, the scores drop.",
        "The errors are caused by the variability of style of some authors in their books.']"
    ],
    "917": [
        "The proposed method for causality extraction is effective in natural language text.",
        "The method is limited by the insufficiency of high-quality annotated data.",
        "The algorithm is generalizable across domains and can handle noisy text.",
        "The algorithm is fast-running and non-combinatorial, making it ideal for large-scale applications.",
        "The presented algorithms have been made available as open-source tools.",
        "The PARSEME experiments did not provide an evaluation against all types of MWEs.",
        "Text partitioning requires some critical mass of training data to achieve high levels of performance.",
        "Future directions of this work will likely involve the collection of larger and more comprehensive data sets.",
        "The current model relies on knowledge of boundary token states and cannot be trained well on MWE lexica alone.",
        "The use of boundary-adjacent words for prediction is a limitation of the present model."
    ],
    "934": [
        "The proposed approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The task described here differs from previous work in that the full (or almost full) discussion thread is available in extracting features characterizing the response to the comment.",
        "The submission context is represented only in terms of the relative timing and graph structure in a discussion thread, and does not use the text within earlier or responding comments.",
        "Prior work has shown that the relevance of a comment to the preceding discussion matters, and clearly the sentiment expressed in responses should provide important cues."
    ],
    "966": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "A distributed multi-domain dialogue architecture can be used to construct generic policies which provide acceptable in-domain user performance, and better performance than can be obtained using under-trained domain specific policies.",
        "The Bayesian policy committee approach gives superior performance to the traditional one-policy-approach across multiple domains and allows flexible selection of committee members during testing.",
        "The basic policy committee model was extended using ideas from multi-agent learning to distribute the reward signal among the committee members, which is particularly useful in real-world scenarios where the domain is a priori unknown.']"
    ],
    "970": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "Using diverse datasets, we robustly demonstrate that the approach we advocate is effective for identification of translationese, even when only little data are available.",
        "The proposed two-phase clustering approach is a robust method for distinguishing original from translated texts.",
        "Our future plans include using various datasets and reduced amount of data for LMs compiled for cluster labeling.",
        "We plan to explore the correlation between the amount of data used for LMs and the scaling factor \u03b1 used for association of a label with a clustering outcome.",
        "We believe that we will be able to show an improvement in the quality of SMT with extremely little supervision.']"
    ],
    "1021": [
        "The inverted factorized model (ISVD) produces the most robust results over data of varying sizes, and across several different test settings.",
        "The neural network-based models and ISVD perform the best for high-frequent items, while the CBOW model, PPMI model, and RI model perform optimally for low-frequent items.",
        "The ISVD model underperforms significantly for low-frequent items, which suggests that it might be interesting to investigate hybrid models that use different processing models or parameterizations for different frequency ranges and data sizes.",
        "None of the standard DSMs work well in situations with small data.",
        "Investigating how to design DSMs that are applicable to small-data scenarios could be an interesting novel research direction.",
        "The results demonstrate that all tested models perform optimally in the medium-to-high frequency ranges, and none of the models perform optimally for low-frequent items.']"
    ],
    "1022": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for fewshot learning.",
        "The best BiLSTM NER model for each of the semantic groups shows balanced precision and recall, and similar F1-scores.",
        "Medical Device and Clinical Finding show a typical performance for a dictionary-based NER system with a high precision and a low recall.",
        "Body Location has relatively high precision and recall values which suggests that this semantic group is well covered by our dictionary of medical terms.",
        "Descriptor shows a very low precision which is the result of a high number of false positives.",
        "The false positives are caused by many Descriptor entries in our dictionary of medical terms that had been automatically extracted from RadLex and MeSH but which do not correspond to the definition of a Descriptor used by the clinicians who produced the labelled data.",
        "Word embeddings may encode information about the compositionality of words as discussed by Mikolov (2013).\"']"
    ],
    "1034": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The proposed method achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what has been used for few-shot learning.",
        "Supervised training with a unidirectional LSTM model can achieve similar results as the state-of-the-art model.",
        "The method of augmenting a supervised sequence transduction objective with an autoencoding objective can improve model performance in semi-supervised training.",
        "The effectiveness of the approach can be demonstrated across multiple semantic parsing tasks.",
        "Training on randomly generated unsupervised data can improve model performance.",
        "Sampling from a logisticnormal distribution instead of a softmax may impact the distribution in the latent space.",
        "Reversing the model to train it with additional labelled data in x is straightforward.",
        "A natural extension would be to perform semisupervised training in both x and y, such as in machine translation tasks where parallel data may be scarce.']"
    ],
    "1037": [
        "The use of lexical and POS information for the first time in multi-label Portuguese temporal text classification.",
        "The approach proposed in this paper is able to predict the publication date of texts, in intervals of both 100 and 50 years, using word unigrams with 99.8% accuracy.",
        "The method is also able to predict the publication date of texts using solely POS tags achieving performance of 90.7% accuracy for intervals of 100 years, and 90.1% accuracy for intervals of 50 years.",
        "The high word unigram results could be the result of chronological topic specificity.",
        "The POS n-grams discussed above, which aren\\'t related to topics but are indicative of grammatical or stylistic change.",
        "Texts from the 19th and 20th centuries contain on average a larger number of adjectives, which reflect stylistic preferences of that period.",
        "As future work, we would like to investigate the question of time intervals (see next section) as well as to create an additional test set comprising a few texts from each time period.\"']"
    ],
    "1046": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The AS Reader may be particularly suitable for training on large datasets.",
        "The second challenge is how to generalize the performance gains from large data to a specific target domain.",
        "Given enough data, the AS Reader was able to exceed the human performance on CBT CN reported by Facebook.",
        "Using more training data can yield performance improvements of up to 14.8% compared to our best ensemble result.\"']"
    ],
    "1084": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "A Bayesian approach to speaker normalization is proposed in this paper, which models the variation of vocal tract length among different speakers using an affine model.",
        "The proposed approach is better suited for cross-gender normalization, as it can improve the performance of gender-dependent normalization more than gender-independent normalization.",
        "The proposed normalization method requires prior knowledge about the transcript of the utterance under test, and a two-pass approach through the recognizer is proposed to solve this problem.",
        "Bayesian estimation method gives better performance compared to other methods.",
        "Currently, methods that use non-Gaussian priors and informative priors are being explored to estimate the speaker normalization parameters.",
        "The possibility of using higher order speaker normalization models in the proposed Bayesian parameter estimation framework is also being investigated.']"
    ],
    "1094": [
        "The proposed approach can significantly boost the performance of a rumour detector by leveraging the context preceding a tweet with a sequential classifier.",
        "The approach outperforms the state-of-the-art rumour detection system introduced by Zhao et al. (2015) that relies on find querying posts that match a set of manually curated list of regular expressions.",
        "The fully automated approach achieves superior performance that is better balanced for both precision and recall compared to the manual approach.",
        "The use of social media and user-generated content (UGC) in journalism and government agencies such as the police and civil protection agencies presents major challenges, including the need to verify the veracity of information before it can be considered fit for use.",
        "The development of tools that can aid in the detection of rumours and determining their likely veracity is vital, and the proposed approach has the potential to address this issue.']"
    ],
    "1101": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Our 36% estimate of human performance on CONTROL shows the difficulty of the general problem, and reveals a gap of 14% between the best language model and human accuracy.",
        "Applying neural readers is a good direction for this task, but they fail on the 17% of instances which do not have the target word in the context.",
        "The Stanford Reader can be easily used to predict target words that do not appear in the context, and other readers can be modified to do so.",
        "Our simple method of dataset creation could be used to create additional training or evaluation sets for challenging language modeling problems like LAMBADA, perhaps by combining it with baseline suppression.']"
    ],
    "1111": [
        "The proposed method outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The exploratory data analysis of the training corpus showed that the fraction of sad songs increases over the years.",
        "Removing stop words from the lyrics improves the performance of the model.",
        "The high precision of the classifier is still satisfactory given the proposed goal of confidently removing sad songs from an extensive music library before performing the genre classification.",
        "A naive Bayes model applied to mood classification based lyrics can predict the positive class (happy) with high precision, which can be useful to filter a large music library for happy music with a low false positive rate.']"
    ],
    "1114": [
        "The authors have introduced a new class of probabilistic neural networks called natural-parameter networks (NPN).",
        "NPN models are flexible and can capture hierarchical relationships among latent variables.",
        "NPN models achieve state-of-the-art performance on various tasks, including classification, regression, and representation learning.",
        "The authors have designed efficient sampling-free backpropagation-compatible algorithms for learning NPN models.",
        "NPN models cannot be used as generative models, and they are not suitable for supporting multiple types of inference.",
        "The authors plan to address these limitations in their future work.']"
    ],
    "1145": [
        "The use of word embeddings can improve the performance of cross-lingual word sense disambiguation (WSD) systems.",
        "The integration of pretrained word embeddings into an existing WSD system can result in competitive or better performance than state-of-the-art systems on many tasks.",
        "The addition of word embeddings as a feature type to IMS resulted in the system performing well, with little effort.",
        "Other publicly available word embeddings, such as Collobert & Weston's embeddings and GLoVe, also consistently enhanced the performance of IMS using the summation feature.",
        "The number of dimensions in the word embeddings did not affect results as much as the scaling parameter.",
        "A simple composition method using summation already gave good improvements over the standard WSD features, provided that a specific scaling method was performed.",
        "The proposed word embedding approach improved the performance of WSD in a cross-lingual setting.",
        "Expanding the existing dictionary with more English words of varying difficulty and including more possible Chinese translations could be a future work to further improve the system.\"]"
    ],
    "1154": [
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "our approach allows lexicality and syntax to interact with each other in the joint search process, it improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "we propose a novel abstractive training mechanism to eliminate the need for extractive labels at training time.",
        "our extractive model achieves competitive performance with state-of-the-art deep learning models.",
        "we plan to further explore combining extractive and abstractive approaches as part of our future work.\"']"
    ],
    "1221": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "Differentiating between dreams and personal stories turned out an easy task.",
        "The analysis of the features used by the Balanced Winnow classifier show that expressions of uncertainty and setting descriptions and narrative verbs are typical for dreams, while time expressions and conversational expressions are typical for the personal stories.",
        "In our exploratory study on discontinuity in dreams, we saw that dream reports indeed use less discourse markers and have a lower entity-based textual coherence.",
        "The fact that the text classifiers obtained such high scores and the topics were significantly different distributed over the samples, can be an indication that the contrasting data sample was not as representative as we had hoped for.",
        "We suspect that a more careful selection over a much larger set of personal stories, and perhaps an additional check to filter out characteristic internet language is needed to create automatic models that focus on the more subtle differences between reported dreams and personal experiences from real life.\"']"
    ],
    "1227": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "A supervised method using unsupervised measures as features could be the best of both worlds, being more robust than embedding-based methods while being more informative than any single unsupervised measure.",
        "Supervised methods that rely on the relation between words, such as path-based information, are more robust than distributional methods.",
        "Unsupervised methods can still play a relevant role, especially if combined with supervised methods, in the decision whether the relation holds or not.",
        "New datasets will be available for the task, which would be drawn from corpora and will reflect more realistic distributions of words and semantic relations.']"
    ],
    "1237": [
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension that enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow information to flow across segments, enabling the model to have knowledge beyond the current segment when selecting answers.",
        "Our approach uses less than 1% of the training data compared to previous systems, achieving competitive performance.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Different features have different impacts depending on the domain, and combining them produces robust and consistent performance across different domains.",
        "Using different numbers of topic terms when computing topic and title relevance for candidate ranking can improve performance, but the difference is marginal.",
        "Computing relevance by first computing the centroid of topic terms before computing the cosine similarity with a candidate title does not provide significant gains.",
        "Graph connectivity over the graphical structure of the Wikipedia categories or similar may have high utility, and is worth exploring in future work.",
        "Methods based on keyphrase extraction such as Zhao et al. (2011) are potentially worth exploring, but it remains to be seen whether notions such as \"interestingness\" benefit topic label selection.']"
    ],
    "1251": [
        "Existing automatic image captioning metrics have strengths and weaknesses, and there is room for improvement in developing more effective evaluation metrics.",
        "The recently proposed WMD document metric is found to be quite effective for image captioning, with high correlations with human scores and less sensitivity to synonym swapping.",
        "Compared to the recent results of significance testing of machine translation and summarization metrics, our results suggest that there remains much room for improvement in developing more effective image captioning evaluation metrics.",
        "Combining different metrics into a unified metric can significantly improve the performance of automatic image captioning evaluation, with the combination of WMD+SPICE+METEOR performing the best.",
        "Incorporating visual and semantic information via multimodal embeddings is useful for the caption evaluation task.']"
    ],
    "1257": [
        "The proposed framework offers many interpretable explanations for various aspects of neural models.",
        "By analyzing the harm this erasure does, the proposed framework provides a way to conduct error analysis on neural model decisions.",
        "The framework has the potential to benefit a wide variety of models and tasks.",
        "The proposed methodology can be used to interpret the decisions of various neural network architectures.",
        "The method can be applied to different NLP tasks, such as POS tagging, NER tagging, chunking, prefix and suffix analysis, and sentiment analysis.",
        "The use of erasure can improve the performance of neural models on certain tasks.",
        "The proposed methodology provides a way to analyze the effect of erasing particular representations in neural networks.",
        "The method can be used to identify the most important representations for a given task.",
        "The proposed framework offers a way to conduct error analysis on neural model decisions, which has the potential to improve the performance of neural models.']"
    ],
    "1261": [
        "The sentiment score of individual tweets can be noisy, but it can be aggregated successfully using networks to study the interactions between users.",
        "The correlation between the sentiment of mentions that a user sends and receives (the in-and out-sentiment) is positive and robust to randomisation tests.",
        "Users with similar sentiment tend to be clustered together in the network, forming distinct groups based on their sentiment and activity level.",
        "Many of the mentions between users in different sentiment groups occurred in the absence of friend/follower links, indicating the existence of topical dialogue across ideological lines.",
        "Sentiment and social structure are related yet distinct, and must be studied together to understand the disposition of users around topics of interest.",
        "Combining sentiment analysis with topic modeling and additional user features can provide a more accurate picture of user disposition.",
        "This work can be extended to incorporate sentiment in opinion dynamics models and the analysis of retweet cascades, and to investigate the calibration of polling data using social structure.']"
    ],
    "1264": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "We proposed a novel autoencoder-like neural network that uses automatically discovered terms in the speech data as a weak top-down supervision signal.",
        "This model was the first neural network model of its kind, resulting in major improvements over previous state-of-the-art approaches.",
        "We proposed a novel segmental Bayesian framework in which potential word candidates of variable duration are modelled as fixed-dimensional acoustic word embedding vectors.",
        "Our full-coverage segmentation system, which imposes a complete top-down segmentation of its entire input, could only be applied to larger vocabularies when taking into account bottom-up knowledge of automatically detected syllable boundaries.",
        "By using the autoencoder features (trained using both bottom-up and top-down knowledge) within the segmentation system, we showed that clusters can be made less speaker-and gender-specific.",
        "This work has shown that a combination of top-down and bottom-up modelling is greatly beneficial in tackling zero-resource problems.\"']"
    ],
    "1282": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The abundance of data is a determining factor for applying deep neural network successfully.",
        "Our model outperformed the AMIA model for certain tags, such as phone or hospital names, despite having a few number of labeled instances for training our network.",
        "The quality of word vectors is another issue in our work, and we are not sure whether the size of our optometry data set is sufficient to achieve good results.",
        "Using pretrained word vectors as initial values and refining/adjusting them by training over our own dataset may be a remedy for our case.",
        "Our network architecture has room for improvement, particularly using multiple bidirectional layers of LSTM to improve capturing the context in different abstraction levels.",
        "Setting hyper-parameters such as dropout, learning rate, and LSTM specific parameters like forget rate is very determining for improving performance.",
        "Conducting experiments on the new i2b2 dataset may be worthwhile.']"
    ],
    "1290": [
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The performance of component classification is greatly improved by a better relation classification.",
        "The effect of better component classification on relation identification is small, because given the correct relation structure of a text, the role of each component can be identified easily.",
        "The low performance of the base relation identification sub-task also matters.",
        "We evaluate the effect of different combination weights and find that for larger \u03b1, the performance for component classification is higher in general; the impact of \u03b1 on relation classification is smaller.\"']"
    ],
    "1299": [
        "Using a small dictionary as their starting seed dictionary, some methods have been proposed to extract bilingual lexicons from comparable corpora.",
        "The effects of using these dictionaries on different types of comparable corpora are evaluated.",
        "A new and interesting challenge was introduced in our work combining different dictionaries creating the seed dictionary.",
        "Using independent dictionaries with and without considering word orders, the proposed method is almost as accurate as our simple combination.",
        "The effect of the comparability degree of the initial comparable corpus is studied using different types of comparable corpora.",
        "A higher degree of comparability in input corpus has a more accurate lexicon despite the fact that the less comparable corpus is larger.",
        "Using a specific corpus may decrease the generality of the extracted lexicon.",
        "A new weighting method has been proposed to increase the efficiency of our dictionary combination.",
        "The jaccard and dice are very similar based on results gathered in (Otero, 2008).",
        "In recent works, the similarity of two vectors, X and Y is computed using one of these similarity measures.\"']"
    ],
    "1311": [
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The context averaging approach, which disregards context word order, suffers in accuracy compared to models that use left and right context words.",
        "The interplay vector in a test instance can be used to disambiguate the preposition sense.",
        "Attachment and complement properties of prepositions can be encoded into context features, disambiguating senses of preposition.",
        "The method relies on no external resources and performs very well on two standard PSD datasets.",
        "The disambiguation readily scales to a large corpus and the resulting sense-specific representations have been shown to capture lexical relationships and aid phrasal paraphrasing."
    ],
    "1337": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Our work could be aligned with the effort to handle low-resource data problems when building the end-to-end neural network model.",
        "Our methods have led to significant improvement over a baseline neural attention model, and our model is also competitive against models that do not use extra linguistic resources.",
        "The proposed ReCoSa model is effective in capturing long-distance dependency relations.",
        "The use of self-attention mechanism improves the quality of multiturn dialogue generation.",
        "The relevant contexts detected by the model are useful for improving the quality of generated response.",
        "The proposed methods can be used to reduce the sparsity problem when training sequence-to-sequence models on a relatively small dataset.']"
    ],
    "1341": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach can achieve competitive performance compared to previous systems (trained using the full dataset) by using less than 1% of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Many of the tasks related to fake news detection have already been tackled, but there is still a need for more contributions, especially those that can be reasonably addressed within the scope of a paper.",
        "Writing style can be used to uncover traces of manipulation and distinguish hyperpartisan news from more balanced news.",
        "The writing styles of otherwise opposing orientations (leftwing and right-wing) are similar, and satire can be distinguished well from other news."
    ],
    "1342": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "We demonstrated that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "By examining our dataset closely, we were able to define baseline and upper bound models for predicting word order and compared the models learned by several algorithms with them and each other.",
        "Given sufficient annotations, any additional theories of the nature of Czech word order could be incorporated into all of the learning models.",
        "We showed that not only does the perceptron perform well in terms of modeling speaker production, including variation, but it is an attractive way to model the actual learning process since it is online and very simple.",
        "The perceptron has the added benefit of being a well-established algorithm in many fields, and its properties are well studied and understood.",
        "In comparing the perceptron to the GLA, we showed empirically that using a Harmonic Grammar and allowing for ganging-up-effects results in a model that is more accurate than a similar OT model in terms of predicting surface forms.']"
    ],
    "1353": [
        "Reinforcement learning can improve parsing accuracy and reduce error propagation in NLP tasks.",
        "The proposed Approximate Policy Gradient (APG) algorithm is efficient and can be applied to high-performance greedy dependency parsing.",
        "The use of reinforcement learning in the training phase can improve parsing accuracy without affecting the original efficiency of the model.",
        "The study demonstrates the first experimental evidence that reinforcement learning can reduce error propagation in an NLP task.",
        "The proposed method is general enough to be applied to other structured prediction problems, such as coreference resolution or semantic role labeling.",
        "The source code of all experiments is publicly available at https://bitbucket.org/cltl/redep-java.']"
    ],
    "1366": [
        "Incorporating the more informative class distribution labels leads to improved performance under certain training setups.",
        "With SLMG, a learning model can update parameters according to a gold-standard that allows for uncertainty in predictions.",
        "SLMG is an easy fix, but it is not a silver bullet for improving generalization.",
        "Under different training settings SLMG can improve performance for the different models.",
        "It is worthwhile to experiment with SLMG to see if and how it can improve performance on other NLP tasks.",
        "NLI is a particularly good use case for SLMG because of the ambiguity inherent in language and the potential disagreements that can arise from different interpretations of text.",
        "Identifying a suitable number using active learning techniques is left for future work.",
        "SLMG requires soft labels, but it does not necessarily require human-annotated soft labels.",
        "Rather, SLMG only requires some measure of uncertainty between training examples as part of the generalization step.",
        "Our results suggest that future work training DNNs to learn a distribution over labels can lead to further improvements.\"']"
    ],
    "1413": [
        "The proposed novel constituent hierarchy predictor outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "The resulting fully-supervised parser achieved state-of-the-art accuracy on three major datasets for the task, evaluating three types of increasingly sophisticated classification models.",
        "The application of meta-classification techniques for NLI achieves state-of-the-art accuracy on different languages and datasets, with the same model configurations achieving the best results across different test sets and corpora.",
        "The ensemble classifier was used to participate in several shared tasks, including the 2015 Discriminating Similar Language shared task and the Complex Word Identification task at SemEval 2016 (Track 11), ranking in second and third place.",
        "The meta-classifier ensemble approach described here was the basis of an entry in the 2016 Computational Linguistics and Clinical Psychology (CLPsych) shared task, ranking in first place among 60 systems.",
        "The possibility of statistical significance testing within NLI is introduced, making available two new sets of predictions to facilitate this.",
        "The use of the classification models described here for various NLP tasks highlights their utility and potential for improving cross-corpus performance.']"
    ],
    "1443": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "MAS and AAS are more stable than HAS for European-English datasets.",
        "In Japanese-English translation, AAS achieved the highest correlation.",
        "MAS performs stably on the WMT datasets.",
        "The 1:1 alignment is too strict to calculate sentence similarity in MT evaluation, while the 1:m (MAS) alignment performs well.",
        "AAS with high threshold values (0.6-0.9) shows stable high correlation across all language pairs.",
        "Appropriate word alignment using word embeddings is helpful in evaluating the MT output.']"
    ],
    "1457": [
        "The graph-structured bidirectional LSTM model leads to improved results in predicting comment popularity compared to a node-independent model.",
        "The model benefits prediction over the extent of the discussion, and language cues are particularly important for distinguishing controversial comments from those that are very positively received.",
        "Responses from even a small number of comments seem to be useful, so it is likely that the bidirectional model would still be useful with a short-time lookahead for early prediction of popularity.",
        "The model can be applied to other social media platforms that maintain a threaded structure or possibly to ci-tation networks.",
        "The model would be useful for other tasks for which the responses to comments are informative, such as detecting topic or opinion shift, influence or trolls.",
        "With the more fine-grained feedback increasingly available on social media platforms (e.g. laughter, love, anger, tears), it may be possible to distinguish different types of popularity as well as levels, e.g. shared sentiment vs. humor.",
        "The model uses a simple bag-of-words representation of the text in a comment; more sophisticated attention-based models and/or feature engineering may improve performance.",
        "The pruning mechanism introduced in the study limits the performance of the model on underpredicted comments.']"
    ],
    "1474": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The use of a projection-based method can effectively attenuate biases in contextualized embeddings without loss of entailment accuracy.",
        "Our taxonomy of common errors in automatically generated image descriptions can be used to quantify the weaknesses of the model and identify possible improvements.",
        "The descriptions that are accurate, are also much more general than the human descriptions, which usually include small, but salient details.",
        "We propose the following rule: if the majority of the human descriptions comments on an aspect of the image that is not addressed by a generated description, then that aspect could be improved.",
        "Our work will encourage researchers in vision & language to investigate the possibility of incorporating a dedicated module to detect color, actions, type of clothing, gender, and scenes.",
        "Our taxonomy of error types can help others to go beyond similarity-based metrics and look at their model's output through a qualitative lens.",
        "The categories are fairly straightforward, but there are cases where it is easy to get confused between a pair of categories, and we provide additional guidelines for difficult cases.\"]"
    ],
    "1480": [
        "The proposed NMT method can translate patent sentences with a large vocabulary of technical terms.",
        "The NMT system outperforms the phrase-based SMT system and the equivalent NMT system without the proposed approach for translating Japanese patent sentences.",
        "The proposed NMT system is expected to improve the translation performance of patent sentences by applying the approach of Bahdanau et al. (2015).",
        "The proposed NMT system is appropriate for translating patent documents that contain many technical terms comprised of multiple words and should be translated together.",
        "The decoding approach of the proposed NMT system achieved the best RIBES performance and human evaluation scores in experiments, while the reranking approach achieved the best performance with respect to BLEU.",
        "A translation with the highest average SMT and NMT scores of the n-best translations produced by NMT and SMT, respectively, is expected to be an effective translation.']"
    ],
    "1518": [
        "The proposed HCLM outperforms a basic LSTM and a baseline HCLM.",
        "The addition of a caching mechanism to the HCLM consistently improves its performance, even on the PTB dataset with no rare or OOV words.",
        "The cache demonstrates consistent value in non-English languages, despite morphological processes generating forms that are related to existing forms.",
        "The model effectively uses the cache to model word reuse, particularly of proper names and frequent words.",
        "The model disfavors copying numbers from the cache, even when they are available.",
        "The proposed character-level language model with an adaptive cache efficiently models word sequences and achieves better perplexity in every standard dataset.",
        "The model performs better than character-level models by modeling burstiness of words in local context.",
        "The model assumes the observation of word segmentation, and therefore is not directly applicable to languages such as Chinese and Japanese, where word segments are not explicitly observable.']"
    ],
    "1524": [
        "The proposed sequence labeling framework with a secondary objective of learning to predict surrounding words for each word in the dataset achieves consistent performance improvements on several common sequence labeling tasks.",
        "The additional language modeling objective provided by the framework leads to more accurate sequence labeling models, particularly on the task of error detection in learner writing, where the label distribution is very sparse and unbalanced.",
        "The use of a bidirectional LSTM with one half trained as a forward-moving language model and the other half as a backward-moving language model, combined with the objective of learning to predict surrounding words, leads to more accurate sequence labeling models.",
        "The architecture was evaluated on a range of datasets covering tasks such as error detection in learner texts, named entity recognition, chunking, and POS-tagging, and it achieved consistent performance improvements on every benchmark.",
        "The language modeling objective provided by the framework allows the system to take better advantage of the available training data, leading to improved performance on sequence labeling tasks.",
        "Future work could investigate the extension of this architecture to additional unannotated resources, or the pre-training or cotraining of more advanced compositional modules.']"
    ],
    "1538": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The use of pseudo labels improves the domain adaptation performance.",
        "The proposed method is robust and can be generalized for the common case where manual diplomatic transcriptions are not available.",
        "The approach can be used for bibliographic study on a much larger class of texts, including those without manually produced transcriptions.",
        "The system is able to include more information and finer-grained information than previous approaches.",
        "The ability to incorporate more evidence at a larger scale offers a more robust approach to compositor identification.",
        "The proposed method can be used to scale the methods of compositor attribution for use across books and corpora.']"
    ],
    "1544": [
        "The proposed neural semantic parser can convert natural language utterances to grounded meaning representations via intermediate predicate-argument structures, and it jointly learns how to parse natural language semantics and the lexicons that help grounding.",
        "The model is more interpretable as the intermediate structures are useful for inspecting what the model has learned and whether it matches linguistic intuition.",
        "The assumption of structural isomorphism restricts the expressiveness of the model, especially since one of the main benefits of adopting a two-stage parser is the potential of capturing domain-independent semantic information via the intermediate representation.",
        "The current framework can handle local structure matching, such as when the mapping between natural language and domain-specific predicates is many-to-one or one-to-many.",
        "The model can perform cross-domain semantic parsing where the first stage of the semantic parser is shared across domains.']"
    ],
    "1550": [
        "Our method can be used to create richly annotated corpora for low-resource languages, which is a significant contribution to linguistic research.",
        "The proposed approach can be applied to over 1000 languages in the Parallel Bible Corpus, making it a valuable resource for crosslingual analysis.",
        "Our method performs well for the crosslingual analysis of the linguistic phenomenon of tense, and we produced analysis results for more than 1000 languages.",
        "We extended existing methodology for leveraging parallel corpora for typological analysis by overcoming a limiting assumption of earlier work.",
        "Our approach requires that a linguistic feature is overtly marked in a few of thousands of languages, rather than being marked in all languages under investigation.",
        "The use of automatic methods for creating richly annotated corpora will advance linguistic research on low-resource languages.']"
    ],
    "1561": [
        "SpeechMod underperforms compared to TextMod",
        "The Kaldi model has been trained on many times more data than contained in VQA1.0",
        "The ASR serves to filter out noise in high dimensions and extract meaningful patterns in the form of text",
        "SpeechMod does not include any mechanisms that explicitly learn semantics in a language",
        "Whether or not forcing the system to learn words will be beneficial is left to future research",
        "Data standardization is helpful for unseen data",
        "The gap in performance between the unseen and full dataset with TextMod is much smaller than in SpeechMod",
        "Text-based VQA performs better than speech-based",
        "SpeechMod merits further study into end-to-end methods\"']"
    ],
    "1562": [
        "The use of complex networks in combination with traditional features can improve the characterization of texts.",
        "The proposed hybrid method combining frequency of words and occurrence of small subgraphs (labelled motifs) can reveal stylistic subtleties in written texts not extracted with only the frequency of the words.",
        "The method could be extended by considering network motifs comprising more than three nodes or other structures present in some textual networks, such as paths and stars.",
        "The approach could be applied in related tasks like the analysis of text complexity or the evaluation of proficiency in language learning.",
        "The proposed method can detect the translation direction, which has a significant impact on statistical machine translation systems.",
        "Translation models trained on texts produced in the same direction of the SMT task usually perform better than those trained on the opposite direction.",
        "Translated sentences are better represented by language models compiled from translated texts.']"
    ],
    "1618": [
        "The proposed system uses deep recurrent neural networks in a sequence-to-sequence setting to learn a bidirectional mapping between human whole-body motion and descriptions in natural language.",
        "The models proposed are probabilistic, allowing for the production of different candidate hypotheses and ranking them accordingly.",
        "The system makes minimal assumptions about both the natural language descriptions and human whole-body motions, requiring minimal preprocessing and no explicit motion segmentation into motion or action primitives or clusters thereof a-priori.",
        "Each model uses a distributed representation, which is shared for all types of motions.",
        "The system is capable of generating rich and detailed descriptions of a large variety of different human whole-body motions.",
        "The system is also capable of generating a similarly large variety of realistic human whole-body motion given a description thereof in natural language.",
        "The performance of the system can be quantified and reported using the Bleu score, which is well-known and frequently used in the context of machine translation.",
        "Each model successfully learns distributed and semantically meaningful latent representations of the given input to produce the desired output.",
        "A limitation of the proposed system is that the input sequence needs to be encoded into a single vector (the context vector c), which becomes especially problematic as the sequence length increases.",
        "Integrating attention mechanisms or hierarchical RNNs into the system would likely improve its performance.",
        "Increasing the size of the KIT Motion-Language Dataset is an important area of future work, as it would allow for the use of more complex models and reduce the risk of overfitting.",
        "Representing human whole-body motion using only the joint values of the kinematic model is insufficient, and incorporating dynamic properties of the motion as well as contact information with the environment and objects involved in the execution of the motion would be an interesting experiment.']"
    ],
    "1634": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Second-order embeddings retain a surprisingly high degree of the discriminative signal encoded by first-order embeddings.",
        "The consistency of second-order embedding performance relative to first-order embeddings suggests that the nearest neighborhood topology of an embedding space contains the lion\\'s share of the important information for these tasks, independent of the values of individual features.",
        "Non-linear transformations help in combining this information with the direct contextual signals from first-order embeddings, but how best to find that combination remains an open question.",
        "The ability to derive a single second-order representation from multiple samples, and the often superior performance achieved by doing so, suggests that this method could be used to reduce some of the variance we observe between different contextual embedding samples trained on the same data.\"']"
    ],
    "1637": [
        "The proposed technique of using low-dimensional trainable speaker embeddings can extend entirely-neural speech synthesis pipelines to multispeaker text-to-speech.",
        "The improved single-speaker model, Deep Voice 2, achieves better quality compared to the previous state-of-the-art model.",
        "The technique can be applied to train both multi-speaker Deep Voice 2 and multi-speaker Tacotron models, and the resulting systems achieve high quality text-to-speech synthesis.",
        "The speaker embedding technique can create high quality text-to-speech systems and generalize effectively to many speakers with small amounts of data.",
        "The technique has the potential to be used for future research directions such as testing the limits of the technique, exploring how many speakers can be generalized, and whether new speakers can be added to a system by fixing model parameters and solely training new speaker embeddings.",
        "The speaker embeddings may be used as a meaningful vector space for text-to-speech synthesis, similar to word embeddings.']"
    ],
    "1650": [
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Experimental results show that our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed LIDM is able to discover an effective initial policy from the underlying data distribution and is capable of revising its strategy based on an external reward using reinforcement learning.",
        "The learnt discrete latent variable interface enables the agent to perform learning using several differing paradigms.",
        "The experiments showed that the proposed LIDM is able to communicate with human subjects and outperforms previous published results.\"']"
    ],
    "1664": [
        "Adding context frames without position features has an interesting impact on the two presented tasks.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The effect of using the various feature sets in our experiments shows that the smallest feature set (prosody) works best.",
        "The features used in this work were chosen to be quite simple, leaving room for further investigation with respect to the acoustic features on the individual tasks.",
        "Speaker normalization in the form of z-scoring can lead to a loss of fine differences in the data and may affect the performance of the CNN.",
        "The model performs well just by learning from simple frame-based features, and that the performance can be increased by adding position indicating features to the input.",
        "Our model generalizes well from a speaker-dependent setup to a speaker-independent setting, yielding high accuracy for pitch accents and phrase boundaries.",
        "The presented method can be readily applied to other datasets, and is quite suitable to the task, especially given its efficiency.']"
    ],
    "1675": [
        "The \"duality\" of question answering (QA) and question generation (QG) tasks is exploited to improve both tasks.",
        "The proposed training framework improves both QA and QG on three datasets.",
        "The use of the \"duality\" as a regularization term influences the learning of QA and QG models.",
        "Simple yet effective QA and QG models are introduced, both of which are neural network based approaches.",
        "The proposed approach improves the efficiency of QA and QG models.",
        "The use of a smoothed language model for both question and answer sentences is effective.",
        "The QG model could be improved by developing more complex neural network architectures to take into account of more information about the answer sentence in the generation process.",
        "An automatic evaluation metric should be investigated to effectively measure the performance of a QG system.']"
    ],
    "1696": [
        "The authors' developed factored system results in significantly less agreement errors overall compared to pure PBMT.",
        "NMT handles all types of agreement better than both pure PBMT and factored PBMT.",
        "The authors' system produces sentences with far fewer errors and a more fluent and grammatical language, which should be helpful for post-editing.",
        "The error taxonomy developed for this research is applicable for the analysis of errors for any translation direction towards a Slavic language.",
        "The methodology can be applied to another language pair (e.g. English-Czech).",
        "Performing more controlled IAA analysis or IAA adjudication would be a promising direction for future work.",
        "Adapting the tagset further, including adding additional layers to the Accuracy branch, would be a promising direction for future work.\"]"
    ],
    "1732": [],
    "1744": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The standard variant of the bag-of-words model is often better or comparable to tf-idf, but the low-dimensional variants of doc2vec are better performers than the high-dimensional one.",
        "The top-ranked variants of word2vec correspond to the feature vector dimensionality of 50 and 75, while the other two settings of 25 and 100 underperform in almost all the experiments.",
        "The network-based model is systematically underperforming regardless of the task or evaluation metrics.",
        "The analysis of the results with respect to the criteria of the document, vocabulary, and class label set size does not reveal clear results, but there are some notable exceptions addressed next.",
        "For smaller documents, bag-of-words is the preferred model regardless of the evaluation metrics, while for larger documents, doc2vec has a slight advantage.",
        "When selecting among word2vec and doc2vec, the latter seems to be a consistently better choice except when observing accuracy on data sets with larger vocabularies and tasks with a moderate number of class labels.']"
    ],
    "1753": [
        "The model performs better than or on par with the multi-task model for most slots, with significant performance gains on slots that have shared semantics with slots in other domains.",
        "For slots that are specific to particular domains, like discount type from bus tickets, the concept tagger usually needs a larger number of training samples to reach the same level of performance.",
        "The concept tagger is able to reach within 10% of the peak performance on appointment time without the need for any in-domain training data.",
        "The model is able to ramp up performance on # seniors with a small amount of in-domain data, despite the presence of a competing slot with similar semantics (# adults) within the same domain.",
        "The concept tagger\\'s performance is worse than our multi-task baseline on pickup location.",
        "A lack of a good contextual representations for the description pickup location and the presence of a competing slot, dropoff location, might be responsible for the performance degradation observed for this slot.",
        "It might be possible to alleviate poor slot representations by fine-tuning the slot representations on small amounts of in-domain training data after starting with representations derived from pre-trained word embeddings or using contextual word embeddings.",
        "Enhancing utterance token representations with an entity linker or a knowledge base are possible extensions of this work that might enable better generalization to new entities.",
        "Exploring use of unlabeled training data from target domains with a domain adversarial loss [25] might be another interesting avenue for exploration.\"']"
    ],
    "1755": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "The Wikipedia-based approaches are effective in improving the generalization capability of NER systems, especially when a system is trained with little human-annotated data.",
        "In the rich-resource scenario, the dictionary feature approach achieves the best improvement.",
        "In both scenarios, the joint approach combining the decoding constraint approach and the post-processing approach achieves relatively robust performance among the Wikipedia-based approaches.",
        "The improvements are larger on unseen entities, and the approaches are especially useful when a system is applied to a new domain or it is trained with little training data.\"']"
    ],
    "1763": [
        "The BIST-COVINGTON model performed well on languages where official training/development sets were available, despite not using any ensemble method or custom tokenization, segmentation, or tagging.",
        "The model ranked in the top ten for LAS for 35 languages, including Arabic, Bulgarian, Buryat, Czech-PUD, Old Church Slavonic, Greek, Spanish, and Swedish.",
        "The model failed on a subset of the PUD treebanks, particularly those in Spanish, Russian, Portuguese, and Finnish.",
        "The main gap came from the Spanish, Russian, Portuguese, and Finnish PUD treebanks, which were parsed with suffixed treebanks.",
        "Our approach worked reasonably well on the set of small treebanks, despite the simplistic strategy followed.",
        "The model did not include ad-hoc dev sets for those languages as part of the final training data, which may have affected the performance.",
        "The cases where the parser did not work well were due to external causes, such as the chosen cross-treebank strategy.",
        "The model can be improved by using external word embeddings and implementing the non-monotonic version of the Covington transition system, together with approximate dynamic oracles.']"
    ],
    "1766": [
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\')",
        "using additional training data, \\'smart\\' features, and hand-crafted resources hurts rather than helps performance",
        "a support vector machine clearly outperforms other classifiers",
        "an SVM is the best choice for the given amount of training data, but with more training data, a neural network-based approach would achieve better results",
        "adding place names explicitly to our model did not yield performance improvements",
        "features designed to capture gender in a more general sense do not yield any benefit over the more specific features",
        "a possible explanation for the lack of benefit from more advanced features than n-grams is that the information is already captured by n-gram features",
        "geographical information in the text can be useful in identifying language variety",
        "features designed to capture gender in a more general sense would likely be useful for a robust, cross-dataset system"
    ],
    "1771": [
        "Deep models have not yet proven to be more effective than shallow models for text classification tasks.",
        "The use of a projection-based method for attenuating biases in word representations is effective.",
        "The performance of SCITE is still limited by the insufficiency of high-quality annotated data.",
        "A shallow-and-wide convolutional neural network at the word level is still the most effective.",
        "Increasing the depth of convolutional models with word inputs does not bring significant improvement.",
        "Deep models outperform shallow networks when the input text is encoded as a sequence of characters.",
        "A new deep model that is an adaptation of DenseNet for text inputs may be effective.\"']"
    ],
    "1790": [
        "The proposed approach achieves new state-of-the-art performance on FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distribution across domains.",
        "The Cosine Annealing Strategy combines the two methods to improve domain adaptation performance.",
        "The proposed approach uses pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The model learns to predict additional linguistic information for each token, allowing it to discover compositional features that can be exploited for error detection.",
        "POS tags, grammatical relations, and error types are the most beneficial auxiliary labels for error detection.",
        "Combining multiple auxiliary labels improves the results further.",
        "The proposed approach using auxiliary labels outperforms other multi-task approaches.",
        "Introducing an additional objective encourages the model to train more general composition functions and better word representations.']"
    ],
    "1791": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The use of dropout on both word embeddings and visual features allowed the model to generalise better, providing consistent improvements in accuracy.",
        "A novel gating component was introduced, which allows the model to actively make use of this component, predicting different gating patterns depending on the input sentence, and substantially improves the overall performance in the evaluations.",
        "The model modifications showed consistent improvements on both tasks, and the relevance assessment model was able to distinguish unsuitable image-sentence pairs on both datasets.",
        "Automated relevance detection of short textual answers to visual prompts can be performed by mapping images and sentences into the same distributed vector space, and it is a potentially useful addition for preventing off-topic responses in automated assessment systems.']"
    ],
    "1798": [
        "There are slight changes in interest classification across networks.",
        "Greater differences bound to the language: as far as our language model of MaI is concerned, pages in Russian are harder to classify than pages in English.",
        "Some of the MaIs we took for research show a strong correlation of classification results, some stand aside.",
        "The Bernoulli model is efficient for classification.",
        "Word frequencies are not as important in classification as the absence or presence of characteristic features.",
        "Common NLP techniques do not work with MaIs as they do in other NLP tasks.",
        "Objects of interest do not necessarily appear in all popular networks.",
        "There are concerns in regard to the size of the dataset, and pages with veracious content representing certain MaIs are not easy to collect."
    ],
    "1803": [
        "Our results indicate that most of the improvement comes from the first two properties.",
        "Perhaps a modified training criterion could be used to encourage the system to generalize more; in the standard setting, the system probably learns to strongly condition the lemma on the tag and avoids the risk of generating new pairs.",
        "The effect here is not due to training the system to also predict morphological tags, which is in contrast with the result of Nadejde et al. (2017) .",
        "It is likely that the two approaches are complementary, the rich information in CCG supertags could bring additional benefit to the morphological generalization that we perform.",
        "We plan to investigate this in future work.\"']"
    ],
    "1819": [
        "The random initialization of the weights has a significant impact on the test performance of the BiLSTM architecture for linguistic sequence tagging.",
        "The choice of word embeddings, optimizer, classifier, and dropout mechanism can have a high impact on the achieved performance.",
        "The use of Adam with Nesterov momentum (Nadam) and a gradient normalization threshold of 1 can result in the best performance.",
        "Variational dropout applied to the output units and recurrent units of an LSTM layer is optimal.",
        "The CRF classifier should be preferred over a softmax classifier.",
        "Multi-task learning can result in a consistent performance increase, especially for tasks that are fairly similar.",
        "The performance variance for multi-task learning is higher than for single task learning.",
        "The selection of hyperparameters has a significant impact on the achieved performance, and finding local minima with low generalization error appears more challenging than for single task learning.']"
    ],
    "1854": [
        "The use of multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our novel end-to-end model for joint slot label alignment and recognition achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The Dynamic Memory Induction Networks (DMIN) model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The extractive metrics provide further insight into the behavior of the models, and the relationships between BLEU score, extractive metrics, and content selection and generation are not as clear-cut as previously thought.",
        "The Conditional Copy system improves the BLEU score, but still far below gold, and content selection and ordering models need further research.",
        "The simple templated model is not as effective as the Conditional Copy model at BLEU, RG, and CS, and reconstruction is quite helpful for improving the joint model.']"
    ],
    "1874": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The im-Situ and MS-COCO datasets, gathered from the web, are heavily gender biased.",
        "Models trained to perform prediction on these datasets amplify the existing gender bias when evaluated on development data.",
        "RBA can significantly reduce bias amplification.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "The degree of bias amplification was related to the size of the initial bias.",
        "Structured prediction models can leverage correlations that allow them to make correct predictions even with very little underlying evidence.",
        "Our work is the first to demonstrate structured prediction models amplify bias and the first to propose methods for reducing this effect, but significant avenues for future work remain.']"
    ],
    "1896": [
        "'We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.'",
        "'A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.'",
        "'The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.'",
        "'Multi-task learning improves the translation up to 1.5 BLEU points and 2 characTER points.'",
        "'We were able to improved the performance of the POS tagging by 30% to 50% relatively.'",
        "'The largest influence on the final performance was given by the training schedule.'",
        "'By adapting the system on the individual tasks, we were able to make most use of available additional resources.'",
        "'The amount of parameter sharing defined by the architecture of the model has less influence on the final performance.'",
        "'The best performance on both tasks was achieved with a model sharing only the encoder between the tasks.'",
        "'In this work, the performance of machine translation task was improved by adopting multi-task training with other source language NLP tasks.'\"]"
    ],
    "1912": [
        "Our attention-RNN is able to provide results that are better than our implementation and comparable to the original values for both Laptops and Restaurants datasets.",
        "We think the variable sentence representation introduced by the attentional component is able to model some of the semantics encoded in these binary features.",
        "Our implemented RNN baseline performs similarly to the original models by (Liu et al., 2015) , although we remained unable to replicate their exact numbers.",
        "Regarding AESC, as shown by our decoupled results, we see all models slowly decreased their performance for aspect extraction, compared with results for AE.",
        "Our attention-RNN outperforms the baseline RNNs by a solid margin.",
        "All models tend to perform poorly for the negative (-) class.",
        "We believe this may be related to the imbalanced nature of the datasets, or due to the additional composition challenges negation involves, which seem to be critical in our dataset.",
        "For AESC, we also observed that SennaEmbeddings did not always provide top performances, being outperformed by other embeddings, even though the former were previously shown to offer the best performance for aspect extraction in all cases.\"']"
    ],
    "1925": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The proposed approach of joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Combining the corpus-based and lexicon-based approaches can improve the accuracy of emotion classifiers.",
        "Label propagation can expand an emotion lexicon in a meaningful way, and graph propagation can rely on task-specific word embeddings.",
        "Good optimization of multiple label propagation parameters can be performed using even larger number of batches and batch size.",
        "Employing GloVe embeddings as an initialization for trainable specialized vectors can improve the performance of the classifier.",
        "Introducing lexical-contrast information into the task-specialization routine using wordnets can be a reliable source for the construction of similarity graphs.']"
    ],
    "1962": [
        "The beam problem can largely be explained by the brevity problem.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The improvements over these models, including importance sampling, word classes, caching and BiRNN, were also introduced and evaluated separately.",
        "The main idea of NNLM is to approximate the probabilistic distribution of word sequences in a natural language using ANN.",
        "NNLM can be successfully applied in some NLP tasks where the goal is to map input sequences into output sequences, like speech recognition, machine translation, tagging and ect.",
        "However, language understanding is another story.",
        "All nodes of neural network in a neural network language model have parameters needed to be tunning during training, so the training of the model will become very difficult or even impossible if the model\\'s size is too large.",
        "An efficient way to enhance the performance of a neural network language model is to increase the size of model.",
        "The performance of NNLM, both perplexity and training time, is expected to be improved dramatically in this way.\"']"
    ],
    "1976": [
        "The results of our experiments suggest that learning short and long-term patterns separately is useful for the performance of an LSTM.",
        "Transferring knowledge from an FFN to an LSTM can improve the performance of an LSTM-based model.",
        "Applying dropout to the input layer can improve the performance of a network.",
        "Using pretrained FFNs as initializations for an RNN-based model can produce gains in accuracy.",
        "The proposed method of using pretrained FFNs as initializations can potentially be applied to any LSTM-based task where a 1-to-1 relation between inputs can first be modeled using an FFN."
    ],
    "1990": [
        "All four analyses support H2: linguistic dissemination was the strongest predictor of monthly frequency changes in growth words.",
        "Linguistic dissemination can be related to theories such as the FUDGE factors (Chesley and Baayen, 2010; Cook, 2010; Metcalf, 2004).",
        "Linguistic dissemination provides an example of \\'diversity of situation.\\'",
        "The effectiveness of linguistic dissemination is exemplified in pairs of semantically similar growth and decline words.",
        "Regarding H1, we generally found a positive role for social dissemination as well, although these results were not consistent across all metrics and tests.",
        "One possible explanation is the inclusion of word categories such as proper nouns in the analysis of Altmann et al. (2011) ; the adoption of such terms may rely on social dynamics more than the adoption of nonstandard terms.",
        "The lower predictive power of thread and user dissemination is also interesting and suggests that subreddits are more socially salient in terms of exposing nonstandard words to potential adopters.",
        "Limitations One limitation in the study was the exclusion of orthographic and morphological features such as affixation, which has been noted as a predictor of word growth (Kershaw et al., 2016) .",
        "Future work should incorporate these features as additional predictors.",
        "Our study also omitted borrowings, unlike prior work in word adoption that has focused on borrowings (Chesley and Baayen, 2010; Garley and Hockenmaier, 2012) .\"']"
    ],
    "2004": [
        "The proposed method achieves competitive performance compared to previous systems using less than 1% of the training data.",
        "The method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed method uses two self-supervised tasks to tackle the training data bottleneck.",
        "The method leverages Neural Architecture Search to incorporate two kinds of losses and improve the efficiency of the model.",
        "The adaptive search finds different models that are suitable for downstream tasks, improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The method learns word embeddings using k-way co-occurrences and captures the types of information captured in a k-way co-occurrence.",
        "The derived relationship between joint probability of more than two words and their embeddings is empirically validated.",
        "The method is effective for tasks that require contextual information such as analogy detection and short-text classification.']"
    ],
    "2012": [
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\")",
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "the Q-learning AMT policy achieved an average Alexa user score substantially above the average score of all teams in the Amazon Alexa Prize competition semi-finals",
        "the policy Off-policy REINFORCE obtained a high number of dialogue turns",
        "significant gains can be achieved w.r.t. both real-world user scores and number of dialogue turns",
        "Our system leverages a variety of machine learning techniques, including deep learning and reinforcement learning.",
        "We have developed a new set of deep learning models for natural language retrieval and generation, including recurrent neural networks, sequence-to-sequence models and latent variable models.",
        "Our best performing system reached an average user score of 3.15, with a minimal amount of hand-crafted states and rules and without engaging in non-conversational activities",
        "the performance is substantially above the average of all teams in the competition semi-finals\"']"
    ],
    "2028": [
        "The proposed ReCoSa model outperforms existing HRED models and their attention variants in multi-turn dialogue generation.",
        "The relevant contexts detected by the ReCoSa model are significantly coherent with humans' judgments.",
        "The use of self-attention mechanism in the ReCoSa model improves the quality of generated responses by capturing long distant dependency relations.",
        "The experimental results show that the ReCoSa model significantly outperforms existing HRED models and their attention variants on both Chinese customer services dataset and English Ubuntu dialogue dataset.",
        "The proposed ReCoSa model has the potential to improve the quality of multiturn dialogue generation by using proper detection methods, such as self-attention.",
        "Introducing topical information or considering detailed content information in the relevant contexts can further improve the quality of generated responses.\"]"
    ],
    "2032": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "However, the performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data (Section 4.4).",
        "We propose a novel nonlinear weighted automata model along with a learning algorithm inspired by the spectral learning method for classical WFA.",
        "Non-linearity can be introduced in two ways in WFA, in the termination function or in the transition maps, which directly translates into the two steps of our learning algorithm.",
        "NL-WFA can lead to models with better predictive accuracy than WFA when the number of states is limited.",
        "NL-WFA are able to capture the complex underlying structure of challenging languages (such as the Dyck language used in our experiments).",
        "NL-WFA exhibit better sample complexity when learning on data with a complex grammatical structure.",
        "We intend to investigate further the properties of NL-WFA from both the theoretical and experimental perspectives.",
        "One natural question is whether we could obtain learning guarantees for some specific classes of nonlinear functions.",
        "We believe that studying the case of more tractable nonlinear functions (e.g. polynomials) could be very insightful.\"']"
    ],
    "2057": [
        "We hope that the dataset developed in this paper can stimulate research that can further approach human agreement.\" (related to the potential for future work and the importance of the dataset)",
        "Our methods outperform the state-of-the-art by over 10% precision, though still being inferior to human agreement by 11.5%.\" (related to the effectiveness of the proposed approaches)",
        "The challenge would be here to find related problems that can be used for transfer learning.\" (related to the potential for future work and the importance of finding new approaches)",
        "We have introduced the problem of property ranking, shown the limitations of the state-of-the-art, and developed approaches that combine classical frequency-based approaches, transfer learning, and semantic similarity.\" (related to the novelty and significance of the proposed methods)",
        "IDs Wikidata articles often contain also a considerable set of ID properties, ranging from IDs stemming from the library domain (VIAF ID, GND ID, SUDOC ID) to web directory and service IDs (Facebook and Twitter username, Rotten Tomatoes ID, ...), that typically make up 10% to 40% of all properties of persons.\" (related to the prevalence and importance of ID properties in Wikidata)",
        "As all semantics in Wikidata is derived by consens, and the data model has virtually no hard constraints, the monitoring of descriptions, intended use and actual use of properties will pose a continuous challenge.\" (related to the ongoing nature of the challenge and the need for continuous monitoring)']"
    ],
    "2075": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptive Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "Incorporating additional architectures does not always improve the performance, and not always become better even if it is a sophisticated architecture.",
        "The simple structure of IOG allows it to be incorporated in any RNN language models, and the experimental results demonstrate that IOG improved the performance of several different settings of RNN language models.",
        "The gate mechanism of IOG can refine the output of an RNN language model by encouraging outputting phrasal verbs such as \"go after\".",
        "The experimental results indicate that IOG can be used with other techniques such as ensemble.']"
    ],
    "2090": [
        "We addressed the problem of limited training data for dialog systems by using sentence selection based on word embeddings and extrinsic evaluation with MAP to select effective models.",
        "Our approach can achieve competitive performance compared to previous systems trained on the full dataset, using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The accuracy of our model on completely new questions is only a few points lower than the accuracy on the main set of questions.",
        "Our manual evaluation shows that the seq2seq model can learn from CQA-derived data, producing accurate answers when answering forum questions.",
        "Our model selection is more accurate than using the loss of the seq2seq model, and performs much better than BLEU.",
        "We plan to explore new methods for selecting data to improve the overall system accuracy further.",
        "We want to experiment with other languages such as Arabic and explore adversarial dialog training and evaluation.\"']"
    ],
    "2093": [
        "Our approach significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Using larger numbers of visemes leads to superior performance in word recognition.",
        "There is no apparent pattern through these pairings, but all speakers are visually unique and how difficult finding a set of cross-talker viseme sets will be when different phonemes require alternative grouping arrangements for each individual.",
        "The conventional wisdom that visemes are needed for lip-reading is not borne out by these experiments.",
        "Larger numbers of visemes outperform smaller numbers, but the curves in Figure 2 are far from monotonic.",
        "The optimal number of visual units for each speaker is not related to any of the conventional viseme definitions, nor is the number of phonemes.",
        "For large numbers of visemes, we are close to phonetic recognition, but we run the risk of visual units which are not visually very distinctive.\"']"
    ],
    "2142": [
        "The proposed method outperforms the simple projection baseline using fast-align on most languages.",
        "The method achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The use of multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "The proposed end-to-end model for joint slot label alignment and recognition does not require external label projection.",
        "The method utilizes pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "The approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The use of cosine annealing strategy combined with clustering promotion mechanism improves the domain adaption performance.",
        "The proposed method reduces the noise of pseudo labels to improve the domain adaption performance in the future.']"
    ],
    "2146": [
        "The proposed method for joint part-of-speech tagging and dependency parsing improves over previous work on these tasks.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Including polysemous relatives in the model improves the accuracy of the system.",
        "The method is effective for a reasonable amount of texts, but may require more training data for optimal performance.",
        "The automatic morphosyntactic annotation may lead to errors, especially in noisy web text.",
        "The need for large corpora is obvious when using only monosemous relatives, but the strategy presented in this paper reaches optimum performance with a reasonable amount of texts.",
        "The accuracy of the system is limited by the quality of the training data and the evaluation environment.']"
    ],
    "2156": [
        "The proposed model outperforms benchmark models across different datasets.",
        "The approach uses a chunking policy network for machine reading comprehension, which enables the model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "The model adds a recurrent mechanism to allow information to flow across segments, enabling the model to have knowledge beyond the current segment when selecting answers.",
        "The approach achieves state-of-the-art performance on FewRel 2.0 dataset.",
        "The model incorporates word-level information by using word vectors as the targets in the training process.",
        "The encoder is an RNN, which explicitly utilizes the word order information.",
        "The decoder predicts all words in a sentence independently, leveraging the benefits from both log-linear models and RNN-based models.",
        "The model uses a suite of techniques for improving context-based unsupervised sentence representation learning, including learning by inferring subsequent contiguous words, mean+max pooling, and tying word vectors with word prediction.']"
    ],
    "2180": [
        "The performance of a language model can be improved by learning a mapping from action-oriented features to visual entities.",
        "Code-switched data corpora exhibit different linguistic characteristics compared to standard monolingual corpora.",
        "The difficulty in collecting code-switched data may contribute to the gains made by our language model.",
        "The SEAME corpus has a high percentage of code-switched bigrams with low counts, which can make it difficult for a language model built from this data to capture context across code-switching boundaries.",
        "The DLM construction in this work discards context across code-switching boundaries in favor of a simpler model.",
        "The unigram distribution of the SEAME corpus is significantly different from that of a standard monolingual corpus, with less-frequent unigrams being more difficult to capture using standard n-gram models.",
        "The DLM partially compensates for this by emulating a \"class-based language model\" using the only class information readily available in the data.",
        "The DLM outperforms the mixed LM by a significant margin when less frequent words appear at switching points.",
        "The DLM performs on par with or slightly worse than the mixed LM within long stretches of monolingual text.",
        "The DLM tends to show improved performance at less frequent switching points and within long stretches of monolingual text.']"
    ],
    "2190": [
        "The model achieves good results on the Croatian language, but not as good as for English.",
        "There is a difference in the performance of models for English and Slavic languages, with Slavic languages being worse.",
        "The differences in morphology between English and Slavic languages affect the results of word embeddings modeling.",
        "Models are better tailored to the English language morphology and have a strict word ordering in sentences.",
        "The incorporation of specificities from Slavic languages into models could be advisable for better modeling of complex morphological structures.",
        "Corpora preprocessing, such as stemming or lemmatization procedures, could also affect word embeddings.",
        "Future research should focus on exploring which Slavic languages specificities to incorporate into models and experimenting with tree structure of sentence during training.']"
    ],
    "2206": [
        "The ConMask model achieves good performance on KGC tasks, as shown by the results in Table 4.",
        "The model successfully ranks the correct entities within the top-3 results in most cases.",
        "The model has some limitations and room for improvement, such as the issue with entities that are similar to the given relationships.",
        "The model can be improved by applying a filter to modify the list of predicted target entities so that entities that are same as the relationship will be rearranged.",
        "The model outperforms other KGC models on metrics such as Mean Rank and MRR.']"
    ],
    "2235": [
        "The current state-of-the-art models still underperform human beings by a large margin on the dataset.",
        "There is considerable room for improvement in several directions, including opinion recognition, cross-sentence reasoning, and multi-document summarization.",
        "The simple paragraph selection strategy employed by baseline systems results in great degradation of the system performance.",
        "The state-of-the-art models formulate reading comprehension as a span selection task, but human beings actually summarize answers with their own comprehension.",
        "The first release of the dataset is far from perfection and leaves much room for improvement, such as annotating opinion tags for description and entity questions.",
        "Proposing new algorithms and models is necessary to tackle real-world reading comprehension problems.",
        "DuReader has three advantages over previous MRC datasets: data sources, question types, and scale.",
        "The dataset is freely available, and a shared competition is organized to encourage the exploration of more models.']"
    ],
    "2250": [
        "The current version of the LiveMedQA system has low performance due to several factors, including misclassification of question types and inaccurate matches of contents.",
        "The system is sensitive to the error generated by the CNN model, which can lead to issues such as overfitting and random behavior on rare classes.",
        "The system introduces errors due to inaccurate matches of contents using hard match or BM25 searcher.",
        "The system prefers candidates from the knowledge base over online sources, leading to a higher likelihood of returning wrong candidates as final results.",
        "There is a significant distribution shift of target labels in the test dataset, with majority of them being medication-related questions, including interaction, side effects, and contraindications.",
        "The system assumes that only one entity exists per question, which can lead to failure in certain types of questions.",
        "The system is optimized for consumer health questions and includes several novel features such as a question type/focus analyzer, domain-specific knowledge base, and structure-aware searcher.']"
    ],
    "2352": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We paid special attention to preventing a catastrophic forgetting of the general language after a model fine-tuning on the user devices.",
        "Our experiments showed that the performance of an initial model of the general English on user data can be improved significantly almost without a performance degradation on the standard English training data.",
        "A combination of on-device training with random rehearsal and server-side model averaging provides the best performance for such distributed finetuning.",
        "We found that a combination of on-device training with random rehearsal and server-side model averaging provides the best performance for such distributed finetuning.",
        "Our method has a reasonable level of differential privacy compared to other solutions.",
        "We still have to note that we provided an empirical estimation of differential privacy which holds with some high probability but not almost surely.\"']"
    ],
    "2380": [
        "Machine learning models of language toxicity such as the GP engine, are effectively able to estimate aggressiveness in written comments.",
        "Adversarial manipulation of the text can deceive these models, increasing their false negative rate.",
        "Preprocessing the comment can alleviate these attacks by restoring its standard sequence and/or polarity representation.",
        "Removing negated toxic predicates or replacing them with affirmative synonyms can restore the correct toxicity.",
        "Our methods can be extended to other Latin-based languages by training their respective word-or character-level n-grams machine learning models coupled with filters of obfuscation-prone customised vocabulary.",
        "Exploring feature spaces derived from conversational networks may improve the effectiveness of our methods.",
        "Ensuring aggression-free, respectful and opinionated online discussions would require a pipeline of text processors working on different levels of abstraction.",
        "Robust toxicity detection will have significant implications for casual, legal or political issues in online communities.\"']"
    ],
    "2388": [
        "Our approaches of LKH-GTSP and Connection Density both have polynomial and approximately similar time complexities.",
        "EARL with either Connection Density or LKH-GTSP can process a question in a few hundred milliseconds on a standard desktop computer on average.",
        "The current approach does not tackle questions with hidden relations, such as \"How many shows does HBO have?\".",
        "Our system is not restricted to a particular knowledge graph and can be applied to other knowledge graphs.",
        "The system can only link taikonaut to dbr:Astronaut, but additional information cannot be captured.",
        "EARL uses synonyms via the grammar inflection forms to tackle the problem of the \"lexical gap\" to a great extent.']"
    ],
    "2390": [
        "The proposed approach can extract keyphrases in any environment, including news articles.",
        "The EmbedRank keyphrase extraction method is fast and enables real-time computation and visualization.",
        "The disjoint nature of the EmbedRank keyphrases makes them highly readable and creates a succinct summary of the original article.",
        "By performing the analysis at the phrase level, EmbedRank opens up the possibility of grouping candidates with keyphrases before presenting them to the user.",
        "The grouping counters the over-generation problem.",
        "The proposed methods are entirely unsupervised, corpus-independent, and only require the current document itself, rather than the entire corpus.",
        "The methods depart from traditional methods for keyphrase extraction based on graph representations of the input text and fully embrace sentence embeddings and their ability to model informativeness and diversity.",
        "EmbedRank can be implemented on top of any underlying document embeddings, provided that these embeddings can encode documents of arbitrary length.",
        "The results obtained with Doc2Vec and Sent2Vec show that EmbedRank based on Sent2Vec consistently improves the state of the art, especially on short and medium-length documents.",
        "A user study shows that users appreciate diversity of keyphrases and raises questions on the reliability of evaluations of keyphrase extraction systems based on F-score.']"
    ],
    "2403": [
        "The Araneum Maximum corpus outperforms the Russian National Corpus (RNC) in semantic similarity evaluation settings.",
        "The Araneum-based model is superior to the RNC in evaluating associative, topical, and hyponymic relations between words.",
        "The performance of the models improves with increasing the size of the training data, but the improvement slows down after the first 100 million words.",
        "The RNC saturates faster than the Araneum in terms of performance improvement.",
        "The semantic relatedness test sets continue to improve even after the point where the models saturate.",
        "The error analysis reveals typical classes of errors and differences between the RNC and Araneum models.",
        "Further research is needed to study how different the RuSSE test sets are from the RuSimLex999 and whether the findings hold for other languages and types of intrinsic evaluation.']"
    ],
    "2404": [
        "Our approach does not exploit expensive-to-build language-specific resources.",
        "Instead of using expensive-to-build language-specific resources, our models require only large corpora for both input languages and a small set of word translation pairs.",
        "As few as 2000 word translation pairs is already enough to obtain high-quality translation matrices, leading to stable cross-lingual STS performance.",
        "The proposed CL STS model exhibits competitive performance as well as stability across different language pairs, including the pair with Croatian as an under-resourced language.",
        "Our resourcelight method performs on par with state-of-the-art models for respective tasks, which are, without exception, much more complex and resource-intensive.",
        "We intend to investigate methods for constructing multilingual semantic spaces that either require no (or fewer) word translation pairs or produce higher quality mappings than the linear translation matrix model.",
        "We intend to build CL STS models for other language pairs, to allow for wider adoption of CL STS in various tasks.",
        "We intend to evaluate the CL STS models in other extrinsic tasks such as cross-lingual document and passage retrieval or cross-lingual text classification.\"']"
    ],
    "2424": [
        "The proposed method achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic Memory Induction Networks (DMIN) can be a learning mechanism more general than what has been used here for few-shot learning.",
        "The approach of combining similarity entropy minimization and adversarial distribution alignment improves the performance of few-shot classification.",
        "The use of pseudo-labeled target-domain data and labeled source-domain data can improve the domain adaption performance.",
        "Bagging sampling technique can provide meaningful diversity to the training data, improving the performance of few-shot classification.",
        "The performance of the system is highly dependent on the use case, and different systems may excel in different scenarios.",
        "Deep learning models, such as CNN and MLP, can also be applied to solve the problem of few-shot classification.",
        "Random forests constructed just on character distributions of values and entropy of attributes provide remarkable results in many cases.",
        "The performance of the system can be improved by exploring a combination of bagging and class imbalance resampling strategies.",
        "Introducing an equivalent of bagging for attribute names may lead to improved performance.",
        "The variance in sizes of data sources and how well each semantic label is represented in the training data can affect the performance of the system.']"
    ],
    "2432": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The performance of SCITE is still limited by the insufficiency of high-quality annotated data.",
        "DRAU can attend to multiple targets and separate background and target objects.",
        "DRAU can predict nonexisting object(s) and attend to the relative location required to answer the question based on textual and visual attention maps.",
        "The recurrent attention mechanism in RAU helps guide the textual and visual attention and is beneficial in dual attention VQA models.",
        "Substituting the visual attention mechanism in other networks improves their performance.']"
    ],
    "2433": [
        "The brevity problem in NMT can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The beam problem in NMT can largely be explained by the brevity problem.",
        "The locally-normalized structure of the model can result in the brevity problem.",
        "The solution to the brevity problem is a very limited form of globally-normalized models for NMT.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized models for NMT.",
        "The more general problem of label bias can be solved by using globally-normalized models for NMT.",
        "The more general solution of globally-normalized models for NMT can be applied to solve the brevity problem.",
        "The locally-normalized structure of the model can result in the brevity problem, which can be solved by using globally-normalized models for NMT.']"
    ],
    "2443": [
        "Our proposed factored system for English-to-Croatian results in significantly less agreement errors compared to pure PBMT.",
        "NMT handles all types of agreement better than both pure PBMT and factored PBMT.",
        "Our NMT system produces sentences with far fewer errors and more fluent and grammatical output, which should be helpful for post-editing.",
        "The error taxonomy developed for this research can be applied for the analysis of errors for any translation direction towards a Slavic language.",
        "The current version of the tagset has been demonstrated to be informative when comparing PBMT to factored PBMT, but NMT has shown itself to produce language that is so fluent that the fine-grained hierarchy in the Fluency branch is of little use.",
        "The most common error type in the NMT output is Mistranslation, which covers both lexical selection and translation of grammatical properties.",
        "Adding additional layers to the Accuracy branch would be a promising direction to follow for an even more nuanced analysis of errors for NMT.']"
    ],
    "2446": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Given the existing empirical findings on the importance of emotion for abstract concepts, this result suggests that direct emotional experiences are necessary for grounding abstract concepts.",
        "On the other hand, some other embodied theories such as WAT theory have argued that social experiences also play an important role in representation of abstract concepts.",
        "The analysis presented in this paper is not comprehensive and has some limitations.",
        "Our analysis is limited within a small set of vocabulary words. To generalize and refine the findings presented in this paper, we have to evaluate a much larger set of vocabulary words that are not included in Binder et al.\\'s dataset.\"']"
    ],
    "2453": [
        "RNN-based autoencoder models (e.g., RNN-AE and RNN-VAE) do not contain sufficient information about the entire sentence in their final hidden state.",
        "We proposed RNN-SVAE to overcome this limitation.",
        "To consider the information of words in the sentence, we constructed a document information vector by combining word vectors of input sentence using attention information.",
        "The proposed RNN-SVAE was verified through three NLP tasks: language modeling, missing word imputation, and paraphrase identification.",
        "RNN-SVAE achieved higher performance than RNN-AE and RNN-VAE for all tasks requiring global latent meaning of the input sentence.",
        "The only exception is missing word imputation for a very short sentence, which does not significantly depend on the global semantic information.",
        "There are some limitations of the current study, such as assuming a specific prior distribution and the risk of learning a model that is far from the actual data distribution.",
        "Using the Bi-RNN structure to find the weight of a word that is not biased on one side of the sentence is necessary for applying RNN-SVAE to one-directional RNN structures.\"']"
    ],
    "2468": [
        "The brevity problem in sequence-to-sequence models can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The beam problem can be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "The accuracy of the model is better than MLP in these experiments.",
        "The model correctly predicted 2 out of 3 genres, but this is considered a misclassification.",
        "The low accuracy can be explained by the multi-label nature of the problem in this paper.",
        "The models appeared to be on the conservative side when predicting genres.",
        "The N o genre ratio confirmed that over 5% of the reviews for the KNN model and over 8% of the reviews for the MLP model did not receive any predicted genre.",
        "When looking at the individual genres for all reviews, the number of wrong predictions are very low, which is promising.",
        "By only looking at text reviews of a movie, there is enough information to predict its genre with an accuracy of 0.554.",
        "This result implies that movie reviews carry latent information about genres.",
        "The complexity of doing prediction on multi-label problems is demonstrated in this paper, both in implementation and data processing but also when it comes to evaluation.']"
    ],
    "2505": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The current WSD network falls short when compared with state-of-the-art WSD algorithms, due to an important factor in the BLSTM network.",
        "Pre-training sense embeddings using a method proposed by Chen et al. [12] showed considerable improvements in the results of some ambiguous words.",
        "The single WSD BLSTM network is language and domain independent, and can be applied to resource-poor languages (or domains) as well.",
        "The network can be evaluated on MSH WSD dataset 8 prepared by National Library of Medicine 9 (NLM).",
        "Considering that for many senses we have at least one (lexically) unambiguous word representing that sense, the network can also be experimented with unsupervised (pre-)training.']"
    ],
    "2506": [
        "The word embeddings partitioning in WESPAD can improve recall but degrade precision. (Supported by experimental results)",
        "The feature ablation study shows that we_distortion and we_partitioning feature sets have the highest impact on F1-measure, with we_partitioning performing better than we_distortion in terms of precision.",
        "The number of positive examples in the training dataset has a considerable impact on the relative improvement of WESPAD in different datasets. (Specifically, WESPAD performs better in PHM2017 with a smaller number of positive examples.)",
        "WESPAD does not require manual feature engineering and can be trained with a relatively small number of positive training examples, making it a valuable tool for extending health monitoring over social data to new diseases and conditions.']"
    ],
    "2525": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Our approach allows for joint disambiguation of entities and relations.",
        "We are able to bridge over implicit relations.",
        "The approach is easy to port to new languages.",
        "The system can be improved over time through permanent system refinement.",
        "The approach is robust to malformed questions and keyword questions.",
        "The identification of relations relies on a dictionary, which could be improved in the future.\"']"
    ],
    "2536": [
        "Many NLP datasets contain annotation artifacts that can inflate model performance.",
        "Supervised models rely heavily on annotation artifacts, particularly in the tendency of some words to serve as prototypical hypernyms.",
        "Annotation biases are prevalent in NLI datasets and can be exploited by models for gaming the benchmark.",
        "The performance of state-of-the-art NLI models can drop drastically when simple adversarial sentences are introduced in the evidence.",
        "The release of the Hard SNLI and MultiNLI test sets provides an opportunity to evaluate NLI models more accurately.",
        "The development of additional challenging benchmarks is encouraged to expose the true performance levels of state-of-the-art NLI models.']"
    ],
    "2543": [
        "The proposed algorithm outperforms the original online algorithm and is comparable in accuracy to batch learning.",
        "The proposed algorithm has a constant calculation time for each step, regardless of the amount of training data.",
        "The algorithm is scalable and can be used for long-term online learning of spatial concepts in large-scale environments.",
        "The algorithm can be extended to learn the topological structure of places and facilitate navigation tasks with human-robot linguistic interactions.",
        "The proposed method can be integrated with other probabilistic models to form a large-scale cognitive model for general-purpose autonomous intelligent robots.",
        "The approach to online learning is extensively useful and can be applied to various other Bayesian models.']"
    ],
    "2558": [
        "The proposed pipeline is one of many possible approaches to the task, and other approaches may be equally valid or potentially better performing.",
        "Applying natural logic inference (Angeli and Manning, 2014) or recent advances in neural theorem proving (Rockt\u00e4schel and Riedel, 2017) could be interesting to test.",
        "Combining question generation (Heilman and Smith, 2010) with a question answering model such as BiDAF (Seo et al., 2016) may be a useful approach.",
        "The sentence-level evidence annotation in the dataset will help develop models that select and attend to relevant information from multiple documents and non-contiguous passages.",
        "The FEVER dataset can be used for claim extraction, generating short concise textual facts from longer encyclopedic texts.",
        "The task of claim extraction can be extended to include the extraction of multiple claims from a single sentence or passage.",
        "The proposed method is not limited to using Wikipedia as the source of information, and systems developed on the dataset should be portable to different textual sources.']"
    ],
    "2563": [
        "The proposed Hybrid Multi-Aspects (HMA) model outperforms other models in the SemEval-2018 Task 11 with an accuracy of 84.13%.",
        "Adding a partial matching feature to the model can improve the accuracy by 1%-2%.",
        "The use of word stemming can help identify words with similar meanings and improve the model's performance.",
        "The model's performance on yes/no questions is better than other question types, indicating that it can handle negation and deeper understanding.",
        "The model's accuracy on reading comprehension tasks is improved by using a human-written title rather than a machine-generated one.",
        "Future work could focus on incorporating external knowledge into the model and solving questions that are less likely to be answered by statistical data.\"]"
    ],
    "2589": [
        "The proposed mechanism for extracting distant supervision data achieves similar performance as the HypeNET network architecture with a simple fast-Text model.",
        "The main difference between the two architectures is the mechanism of producing high-dimensional representations, with HypeNET using LSTMs to maintain dynamic context, and fastText using a fixed window size for ngrams.",
        "Dynamic-length context modeling did not bring any gains.",
        "The effect of grouping supports and satellites nodes features for various relations is evaluated, providing a solid ground for building RE systems for more relations.",
        "There are obvious extensions to the current approach, such as using an ensemble-based method for grouping supports, and reducing sources of noise.",
        "The system should be generalized to cover a large number of relations, and the main obstacle is combining the results of multiple RE systems into a single classifier.",
        "Distant supervision as a method introduces some errors due to not all sentences expressing the fact they mention.",
        "Manual annotations can be used to assist or replace distant supervision.']"
    ],
    "2610": [
        "The proposed approach achieves state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The Cosine Annealing Strategy combines the two methods to improve domain adaptation.",
        "The approach uses pseudo-labeled target-domain data and labeled source-domain data to train the few-shot classifier.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "RNNs trained on a generic language-modeling task can predict long-distance number agreement with high accuracy, even in nonsense sentences and across multiple languages.",
        "The constructions considered are quite infrequent and vary in context, indicating that RNNs are not simply memorizing frequent morphosyntactic sequences.",
        "LM-trained RNNs can construct abstract grammatical representations of their input, suggesting that the input contains enough information to trigger syntactic learning in a system.",
        "The study plans to adapt methods to inspect information flow across RNN states and expand the empirical investigation by focusing on other long-distance phenomena.",
        "The approach may be useful for probing more sophisticated syntactic capabilities and shedding light on the theoretical analysis of underlying linguistic structures.']"
    ],
    "2675": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The proposed method uses a self-attentive BiLSTM-CRF-based solution for causality extraction.",
        "The method introduces a multihead self-attention mechanism to learn the dependencies between cause and effect.",
        "The approach is based on a sequence tagging problem, and the proposed method delivers a self-attentive BiLSTM-CRF-based solution for the causality extraction.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "The authors plan to develop annotated datasets from multiple sources based on existing datasets and their causality tagging scheme.",
        "The authors plan to combine their method with distant supervision and reinforcement learning to achieve better performance without having to build a high-quality annotated corpus for causality extraction.']"
    ],
    "2694": [
        "The proposed methods can forecast both the presence and intensity of hostility in Instagram comments with high accuracy.",
        "The methods can distinguish between posts that will receive many hostile comments and those that will receive few or none, prioritizing specific posts for intervention.",
        "The proposed approach can be used to manage moderation queues and improve existing comment management features, such as offensive comment filtering and comment controls.",
        "The methods can identify predictors of future hostility, such as the post's author has received hostile comments in the past, the use of user-directed profanity, the number of distinct users participating in a conversation, and trends in hostility thus far in the conversation.",
        "The proposed approach can be extended to other platforms, such as photo-sharing sites like Instagram, by using image classification algorithms to identify image attributes that predict hostile comments.",
        "The methods can be improved by inferring more detailed user attributes (e.g., age, gender, ethnicity, etc.) to provide additional context for forecasting.",
        "The proposed approach can be used to extract insights into the effectiveness of de-escalation strategies, such as the effectiveness of different responses to hostile comments."
    ],
    "2714": [
        "Minimizing a convex relaxation of the CSLS loss significantly improves the quality of bilingual word vector alignment.",
        "Removing the orthogonality constraint does not degrade the quality of the aligned vectors.",
        "The number of nearest neighbors has an impact on the performance of the aligned vectors, with 10 nearest neighbors being optimal.",
        "The CSLS criterion and the RCSLS loss are sensitive to the number of nearest neighbors.",
        "Replacing CSLS by NN leads to a smaller drop for RCSLS than for competitors, suggesting that RCSLS transfers some local information encoded in the CSLS criterion to the dot product.",
        "The performance of all methods drops when exact string matches are removed.']"
    ],
    "2750": [
        "The model does not learn to examine the relation between the answer span and the relation subject unless the training data requires it.",
        "Other models may show different patterns of strength and weakness.",
        "It is possible to adapt a QA model to the slot filling task without having to understand and modify its internal structure and implementation.",
        "The unmodified BiDAF model is almost as effective as the Levy et al. (2017) model in terms of zero-shot F1 on the original UWRE test set.",
        "FastQA's performance is substantially worse than the unmodified BiDAF model, but it is extremely accurate on the challenge test set.",
        "The unmodified BiDAF and FastQA architectures have complementary strengths on the two evaluations.",
        "Standard QA models and data can be easily reused on the slot-filling task using some straightforward data pre-processing. These recycled models were reasonably effective in the reduced data regime and robust on a new test set containing challenging examples.\"]"
    ],
    "2760": [
        "The incorporation of transitivity constraints in inference is widely used, but this work proposes to incorporate these constraints in the learning phase as well.",
        "The proposed method is based on Chang et al. (2012) \\'s constraint-driven learning (CoDL), which is the same as the intermediate System 7 in Table 2.",
        "The current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data and learn in the incidental supervision framework.",
        "The P used in this work is TBAQ, where only 12% of the edges are annotated.",
        "Every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down.",
        "A more general question is to find out the optimal ratio of graph annotations.",
        "Partial annotation is only one type of annotation imperfection, and if the annotation is noisy, we can alter the hard constraints derived from P and use soft regularization terms.",
        "If the annotation is for a different but relevant task, we can formulate corresponding constraints to connect that different task to the task at hand.",
        "Being able to learn from these \"indirect\" signals is appealing because indirect signals are usually order of magnitudes larger than datasets dedicated to a single task.",
        "Temporal relation (TempRel) extraction is important but TempRel annotation is labor intensive.",
        "While fully annotated datasets (F) are relatively small, there exist more datasets with partial annotations (P).",
        "This work provides the first investigation of learning from both types of datasets, and this preliminary study already shows promise.']"
    ],
    "2761": [
        "Our approach for objective artifact assessment using spoofing countermeasures is reference-free and text-independent, allowing for comparison of different VC systems in terms of their artifacts.",
        "The proposed approach has clear potential as a convenient and complementary tool to automatically assess the amount of audible and non-audible speech artifacts.",
        "The obtained results indicate that waveform filtering, SuperVP, and Griffin-Lim methods have relatively less artifacts, and these methods should be included in future ASVspoof datasets.",
        "Perceptually convincing VC samples based on Wavenet have detectable artifacts, implying that the current best VC samples may fool human ears but not necessarily the CM systems.",
        "There is no system that would perfectly fool both humans and CM systems yet, providing an opportunity for future research to improve the robustness of anti-spoofing countermeasures.",
        "It would be interesting to compare the performance of our approach with standard objective artifact measures used in assessing speech codecs and speech enhancement methods, and to other spoofing countermeasure frontends besides CQCCs.",
        "One-class approaches that require human training speech only may provide a stronger correlation to the MOS scores, and it would be interesting to revisit them in the context of artifact assessment.']"
    ],
    "2771": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The beam problem can be alleviated or eliminated by solving label bias in general.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized model.",
        "Solving the brevity problem leads to improvement in BLEU gains, but there may be remaining room for improvement by solving label bias in general.",
        "Our method for learning the parameters of the corrections to the model is helpful and easy.",
        "The sentiment modification task is not well-suited for evaluating style transfer.",
        "The BST model preserves meaning worse than the baseline, but with a mild hit in the style transfer accuracy.",
        "Our novel approach to the task of style transfer with non-parallel text aids grounding the meaning of the sentences and weakening the style attributes.",
        "Our model outperforms the baseline in all the experiments of fluency, and in the experiments for meaning preservation in generated sentences of gender and political slant.",
        "The technique of learning a latent content representation using machine translation techniques is suitable not just for style transfer, but for enforcing style, and removing style too.']"
    ],
    "2777": [
        "The use of convolutional neural networks (CNNs) architectures and biomedical word embeddings can improve the automatic categorization of sentences relevant to adverse drug reactions (ADRs) in case reports present in the biomedical literature.",
        "The ADE corpus, which consists of sentences coming from 2972 MEDLINE case reports labelled by trained annotators, contains duplications that can lead to overoptimistic performance estimates if not properly de-duplicated.",
        "Using biomedical word embeddings, as opposed to general purpose word embeddings, can improve the performance of the algorithm for sentence classification.",
        "The architecture proposed by Huynh outperformed the architecture proposed by Hughes in this task and dataset in every metric.",
        "Inter annotator agreement (IAA) is a measure of the potential noise in the inputs of human annotators, and it can be informative of the potential noise in the inputs used to build the dataset.",
        "The IAAs for partial matches of adverse events ranged between 0.77 and 0.80, indicating that aiming for near perfect predictions may be unrealistic due to considerable disagreement between human annotators.",
        "Sentence de-duplication, pre-processing, choice of word embeddings, and neural network architectures are important factors that can greatly influence the performance of the algorithms performing these tasks.",
        "Future work could include the use of exhaustive, grid-based or reinforcement-learning based search for more optimal CNN architectures, as well as the evaluation of architectures other than CNNs.']"
    ],
    "2821": [
        "The proposed method outperforms benchmark models across different datasets.",
        "The approach enables a model to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "The recurrent mechanism allows information to flow across segments, enabling the model to have knowledge beyond the current segment when selecting answers.",
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The time complexity of decoding algorithm is O(n^2), which is more efficient than graph-based parsers.",
        "Top-down Parsing allows for more efficient parsing and captures information from the whole sentence and all previously derived subtrees.",
        "Experimental results show the effectiveness of the proposed parser across 20 languages and achieve state-of-the-art performance on 21 corpora.",
        "There are potential directions for future work, such as exploring reinforcement learning approaches to learn an optimal order for the children of head words.']"
    ],
    "2829": [
        "Our algorithm achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our algorithm builds on advantage actor-critic learning for an interactive translation process with a human in the loop.",
        "The advantage over previously presented algorithms for interactive-predictive NMT is the low human effort for producing feedback, even further reduced by an active learning strategy to request feedback only for situations where the actor is uncertain.",
        "We showcased the success of BIP-NMT with simulated feedback, with the aim of moving to real human feedback in future work.",
        "Our algorithm is in principle not limited to the application of NMT, but can furthermore be extended to other structured prediction or sequence generation tasks.\"']"
    ],
    "2835": [
        "Experimental results show that our approach achieves competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The representation extractor is used to encode unlabeled target-domain data into features, which are passed to a k-means cluster miner to generate pseudo labels.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "The lattice method is fully independent of word segmentation, yet more effective in using word information thanks to the freedom of choosing lexicon words in a context for NER disambiguation.",
        "Our approach can potentially learn to select more correct words during NER training.",
        "The quality of the lexicon may affect the accuracy of our NER model since noise words can potentially confuse NER.\"']"
    ],
    "2837": [
        "the learning rate set for training, the hidden unit selected for the activation function, the optimization criterion and the amount of dropout applied to the hidden connections all have a drastic effect on overall accuracy and training time",
        "a lower learning rate achieved the best performance in terms of convergence speed and BLEU score",
        "the V100 was able to execute more words-per-second than the P100 in all cases",
        "LSTM hidden units outperformed GRUs in all cases",
        "the amount of dropout applied on a network in all cases prevented the model from overfitting and achieve a higher accuracy",
        "the multidimensionality of hyper-parameter optimization poses a challenge in selecting the architecture design for training NN models",
        "developing optimization methods to evaluate how to best select hyper-parameters",
        "static analysis of the computational graph that represents a NN in terms of instruction operations executed and resource allocation constraints, one could derive execution performance for a given dataset without running experiments\"']"
    ],
    "2841": [
        "The two chosen multidomain approaches outperform the regular approach of uniform translation and domain-tuning. (supporting evidence: results from experiments)",
        "The parameter sharing effect discussed in Google\\'s zero-shot article would benefit domain translation. (supporting evidence: results from experiments)",
        "The translation scores even outperform domain-tuning approach, which could be explained by the same parameter sharing. (supporting evidence: results from experiments)",
        "Adding domains as an input feature can have a stronger effect on the translation scores. (supporting evidence: results from experiments)",
        "Concatenating the domain feature embedding with word embedding at each timestep improves model performance. (supporting evidence: results from experiments)",
        "The unsupervised tagging approach ensures better domain assignment to each new sentence and can efficiently incorporate new data from various small domains to fortify each of the learned \"domain\" (clusters). (supporting evidence: results from experiments)",
        "The performance of unsupervised domain tagged model indicates that there is grounds to substitute the pre-defined domain approach with automatically assigned domain approach. (supporting evidence: results from experiments)']"
    ],
    "2842": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The weak supervision and the strategy of training data construction are important to the success of the proposed learning approach.",
        "Training data construction plays a more crucial role than weak supervision.",
        "Updating the Seq2Seq model with policy-based reinforcement learning can improve the matching model.",
        "The number of response candidates affects the performance of the learned models, and our approach can still improve the performance of matching models with 2 candidates."
    ],
    "2860": [
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "Removing the memory cell degrades performance drastically, while removing the S-RNN makes little to no difference in the final performance, suggesting that the memory cell alone is largely responsible for the success of LSTMs in NLP.",
        "The additive recurrent connection in the memory cell -and not the multiplicative recurrent connections in the content layer or in the gates -is the most important computational element in an LSTM.",
        "LSTMs suffer little to no performance loss when removing the S-RNN, providing evidence that the gating mechanism is doing the heavy lifting in modeling context.",
        "The realization allows us to mathematically relate LSTMs and other gated RNNs to attention-based models.",
        "Casting an LSTM as a dynamically-computed attention mechanism enables the visualization of how context is used at every timestep, shedding light on the inner workings of the relatively opaque LSTM.\"']"
    ],
    "2867": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Our model significantly reduces repetition compared to the conventional seq2seq and its repetition rate is similar to the reference's.",
        "Our model is able to generate summaries of higher diversity with less repetition.",
        "The convolutional gated unit performs global encoding on the source side information so that the core information can be reserved and the secondary information can be filtered.\"]"
    ],
    "2879": [
        "The training data used for machine learning models can contain biases, which can be reflected in the predictions made by the model.",
        "Different parts of a machine learning system can contribute to bias, including the labeled and unlabeled datasets used to learn different parts of the model, the language resources used, and the learning method used.",
        "The use of word unigrams as a baseline system in the analysis showed small biases in gender and race, with \u2206-spreads of 0.09 to 0.2 on gender sentence pairs and less than 0.002 on race sentence pairs.",
        "The system predicted higher intensity scores on sentences with male noun phrases than on sentences with female noun phrases for certain emotion intensity prediction tasks, but the reverse was true for the task of valence prediction.",
        "The training data can contain biases in the form of unigrams associated with a particular gender or race tending to appear in tweets labeled with certain emotions.",
        "Different learning methods in combination with different language resources can accentuate, reverse, or mask the bias present in the training data to different degrees.']"
    ],
    "2887": [
        "The proposed method (LVeGs) achieves competitive performance compared to previous systems by using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The model based on gaze features and part-of-speech information achieves accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerge as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Incorporating contextual information of words and constituents is a crucial technique that can be found in most state-of-the-art approaches to parsing or POS tagging.",
        "Using a single continuous space for subtypes of all the nonterminals allows for modeling similarity between nonterminals.",
        "LVeGs can subsume latent variable grammars and compositional vector grammars as special cases.",
        "The partition function and the expectations of fine-grained production rules in GM-LVeGs can be efficiently computed using dynamic programming, making learning and inference feasible.']"
    ],
    "2896": [
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR).",
        "The performance of the method is not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "The method is able to extract Semitic roots with high accuracy.",
        "The scheme is robust and can be used to analyze the semantics of prepositions and possessives in English.",
        "The method has good interannotator agreement and can be used to provide initial supervised disambiguation results.",
        "Future work will develop methods to scale the annotation process beyond requiring highly trained experts; bring this scheme to bear on other languages; and investigate the relationship of our scheme to more structured semantic representations, which could lead to more robust models.']"
    ],
    "2897": [
        "The model based on gaze features and part-of-speech information achieves similar accuracy as the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features are the most discriminative ones for disambiguating categories.",
        "The use of a multi-label classifier trained to predict visual entities from action features can improve the performance of captioning compared to the raw features themselves.",
        "Basic entity models tend to perform poorly, especially when the entity grids are sparse and there are few meaningful entity transitions between sentences.",
        "Neural network models almost always outperform other models in coherence tasks.",
        "The sentence ordering task is not an accurate measure of coherence, as neural models often outperform the random baseline by a reasonable margin.",
        "The large number of parameters in PARSEQ may explain why it is sometimes outperformed by SENTAVG.",
        "The use of a simple and effective method like SENTAVG or PARSEQ can be beneficial for future work.",
        "Future evaluations should move away from the sentence ordering task to more accurately predict the success of systems in real-world conditions.",
        "Future annotation efforts should leverage expert raters to improve the quality of annotations, as this task is difficult for untrained workers on crowdsourcing platforms.']"
    ],
    "2967": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans\\' judgments.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "Increasing the number of attributes in features was not helpful in enhancing the prediction of the ratings.",
        "The best features were top 600 and 900 Information Gain words with the highest average accuracy, 82 percent, confidence of the classifier, 64 percent, and f-score, 90 percent.",
        "Running the test data showed that sometimes the words of the largest group or domain can dominate the selected features like info gain and may result in biased classifiers.",
        "We found that TF-IDF is not always the best metric for extracting the most salient words from the document.",
        "We plan to explore other algorithms and approaches in the future, such as using word embedding and LSTM or CNN.",
        "Adding other features such as syntactic features, the length of the reviews, and n-gram words to our analysis can help improve the accuracy.",
        "In our future work, we plan to add social media texts as new features to the rating prediction, which can tremendously help in sparse matrix situations.\"']"
    ],
    "2968": [
        "The proposed architecture uses both audio and text data to analyze sentiments and emotions, but not all data sources contain textual information. (i)",
        "Automatic speech recognition provides a potential solution for generating textual information from vocal signals. (ii)",
        "Word alignment can be easily applied to human speech, but it is difficult to align visual information with text, especially if the text only describes the video or audio. (iii)",
        "The limited amount of multimodal sentiment analysis and emotion recognition data is a key issue for current research, especially for deep models that require a large number of samples.",
        "Our model outperforms state-of-the-art methods and provides effective visualization of modality-specific features and fusion feature interpretation.']"
    ],
    "2970": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Hard-attention methods achieve better scores than Seq2Seq in all automatic evaluation metrics.",
        "Self-Attention & Max achieved the best scores in the metrics among hard-attention methods.",
        "Random Hard-Attention did not satisfy the expectation, and two hard-attention methods did not guarantee a good seed of diversity to generate a response.",
        "Using MMI tends to avoid safe responses, and such avoidance often leads to Bad responses.",
        "Self-Attention & Max using MMI sometimes led to Mediocre responses.",
        "Non-use of MMI does not show significant differences between the methods.",
        "The method based on self-attention tends to select an important vector as a seed among hidden vectors."
    ],
    "3004": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Integrating character-based word representations into a baseline CNN model for relation extraction improves the baseline to attain state-of-the-art performance.",
        "Our models are suitable candidates to serve as future baselines for more complex models in the relation extraction task.",
        "The electrocardiograms (ECG) of 99 cocaine-abusing patients were compared with the ECGs of 50 schizophrenic controls.",
        "Following Gu et al. (2016) , Li et al. (2016b) and Gu et al. (2017) , these are derived from either (i) a pair of entity mentions that has been positively classified to form a CID relation based on the document or (ii) a pair of entity mentions that co-occurs in the document, and that has been annotated as having a CID relation in a document in the training set.",
        "In an article, a pair of chemical and disease concept identifiers may have multiple entity mention pairs, expressed in different relation mentions.\"']"
    ],
    "3015": [
        "The proposed method has a positive effect on learning the encoder.",
        "The proposed method uses low-frequency words instead of high-frequency words in the training corpus.",
        "Increasing OOV improves the learning of encoder-decoder models.",
        "The proposed method excludes words that may interfere with the learning of encoder-decoder models.",
        "The proposed method selects the words that are more suitable for learning encoder models by considering contextual information.",
        "This method is effective for not only machine translation but also grammatical error correction.",
        "The study employed a symmetric matrix to express relationships between words.",
        "In future research, the method will be developed by using vocabulary obtained by designing an asymmetric matrix to incorporate syntactic relations.']"
    ],
    "3043": [
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The experiments show that our sparsifying methods have little impact on the training quality while decreasing the time to train LSTM models when compared to dense training.",
        "Our fine-grained approach allows for more aggressive sparsity settings, but is less efficient, while our coarse technique is faster, but cannot be used as aggressively.",
        "The small performance gap can be mitigated by mixing the dense training with the sparse training, at the cost of less speedup.",
        "Our sparsifying methods are tested on large-scale problems and achieve good speedup against the state-of-the-art training hardware and software.",
        "With the dense-after-sparse training method, we can train the LSTM models to the same quality as models using the regular dense training with the same number of training steps.",
        "The sparse LSTM backward propagation is 49% faster than the dense counterpart on GPUs.",
        "Experiments also demonstrate the dense after sparse training method can achieve comparable results in a shorter time span than using the pure dense training for large-scale models.\"']"
    ],
    "3075": [
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "the correlation coefficient of corpus log-frequency with a word familiarity list was 0.57 to 0.74 for English using the MRC familiarity list, and 0.45 to 0.72 for Japanese with the Amano list.",
        "the log-frequency of larger corpora was more strongly correlated with familiarity ratings than that of smaller corpora.",
        "the log-frequency was more strongly correlated with the familiarity ratings when the corpus consisted of spoken rather than written data.",
        "a log-frequency list -if obtained from a large-scale corpus-could be used for a pseudo-measurement of familiarity scores.\"']"
    ],
    "3092": [
        "The proposed recurrent chunking mechanisms enable a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "The approach outperforms benchmark models across different datasets.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "The early peak of the P600 effect is derived from the RNNG's composition function, and not from the N400 effect.",
        "The absence of an N400 effect in this analysis could be attributable to the choice of electrodes or the modality of the stimulus narrative.",
        "The model improves, but the improvement does not reach the \u03b1 = 0.002 significance threshold imposed by our Bonferroni correction.",
        "The lack of an N400 effect in the study could be due to the choice of electrodes or the modality of the stimulus narrative.",
        "The results suggest an approach to the overall modeling task that involves using both grammar and processing strategy to interpret the unified model at different times or places within the brain.\"]"
    ],
    "3102": [
        "The theoretical contribution of this paper is to show that the main bottleneck in SZO optimization can be reduced to the expected number of active features in sparse structured prediction scenarios.",
        "Using a smooth annealed criterion yields similar results as the discontinuous deterministic MAP criterion, and allows to match our experiments with our (and other existing) theory that assumes at least Lipschitz continuity of perturbed functions.",
        "Non-asymptotic convergence analysis for discontinuous SZO is an interesting open problem.",
        "Minimizing the upper bound in h gives h* = \u03b1R n(n + 4) 2 L 3 0 (N + 1), which guarantees an -accuracy on the left-hand side of eq. (15).",
        "To guarantee an -accuracy, we need N \u2265 O( Training for bandit learning is done by cold starting the models from w 0 = 0.",
        "The SFO method functions as an upper bound for the SZO methods, and among SZO methods, a clear advantage in convergence speed is observed for the former.",
        "Updates based on (SPARSE) two-point function evaluation converge fastest.']"
    ],
    "3106": [
        "The system significantly lowers the requirements for multispeaker TTS training data.",
        "Given sufficient speaker diversity in the synthesizer training set, speaker transfer quality could be significantly improved by increasing the amount of speaker encoder training data.",
        "The proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder (along with its very high inference cost).",
        "The model has learned to utilize a realistic representation of the space of speaker variation.",
        "Given sufficient training data, this could be addressed by conditioning the synthesizer on independent speaker and accent embeddings.",
        "The model is able to generate realistic speech from fictitious speakers that are dissimilar from the training set.",
        "The system significantly simplifies the training configuration of the synthesizer network compared to [10] since it does not require additional triplet or contrastive losses.",
        "Improving speaker similarity given more than a few seconds of reference speech requires a model adaptation approach as in [2] , and more recently in [5] .\"']"
    ],
    "3129": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Our approach can extract parallel sentence pairs from comparable corpora, which is a considerable resource for SMT and NMT systems.",
        "Using two different sentence encoders trained by mutual information neural estimation (MINE) could improve the flexibility and performance of our approach.",
        "Our approach is easily scalable across multiple language pairs and can be applied to a larger set of language pairs, including distant language pairs with limited parallel resources.']"
    ],
    "3137": [
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\')",
        "our approach outperforms benchmark models across different datasets",
        "the proposed RL framework outperforms the existing methods by 55% in terms of execution error",
        "the scheduled mechanism takes both generalization and data efficiency into account",
        "by utilizing an adaptive scheduling mechanism which alternates between LfD (imitation learning) and conservative policy updates, the RL agent is able to maintain a high-entropy training policy for sufficient exploration without sacrificing the learning efficiency",
        "the proposed scheduled RL algorithm is superior to both cross-entropy training and mixed-loss training",
        "our RL algorithm does not provide much improvement on the seen scenes\"']"
    ],
    "3146": [
        "The NMT system requires a sequence of unsegmented symbols and their aligned sentence translations to provide segmentation.",
        "The AUD method chosen to encode the speech input has an impact on the quality of the final segmentation.",
        "The SVAE model extracts more consistent pseudo-phone units than the HMM model, although it misses some boundaries.",
        "Shorter sequences of symbols are easier to segment.",
        "The HMM model uses more symbols to represent an utterance, while the SVAE model offers a more concise representation.",
        "The MBN SVAE is performing best on the task, with an increase in performance of about 0.8% in all cases.",
        "The attention-based segmentation technique is more robust for word boundary detection than the monolingual (Bayesian) approach.']"
    ],
    "3159": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The ConMask model has limitations and room for improvement, such as entities with similar relationships being ranked highly.",
        "The method of using a filter to modify the list of predicted target entities can be applied to improve the performance of the ConMask model.",
        "The use of a projection-based method can attenuate biases in word representations.",
        "The approach of debiasing the first (non-contextual) layer alone can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The alignment of WordNet to the foundational ontology DOLCE led to a more rigorous version of the WordNet noun taxonomy.",
        "The resulting alignment was used in the implementation of a semantic annotation framework, the FO Tagging, which showed an increase in accuracy compared to the SuperSense Tagger.",
        "The introduction of the verb alignment can help in the execution of semantic tasks involving natural language processing, such as text entailment and question answering.",
        "The DOLCE classes provide a suitable semantic representation for such tasks, and the evaluation has shown that even with a straightforward word sense disambiguation technique, the WN-DOLCE alignment is possible to annotate text with high accuracy.']"
    ],
    "3165": [
        "We propose a dictionary-guided editing networks model for generating paraphrase sentences through editing the original sentence.",
        "Our model is able to replace some words or phrases in the original sentence based on the dictionary and makes necessary changes to ensure the new sentence is grammatically correct and fluent.",
        "Experiments on the Quora and MSCOCO datasets demonstrate that the dictionary-guided editing networks significantly improves the existing generative models for paraphrase generation from scratch.",
        "The dictionary-guided editing networks can also be applied to other text generation tasks, such as the text style transfer.",
        "We find that the pair (\\'a tv\\', \\'a television\\') has larger attention scores where the decoder generates the television word.",
        "Our approach outperforms benchmark models across different datasets.\"']"
    ],
    "3187": [
        "The use of chart constraints can significantly improve parsing speed and accuracy.",
        "Chart constraints prune useless spans out more directly and completely than other pruning techniques, leading to a further boost in parsing speed.",
        "The effectiveness of chart constraints is not limited to specific grammar formalisms, but can be applied to arbitrary chart parsers, including those for grammar formalisms that describe objects other than strings.",
        "Chart constraints can be applied to parse trees, even if they are not generated using a chart-based parser.",
        "The primary challenge in implementing chart constraints is developing a high-precision tagger that identifies allowable subgraphs, which may require moving beyond LSTMs.",
        "Chart constraints can also speed up parsing algorithms that do not use charts, although the extent of the benefit may vary depending on the specific parser and grammar formalism used.']"
    ],
    "3207": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.",
        "Tagging ambiguity is mostly local while dependency parsing heavily depends on POS tags to predict syntactic structures.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "Including polarity and intensity classification as auxiliary tasks supports a fine-grained sentiment analysis and illustrates that individual modalities differ when conveying sentiment.",
        "The visual modality is weaker at conveying the intensity aspect of sentiment, while the vocal and verbal modalities are weaker at conveying the polarity aspect of sentiment.",
        "Decomposing sentiment scores into polarity and intensity provides detailed understanding on how individual modalities and multimodal information convey these two aspects of sentiment.']"
    ],
    "3237": [
        "The language modality is the most discriminative and important for learning multimodal representations.",
        "The baseline unimodal text approach outperforms the multimodal approach.",
        "The merging of modalities into a shared representation space decreases the resolution of the text domain and reduces the modeling power of the domain.",
        "The top-performing multimodal model is one that incorporates the text domain.",
        "The learned representations were quite poor when it came to their use in prediction, especially those learned using only audio and video.",
        "The representation losing the resolution of the original two domains from which the original source embedding was learned and instead being focused on learning the best representation to predict the final modality.",
        "The intermediate multimodal representations can then be used for multimodal downstream tasks, and the multimodal representations learned from the Seq2Seq modality translation method are highly informative and achieve improved performance on multimodal sentiment analysis.']"
    ],
    "3262": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Using the measure of gain can help when faced with a choice between similarly performing models on a limited budget.",
        "Examining the gain of a predictive model can help answer several important budgetary questions, such as which model will yield the highest profit at the earliest time or which model will yield the highest number of positive annotations given our budget.",
        "The same considerations can be applied to the task of creating any other linguistic resource or many other NLP tasks as long as the pipeline integrates a probabilistic model.",
        "Calculating gain can show which system would likely yield the highest evaluation scores, which could help inform their decision on which system to submit for evaluation.",
        "Performing gain calculations will not invariably undermine the results of global F-score evaluations, but it warrants more specialized consideration when it comes to model selection and external constraints such as budget.\"']"
    ],
    "3280": [
        "The proposed Distinctive-attribute Extraction (DaE) method for image captioning improves the performance of the SCN-LSTM scheme by significant margins across all metrics.",
        "DaE accurately figures out the situation that the clock is \"displayed\" over the \"window.",
        "The proposed method can be applied to other base models that use attribute to improve their performance.",
        "DaE detects that the picture is a \"store\" or a \"shop,\" and accurately figures out the situation.",
        "In (g), DaE extracts both \"sign\" and its message \"stop.",
        "In (h), there is a red stop sign next to a man. DaE extracts both \"sign\" and its message \"stop.",
        "In (i), DaE extracts the word \"frost\" that exists only in its vocabulary and does not exist in the vocabulary of SCN.",
        "In (j), DaE extracts key objects and place such as \"microwav\", \"kitchen\", \"sink\", etc.",
        "In (k), a man is standing in front of a computer monitor or laptops. DaE detects \"comput\" and \"laptop,\" which are not detected by SCN.",
        "In (l), a pair of scissors placed in a plastic packing case is taken close up. DaE extracts \"scissor\" which is the main object of the picture as the highest score.']"
    ],
    "3306": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Our proposed method can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our approach significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The best-performing ASR system suffers from false alarms despite providing the best detection performance using a duration-based metric.",
        "Investigating smoothing techniques for language tag switches to reduce these false alarms remains as future work.",
        "The most common language tag errors are orthographically identical short filler words with the same meaning both in Frisian and Dutch.",
        "The orthographically identical words induce a 2%-2.5% absolute WER difference between the ASR results with and without language tags.']"
    ],
    "3364": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "The spectral clusters exhibit virtually identical performance to the Brown clusters, both outperforming the other clustering methods.",
        "The use of the spectral method for lexicon clustering may serve to motivate the use of the spectral method for lexicon clustering.",
        "The representation produced by the spectral method may be used to produce more features or characterize the relations between the clusters.",
        "Spectral clusters and Brown clusters complement each other rather than completely overlap.",
        "The two models agree on only 61.4% of the test samples in the SRL task, implying that each of them may be better suited for different cases in the same task.\"']"
    ],
    "3385": [
        "The proposed HRL framework outperforms various interactive baselines in producing more accurate recipes while asking the user fewer questions in general.",
        "The HRL agent can be trained with real users, but it is too costly to be practical.",
        "The HRL framework can be easily generalized to resolve ambiguities in other semantic parsing tasks.",
        "The proposed method models the dependencies between subtasks and enables temporal abstractions over the state and action space.",
        "The HRL agent can be trained with real users in simulation, or fine-tuned with real users.",
        "The HRL framework can be further improved by reducing the user interaction turns.",
        "The proposed method has the potential to be applied to other semantic parsing tasks such as knowledge-based question answering.",
        "The HRL agent can be trained with real users in the loop, or with simulated users.",
        "The HRL framework can be improved by modeling real user noises in simulation.",
        "The HRL agent can be trained with crowdsourced more diverse component descriptions as user answers for training.']"
    ],
    "3391": [
        "The need for suprasentential context in human evaluation of machine translation is emphasized, as raters have difficulty discriminating between human and machine translation when evaluating sentence-level adequacy.",
        "Professional translators are more effective than crowd workers in evaluating translation quality.",
        "Document-level evaluation unveils errors that are hard or impossible to spot in sentence-level evaluation, such as mistranslation of an ambiguous word or errors related to textual cohesion and coherence.",
        "HUMAN is consistently preferred in document-level evaluation over MT, even when raters are native speakers of the source language.",
        "The availability of document-level context still has a strong impact in the fluency condition.",
        "MT tends to be more literal than HUMAN and is judged more favourably by raters in the bilingual condition due to L1 interference.']"
    ],
    "3410": [
        "The use of a graph embedding space as a target for mapping text to entities is an effective approach.",
        "Using a compositional architecture with a dynamic disambiguation mechanism improves the performance of the model.",
        "The effectiveness of the textual feature mechanism is demonstrated in every task attempted, but to different extents.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Using a five-action transition system and developing three classifiers to resolve structural, tagging, and labeling conflicts improves the accuracy of joint POS tagging and dependency parsing.",
        "The attentional mechanism of Fig. 4 handles subtle variations of each distinct sense due to polysemy.",
        "Homonymy issues can be resolved by design in the model, with each point in the target space corresponding to a well-defined unambiguous concept or synset.",
        "The best performance on the classification task came with \u03bb values between 0.50 and 0.75, i.e., by walks visiting more entity nodes than textual nodes.']"
    ],
    "3412": [
        "Our approach outperforms previous state-of-the-art models in terms of both accuracy of style transfer and quality of input-output correspondence.",
        "We have presented a two-stage joint training method to boost source-to-target and target-to-source style transfer systems using non-parallel text.",
        "Our method outperforms previous state-of-the-art models in terms of both accuracy of style transfer and quality of input-output correspondence.",
        "We plan to further investigate the use of our method on other style transfer tasks.",
        "Our approach can be used for a variety of style transfer tasks, including but not limited to text-to-text style transfer.",
        "Our method has the potential to improve the quality of automatic evaluation for this task.",
        "We are interested in designing more accurate and complete automatic evaluation for this task.\"']"
    ],
    "3419": [
        "The proposed method, PReFIL, surpasses prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL exceeds human baseline for FigureQA, but the results are more nuanced for DVQA due to OCR model variations.",
        "The visual attention mechanism learns to assign high weights to words that have corresponding visual semantics in the image.",
        "The visual grounding attention captures the dependency between the words that have strong visual semantic relatedness.",
        "The use of a sequence-to-sequence model with encoded visual dependence information strengthens the connection between the words with visual semantic relatedness, mitigating the problem of standard sequenceto-sequence models tending to forget distant history.",
        "VAG-NMT outperforms all the other methods on the IKEA dataset which has long sentences.",
        "The attention mechanism and visual shared space capture the visual dependence between the word \"tennis\" and \"racquet\".",
        "VAG-NMT translates prepositions better than Text-Only NMT.",
        "PReFIL mistranslates the verb phrase \"sticking out\" to \"springt aus\" which means \"jump out\" in German, while Text-Only NMT translates to \"streckt aus\", which is correct.']"
    ],
    "3424": [
        "Our approach outperforms sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "Our proposed algorithm significantly outperforms the existing approach of multilingual, transfer learning in low-resource neural machine translation across all the language pairs considered.",
        "The proposed approach opens new opportunities for neural machine translation, including incorporating various extra sources of data and accommodating existing and future neural machine translation systems.",
        "Our approach is a principled framework for incorporating various extra sources of data, such as source-and targetside monolingual corpora.",
        "Our approach is a generic framework that can easily accommodate existing and future neural machine translation systems.\"']"
    ],
    "3425": [
        "The use of gaze features and part-of-speech information can achieve accuracy similar to that of linguistic-based models and state-of-the-art systems, without the need for text processing.",
        "Late gaze features are the most discriminative ones for disambiguating categories.",
        "The addition of syntactic features such as POS and chunking information does not have a clear positive effect on performance.",
        "The match/partial match between words and entries in the chemical gazetteer is a good indicator for the presence of chemical entities.",
        "The use of character-level word embeddings, such as CNN-char or LSTM-char, can improve the performance of biomedical named entity recognition.",
        "There is little difference between the two approaches (CNN-based and LSTM-based character-level word embeddings) for biomedical named entity recognition.",
        "The CNN embeddings show a substantial advantage in reduced training complexity.']"
    ],
    "3438": [
        "The proposed architecture achieves state-of-the-art results for SciTail and strong results in the SNLI and MultiNLI sentence-encoding category.",
        "Our model outperforms InferSent in nearly all cases with substantially reduced confusion between classes of inferential relationships.",
        "Our approach is robust across the various categories and outperforms InferSent on antonyms and negations that require a good level of semantic abstraction.",
        "The model achieves great generalization capability and outperforms InferSent on 7 out of 10 downstream and 8 out of 10 probing tasks, and SkipThought on 8 out of 9 downstream tasks.",
        "Our model performs well across all the conducted experiments, which highlights its applicability for various NLP tasks and further demonstrates the general abstractions that it is able to pick up from the NLI training data.",
        "The success of the proposed architecture raises a number of other interesting questions, such as understanding what kind of semantic information the different layers are able to capture and how they differ from each other, and whether other architecture configurations could lead to even stronger results in NLI and other downstream tasks.']"
    ],
    "3469": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans\\' judgments.",
        "The integration of premises should be complemented by external knowledge to provide sufficient information to verify the correct hypothesis.",
        "One limitation of this work was the heavy cost of the annotation.",
        "In future research, we plan to explore a method for automatically classifying reasoning types.\"']"
    ],
    "3490": [
        "The inclusion of lexicons results in higher coverage and is part of the explanation for the improvement of DSDS.",
        "Our model benefits from the lexicon beyond its content, with OOV accuracy improving even for words not present in the lexicon.",
        "Adding lexicon information always helps, even when only 1k entries are available.",
        "Using pre-trained word embeddings benefits the tagger compared to label propagation through Europarl.",
        "DSDS is almost on par with the use of much smaller and noisier data sources.",
        "Our model outperforms DAS on four languages: Czech, French, Italian, and Spanish.']"
    ],
    "3497": [
        "Adaptation greatly improved an RNN LM\\'s word prediction accuracy.",
        "The adapted model was psycholinguistically plausible, in two senses.",
        "Using materials that teased apart lexical content from syntax, we showed that the model adapted both its lexical and its syntactic predictions.",
        "Our gradient-based updates naturally incorporate the error-driven nature of syntactic adaptation.",
        "The simplicity of our adaptation method makes it attractive for use in modeling human expectations.",
        "Since adaptive surprisal is strictly superior to non-adaptive surprisal in modeling reading times, it would be a stronger baseline in analyses that aim to demonstrate the contribution of factors other than predictability.",
        "We used a simple neural adaptation approach, where we performed continuous gradient updates based on the prediction error on the adaptation sentences.",
        "An alternative approach to neural LM adaptation uses recent RNN states in conjunction with the current state to make word predictions.",
        "We reverted to the base model after the end of each text in our experiments, forgetting any text-specific adaptation.\"']"
    ],
    "3535": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The use of crossword data in the training set resulted in an improved performance versus the base model.",
        "Including crossword data in the training set resulted in the most significant improvement in performance.",
        "Using a bidirectional LSTM as a gloss encoder and taking an average of LSTM hidden states across the sequence improved the model's performance.",
        "The model was able to output the correct word in some cases, and applied affixes correctly in novel contexts.",
        "Fine-tuning fully-trained models on just the crossword dataset may improve crossword performance.",
        "Investigating all hyperparameters through grid search may find the optimum parameters for the model.",
        "Adding a decoder to the model could allow for character-level and/or multiple-word output, as well as an unlimited output vocabulary.\"]"
    ],
    "3543": [
        "The presence of obfuscated words is a significant problem in natural language processing tasks.",
        "Obfuscated words can be used to mask hate speech or abusive language in online datasets.",
        "The use of character n-gram features enhances the performance of RNN-based methods for natural language processing tasks.",
        "Incorporating a character-based word composition model yields state-of-the-art results on all datasets.",
        "The inclusion of obfuscated words in online datasets can lead to misclassification by machine learning models.",
        "Our task-tuning process for embeddings is effective in capturing the semantics of obfuscated words.",
        "Our word composition model correctly infers the semantics of obfuscated words, even when they are formed by concatenation of words.']"
    ],
    "3551": [
        "The proposed model outperforms existing HRED models and attention variants.",
        "The model significantly improves the quality of multiturn dialogue generation by using proper detection methods, such as self-attention.",
        "The detected relevant contexts are coherent with human judgments.",
        "The feature-based model is effective but cannot capture semantic relations like scripts and frames.",
        "The KCE model uses kernels and embeddings to capture these relations, leading to better performance.",
        "The automatic method inevitably introduces noise to the dataset, but the scale enables studying complex event interactions.",
        "The salience model finds and utilizes a variety of discourse relations, such as script chains and frame argument relations.",
        "The core message is that a salience detection module automatically discovers connections between salience and relations, beyond prior centering analysis work.",
        "The results empirically reveal many interesting connections between discourse phenomena and salience.",
        "The findings suggest that core script information may reside mostly in salient events."
    ],
    "3554": [
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Our generation framework may also be applied to generate open-domain QA datasets given the availability of other NLP resources.",
        "Using our reverse engineering technique for QA dataset generation, we can automatically generate questions and their corresponding logical forms from existing NLP annotations on Wikipedia.",
        "The complexity of the generated dataset can be further extended by using a coreferred or lexical variant of the original entity in the question-logical form generation, combining two or more question templates, or generating questions with entities not related to the context in the passage.\"']"
    ],
    "3575": [
        "We achieved an improvement of 9 BLEU points for a Spanish-English ST model with 20 hours of parallel data and 300 hours of English ASR data.",
        "Moreover, the pre-trained model trains faster than the baseline, achieving higher BLEU in only a couple of hours, while the baseline trains for more than a day.",
        "We outperformed baseline models to obtain a BLEU score of 7.1 and precision/recall of about 25% on a real low-resource language, Mboshi, with only 4 hours of parallel data.",
        "Our analysis indicated that, other things being equal, transferring both encoder and decoder parameters works better than just transferring one or the other.",
        "Pre-training using a large ASR corpus from a mismatched language will therefore probably work better than using a smaller ASR corpus that matches the output language.",
        "On the speech side, it might be even more effective to use multilingual training; or to replace the MFCC input features with pre-trained multilingual features, or features that are targeted to low-resource multispeaker settings.",
        "On the language modeling side, simply transferring decoder parameters from an ASR model did not work; but an alternative, and perhaps better, method would be to use pre-trained decoder parameters from a language model, as proposed by Ramachandran et al. (2017) , or shallow fusion (G\u00fclc \u00b8ehre et al., 2015; Toshniwal et al., 2018a) .\"']"
    ],
    "3576": [
        "The authors' approach achieves state-of-the-art performance on a benchmark dataset.",
        "The approach is effective in reducing the task-specific training size.",
        "The use of an energy-based model (EBM) improves the performance of the model.",
        "The authors propose a novel approach to tackle the word segmentation and morphological tagging problem in Sanskrit.",
        "The proposed model outperforms the current state-of-the-art method.",
        "The use of external knowledge in the form of morphological constraints is effective in reducing the task-specific training size.",
        "The approach does not rely on proper normalization, which allows for a more flexible search space.",
        "The authors observe that the first three models in Table 1 .B often end up predicting an identical surface-form with an incorrect morphological tag, thus affecting the WP3T scores.\"]"
    ],
    "3578": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans\\' judgements.']"
    ],
    "3595": [
        "The number of hallucinated objects in current captioning models is significant, ranging from 5.5% to 13.1% of MSCOCO objects.",
        "Hallucination does not always agree with the output of standard captioning metrics, and the popular self-critical loss can increase CIDEr score while also increasing the amount of hallucination.",
        "The CHAIR metric complements standard sentence metrics in capturing human preference, and attention lowers hallucination but may not be due to the attention mechanism itself.",
        "Models with stronger image consistency tend to hallucinate fewer objects, suggesting that strong visual processing is important for avoiding hallucination.",
        "The design and training of captioning models should be guided by cross-entropy loss, standard sentence metrics, and image relevance, including the CHAIR metric and other image relevance metrics.",
        "Incorporating visual information in the form of ground truth objects in a scene can help better understand the performance of captioning models.']"
    ],
    "3610": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The assumption that the label preserves in most cases is verified empirically by results in Table 1.",
        "The performance differences due to swapping are very similar in contradiction case and neutral case in most of the methods tested, leading to a conjecture that the label preserving property of swapping operation will be similar for contradiction and neutral in the data set.",
        "Our swapping testing and robustness testing indicate that DGA (Chen et al. 2017) and KIM (Chen et al. 2018 ) are powerful at the semantic level and robust to the confounding factors.",
        "ADV (Minervini and Riedel 2018) is also a promising method, but some more detailed studies are recommended to be conducted for the interesting properties our metrics revealed.\"']"
    ],
    "3648": [
        "The introduced dataset contains content from a white supremacist forum, which may contain offensive language and hints of hate speech.",
        "The annotation process for hate speech is subjective and requires careful consideration of what constitutes hate speech and what does not.",
        "The current definition of hate speech as deliberate attacks lacks robustness and is open to interpretation.",
        "There is a need for a more precise definition of what constitutes an attack, and whether certain vocabulary (e.g. \"nigger\") should be considered an attack.",
        "The annotation granularity is sentence level, but it is arguable whether a comment containing a single hate-sentence can be considered \"hateful\" or not.",
        "The dataset provides the full set of sentences per comment with their annotations, allowing for different levels of analysis and interpretation.",
        "The label \"RELATION\" is used to indicate when two or more sentences need each other to be understood as hate speech, but it has been seldom used.",
        "Additional context is needed to label a sentence, and studying context dependency can help annotators distinguish hate speech more easily over time.']"
    ],
    "3658": [
        "Our approach outperforms benchmark models across different datasets.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The model obtained best results on both tasks at official evaluation.",
        "We have also experimented with a range of other models, including Poisson regression and neural models.",
        "The success of the simple linear models over more complex (e.g., neural networks) is in line with our experiences in a diverse set of text classification tasks.",
        "The results indicating superiority of simple linear models, however, should not be considered as conclusive since exploration of more complex models was not thoroughly performed due to time limitations.",
        "The promise of this line of work, namely, predicting mental health from language samples is interesting scientifically and it may also have important applications in monitoring public and personal health.",
        "Predicting future mental health is even more interesting as it may allow the clinicians to identify preventive interventions.",
        "The method is even more relevant and easily applicable due to the increase in the longitudinal collection of language output in the last few decades.']"
    ],
    "3793": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "BISON solves the issues of text-based image retrieval tasks that erroneously assume that all unlabeled images are negative examples for the text query.",
        "BISON has the advantage that the evaluation is more reliable, easily interpretable, and focuses more on \"fine-grained\" visual content.",
        "The relative ranking of modern systems in terms of retrieval or captioning scores is nearly identical to the ranking of those systems in terms of BISON.",
        "Existing captioning scores suggest that systems possess superhuman capabilities, which contradicts human assessments of the quality of these systems.",
        "The BISON scores of current systems appear to be better aligned with human assessments of the quality of these systems.",
        "The binary image selection task will foster research into models that go beyond coarse-level matching of visual and linguistic content by rewarding systems that can perform visual grounding at a detailed level.']"
    ],
    "3832": [
        "The proposed approach achieves state-of-the-art performance on the FewRel 2.0 dataset.",
        "The approach uses a novel combination of Cosine Annealing Strategy and pseudo-label generation to improve domain adaptation.",
        "The use of pseudo labels leads to strong improvements in reading comprehension tasks.",
        "Future work should investigate end-to-end approaches or develop alternative approaches that generate titles more similar to how humans write titles.",
        "The proposed approach learns a highly multimodal normalizing flowbased continuous distribution.",
        "The use of continuous prior and inputless decoder imparts interesting properties that are worth further study.",
        "The combination of continuous prior and inputless decoder can be especially useful for GANs, which have traditionally encountered challenges in their application to discrete data such as text.",
        "The flow framework can be used to model data when D > 1, and future work should investigate the limits of this approach.",
        "The 1D case fails, and it will be important to understand how this failure relates to the higher dimensional cases.']"
    ],
    "3854": [
        "Erroneous words show longer reading times and are more likely to be fixated.",
        "Higher error rates lead to increased reading times and more fixations, even on words that are correct.",
        "Transpositions lead to an increased fixation rate compared to misspellings.",
        "Whether the previous word is fixated or not modulates the effect of error and error rate.",
        "A mixed-effects analysis with word forms as random effects showed no significant difference in the lengths of error words and their correct versions.",
        "Comparing the erroneous words of the two error types, we found that they differ in mean length.",
        "Transpositions cause more reading difficulty than misspellings.",
        "All words are more difficult to read when the context of a word is degraded by errors.\"']"
    ],
    "3859": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "Models trained from multi-thematic, big, and rich corpora achieve highest performance on word analogy tasks.",
        "Glove slightly outruns Word2vec when used on very big text corpora.",
        "The size of the training corpus is important for analyzing tweets or song lyrics, and the thematic relevance of the corpus can affect the performance on movie and phone reviews.",
        "Post-processing techniques such as \"injecting\" intelligence of lexicons inside word vectors can improve performance on song lyrics.",
        "Using Glove instead of Word2vec can improve performance on big text corpora.",
        "Training models on large text bundles can improve the quality of word embeddings for sentiment analysis tasks.",
        "Wikipedia texts should be included in word embedding models to improve their performance in semantic and syntactic analogy tasks.']"
    ],
    "3885": [
        "Our model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We used word-based micro precision, recall, and F-score to evaluate the quality of the model.",
        "The attention mechanism is effective in capturing the dependencies among output labels.",
        "Adding the CRF layer to capture the dependency among output labels is useful.",
        "Our model is able to assign substantial weights to many neutral words based on the aspect, contributing to its effectiveness over other baselines.",
        "The model works for both single and multiple aspect sentences and improves phrase discovery by leveraging the latent interactions among the aspect and opinion words based on the content and location.']"
    ],
    "3921": [
        "The accuracy of the VLAWE document representation can decrease by up to 1% due to the random choices involved in the k-means clustering algorithm and cross-validation procedure.",
        "The VLAWE representation based on 3000 components is robust to the choice of k, always surpassing the state-of-the-art approach (Cheng et al., 2018).",
        "The compact VLAWE representations proposed in this paper are competitive with respect to the state-of-the-art methods on five benchmark data sets.",
        "Reducing the number of clusters in the k-means clustering algorithm can lead to more compact VLAWE representations, but the differences are insignificant.",
        "Applying Principal Component Analysis (PCA) to reduce the dimension of the feature vectors can also lead to more compact VLAWE representations.",
        "The VLAWE representation based on 3000 components is robust to the choice of k, and the accuracy tends to increase slightly as the number of clusters increases from 2 to 30.']"
    ],
    "3931": [
        "Our proposed method, EAT, is a novel semantic representation format for NLP that directly builds on theoretical ideas behind the conjunctivist framework.",
        "EAT has several benefits compared to alternative semantic representation formats like AMR or MRS, including its simplicity and versatility.",
        "EAT's linearity and lack of metapredicates make it easy to navigate and use as direct input to an encoder-decoder network.",
        "The current variant of EAT implements the basic version of Pietroski's conjunctivist system, but additional semantic types and combinatory mechanisms would be needed to account for various non-conjunctive aspects of semantics.",
        "Our reason for not implementing these additional features was that the information is not available in the syntactic parse alone, but including such information to EAT would be easy by adding markers of first-vs. second-order interpretation and Theme vs. Content interpretation to the Boolean features.",
        "We applied EAT to three NLP tasks: parallel corpus extraction between grammatical classes, text reconstruction from EAT, and grammatical transformation, and our results provide strong baselines to compare against in future work.\"]"
    ],
    "3941": [
        "The models evaluated in the previous section show that eye-tracking data contain valuable semantic information that can be leveraged effectively by NER systems.",
        "While the individual datasets are still limited in size, the largest improvement is observed in the models making use of all the available data.",
        "The combined type-level feature aggregation from all datasets does not yield the best results, since each sentence in these corpora already has accurate eyetracking features on toke-level.",
        "Type aggregation evidently reduces the fine-grained nuances contained in eye-tracking information and eliminates the possibility of disambiguation between homographic tokens.",
        "The trained NER models can be applied robustly on unseen data.",
        "Our results highlight the benefits of leveraging cognitive cues such as eye movements to improve entity recognition models.",
        "The manually annotated named entity labels for the three eye-tracking corpora are freely available.",
        "The type-aggregated gaze features are effective in cross-domain settings, even on an external benchmark corpus.",
        "The results of these type-aggregated features are a step towards leveraging eye-tracking data for information extraction at training time, without requiring real-time recorded eye-tracking data at prediction time.\"']"
    ],
    "3948": [
        "The proposed vocabulary selection algorithm can find sparsity in the vocabulary and dynamically decrease its size to contain only the useful words.",
        "The commonly adopted frequency-based vocabulary selection is already a very strong mechanism, but applying the proposed VVD can further improve the compression ratio.",
        "The training of text classification takes longer than canonical cross entropy objective due to the stochasticity of VVD.",
        "With the increase in full vocabulary size, the convergence time of VVD increases sub-linearly, while the convergence time of Cross Entropy remains consistent.",
        "The proposed two-step vocabulary reduction can dramatically decrease VVD's training time.",
        "Using a hybrid methodology that combines VVD with a two-step vocabulary reduction can improve the evaluation speed.",
        "The current evaluation method of drawing each vocabulary size from 1 to V and performing V times of evaluation is not practical due to limited computational resources.",
        "The proposed method of increasing the interval exponentially to cover more samples at extremely low vocabulary size is affordable and can achieve a reasonably accurate estimation of ROC with only O(log(|V |)) sample points.\"]"
    ],
    "3959": [
        "We presented the first study of the connections between the severity of cybersecurity threats and language that is used to describe them online.",
        "Our corpus supports the development of automatic classifiers with high precision for this task.",
        "We demonstrate the value of analyzing users\\' opinions about the severity of threats reported online as an early indicator of important software vulnerabilities.",
        "Using our predicted severity scores, we show that it is possible to achieve a Precision@50 of 0.86 when forecasting high severity vulnerabilities, significantly outperforming a baseline that is based on tweet volume.",
        "Reports of severe vulnerabilities online are predictive of real-world exploits.",
        "CVSS ratings are widely used as standard indicators for risk measurement in practice, but they have limitations.",
        "Our work provides one such additional source of information for helping to prioritize threats.\"']"
    ],
    "3960": [
        "The dynamic encoding has a positive effect on 7 out of 10 treebanks.",
        "Casting the constituent-parsing-assequence-labeling problem as MTL surpasses the baseline for all tested treebanks.",
        "Our models are both faster and more accurate than existing sequence tagging or sequence-to-sequence models.",
        "We outperform other approaches that were not surpassed by the original sequence tagging models in terms of F-score.",
        "The baseline parses 70 sents/s on the CTB, while the full model processes up to 120.",
        "The proposed techniques can generalize on heterogeneous settings.",
        "We outperform the current best model for Basque, Hebrew and Polish, and for Swedish.",
        "Casting the problem as MTL reduces the parsing time for all tested treebanks.",
        "Designing methods to handle multi-word expressions could lead to better results for low-resource languages.']"
    ],
    "4013": [
        "Our model achieves competitive performance compared to previous systems using less than 1% of the training data.",
        "Our approach significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The order of elements in English is predominantly SVO, but constructions with the verb preceding the subject do exist.",
        "Languages differ in word order inside other types of phrases, including noun phrases and adpositional phrases.",
        "Consistent reordering across categories may improve the model's performance.",
        "The fact that the agreement dependency between the subject and the verb was more challenging to establish in the SOV order compared to the SVO order is consistent with the hypothesis that SVO languages make it easier to distinguish the subject from the object.",
        "There was not a clear relationship between the prevalence of a particular word order in the languages of the world and the difficulty that our models experienced with that order.\"]"
    ],
    "4042": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "The advantage of the pre-trained BERT model in dealing with sentence pair classification tasks comes from both unsupervised masked language model and next sentence prediction tasks.",
        "Directly fine-tuning the pre-trained BERT on TABSA does not achieve performance growth, but separating the target and aspect to form an auxiliary sentence and transforming TABSA into a sentence pair classification task allows for full utilization of the BERT model's advantage.",
        "Our approach is not limited to TABSA, and this construction method can be used for other similar tasks.",
        "The modeling of label information in BERT-pair models, such as BERT-pair-QA-B and BERT-pair-NLI-B, likely contributes to their better AUC values on sentiment classification.",
        "Our conversion method of transforming (T)ABSA from a single sentence classification task to a sentence pair classification task using an auxiliary sentence can be applied to other similar tasks.\"]"
    ],
    "4088": [
        "Our model shows a great improvement for low-frequency words.",
        "Using pretrained FastText embeddings on the decoder rather than the encoder improves performance.",
        "Initializing the decoder with the embedding results in an increase of +1.80 BLEU.",
        "Multitask learning with raw images would not help the predictive model.",
        "Debiasing images is an essential preprocessing for NMT with embedding prediction to use images effectively in multitask learning scenario.",
        "Pretrained word embeddings improve the performance in multimodal translation tasks, especially when translating rare words.",
        "Incorporating visual features into contextualized word embeddings can be beneficial for future improvements.']"
    ],
    "4108": [
        "The weighted classification F1 score is 0.22, suggesting that the task is complex and probably due to the highly imbalanced data set.",
        "The model has difficulties classifying locations, and this is reflected in the table.",
        "The definition of a province is sharper than that of location.",
        "The more precise a concept boundary, the higher the classification performance tends to be.",
        "The model activates many meaningful concepts beneath the threshold of 0.5, and thus 0.5 might not be appropriate to determine class membership.",
        "NVC deactivates unrelated concepts, such as personality, finding, filling, great, and work that, according to cosine similarity.",
        "NVC replaces speaker, phone, and organ with more meaningful concepts.",
        "The top 25 NVC concepts are more fitting than the top 25 cosine concepts.']"
    ],
    "4127": [
        "Our adversarial training approach produces more useful and specific questions compared to both a model trained using maximum likelihood objective and a model trained using utility reward based reinforcement learning.",
        "We find that our adversarial training approach produces more useful and specific questions compared to both a model trained using maximum likelihood objective and a model trained using utility reward based reinforcement learning.",
        "Our novel approach to the problem of clarification question generation improves upon the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Our approach uses a sequence-to-sequence model to generate a question given a context and a second sequence-to-sequence model to generate an answer given the context and the question.",
        "We use the observation of Rao and Daum\u00e9 III (2018) that the usefulness of a clarification question can be measured by the value of updating a context with an answer to the question.\"']"
    ],
    "4143": [
        "significant improvements over the baseline models",
        "improvements can be achieved without requiring large amounts of recorded data",
        "the combination of gaze and EEG features decreases the signal-to-noise ratio",
        "EEG signals be preprocessed and denoised more efficiently for NLP tasks",
        "how to combine the potential of exceptionally good single-subject models and multi-task learning",
        "improving NLP tasks with eye-tracking and electroencephalography data",
        "no recorded data is required during prediction time",
        "great potential in improving NLP tasks and facilitate insights into language processing"
    ],
    "4144": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Increasing the number of glimpses for a RAM model has little effect on overall accuracy, while increasingly deep CNNs do show increased performance.",
        "The psychophysical model of approximate number fits network data well, with some Weber fractions being near those found for human participants.",
        "The primary qualitative difference between model performance and human performance is that the models do roughly equally well on both column image types, whereas humans are significantly better on column sorted as opposed to column mixed trials.",
        "These results exhibit initial promise in using neural models as cognitive models in psychosemantics.",
        "More detailed hyper-parameter searches may improve fit with the human data, thus allowing us to use the models to generate predictions.",
        "RAM model performance could be improved by giving the network a low-resolution version of the whole image to help it make location choices.",
        "The depth manipulation for CNNs was designed to reflect increased information processing capacity as duration increases, and one could control for capacity by making the deeper networks narrower or the shallow networks wider, and seeing if depth still has an effect.",
        "To better understand what strategies the models are using to solve the task, techniques such as transfer learning and diagnostic classifiers could be applied to our models.",
        "Independent neural models that exhibit ANS-like behavior or are trained on other image processing tasks could be used in this task.']"
    ],
    "4173": [
        "We have obtained promising results where the input length can be reduced by more than 20% at the cost of less than 3 points of UAS.",
        "Our work can be a starting point for developing templates that in the future can significantly speed up parsing time by avoiding redundant syntactic analyses at the minimal expense of accuracy.",
        "The present study has investigated two approaches, and the second technique is still in a preliminary stage, but requires some refinement.",
        "We plan to use both PoS tags and lemmas in our templates, and the sparsity problem in finding ngrams involving lemmas will be tackled by augmenting training data with parsed sentences.",
        "Our approach is generic enough to be applied to practically any kind of parser, and as the technique reduces the input length received by the parser, speed gains can be expected to be larger on parsers with higher polynomial complexity.\"']"
    ],
    "4204": [
        "The proposed model, KALM, outperforms two baseline models (NE-LM and a supervised CRF-biLSTM) in both unidirectional and bidirectional settings on two datasets.",
        "The improvement of KALM over NE-LM is larger in the unidirectional setting compared to the bidirectional setting.",
        "Training KALM on more unlabeled data further reduces the perplexity, and adding prior information as to whether a word represents different entity types helps to bring the F1 score to 0.76.",
        "The unsupervised model KALM can be trained on large corpora, and adding the Wikitext-2 corpus improves the NER score to 0.84.",
        "The best KALM model almost always scores higher than biLSTM without the CRF loss, and the ablation experiment shows that KALM is sensitive to the quality of the knowledge base.",
        "The proposed Knowledge Augmented Language Model (KALM) extends a traditional RNN LM with information from a Knowledge Base, and the latent type information learned by the model can be used for high-accuracy NER systems.",
        "The modeling paradigm of KALM opens the door for end-to-end deep learning systems that can be enhanced with latent modeling capabilities and trained in a predictive manner end-to-end.']"
    ],
    "4211": [
        "The APE model improved human evaluations while lowering BLEU scores by 6.",
        "Evaluating both source-language original and target-language original test sets provides a higher-precision view of the strengths and weaknesses of different modeling techniques.",
        "The current scoring method may hide certain effects, such as improvements in naturalness of output.",
        "Using multi-reference BLEU or aligning sentence pairs from monolingual data sets can provide a more accurate assessment of translation quality.",
        "The APE model can be used on top of any new advance in the field without need for re-training.",
        "The APE model only requires a subset of 24M training examples to train the model.",
        "The model can reveal systematic problems with reference translations and provide finer-grained BLEU reporting.']"
    ],
    "4228": [
        "AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improves the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "RE-NET outperforms all the static and temporal methods and our extensive analysis shows its strength.",
        "Developing a fast and efficient version of RE-NET is an interesting future work.",
        "Modeling lasting events and performing inference on the long-lasting graph structures is an interesting future work.\"']"
    ],
    "4235": [
        "Motivated by data scarcity for the GEC task, we present two contrasting approaches for generating large parallel corpora from the same publicly available data source.",
        "We believe both techniques offer promising research avenues for further development on the task.",
        "Models trained exclusively on minimally filtered English Wikipedia revisions can already be valuable for the GEC task.",
        "Implementing more complex filtration in order to reduce the noise in the generated dataset will likely be a productive avenue to increase the value of this approach.",
        "The performance achieved by the reported Wikipedia revisions-trained models, both with and without finetuning, may be used as a baseline by which to evaluate smaller, cleaner datasets drawn from Wikipedia revisions.",
        "Round-trip translation takes advantage of the advanced state of the task of Machine Translation relative to GEC by leveraging extant translation models as a source of grammatical-style data corruption.",
        "In our experiments with round-trip translation, we used target sentences drawn from Wikipedia to maintain a reasonable comparability between the two techniques.",
        "We observe that pooling two diverse data sources used to train competitively performing models on the same task can degrade performance.",
        "This suggests that within datasets useful for a specific task, there may be greater value to be discovered in finding optimal partitions of the data for training models which can then be combined using ensembles.\"']"
    ],
    "4241": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our approach outperforms benchmark models across different datasets.",
        "We have found that better OCR methods lead to better results for DVQA.",
        "The large amount of data the model has to be exposed to in order to learn the FOIL classification task and by the unstable results over training epochs.",
        "None of the models transfers their encoding skills with high confidence, but again the VQA model does it to a lower extent.",
        "The RSA analysis confirms the higher similarity of the multimodal spaces generated by the ReferIt and GuessWhat encoders and the high similarity between the VQA space and the space produced by the randomly initialized encoder.",
        "From the NN analysis, it appears that for all models (except for the one fully trained on the FOIL task) the visual modality has higher weight than the linguistic one in the construction of the multimodal representations.\"']"
    ],
    "4275": [
        "The proposed BERT-BiDAF hybrid architecture achieves decent performance on a difficult problem (SQuAD 2.0) and verifies the authors\\' arguments through ablation experiments.",
        "The main difficulties in improving the model\\'s performance are overfitting and high variance.",
        "The authors plan to dig deeper into the difficulties they are facing and consider question representation as a possible solution.",
        "The political parties identified as \"Islamist\" won 75% of the total seats in the Egyptian parliamentary election, 2011-2012.",
        "The reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading to doubts about the identity of the Black Death.']"
    ],
    "4277": [
        "The SENVAE model has a shallow generative story but is hard to use due to its strong generator component.",
        "Many techniques in the literature perform reasonably similarly, with some requiring a considerable hyperparameter search.",
        "Our proposed optimization technique is simple enough to tune and superior to annealing and word dropout.",
        "The typical RNNLM is hard to improve upon in terms of log-likelihood of gold-standard data.",
        "We hope that this work will pave the way to deeper-in statistical hierarchy-generative models of language.",
        "The SEN-VAE is not a model, but it is a crucial building block in the pursuit of hierarchical probabilistic models of language.",
        "We employ an embedding layer, one or more GRU cells, and an affine layer to map from the dimensionality of the GRU to the vocabulary size.",
        "Compared to RNNLM, we modify f only slightly by initializing GRU cell(s) with h0 computed as a learnt transformation of z.']"
    ],
    "4288": [
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "Using a manually labeled dataset of literal and non-literal image-caption pairs, we casted the problem of gist detection as a ranking task over the set of concepts provided by an external knowledge base.",
        "Our method improves the performance for all types of pairs, a finding which is in line with research on multimodal approaches for other related tasks.",
        "Using features and concepts from both modalities (image and caption) improves the performance.",
        "A feature ablation study shows the complementarity nature and usefulness of different types of features, which are collected from different kinds of semantic graphs of increasing richness.",
        "Our experiments show that the candidate selection and ranking of gist concepts is a more difficult problem for non-literal image-caption pairs than for literal image-caption pairs.",
        "Gist image identification is a small, yet arguably crucial part of the much bigger problem of interpreting images beyond their denotation.\"']"
    ],
    "4291": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but the results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The approach of using a projection-based method for attenuating biases in word representations was found to be effective for the static GloVe embeddings.",
        "The simple approach of debiasing the first (non-contextual) layer alone can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Genie produces a syntactically correct and type-correct program for 96% of the inputs, indicating that the model can learn syntax and type information well.",
        "Genie identifies correctly whether the input is a primitive or a compound with 91% accuracy, and can identify the correct skills for 87% of the inputs.",
        "The main source of errors is due to the difficulty of generalizing to programs not seen in training.",
        "Using a formal VAPL language to represent the capability of a virtual assistant and use a neural semantic parser to directly translate user input into executable code can be effective.",
        "Genie can be used to create cost-effective semantic parsers for new virtual assistant capabilities.']"
    ],
    "4314": [
        "The proposed task of fine-grained argument unit recognition and classification (AURC) has good quality in terms of annotator agreement.",
        "The AURC-8 benchmark is released for this task, and it is the first to show that information-seeking AM can be applied on very heterogeneous data.",
        "The approach does not depend on correct sentence boundaries, and it increases the recall of arguments and makes their representation more accurate for the user as well as for downstream tasks.",
        "The template defined in Section 3.4 can be used to reconstruct complete arguments for most of AURC-8.",
        "The approach is well-suited for noisy data from social media or continuous speech data, e.g., political debates.",
        "The fine-grained argument annotations should be explored in downstream applications like claim validation and fact checking.",
        "The retrieval and annotation pipeline can be used to annotate argument units for additional topics, thus enabling improved domain transfer.']"
    ],
    "4318": [
        "Our model operates in the quaternion space and has capability in modeling several key relation patterns.",
        "The flexibility and representational power of quaternions enable us to model major relation patterns at ease.",
        "Similar to ComplEx, our model can model both symmetry (r(x, y) \u21d2 r(y, x)) and antisymmetry (r(x, y) \u21d2 r(y, x)) relations.",
        "The symmetry property of QuatE can be proved by setting the imaginary parts of W r to zero.",
        "Our model has fewer free parameters than multiple recent strong baselines.",
        "Empirical experimental evaluations on four well-established datasets show that QuatE achieves an overall state-of-the-art performance, outperforming multiple recent strong baselines.\"']"
    ],
    "4355": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The N-VAE frequently returns better results than the original LDA model as well as the LFLDA on both the topic coherence evaluation and document clustering task.",
        "Word2Vec helps the model to reach better topic coherence scores than GloVe, while GloVe is evidently more beneficial to the document clustering task.",
        "The NPMI employs a sliding window over tokens in an external corpus (Wikipedia) to calculate the cooccurrences between pairs of words, while the Word2Vec training method emphasizes capturing the relationships between a word and its neighbors, i.e., local co-occurrences.",
        "The effect of BN is two-fold: first, it helps solve the component collapsing problem of the model caused by the rapid saturation of the softmax function; and second, it permits the employment of high learning rates which speeds up convergence.",
        "Our model not only succeeds in securing a time advantage but also consumes less memory since it is trained using mini batches instead of loading the entire corpus into the memory, which makes it extremely useful for inducing topics of a large dataset of microtexts, e.g., posts on social media.\"']"
    ],
    "4364": [
        "The existing event detection methods are not satisfactory in performance because they rely on lexical and syntactic features from dependency parsing, leading to limited progress in the field.",
        "The proposed bottom-up event detection framework using deep learning techniques can overcome the error propagation and extra annotations of trigger-based approaches.",
        "The use of a bottom-up approach and multi-output neural network for directed event detection can significantly lessen the sensitivity of event detection.",
        "The proposed method is suitable for general event extraction and can be used in a wide range of tasks, including biomedical text mining.",
        "The method is not sensitive to hyperparameters and can achieve outstanding performance.",
        "The proposed method has the potential to fundamentally benefit downstream tasks in biomedical text mining with broad impacts.']"
    ],
    "4410": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but the results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The strong results suggest that the community is ready for more difficult CQA datasets.",
        "The current datasets omit human-generated charts and natural language questions, which limits the ability of algorithms to handle real-world scenarios.",
        "Document-level CQA is necessary to understand charts in documents and to improve the retrieval of information from charts.']"
    ],
    "4420": [
        "Our approach outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Experimental results demonstrate that our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first rank unlabeled domain training samples based on their similarity to in-domain data, and then adopt a probabilistic curriculum learning strategy.",
        "Curriculum learning models can improve over the standard continued training model by up to 3.22 BLEU points and can take better advantage of distant and noisy data.",
        "The improvement is mainly due to better scoring of words that acquire a new sense or have a different score distribution in the new domain.",
        "Our approach is effective for several reasons, including providing a robust way to augment the training data with samples that have different levels of similarity to the in-domain data.",
        "The method implements best practices that have shown to be helpful in NMT, e.g. bucketing, mini-batching, and data shuffling.",
        "Curriculum learning can scale with more in-domain data-it consistently outperforms the standard training policy, but with less improvement.\"']"
    ],
    "4429": [
        "The beam problem in automatic speech recognition (ASR) can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains, but it remains to be seen how much improvement remains to be gained by solving label bias in general.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset, and it is unclear whether more general globally-normalized models can be trained in a similarly inexpensive way.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "The use of self-attention mechanism in our model allows for effective capture of long distant dependency relations, leading to improved performance in multi-turn dialogue generation.",
        "The proposed ReCoSa model is able to detect relevant contexts that are significantly coherent with humans' judgments, which can be useful for improving the quality of multiturn dialogue generation.",
        "The use of proper detection methods, such as self-attention, can improve the quality of multiturn dialogue generation by using relevant contexts.\"]"
    ],
    "4448": [
        "different classes of books display different patterns of semantic flow",
        "the proposed framework can be used to identify semantic flow patterns that are able to discriminate distinct classes of texts",
        "the results obtained here suggest that the semantic flow could play an important role in other NLP tasks",
        "the adopted network representation could be adapted and used as an auxiliary tool to study complex brain and cognitive processes",
        "the analysis of other stylometric tasks could benefit from the use of a semantic flow analysis",
        "the proposed framework can be used to identify patterns of transition between semantic groups (communities)",
        "the accuracy rate was high when discriminating investigative and philosophy books, and a significant performance was obtained when discriminating books published in distinct epochs\"']"
    ],
    "4451": [
        "HellaSwag is a challenging testbed for state-of-the-art NLI models, even those built on extensive pretraining.",
        "The existence of a Goldilocks zone of text complexity - in which generations are nonsensical, but existing state-of-the-art NLP models cannot tell the difference.",
        "The dataset is adversarial to the most robust models available, even when models are evaluated on items from the training distribution.",
        "The inner workings of pretrained models can be provided insight into by constructing the dataset through adversarial filtering.",
        "The path for NLP progress going forward is towards benchmarks that adversarially co-evolve with evolving state-of-the-art models.']"
    ],
    "4456": [
        "Performance Gap: There exists a performance gap between the development set and the test set in IRNet, as shown in Table 1.",
        "Limited Data: The limited number of data (1034 in development, 2147 in test) may cause the performance gap.",
        "Different Distributions: The SQL queries in Hard and Extra level have different distributions in the development and test sets.",
        "Pseudo Test Set: Constructing a pseudo test set from the official training set can help verify the hypothesis that the performance gap is caused by the different distributions of SQL queries.",
        "No Performance Gap: Other approaches do not exhibit the performance gap because of their relatively poor performance on the complicated SQL queries.",
        "Limited Support: SemQL has limitations, such as not supporting self-joins in the FROM clause of SQL and not completely eliminating the mismatch between NL and SQL.",
        "Effectiveness: Experimental results demonstrate the effectiveness of IRNet despite its limitations.",
        "Future Work: Designing an effective intermediate representation to bridge NL and SQL is a promising direction for future work.']"
    ],
    "4460": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach to joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "The performance of our approach depends to a certain extent on the ease of annotation for each lemma.",
        "The correlation between Uiaa and Umid is negative, suggesting that words with higher disagreement tend to exhibit a higher proportion of mid-range judgments.",
        "Using BERT target word embeddings results in better correlation with Uiaa per target word.",
        "Average Usim values for the word suffer do not exhibit high variance, suggesting that a strong correlation is harder to obtain.",
        "The by-lemma approach, which takes into account the specificities of each lemma, can be useful for determining the appropriate meaning representation for each lemma.",
        "Automatic substitutions can be used as an alternative to manual annotation when combined with embedding-based features.",
        "BERT offers a much more straightforward solution to the Usim prediction problem than using automatic Usim predictions for estimating word sense partitionability.']"
    ],
    "4483": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "MCScript2.0 is a new machine comprehension dataset with a focus on challenging inference questions that require script knowledge or commonsense knowledge.",
        "The theme of the party was \"Something Blue\".",
        "The invitation included a picture of the host and had blue and activities with titles related to the theme.",
        "The letter was mailed to the company headquarters with a stamped and addressed envelope.",
        "The receipt from the restaurant was printed out.",
        "The signatures were placed after putting the letter in the envelope.",
        "The letter was signed on the final line.']"
    ],
    "4486": [
        "The primary difference between models like Transformer, attention-based GRU or CNN and attention-less LSTM is the degree to which the attention, or more specifically the scoring function, is learned.",
        "The more the model is able to learn the scoring function, and thus the better it can make use of attention, the more effective the attention mechanism becomes.",
        "Attention mechanism seems to be a fundamental property of the systems that learn, much like backpropagation.",
        "Using non-recurrent attention architectures like the CNN and the transformer not only improves the performance but also significantly reduces the running time for a single pass through the model.",
        "Transformer performs the best in the sequence to sequence mapping tasks. By making use of attention, it is able to capture long-range dependencies and consistently decrease the loss without showing any instability that other models show.",
        "Unlike transformer, the CNN and GRU attention models take much longer to train and produce worse results. Additionally, these models are unstable, meaning that over-training them causes unanticipated results.",
        "The LSTM model without attention shows practically no learning and should not be used for any practical applications.']"
    ],
    "4500": [
        "The embeddings of GloVe, ELMo, and BERT encode gender, religion, and nationality biases.",
        "A projection-based method can be used to attenuate these biases.",
        "The method works for static GloVe embeddings.",
        "The approach can be extended to contextualized embeddings (ELMo, BERT) by debiasing the first non-contextual layer alone.",
        "This simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Our system's automated results on the E2E task exceed that of the winning system.",
        "Splitting apart utterance planning and surface realization in a fully neural system may have potential benefit.",
        "Our intuition is that by loosely separating the semantic and syntactic tasks of sentence planning and surface realization, our models are more easily able to learn alignments between source and target sequences in each distinct task than in a single model.",
        "The design of our symbolic intermediate representation is such that additional training data can be easily collected for the surface realization model.",
        "Adding a semantic reranker to our system could likely help improve performance of the utterance planning step.\"]"
    ],
    "4510": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Adding ELMo to a straightforward PyTorch implementation of LSTM may be easier now than at the time of performing the tests.",
        "The solution for any practical case has to be tailored to its characteristic error patterns.",
        "Available corpora can be \"boosted\" (Ge et al., 2018) , i.e. expanded by generating new errors consistent with a generative model inferred from the data.",
        "The possibility of using measurements in spaces of semantic vectors was already mentioned in this article.",
        "Non-word errors can be easily detected with comparing tokens against reference vocabulary, but in practice one should have ways of detecting mistakes masquerading as real words and fixing bad segmentation (tokens that are glued together or improperly separated).']"
    ],
    "4545": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach outperforms benchmark models across different datasets.",
        "SPGD produces higher-quality, more interpretable perturbed sequences than previous fast-gradient methods for text without sacrificing final classifier accuracy.",
        "The set of adversarial sequences generated by SPGD and its predecessors represents only a small subset of the set of all possible adversarial sequences.",
        "Recent work has attempted to address these restrictions using autoencoders and SCPNs, but such approaches are limited by the ability of latent-variable generative text models to encode and decode very long sequences (such as those in IMDB) with high reconstruction accuracy.",
        "More work needs to be done here.\"']"
    ],
    "4550": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The result that SWITCHP is correlated with human judgments of local topic quality best parallels the intuition.",
        "Topic consistency should not be used to the exclusion of other measures of topic model quality.",
        "Future models should be evaluated with respect to global topic quality and local topic quality, in addition to downstream tasks.",
        "The proposed metric, topic switch percent (or SWITCHP), correlates well with human evaluation."
    ],
    "4584": [
        "Pretraining with NUR or NUG leads to strong results when fine-tuning on the other task. (supporting evidence: all of the results)",
        "The two tasks, NUR and NUG, are complementary. (supporting evidence: the similarity of the two tasks)",
        "Pretraining with either NUG or NUR improves performance, leads to faster convergence, and facilitates domain generalizability. (supporting evidence: all of the results, especially in low-data scenarios)",
        "The representations learned by NUG are very general and transfer well to many downstream tasks. (supporting evidence: the strong results when fine-tuning on other tasks)",
        "InI and MUR, two novel pretraining objectives, consistently show strong improvement for the downstream NUG task. (supporting evidence: the large improvements over directly training for the downstream task)",
        "The proposed objective allows the model to extract stronger and more general context representations from the same data. (supporting evidence: the improved performance with reduced data experiments)",
        "Pretraining on a larger external data will result in further performance gains, but it is challenging to identify a sufficient corpus. (supporting evidence: the reduced data experiments show that pretraining on a larger corpora results in strong performance on smaller task-specific datasets)']"
    ],
    "4588": [
        "The presence of disfluencies in speech can negatively impact machine translation applications.",
        "Previous work has relied on separate step-by-step approaches to remove disfluencies in speech translation, which is not feasible with end-to-end models.",
        "The authors have extended the text baseline to speech input and provided first results for direct generation of fluent text from noisy disfluent speech.",
        "Fluent training data is expensive and unlikely to be available for every corpus and domain, so alternative approaches are needed.",
        "The authors hope to reduce the dependence on fluent target data during training through decoder pretraining on external non-conversational corpora or multitask learning.",
        "Standard metrics alone do not tell the full story for this task; additional work on evaluation metrics may better demonstrate the differences between such systems.",
        "The authors have used a novel approach to downsample feature dimensionality by a factor of 3 using a ConvLSTM layer.",
        "The proposed approach achieves new state-of-the-art on FewRel 2.0 dataset.']"
    ],
    "4602": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The ConMask model has some limitations and room for improvement, such as the nature of the relationship-dependent content masking, which can lead to incorrect predictions.",
        "The use of a filter to modify the list of predicted target entities can be applied to rearrange entities that are similar to the given relationships.",
        "The ConMask model mitigates catastrophic forgetting and outperforms baseline methods on text classification and question answering.",
        "The proposed method introduces a lifelong language learning setup and an episodic memory model that performs sparse experience replay and local adaptation to continuously learn and reuse previously acquired knowledge.",
        "The use of local adaptation helps improve the performance of the model, as shown in two examples (e.g., David Niven and dj Kool Herc).']"
    ],
    "4604": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We release our MultiATIS++ corpus to facilitate future research on cross-lingual NLU to bridge the gap between cross-lingual transfer and supervised methods.",
        "Regularizing data without a teacher may be helpful for multiple task distributions to work together in the same model.",
        "In experiments, we found that data-discarding pace functions seem to work best when they simultaneously decay down to their respective floors.",
        "Adaptively adjusting them seems an interesting future work.\"']"
    ],
    "4640": [
        "The proposed approach achieves competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed chunking policy network for machine reading comprehension enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "The u t = 1 language model is comparable to the LSTM on perplexity and classification accuracy, but it ultimately achieves worse scores than the baseline.",
        "The pushing behavior of the model reflects subcategorization properties of lexical items that play an important role in determining their syntactic behavior.",
        "The stack at least partially alleviates the difficulty experienced by the LSTM classifier in handling syntactically complex inputs.",
        "Stack RNNs trained on corpora of natural language text learn to encode sentences in a hierarchically organized fashion.",
        "Using the stack RNN to predict the grammatical number of a verb results in better hierarchical generalizations in syntactically complex cases than is possible with stackless models.",
        "The stack RNN model yields comparable performance to other architectures, while producing structural representations that are easier to interpret and show signs of being linguistically natural.']"
    ],
    "4655": [
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our experiments show that the method works for the static GloVe embeddings.",
        "We extend the approach to contextualized embeddings (ELMo, BERT) by debiasing the first (non-contextual) layer alone and show that for the well-characterized gender direction, this simple approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Automatic evaluation scores do not reflect the quality improvements.",
        "Classic automatic evaluation metrics fail to capture the signal in human judgments for the proposed visual story post-editing task.",
        "We first use the human-edited stories as references, but all the automatic evaluation metrics generate lower scores when human judges give a higher rating.",
        "When comparing among machine-edited stories (y and |), among pre-and post-edited stories (z and }), or among any combinations of them (~, and ), all metrics result in weak correlations with human judgments.",
        "These results strongly suggest the need of a new automatic evaluation metric for visual story postediting task.\"']"
    ],
    "4660": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "Knowing the topic, target, and sentiment expression is important for accurately predicting the sentiment of tweets.",
        "Including information about the topic and sentiment expression improves the performance of a baseline classifier by more than 10% absolute.",
        "There is a significant improvement (13-14%) when adding the sentiment expression feature, which suggests some dependency between sentiment polarity and how sentiment is expressed.",
        "Developing accurate machine learning models that leverage the existing annotation to perform both overall and target-based sentiment in Arabic tweets is a potential future work.",
        "Investigating cross-topic and cross-dialect solutions to mitigate the amount of required resources for sentiment analysis on any given piece of text is an interesting future work.']"
    ],
    "4678": [
        "The beam problem can largely be explained by the brevity problem.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our proposed baseline models display competitive performance on supervised multimodal prediction and outperform classical deep autoencoders for semisupervised multimodal prediction.",
        "Our strong baseline models provide new benchmarks for future research in multimodal learning.",
        "By Taylor expansion, we have that f v(i) (m s ) (56) \u2248 f v(i) (0) + \u2207 ms f v(i) (0).",
        "We have that: EQUATION By our symmetric paramterization of the acoustic features, we have that: EQUATION.",
        "Our choice of a Gaussian likelihood for the visual and acoustic features introduces a squared term W \u03c3\u22ba v (v -b \u00b5 v ) \u2297 (v -b \u00b5 v ) to account for the 2 distance present in the Gaussian pdf.\"']"
    ],
    "4711": [
        "We have proposed a simple but effective model, the Visually Grounded Neural Syntax Learner, for visually grounded language structure acquisition.",
        "Our approach to grounded language learning produces parsing models that are both accurate and stable, and that the learning is much more data-efficient than a state-of-the-art text-only approach.",
        "The results suggest multiple future research directions, such as considering structured representations of both images and texts, disentangling syntax and semantics, and automatically acquiring inductive biases from data.",
        "Our best model is based on the head-initial inductive bias, and it may be possible to extend our approach to other linguistic tasks such as dependency parsing, coreference resolution, and learning pragmatics beyond semantics.",
        "The current approach has thus far been applied to understanding grounded texts in a single domain, but its applicability could be extended by learning shared representations across multiple modalities or integrating with pure text-domain models.\"']"
    ],
    "4714": [
        "The diachronic and synchronic evaluation tasks we introduced were solved with impressively high performance and robustness.",
        "We introduced Word Injection to overcome the need of (post-hoc) alignment, but find that Orthogonal Procrustes yields a better performance across vector space types.",
        "The overall best performing approach on both data suggests to learn vector representations for different time periods (or domains) with SGNS, to align them with an orthogonal mapping, and to measure change with cosine distance.",
        "We further improved the performance of the best approach with the application of mean-centering as an important pre-processing step for rotational vector space alignment.",
        "A Pre-processing and Hyperparameter Details Corpora.",
        "For all corpora, we removed words below a frequency threshold t.",
        "We then created two versions: \u2022 a version with minimal pre-processing, i.e., with punctuation removed and lemmatization (L ALL ) \u2022 a stronger preprocessed version with only content words.",
        "While our implementations of the count-based vectors have a stable window of size n, SGNS has a dynamic context window with maximal size n (cf. Levy et al., 2015) and SCAN has a stable window of size n, but ignores all occurrences of a target word where the number of context words on either side is smaller than n.",
        "We set the number of dimensions d for SVD, RI and SGNS to 300.",
        "We trained all SGNS with 5 epochs.",
        "For PPMI we set \u03b1 = .75 and experimented with k = {1, 5} for PPMI and SGNS.",
        "For RI and SGNS we experimented with t = {none, .001}.",
        "We set the number of iterations for the Gibbs sampler to 1,000.",
        "We used 1, 000 iterations for the Gibbs sampler and set the minimum amount of contexts for a target word per time period min = 0 and the maximum amount to max = 2000.\"']"
    ],
    "4725": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but the results for DVQA were more nuanced due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods lead to better results for DVQA.",
        "The current state-of-the-art datasets are not challenging enough for future CQA systems.",
        "Document-level CQA is necessary for better understanding of charts in documents.",
        "The proposed system, PReFIL, improves the state-of-the-art and surpasses human accuracy on two datasets.",
        "The community is ready for more difficult CQA datasets."
    ],
    "4726": [
        "correctly classifying the minority classes Unable and Unclear is not trivial",
        "the lack of data for training those classes",
        "semantic ambiguity - even for humans",
        "an important area of confusion is when actions are hypothetical",
        "the use of an assistive device can cause semantic problems",
        "different ensembling parameters should be considered to take better advantage of the rule-based system\\'s strengths",
        "machine learning approaches with lexical features perform surprisingly well on the task",
        "an ensembled approach sets a strong baseline of 77.9% macro F1 for our dataset",
        "incorporating contextual representations such as ELMo and BERT into our models",
        "this is the first work expanding on the problem of clinical negation detection to complex interactions between individuals and their environments\".']"
    ],
    "4734": [
        "We propose a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "We have shown that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "We attempt to facilitate the development of systems that aid in better organization and access to information.",
        "The dataset presented here is not intended to be exhaustive, nor does it attempt to reflect a true distribution of the important claims and perspectives in the world.",
        "When we ask crowd-workers to evaluate the validity of perspectives and evidence, their judgment process can potentially be influenced by their prior beliefs.",
        "To avoid additional biases introduced in the process of dataset construction, we try to take the least restrictive approach in filtering dataset content beyond the necessary quality assurances.",
        "Our hope is that this dataset would bring more attention to this important problem and would speed up the progress in this direction.\"']"
    ],
    "4735": [
        "The proposed method is compared favourably with a number of existing methods.",
        "It is worth to emphasize that rVAD obtained the promising performances across databases and tasks by using the same parameters, indicating the good generalization ability of rVAD.",
        "Pitch is a good indicator or anchor for locating speech segments.",
        "A posteriori signal-to-noise ratio (SNR) weighted energy difference is an effective measure for segmenting speech in noisy environment.",
        "The proposed method performs well in both clean and noisy conditions and for both VAD itself and SV.",
        "The generalization ability across databases, noisy conditions and tasks was proofed as well.",
        "Future work includes investigating the optimal configurations of rVAD for different applications.",
        "The performance of rVAD on automatic speech recognition is worth to study as well.\"']"
    ],
    "4741": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "We demonstrated that our approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The wage gap does not have to do with the gender pay gap, but rather that women are more likely to be able to take care of their children than their male counterparts.",
        "It is not a matter of the wage gap, it is a matter of opinion, and it is the job of the employer to make sure that the job is not the same as the other.",
        "Our model generates more proper arguments with richer content than nontrivial comparisons, with comparable fluency to human-edited content.\"']"
    ],
    "4754": [],
    "4760": [
        "The proposed method, Dynamic Memory Induction Networks (DMIN), achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The use of dynamic memory as a learning mechanism has the potential to be more general than what has been used here for few-shot learning.",
        "The model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The dataset consists of 1,268 miniclips and 14,769 actions, with 4,340 of them being labeled as visible.",
        "The multimodal model outperforms the use of one modality at a time.",
        "The approach of labeling actions in videos based on the language that accompanies the video has the potential to create a large repository of visual depictions of actions, with minimal human intervention, covering a wide spectrum of actions that typically occur in everyday life.",
        "The dataset and the code introduced in this paper are publicly available at http://lit.eecs.umich.edu/downloads.html.']"
    ],
    "4778": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data, particularly for the task of causality extraction.",
        "The class distribution of the customer service dataset used in previous work was different from the current dataset, with a larger number of negative tweets, which may have affected the performance of the lexicon-based SVM.",
        "The antonym-based method improves the detection of negation in customer service conversations, as shown by the example in Row 2 of Table 9.",
        "There is room for improvement for the positive class but negation handling may not be enough; a combination of negation and sarcasm may be a useful direction to explore in future for customer service conversations.']"
    ],
    "4780": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The main limitation of composition functions is that they rely on the assumption of compositionality, which often does not hold.",
        "While in this work we focused on compositional noun compounds, the meaning of many noun compounds is not a straightforward combination of the meanings of their constituents.",
        "It is not certain that the representations we tested would be able to address the complexity of longer noun compounds, which, among other things, also require uncovering the syntactic head-modifier structure.",
        "Contextualized Word Embeddings are dynamic word embeddings computed for words given their context sentence.",
        "Recently, Shwartz and Dagan (2019) found that while these representations excel at detecting non-compositional noun compounds, they perform much worse at revealing implicit information such as the relationship between the constituents.\"']"
    ],
    "4806": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our model can effectively capture the long distant dependency relations.",
        "The proposed ReCoSa model is useful for improving the quality of multiturn dialogue generation.",
        "The relevant contexts detected by our model are significantly more accurate than those of existing HRED models and its attention variants.",
        "Our model achieves better results in terms of overall generation quality, aspect coverage, and fluency.",
        "Future work includes integrating more kinds of syntactic features from linguistic analysis such as dependency parsing.\"']"
    ],
    "4822": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "There is a type of downward inferences to which every model fails to provide correct answers, such as the contrast between few and a few.",
        "BERT does not understand the difference between the downward operator few and the upward operator a few.",
        "The MultiNLI training set contains only 77 downward inference problems.",
        "Certain pragmatic factors can block people from drawing downward inferences in naturally occurring texts.",
        "The MED dataset is useful for testing state-of-the-art NLI models and improving their generalization ability in monotonicity reasoning.']"
    ],
    "4823": [
        "Our approach outperforms traditional demographic data-based models for health-related predictions.",
        "Our method discovers novel correlations between open-vocabulary topics and health variables.",
        "Our model identifies known and novel risk or protective factors in the form of topics.",
        "Our approach significantly outperforms previous models based on topic models such as LDA or traditional statistical models.",
        "Our method captures language features on a community scale.",
        "Our approach was able to predict target variables tied to very different age-groups, which is encouraging and supports the robustness of our approach.",
        "Our method raises the question of how these findings can be translated to the individual person.",
        "Future research should address the applicability of our model to textual data other than Twitter and potentially from non-social media sources, to communities that are not geography based, to the time evolution of topics and health/lifestyle statistics, as well as to targets that are not health related.']"
    ],
    "4825": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but the results for DVQA were more nuanced due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The strong results suggest that the community is ready for more difficult CQA datasets.",
        "Charts in the wild may contain variations that are not captured by current datasets, and human-generated questions are necessary for handling such variations.",
        "Document-level CQA requires information from the rest of the document to answer questions about the chart.",
        "BERT base and BERT large consistently outperform ELMo and Flair embeddings.",
        "The usage and distribution of English may vary across corpora, yielding different results on downstream tasks.",
        "The training objective of BERT may be responsible for its specific performance profile.']"
    ],
    "4834": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our first strategy selects which words should be turned into a gap, and the second strategy learns to increase or decrease the size of the gaps.",
        "Both strategies can effectively manipulate the C-test difficulty, as both the participants\\' error rates and their perceived difficulty yield statistically significant effects.",
        "Manipulating the gaps\\' size and position does not only influence the C-test difficulty, but also addresses different competencies (e.g., requires more vocabulary knowledge or more grammatical knowledge).",
        "Future manipulation strategies that take the competencies into account have the potential to train particular skills and to better control the competencies required for a placement test.",
        "We can work with any given text and thus provide C-tests that do not only have the desired difficulty, but also integrate the learner\\'s interest or the current topic of a language course.\"']"
    ],
    "4845": [
        "The proposed method, SDM, represents sentence meaning with formal structures derived from DRT and including embeddings enriched with event knowledge.",
        "The SDM model represents events and their prototypical participants with distributional vectors linked in a network of syntagmatic relations extracted from parsed corpora.",
        "The compositional construction of sentence meaning in SDM is directly inspired by the principles of dynamic semantics.",
        "Current methods for representing sentence meaning generally lack information about typical events and situations, while SDM rests on the assumption that such information can lead to better compositional representations.",
        "The SDM model integrates event information activated by lexical items, which improves the performance on both the evaluation datasets.",
        "The reported scores on the DTFit dataset showed that not only SDM improves over simple and smoothed additive models, but also that the increase in correlation concerns the dataset items rated as most typical by human subjects.",
        "The fact that the best performing word embeddings in our framework are the Skip-Gram ones is somewhat surprising, and against the finding of previous literature.",
        "The dimensionality of the embeddings could be an important factor, much more than the choice of training them on syntactic contexts.']"
    ],
    "4847": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Sampling from 50-best lists outperforms beam search, albeit at a higher computational cost.",
        "Restricting the search space to not consider low-probability outputs can improve translation quality.",
        "Re-training the generator model without label smoothing or by restricting the search space to not consider low-probability outputs can improve translation quality.\"']"
    ],
    "4850": [
        "Our approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The proposed Cosine Annealing Strategy combines two methods to improve domain adaptation performance.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "Pseudo-labeled target-domain data is used to train the few-shot classifier.",
        "Our approach demonstrates promising results, but there is a limitation in that the current method relies on instructions provided by a language supervisor.",
        "The language supervisor can be replaced with an image-captioning model and question-answering model for real image observations.",
        "The instruction set used is specific to our problem domain, providing a substantial amount of pre-defined structure to the agent.",
        "Our experiments suggest that both would likely yield an HRL method that requires minimal domain-specific supervision, while yielding significant empirical gains over existing domain-agnostic works.",
        "Our work represents a step towards RL agents that can effectively reason using compositional language to perform complex tasks.']"
    ],
    "4854": [
        "The proposed Attention Guided Graph Convolutional Networks (AGGCNs) achieve state-of-the-art results on various relation extraction tasks.",
        "Unlike previous approaches, AGGCNs operate directly on the full tree and learn to distill the useful information from it in an end-to-end fashion.",
        "The proposed framework has multiple venues for future work, including using the proposed framework to perform improved graph representation learning for graph related tasks.",
        "The current model, GCN over pruned tree, predicts the relation to be response rather than sensitivity, and the reason is that the pruned tree misses crucial information (i.e., partial response).",
        "The AGGCN model predicts the correct relation for this instance by including an attention-guided layer, which is able to distill relevant information from the full tree in an end-to-end fashion.",
        "The AGGCN model can maintain a balance between including and excluding information in the full tree for learning a better graph representation.']"
    ],
    "4874": [
        "Pre-processing can have a strong effect on performance, sometimes more than modeling techniques, as is the case of i2b2.",
        "Concept types seem to offer useful information, perhaps revealing more general semantic information in the sentence that can help with predictions.",
        "Fine-grained Gold standard annotated concept types are most beneficial, but those from automatically extracted packages may also be useful as long as they consist of multiple types.",
        "Punctuation and digits may hold more importance in biomedical settings, but stop words hold significance in all settings.",
        "Reporting on one test set score can be problematic due to split bias, and a cross validation approach with significance tests may help ease some of this bias.",
        "Contextualized embeddings are generally helpful, but the featurizing technique is important: for CNN models, concatenating them with the word embeddings before convolution is most beneficial.",
        "Picking the right hyperparameters for a dataset is important to performance, and suggesting an initial manual hyperparameter search based on cross validation significance tests may be sufficient in most cases.",
        "Random search is a reasonable automated option for hyperparameter tuning, but requires more experience for picking the right search space and the right distributions for the hyperparameters.']"
    ],
    "4881": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "The use of self-attention mechanism can effectively capture the long distant dependency relations.",
        "The proposed ReCoSa model outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly more accurate than those detected by previous models.",
        "The use of proper detection methods, such as self-attention, can improve the quality of multiturn dialogue generation.\"]"
    ],
    "4898": [
        "Our proposed query-oriented summarization approach extracts important and query-relevant sentences of a corpus based on the definition of a fuzzy hypergraph over sentences, capturing semantic similarities between sentences.",
        "Existing graph and hypergraph-based summarizers rely on lexical similarities between sentences, which fail to capture semantic similarities.",
        "Our approach uses a probabilistic topic model to capture semantic relationships between sentences, and the resulting topics are modeled as hyperedges of a fuzzy hypergraph in which nodes are sentences.",
        "The algorithm simultaneously maximizes individual Relevance scores and joint Topical Coverage, which encourages the topical diversity of the resulting summary.",
        "Our topic-based fuzzy hypergraph model and sentence selection algorithm contribute to an improvement in the content coverage of the summaries, as measured by ROUGE scores.",
        "The thorough comparative analysis with other graph-based summarizers and summarizers presented at DUC contest demonstrates the superiority of our method in terms of content coverage.",
        "Future research directions include investigating how to adapt the model for related tasks including update summarization and community question answering, and incorporating sentence fusion and compression into the fuzzy hypergraph-based method.']"
    ],
    "4904": [
        "Our results show that a models with either (1) a completely unique decoders for each target language or (2) unique decoder attention parameters for each target language clearly outperform models with fully shared decoder parameters.",
        "The ZERO-SHOT PIVOT evaluation is the outlier in our results, with the EMB system outperforming the others.",
        "It is plausible that the language-independence of encoder output could be correlated with the amount of sharing in the decoder module.",
        "Because most non-English target tasks only have parallel training data in English, a unique decoder for those tasks only needs to learn to decode from English, not from every possible source task.",
        "The ATTN model, which partially shares parameters across target languages only slightly outperforms the DEC model globally, because of the improved performance of the ATTTN model on the lowest-resource tasks.",
        "Multi-lingual encoders still learn to share information across languages, even when trained using decoders that are unique to each target task.\"']"
    ],
    "4908": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Open-domain argument search is a challenging research problem, and previous methods have achieved low performance scores.",
        "Integrating topic information into the transformer network of BERT can significantly improve the performance of identifying pro-arguments.",
        "A good argument similarity function is only the first step towards argument clustering, and more realistic datasets are required for end-to-end evaluation.",
        "Arguments can address multiple aspects and therefore belong to multiple clusters, which is not possible to model using partitional algorithms.",
        "Future work should study the overlapping nature of argument clustering.']"
    ],
    "4909": [
        "We provide a detailed analysis of how translationese phenomena can adversely affect machine translation results.",
        "Our analysis shows that in general, translated text is longer than text originally written in a given language.",
        "Translationese is a problem for evaluation of systems, in particular in terms of comparison of system performance with automatic metrics such as BLEU.",
        "We recommend avoiding the use of test data that was created via human translation from another language.",
        "No previous work has aimed to provide more certainty about conclusions of human parity in MT.",
        "Our analysis provides missing analysis that should be included in the planning stage of future human evaluations of MT.",
        "We provide a checklist for planning upcoming MT evaluations, including the direction of test data creation, human judge reliability, testing level, testing language pairs, and translation sample size.\"']"
    ],
    "4935": [
        "The proposed measure of irregularity based on wug-testing is consistent with human judgments.",
        "The model is able to recover the correct inflected forms at a high rate for a subset of languages.",
        "Average irregularity varies significantly between languages.",
        "Irregularity is correlated with frequency both at the level of individual forms and at the level of lexemes.",
        "The correlation between the level of individual forms and the level of lexemes differs, with low-frequency forms free-riding on higher-frequency members of the lexeme.",
        "The findings provide credence to models of linguistic structure that group words together by their lexeme or stem.",
        "The wug-test techniques provide a general way of studying regularity and predictability within languages and may prove useful for attacking other difficult problems in the literature, such as detecting inflectional classes.",
        "By measuring which words or lexemes are most predictable from one another, a general picture of morphological relatedness within a language can be built in a bottom-up way.']"
    ],
    "4964": [
        "Our study is designed as a proof-of-concept and the main objective of this work is to study the utility of using SOTA approaches for generating artificial EHR data and to evaluate the impact of using this to augment real data for common NLP tasks in the clinical domain.",
        "Our results are promising, with most meaning preserved in the generated texts, and using generated text to augment real data in the training phase improved results for both extrinsic evaluation tasks (phenotype classification and temporal relation classification).",
        "Using only generated data was comparable to those using only real data, further indicating usefulness.",
        "This is the first study looking at the problem of generating longer clinical text, and that is extrinsically evaluated on two downstream NLP tasks.",
        "Although the MIMIC data is comprehensive, it represents a particular type of clinical documentation from an ICU setting, and further work is needed to extend to other clinical domains.",
        "If artificial data was to be used for further downstream tasks, particularly those that are intended to support secondary uses in a clinical research setting, further analysis is needed to assess the clinical validity of the generated text.",
        "We will invite clinicians to review the generated text with a focus on clinical validity aspects, and study further downstream NLP tasks.",
        "We propose a generic methodology to guide the generation in both cases, and our experiments show the utility of artificial data for neural NLP models in data augmentation setups.']"
    ],
    "5001": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "We have explored simple and effective ways to alleviate or eliminate the beam problem.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Ramp loss objectives show promise for neural sequence-to-sequence learning, especially when it comes to weakly supervised tasks where the MLE objective cannot be applied.",
        "Bipolar RAMP operates in both regions of the search space when extracting supervision signals from weak feedback.",
        "MRT can be turned into a bipolar objective by defining a metric that assigns negative values to bad outputs.",
        "The ramp loss objective is still superior as it is easy to implement and efficient to compute.",
        "Our novel token-level ramp loss objective RAMP-T can obtain further improvements over its sequence-level counterpart because it can more directly assess which tokens in a sequence are crucial to its success or failure.\"]"
    ],
    "5006": [
        "The ConMask model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The use of multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "The proposed end-to-end model for joint slot label alignment and recognition achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The system achieved bit rates that were significantly higher than the current state of the art in BCI communication.",
        "However, communication accuracies are currently insufficient for a practical BCI device, so future work must focus on improving these and developing an interface to present feedback to users."
    ],
    "5015": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Using SQuAD and Natural Questions as examples, we construct the ReQA SQuAD and ReQA NQ tasks, and evaluate several models on sentence-and paragraph-level answer retrieval.",
        "We find that a freely available neural baseline, USE-QA, outperforms a strong information retrieval baseline, BM25, on paragraph retrieval, suggesting that end-to-end answer retrieval can offer improvements over pipelined systems that first retrieve documents and then select answers within.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.\" (Section: \"In this paper, we propose a new multi-turn dialogue generation model, namely ReCoSa.\")",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans\\' judgements.\" (Section: \"In this paper, we propose a new multi-turn dialogue generation model, namely ReCoSa.\")",
        "Using SQuAD and Natural Questions as examples, we construct the ReQA SQuAD and ReQA NQ tasks, and evaluate several models on sentence-and paragraph-level answer retrieval.\" (Section: \"In this paper, we introduce Retrieval Question-Answering (ReQA) as a new benchmark for evaluating end-to-end answer retrieval models.\")",
        "We find that a freely available neural baseline, USE-QA, outperforms a strong information retrieval baseline, BM25, on paragraph retrieval, suggesting that end-to-end answer retrieval can offer improvements over pipelined systems that first retrieve documents and then select answers within.\" (Section: \"Using SQuAD and Natural Questions as examples, we construct the ReQA SQuAD and ReQA NQ tasks, and evaluate several models on sentence-and paragraph-level answer retrieval.\")']"
    ],
    "5017": [
        "Parallel computing is a necessity for ESGD",
        "The reported experiments are carried out in a distributed manner where SGD and fitness evaluation are conducted on multiple GPUs in parallel",
        "The wall clock time of ESGD is about the same as that an end-to-end vanilla SGD run",
        "A healthy population diversity is crucial for good performance of ESGD",
        "Using complementary optimizers and employing very short SGD and ES updates in each ESGD generation can prevent pre-mature convergence in ES and give good chances to produce better offsprings",
        "The initial parent population is created using models with randomized weights, which seems to be hurtful in the sense of fitness but in the long run it helps to establish diversity in the population and turns out to be better than just slightly perturb the initial anchor model to create the population",
        "Anchor models can help to accelerate the evolution process as the good \\'genes\\' of an anchor can spread out (with probability) to the next generations until it is replaced by another anchor with better \\'genes\\'",
        "With model backoff, elitist and anchor switching, it is guaranteed that the fitness of the best model of the population will never degrade",
        "The objective to optimize under ESGD is fitness which is CE loss in this work",
        "In essence, there is no difference from other deep learning optimization problems except that it is population based and makes use of complementary optimizers and gradient-aware/gradient-free algorithms\"']"
    ],
    "5036": [
        "We have presented an approach to systematically mine for parallel sentences in the textual content of Wikipedia, for all possible language pairs.",
        "Our approach uses a recently proposed mining approach based on massively multilingual sentence embeddings and a margin criterion.",
        "The same approach is used for all language pairs without the need of a language specific optimization.",
        "We make available 135M parallel sentences in 85 languages, out of which only 34M sentences are aligned with English.",
        "We were able to mine more than ten thousands sentences for 1620 different language pairs.",
        "This corpus of parallel sentences is freely available and can be used to improve the performance of NMT systems on low-resource languages.",
        "The mined texts could be used to first retrain LASER's multilingual sentence embeddings with the hope to improve the performance on low-resource languages, and then to rerun mining in Wikipedia.",
        "We also plan to apply the same methodology to other large multilingual collections, such as the WikiMatrix corpus, which is expected to have mostly well-formed sentences and not contain social media language.\"]"
    ],
    "5086": [
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "there is still a good deal of room for improvement in large-scale evaluation of vision and language integration systems, confirming that they still fall behind human performance by a large margin.",
        "designing novel evaluation measures and architectures that can adequately deal with the complexity of vision and language integration problems has the potential to address some of the challenges.",
        "our efforts in publishing this survey will help to systematize future research papers and also investigate the unsolved problems that are hindering the progress of effective integration of vision and language modalities.\"']"
    ],
    "5152": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "PReFIL exceeded the human baseline for FigureQA, but results were more nuanced for DVQA due to OCR model variations.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Charts in the wild: The charts in FigureQA and DVQA were methodologically generated, but human-generated charts in real-world business and scientific documents can contain variations that these datasets omit.",
        "Human-generated questions: The questions in both FigureQA and DVQA were created with templates, which do not capture all the nuances of natural language.",
        "Document-level CQA: FigureQA and DVQA have well-defined image regions, but information in the rest of the document may be necessary to answer questions about the chart.",
        "Our work has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments.']"
    ],
    "5159": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach to joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "We have introduced an additional slot carryover model and showed its impact on the model performance.",
        "Incorporating deep contextual word embeddings and combining the traditional fixed vocabulary approach significantly improved the joint goal accuracy on MultiWOZ-2.0.",
        "The field of machine reading comprehension has made significant progress in recent years, and human conversation can be viewed as a special type of context.",
        "Our proposed model can help dialog-related tasks benefit from modern reading comprehension models.",
        "Predicting whether a slot of state is none or not is an important finding that can inspire future dialog state tracking research.']"
    ],
    "5162": [
        "The CO-ATTN model assigns reasonable attention scores based on the relevance of the sentence to the source article.",
        "Sentence length does not determine attention score; instead, it depends on the number of important pieces of evidence in the sentence.",
        "The attention score accurately reflects the importance of sentences.",
        "Examining RTA data helps us understand the attention score assigned by our model.",
        "Bolded examples extracted by the expert from the source article that the student includes in the essay are important evidence.",
        "Sentence 3 has many details related to the source article, but it still has no evidence directly from the source article.",
        "Sentence 4 mentions \"The author did convince me that winning the fight against poverty is achievable in our lifetime\" which comes from both the prompt and the source article, but this statement is so general that almost every student mentions this statement in the essay.",
        "Sentence 5 is short, but it mentions one piece of evidence.",
        "Sentence 6 talks about farming, which is a topic from the source article.",
        "Sentence 7 also mentions conditions of hospitals nowadays, and sentence 8 talks about the school."
    ],
    "5163": [
        "The reported performance of prosodic prominence prediction is still quite low, even for state-of-the-art systems based on large pre-trained language models such as BERT.",
        "The annotation method for prosodic prominence has been shown to be quite robust, but errors in automatic alignment, signal processing, and quantization introduce noise to the labels.",
        "The noise in the labels can affect the test results, and manual correction of a part of the test set could be beneficial.",
        "Different speakers have different accents, reading proficiency, and reading tempo, which all impact the consistency of the labeling as the source speech data contains in total samples from over 1200 different speakers.",
        "The authors have introduced a new NLP dataset and benchmark for predicting prosodic prominence from text, which is the largest publicly available dataset with prosodic labels.",
        "BERT outperforms other models with just up to 10% of the training data, highlighting the effectiveness of pre-training for the task.",
        "The implicit syntactic or semantic features BERT has learned during pre-training are relevant for the specific task of predicting prosodic prominence.",
        "The authors will focus their future research activities on two fronts: improving the quality of prominence annotation and adding prosodic boundary labels, and developing methods and models for improved prediction of prosodic prominence.']"
    ],
    "5173": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Experimental results show that our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Neural models are not able to capture the more subtle interplay between affordances and properties.",
        "Priorless models that learn from statistical associations falter.",
        "The depth of networks used in models such as ELMo and BERT leads to complex inter-parameter structure, but the latent semantic patterns that describe physical commonsense are much weaker than more superficial patterns that arise due to grammar or domain.",
        "The rise of physics engines improves our ability to model physical inferences.",
        "Exploring the mechanisms underlying communication using an implicit shared world model will require developing access to such a world model or exposing algorithms to predictions of that world model by directly querying humans.",
        "Bridging the inductive biases learned from simulation and those discovered by scientists will lead to a more cohesive model of commonsense physics.']"
    ],
    "5178": [
        "The sentiment cannot substitute the stance in general, as shown by the words choice gap and the mismatch between in-favor and positive stance. (supporting evidence: Appendix A)",
        "Using sentiment purely to predict public opinion can lead to misleading results and truncated findings. (supporting evidence: the failure of sentiment to distinguish supporter viewpoints)",
        "The negative sentiment can help discover against stances, but it will be mixed with a proportion of supporter viewpoints. (supporting evidence: the noticeable disparity between sentiment and stance for a given topic)",
        "The sophistication of stance detection cannot be simply captured using the sentiment polarity. (supporting evidence: the failure of sentiment to capture the real stance)",
        "Researchers should be more cautious when identifying viewpoints toward an event and take into account the clear difference between sentiment and stance. (supporting evidence: the mismatch between in-favor and positive stance)",
        "The use of sentiment purely can lead to truncated results, and negative words tend to be similar to against words while the matching cases are minuscule. (supporting evidence: the failure of sentiment to capture the real stance and the noticeable disparity between sentiment and stance)']"
    ],
    "5268": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency in inference time and parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The graph-based parser is now as accurate as the transition-based parser on shorter dependencies and dependencies near the leaves of the tree, thanks to improved representation learning that overcomes the limited feature scope of the first order model.",
        "Both parsers improve their accuracy on longer sentences, with some models for some languages in fact being more accurate on medium-length sentences than on shorter sentences."
    ],
    "5291": [
        "Defensive distillation does not have the same effect for text classification as it has for image classification.",
        "The robustness against adversarial examples of networks trained with defensive distillation increases only slightly.",
        "Gradient masking [25, 20] does not have a significant effect on our algorithm.",
        "Other methods based on gradient masking are also not effective in the text domain.",
        "The exact characteristics of the gradient are not important and the gradient masking itself has only a minimal effect on our algorithm.",
        "If this hypothesis is true, this might mean that other methods based on gradient masking are also not effective in the text domain.",
        "The question still remains open whether different methods from image classification can be successfully transferred to text classification or whether completely new approaches must be developed.\"']"
    ],
    "5304": [
        "The proposed approach for unsupervised lemmatization based on agglomerative clustering achieves promising results, surpassing the baseline on most of the evaluation datasets.",
        "The approach is able to capture global sentential information and outperform the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "The method is effective in attenuating biases in word representations using a projection-based approach, especially for the well-characterized gender direction.",
        "The approach can handle different languages, including analytical languages with practically no inflection, and achieve an improvement over the baseline on most of the datasets.",
        "The method is able to capture the relationship between word forms and their meanings using a combination of edit distance and embedding similarity, leading to improved performance on some datasets.",
        "The approach can be further improved by actively looking for regularities in the clusters to recognize inflectional paradigms and refine the clusters.",
        "The use of contextual word embeddings, such as BERT, could potentially solve the issue of homonymy and improve the approach's performance.\"]"
    ],
    "5307": [
        "Our results suggest that even the smaller base BERT model is significantly overparametrized.",
        "Disabling both single and multiple heads is not detrimental to model performance and in some cases even improves it.",
        "We found no evidence that attention patterns that are mappable onto core frame-semantic relations actually improve BERT\\'s performance.",
        "Fine-tuned BERT does not rely on this piece of semantic information and prioritizes other features instead.",
        "We proposed a set of methods for analyzing self-attention mechanisms of BERT, comparing attention patterns for the pre-trained and fine-tuned versions of BERT.",
        "Our most surprising finding is that, although attention is the key BERT\\'s underlying mechanism, the model can benefit from attention \\'disabling\\'.",
        "There is redundancy in the information encoded by different heads and the same patterns get consistently repeated regardless of the target task.",
        "We believe that these two findings together suggest a further direction for research on BERT interpretation, namely, model pruning and finding an optimal sub-architecture reducing data repetition."
    ],
    "5314": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "There is a large positive correlation between APS and the likelihood of correct classification for samples containing intensifiers.",
        "The absence of significance in the case of discourse markers and presence of social process words implies that the normal classifier is still able to learn reliable representations invariant of stress.",
        "Decorrelating spontaneity has a significant impact on the performance of the model, with the adversarial model outperforming the normal model for certainty and hesitation categories.",
        "The use of words in the certainty category and all hesitation categories for activation benefits from the model trained adversarially to decorrelate spontaneity and emotion representation.",
        "Spontaneous speech has been shown to have more of certain linguistic properties, such as words in the certainty category, that are beneficial for the model's performance.\"]"
    ],
    "5348": [
        "The accuracy of the described QA system benefits from our re-ranking approach.",
        "Our re-ranking method can improve the performance of already deployed QA systems.",
        "The performance gain is small, and there may be several reasons for this.",
        "Integrating spell-checking in our re-ranking method proved to be effective.",
        "The re-ranking model is based on very simple features.",
        "More advanced features or models could improve the ranking performance.",
        "The re-ranking algorithm cannot choose answer candidates beyond the top-10 results.",
        "A meta-model can be used to estimate weaknesses of the QA model and guide data labelling for a targeted improvement.",
        "Data labelling and incremental model improvement can be scaled by crowdsourcing.",
        "Crowd-supervised re-ranking allows us to train improved re-ranking models.",
        "A meta-model that detects queries which are prone to error can be used to guide the sample selection for costly human data augmentation and creation.']"
    ],
    "5358": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our iterative attention mechanism allows for the recognition of ASL fingerspelling in the wild without relying on hand detection, segmentation, or pose estimation modules.",
        "The new data set of fingerspelling in the wild with crowdsourced annotations is larger and more diverse than any previously existing data set.",
        "Training on the new data significantly improves the accuracy of all models tested.",
        "Our iterative attention approach is applicable to other fine-grained gesture or action sequence recognition tasks.",
        "The signing hand(s) are spatially close to the face during fingerspelling, so we crop a region centered on the bounding box which is 3 times larger.",
        "Scaling the original frame based on the size of the face bounding box is used to make the scale of hands in different input sequences roughly uniform.']"
    ],
    "5371": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our approach outperforms benchmark models across different datasets.",
        "Acknowledgements meet two very helpful criteria: they use emotive language visibly different from the main body of a scientific manuscript, and they have a very standard and narrow communicative function.",
        "Examples can be difficult to separate from e.g. remark and proposition.",
        "Our investigations have shown that a careful grouping of related classes, while retaining 99% of available annotated data, is essential for reaching state-of-the-art performance with known models on this task.",
        "We have demonstrated that the performance of the same baseline model improves from 0.67 to 0.91 F1 score through this type of empirical curation.",
        "Achieving a high F1 score in the announcement of the task gives us some confidence of data quality and experimental design that allow for state-of-art methods to compete.",
        "We do not have the capacity to provide a real human evaluation on the task as posed, in order to set a natural \\'best\\' baseline, which would certainly be less than a perfect score.\"']"
    ],
    "5385": [
        "Learning the difficulty parameters of items and the ability parameters of DNN models allows for more nuanced interpretation of model performance and enables us to filter training data so that DNN models can be trained on less data while maintaining generalization as measured by test set performance.",
        "IRT models with machine RPs can be fit not only for NLP data sets but also data sets in other machine learning domains such as computer vision.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans\\' judgments.",
        "Using the previously discarded data to learn IRT models and estimate latent difficulty and ability parameters can be used to improve a variety of tasks such as model selection, data selection, and curriculum learning strategies.",
        "It is possible to fit IRT models using RPs from DNN models. Prior work relied on human RPs to investigate the impact of difficulty on model performance (Lalor et al., 2018 ), but it is now possible to conduct similar IRT analyses with machine RPs.",
        "This work also opens the possibility of fitting IRT models on much larger data sets. By removing the human bottleneck, we can use ensembles of DNN models to generate RPs for large data sets (e.g. all of SNLI or SSTB instead of a sample).\"']"
    ],
    "5408": [
        "Increased model size had limited benefits; models with 400 hidden units performed significantly better than smaller models, but further increases in network size had no effect.",
        "We found a striking difference in agreement accuracy between short and long coordinated verb phrases: performance on short phrases was poorer.",
        "Other studies suggest that Transformer models suffer from similar problems as the LSTMs we have analyzed.",
        "Dramatically increasing the pre-training corpus for a BERT-like model from 562M words to 18G words only leads to a modest improvement in its natural language inference accuracy.",
        "Learning syntax from realistic amounts of data-in particular the amount of data available to humans when they learn language-may require syntactically structured architectures or explicit syntactic supervision.",
        "The security guards like injured himself/*themselves .",
        "Corpus Size 2m 10m 20m 40m 80m (c) Reflexives: Across Figure 5 : Language model agreement performance between reflexive pronouns and their antecedent in simple transitive sentences (5a), when agreement occurs within a sentential complement (5b), and when there is an intervening subject relative clause (5c).",
        "Human results are those reported by Marvin and Linzen (2018) .",
        "Table 4 : Training tokens needed for LSTMs to achieve 99.99% accuracy in each condition that does not presently reach 99% accuracy.\"']"
    ],
    "5439": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Experimental results on the commonly used English Switchboard test set show that our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "When the size of pre-training data is under 25k (training data size of DRCD), we can see that there is no much difference whether we use Chinese or English data for pre-training, and even the English pre-trained models are better than Chinese pre-trained models in most of the times.",
        "When there is no training data available for the target language, firstly, we provide several zeroshot approaches that were initially trained on English and transfer to other languages.",
        "The proposed model could give consistent and significant improvements over various state-of-the-art systems by a large margin and set baselines for future research on CLMRC task.",
        "Future studies on cross-lingual machine reading comprehension will focus on ... cross-lingual machine reading comprehension without the translation process, etc.\"']"
    ],
    "5462": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Our content planner first identifies salient keyphrases and a proper language style for each sentence, then the realization decoder produces fluent text.",
        "Our model considers three tasks of different domains on persuasive argument generation, paragraph generation for normal and simple versions of Wikipedia, and abstract generation for scientific papers.",
        "Our model obtains significantly better BLEU, ROUGE, and METEOR scores than nontrivial comparisons.",
        "Human subjects also rate our model generations as more grammatical and correct when language style is considered.\"']"
    ],
    "5466": [
        "The proposed method for mitigating gender bias in text data successfully mitigates indirect gender bias and maintains the interpretability of the space.",
        "The Names Intervention technique is effective in reducing the cluster purity of previously biased words, with an average reduction of 49% in cluster purity.",
        "Counterfactual Data Substitution is a faster and more efficient method for mitigating gender bias compared to other techniques.",
        "The reliance on predefined lists of gender words is a fundamental limitation of all the methods compared.",
        "The use of pairings in bias mitigation can perpetuate a male reading of certain words, leading to counterproductive results.",
        "The strict use of pairings imposes a gender binary and ignores non-binary identities.",
        "Future work should extend the Names Intervention to names from other languages and explore the possibility of a many-to-one mapping or a probablistic approach.",
        "The Names Intervention could also be used to mitigate racial biases, but finding pairings could prove problematic.']"
    ],
    "5496": [
        "We propose two self-supervised tasks to tackle the training data bottleneck.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The reference translations are insufficient for the task, and automatic MT evaluation is effectively useless for assessing terminological subtleties.",
        "Terminology lists may be a good help for both MT and MT evaluation, but an interactive system supporting a domain expert in manual correction of terminological choices is the only practically possible ultimate solution for translation.",
        "The main problem was that all the systems made the same (and from the readers\\' perspective, the most severe) translation error by translating the terms \\'tenant\\' and \\'lessee\\' using the same Czech word \\'n\u00e1jemce\\', which made the whole text incomprehensible.\"']"
    ],
    "5501": [
        "The ConMask model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The system serves as a demonstration of the stunning progress that NLP technology has made in the last two years.",
        "Aristo has achieved surprising success on a formidable problem by leveraging large-scale language models.",
        "The system is not familiar with certain types of questions, such as those requiring diverse pieces of evidence to be combined, reading comprehension, meta-questions, and arithmetic.",
        "Science exams are just one of many different, partial indicators of progress in broader AI.",
        "Aristo not only answers questions correctly but also changes its answer based on the context, suggesting that some form of reasoning is occurring.",
        "Other work has found that neural systems can learn systematic behavior, and these emergent semantic skills are a key contributor to Aristo's scores reaching the 90% range.",
        "Large-scale language model architectures have brought a dramatic, new capability to the table that goes significantly beyond just pattern matching and similarity assessment.\"]"
    ],
    "5530": [
        "The chatbot developed by the authors provides a safe platform for survivors of sexual harassment to share their experiences and receive proper assistance.",
        "The chatbot uses the Telegram API and combines two models to classify harassment types with over 80% accuracy.",
        "The chatbot also performs named entity recognition (NER) with satisfactory results for location and dates, and good results for time events.",
        "The slot-filling based chatbot encapsulates the classification and NER frameworks into the dialogue flow.",
        "The authors plan to improve the interaction with the chatbot by validating the script flow through specific focus groups, and explore more technical and security possibilities in the future.",
        "The chatbot aims to provide adaptable and less \"linear\" functionality, showing empathy when needed.']"
    ],
    "5544": [
        "The ability of ELMo contextualised word embedding models to disambiguate word senses depends on the nature of the training data.",
        "Using lemmatized training data yields small but consistent improvements in the word sense disambiguation task for rich-morphology languages like Russian.",
        "Better WSD scores of lemma-based models are related to their better handling multiple word forms in morphology-rich languages.",
        "Lemmatization is not a silver bullet, and it can even hurt the performance in other tasks where inflectional properties of words are important.",
        "The nature of the language being used matters, and differences in this nature can result in different decisions being optimal or sub-optimal at the stage of deep learning models training.",
        "English is not representative of all languages on Earth, and more research is needed to analyze other languages.",
        "Including more languages in the analysis will help to verify the claim that inflection differences are important for training deep learning models across human languages in general.']"
    ],
    "5545": [
        "Our results show that exploiting core components of the Transformer to embed linguistic knowledge leads to higher and consistent gains than previous approaches.",
        "Conversely, dependency-aware self-attention mechanisms (LISA and PASCAL) best embed syntax, for all corpus sizes, with PASCAL consistently outperforming other all approaches.",
        "Our implementation of the multi-task approach by Currey and Heafield (2019) where a standard Transformer learns to both parse and translate source sentences.",
        "We adapt Linguistically-Informed Self-Attention (LISA; Strubell et al. 2018) to NMT.",
        "The model is trained to maximize the joint probability of translations and parent positions.",
        "On small-scale data, we train for 20K steps and use a dropout probability P drop = 0.3 as they let the Transformer baseline achieve higher performance on this size of data.",
        "Our baseline outperforms the one in Currey and Heafield (2019) by +3.5 BLEU.\"']"
    ],
    "5603": [
        "The use of a projection-based method can effectively attenuate biases in contextualized embeddings without loss of entailment accuracy.",
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Combining multi-head attention with convolution provides an effective performance compared to individual components.",
        "The addition of convolutions after the multi-head makes the model more expensive, but the lower representation dimension of the filters reduces the cost.",
        "Multi-head attention identifies long dependencies in extracting relations and events, while convolutions provide the additional benefit of capturing more local relations, which improves the performance of existing approaches.",
        "The finding that CNN-before-MHA is outperformed by MHA-before-CNN is interesting and could be used as a competitive baseline for future work.']"
    ],
    "5628": [
        "Our experiments provide some guidance for managing the budget of constructing a new dialog dataset.",
        "For dialog tasks that have more complex dialog states and action space like LaptopNetwork, supervision from all four modules leads to much higher performance and requires significantly fewer number of dialogs (e.g., 40% in LaptopNetwork).",
        "In CamRest676 for example, we obtain annotations for natural language understanding by calculating the difference of the current and previous dialog states.",
        "We propose Modular Supervision Network (MOSS), an end-to-end trainable framework that incorporates supervision from various intermediate dialog system modules.",
        "Our experiments show that the more supervision the model has, the better the performance.",
        "If more supervision is included, the model needs less number of training dialogs to reach state-of-the-art performance.",
        "In addition, such benefit is observed even larger when the dialog task has a more complex dialog state and action space, for example, LaptopNetwork.\"']"
    ],
    "5636": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Humans correctly answer questions while reading only 20% of the sentences in the full passage, showing the potential of our approach for assisting humans in question answering tasks.",
        "Agent-selected evidence is generalizable.",
        "We exploit these capabilities by employing evidence agents to facilitate QA models in generalizing to longer passages and out-of-distribution test sets of qualitatively harder questions.",
        "Passage (DREAM) W: What changes do you think will take place in the next 50 years? M: I imagine that the greatest change will be the difference between humans and machines.\"']"
    ],
    "5651": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgements.",
        "Improving the out-of-domain performance could come at the expense of decreased in-domain performance, especially for DFL.",
        "Increasing \u03b3 increases debiasing and thus hurts in-domain accuracy on SNLI, but out-of-domain accuracy on the SNLI hard set is increased within a wide range of values.",
        "Our debiasing methods reduce the correlation between the element-wise loss of the debiased models and the loss of a bias-only model on the considered datasets.",
        "Compared to the baselines, our debiasing methods, DFL and PoE, reduce the correlation to the bias-only model, confirming that our models are effective at reducing biases.",
        "Our proposed debiasing techniques are model agnostic, simple, and highly effective.",
        "Our debiasing techniques result in better generalization to other NLI datasets."
    ],
    "5669": [
        "The results on synthetic datasets are on par with or surpass state-of-the-art published systems.",
        "The structural models are effective in preserving salient source relations in summaries.",
        "The performance on Human100 is lower overall, but the findings are encouraging.",
        "The property of the data that different scenarios for the same topic have a high vocabulary overlap can be seen in Human100.",
        "Shallow processing based models are penalized while deeper semantics are encouraged.",
        "A larger dataset would be useful to strengthen quantitative analysis.",
        "Variable-size scenarios would be more realistic for evaluating the more general case.",
        "Improving the crowdsourcing technique in future work is important.",
        "Scenarios which exhibit more nuanced conflicting points that capture a wider range of cues are targeted by the Active Interpretation of Disparate Alternatives (AIDA) program.",
        "Taking sentence order into account and encoding connections between words can lead to sizable performance improvements.']"
    ],
    "5694": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Developing annotated datasets from multiple sources based on existing datasets and the proposed causality tagging scheme can improve the performance of SCITE.",
        "Combining SCITE with distant supervision and reinforcement learning can achieve better performance without needing a high-quality annotated corpus for causality extraction.",
        "The inability to locate evidence of SDIs/SSIs can challenge clinician ability to advise patients and cause risks for consumers of dietary supplements.",
        "Extracting evidence for SDIs/SSIs from a large corpus of scientific literature can offset some of these risks.",
        "NLP techniques can be extraordinarily useful for extracting information and relationships specific to an application domain in healthcare.",
        "Re-purposing existing labeled data from related domains can derive maximum utility from curation efforts.",
        "The lack of regulation in the supplement space introduces dangers for the many users of these supplements.",
        "Claims of interactions are difficult to validate without links to source evidence.",
        "An NLP pipeline can be used to detect SDI/SSI evidence from scientific literature, leveraging UMLS identifiers, scispaCy for NER and entity linking, BERT-based language models for classification, and labeled data from a related domain for training.']"
    ],
    "5701": [
        "The proposed MANN-inspired models for machine translation have achieved state-of-the-art results on many language pairs.",
        "The NTM Style Attention and Memory-Augmented Decoder extensions have improved the translation quality by 0.2-0.5 BLEU on the low resource Vietnamese\u2192English translation task and 0.3-0.9 lower BLEU on the Romanian\u2192English translation task.",
        "A content-based addressing mechanism is sufficient to encode a strategy of monotonic iteration through source sentences, and extending the memory capacity of the decoder does not offer an advantage.",
        "The pure MANN model performs marginally better than the attentional encoder-decoder for the Vietnamese\u2192English translation task, but performs 0.3-1.9 BLEU worse for the Romanian\u2192English translation task.",
        "MANNs in their current form do not improve over the attentional encoder-decoder for machine translation.",
        "The pure MANN model is very general and does not incorporate any domain-specific knowledge.",
        "With the development of improved MANN architectures, MANNs could achieve state-of-the-art results for machine translation.']"
    ],
    "5703": [
        "The model based on gaze features and part-of-speech information achieves accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerge as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "ESuLMo with 500 subwords outperforms the original ELMo, and BPE is consistently better than ULM on all evaluations under the same settings.",
        "BPE can give static subword segmentation for the same word in different sentences, while ULM cannot.",
        "ESuLMo is sensitive to segmentation consistency.",
        "The overlapping rates of the two algorithms are similar.",
        "Our model can outperform the original ELMo in terms of PPL.",
        "Subwords can represent word better than characters to let ESuLMo more effectively promote downstream tasks than the original ELMo.']"
    ],
    "5705": [
        "The best performing setup uses BERT-German with metadata features and author embeddings, which leads to better classification performance.",
        "Including related data or making certain characteristics more explicit helps improve the classification performance in a low-resource scenario.",
        "The blurbs do not provide summary-like abstracts of the book, but instead act as teasers, intended to persuade the reader to buy the book.",
        "The BERT model trained for German outperforms the multilingual BERT model by a significant margin for sub-task A and sub-task B.",
        "Precision is considerably higher than recall for all setups, which may be due to the fact that for some of the 343 labels in sub-task B, there are very few instances.",
        "The model has a tendency to higher precision rather than recall in sub-task B, and this may be improved by taking the most detailed label as correct and manually fixing the higher level labels accordingly.",
        "An MLP with more and bigger layers could improve the classification performance, but this would require more training data.']"
    ],
    "5708": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness study confirms that the proposed Ad-aBERT can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "RL fine-tuning of language models can be applied to four NLP tasks, including stylistic continuation with high sentiment or physically descriptive language, and summarization on the CNN/Daily Mail and TL;DR datasets.",
        "The use of human reward learning to natural language tasks is important both from a capability and safety perspective.",
        "Purely supervised training is insufficient to correct mistakes that arise when sampling from trained policies, and RL training to programmatic reward functions such as BLEU or ROUGE is insufficient.",
        "Interactive tasks such as dialogue are particularly relevant for RL fine-tuning of language models, as it is difficult to define the goal of a dialogue without the human participant.']"
    ],
    "5733": [
        "Despite the scientific effort...we still have no adequate model of this process.",
        "A biased parser that is well-matched to English structures appears to show that LSTMs operate more syntactically than they may actually do.",
        "The goal is not simply to obtain the best accuracy, but to do so in a plausibly language agnostic way.",
        "If one\\'s goal is not to understand the general problem of structure, but merely to get better parsers, language specific biases are certainly on the table.",
        "We argue that the work should make these goals clear, and that it is unreasonable to mix comparisons across models designed with different goals in mind.",
        "The works employing this parser represent a meaningful step forward in the important scientific question of how hierarchical structure is inferred from unannotated sentences.",
        "Children demonstrate knowledge of word order from their earliest utterances.",
        "Even prelexical infants are aware of word order patterns in their first language before knowing its words.\"']"
    ],
    "5747": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "The dimensionality of word vectors significantly affects the performance in the similarity task using the NMF method of factorization.",
        "Decreasing the dimensionality of the NMF method to 100 may give better results.",
        "NMF performs poorly in the analogy task due to the non-negativity of the word vectors produced.",
        "Using R is better than using Q in the QR decomposition for the similarity and analogy tasks.",
        "The truncated SVD provides a far better solution than the NMF and the truncated QR in both similarity and analogy tasks.']"
    ],
    "5770": [
        "The Bi-CS model achieves good performance consistently, outperforming all models in LM and performing second best on the categorization task.",
        "The Bi-CS model requires the least data and supervision requirements among all models.",
        "The performance of the models is task-dependent, and the models show almost opposite ordering of performances (excluding Bi-CS).",
        "The use of word embeddings improves the performance of the categorization and CS LM tasks.",
        "The proposed Bi-CS model is simple yet effective, and it only requires monolingual corpora along with a small amount of CS data.",
        "The Bi-CS model outperforms the existing approaches on the intrinsic evaluation.",
        "Incorporating bilingual word embeddings into language modeling for automatic speech recognition is a future work that may improve the performance.']"
    ],
    "5784": [
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "The impact of vocabulary size on BERT performance:",
        "Alternative vocabulary pruning strategies:",
        "The efficacy of mixed-vocabulary training over vanilla distillation:"
    ],
    "5794": [
        "AutoJudge correlates well with human judgments and is useful for measuring the progress of a dialogue system.",
        "AutoJudge generalizes well to unseen strategies for the same domain.",
        "AutoJudge can be applied to deployed systems where gold-standards are not available.",
        "AutoJudge shows promising results when applied as an answer selection module.",
        "The main reason why AutoJudge cannot properly handle bad utterances generated during the initial phase of reinforcement learning is because there are different types of \"bad\" utterances, and the training mechanism of AutoJudge needs to be adapted.",
        "Trained metrics suffer from instabilities, which might be caused by the size of the dataset.",
        "The correlation between human judgments and the outputs of AutoJudge is high, but it is not clear which aspects of the context or the response are relevant for the predicted rating.",
        "There is no clear definition for \"adequate\" responses in conversational dialogue systems, and this is an important future work problem that needs to be addressed.']"
    ],
    "5812": [
        "COMB1:LID-MonoLT and COMB2:MonoLT-LID outperform the baselines.",
        "The performance increases slightly from 77.41% to 77.66% for the combined conditions.",
        "The results for MonoLT-SVM are the highest for the combined conditions for MSA-EGY.",
        "The worst results are for condition MonoLT-Conf.",
        "Almost all the accuracies achieved by the combined conditions are higher than the Spanglish data set\\'s baselines.",
        "The only combined condition that is lower than the baselines\\' of the Spanglish data set is \\'COMB2:MonoLT-LID\\' condition.",
        "The boosts in accuracy are of at least 2% for all accuracy results of the combined conditions for the Bangor corpus and its baselines.",
        "The trends seem to be the same between the two language pairs.",
        "Both language pairs achieve the highest performance with MonoLT-SVM and worse results with MonoLT-Conf.",
        "The gains in performance from using a learning algorithm are likely due to the fact that the learner is taking advantage of both monolingual tagger outputs and is able to go beyond the available tags for cases where there are errors in both.\"']"
    ],
    "5830": [
        "Our proposed model outperforms benchmark models across different datasets.",
        "The structural models are on-par with or surpass state-of-the-art published systems.",
        "Our simplest proposed model achieved the largest improvements over the baseline when it came to the average embedding similarity and vector extrema similarity.",
        "The baseline models tend to generate responses containing slots and values for the wrong domain.",
        "Including a simple retrieval step leads to very large gains in the success of providing the inform/request slots.",
        "Our proposed model performs better than the state-of-the-art when it comes to providing the request slots, but still falls behind the state-of-the-art when it comes to providing inform slots.",
        "Our model remains competitive without requiring turn labels.",
        "Generating fluent and precise responses is crucial for creating goal-oriented dialogue systems, but this can be a very difficult task.",
        "The Exemplar-HRED model becomes more accurate in responding for the correct domain.",
        "Adding a simple retrieval step improves over multiple strong baseline models on word overlap metrics and achieves competitive performance for inform/request metrics without requiring dialog act annotations.']"
    ],
    "5840": [
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "BERT achieves higher accuracy than prior contextualized word representations models, such as context2vec and ELMo.",
        "Our BERT WSD models outperform prior neural WSD models by a large margin.",
        "The attention mechanism in transformer captures the surrounding words, and the hidden layer outputs can be viewed as contextual features.",
        "Incorporating BERT with our proposed strategies achieves significantly higher scores than prior neural and feature-based WSD approaches.\"']"
    ],
    "5844": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The performance of state-of-the-art models substantially deteriorates as the size of the accompanying context increases.",
        "The model operates over explicit relational knowledge, connecting documents and sentences extracted from large text corpora.",
        "DGN outperforms a baseline adopting a sequential reading strategy.",
        "When trained to retrieve just supporting facts, the performance of the baseline degrades by \u224820%.",
        "The approach can be combined with structured and distributional sentence representation models.",
        "Investigating the role and impact of different structured sentence representation models within the inference process is a future research direction.",
        "Injecting richer structured knowledge in the model, allowing for fine-grained message passing and improved representation learning, is a potential research direction.",
        "Scaling the approach to massive text corpora such as the whole Wikipedia is an important future research direction.']"
    ],
    "5845": [
        "document-level NMT with a thorough qualitative analysis and expose the limit of its improvements in terms of context length and model complexity.",
        "a dominant cause of the improvements by document-level NMT is actually the regularization of the model.",
        "Not all of the words in the context are used in the model; we leave out redundant tokens without loss of performance.",
        "A long-range context gives only marginal additional improvements.",
        "Word embeddings are sufficient to model document-level context.",
        "For a fair evaluation of document-level NMT methods, we argue that one should make a sentence-level NMT baseline as strong as possible first.",
        "Targeted test sets (Bawden et al., 2018; Voita et al., 2019) might be helpful here to emphasize the document-level improvements.",
        "However, one should bear in mind that a big improvement in such test sets may not carry over to practical scenarios with general test sets, where the number of document-level errors in translation is inherently small.\"']"
    ],
    "5864": [
        "The proposed architecture successfully tackles the multi-task problem with results that are promising in terms of usability and applicability.",
        "The architecture models semantics through multiple Dialogue Acts and Frame-like structures in an end-to-end fashion, providing rich representations across the whole network.",
        "The approach is scalable and can be applied to a dataset addressing similar problems, outperforming off-the-shelf tools.",
        "The shortcut connections help in the more fine-grained tasks, successfully encoding richer representations.",
        "CRFs help when longer spans are being predicted, especially in upstream tasks.",
        "The seq2seq design allows for a multi-label approach, enabling the identification of multiple spans in the same utterance that might evoke different dialogue acts/frames.",
        "The current seq2seq approach carries some limitations, especially on the Frame Semantics side, as it does not support nested predicates well.",
        "The authors are working on modeling nested predicates through the application of bilinear models.']"
    ],
    "5876": [
        "The traditional criteria for basic color terms are not robust.",
        "The ability to produce color templates challenges the notion of monomorphemicity.",
        "Many color words have back-translations that violate abstractness.",
        "The heterogeneity of color naming data contests the salience requirement.",
        "The traditional sequence of color terms and the coarse division between basic and non-basic colors are not supported by empirical evidence.",
        "Color terms can be decomposable, and we can use this to generate missing color words.",
        "Cross-lingual patterns of word formation can be exploited for future work.",
        "The divide between basic and secondary color terms is blurred, and models should employ graded membership models like fuzzy set theory.",
        "This paper provides empirically-grounded computational linguistic metrics with evidence from 2491 languages.",
        "The aggregation of features correlates strongly with the Berlin and Kay basic/secondary color term partition, and predicts the Berlin and Kay hypothesized universal acquisition sequence.']"
    ],
    "5899": [
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "the use of visual information for the model still remains a challenging task.",
        "adversarial approaches more upstream in the pipeline like in the visual features extraction (as previously investigated in (Delbrouck and Dupont, 2017a) ).",
        "the use of attention-based encoder-decoder models can improve the accuracy of the model.",
        "the objective function is given by: EQUATION ..."
    ],
    "5936": [
        "The proposed approach to learning embeddings of social media users is robust and can handle both novel users and data drawn from future time periods.",
        "The proposed embeddings are learned using a metric learning coupled with a novel training regime, which results in invariant user representations.",
        "The approach can be scaled up to Web-scale datasets consisting of millions of users, as has been successfully done for face recognition.",
        "The proposed method can be used for open-world author ranking tasks by learning a vector space with a meaningful distance.",
        "The approach can be extended to explore cross-domain author attribution and community composition beyond the scope of this paper.",
        "The proposed model presents a double-edged sword, as it could be used to identify authors with legitimate reasons to remain anonymous, such as political dissidents, activists, or oppressed minorities.",
        "The source code will be released to encourage positive applications and not shared with the broader community.']"
    ],
    "5948": [
        "The authors have advanced the field of topical content analysis and the study of public health mentions in tweets, particularly with regards to tobacco products and e-cigarettes.",
        "The authors have collected a dataset of 3144 tweets related to tobacco and e-cigarettes, which they have analyzed using a novel approach based on slang terminology.",
        "The inclusion of tweets into the corpus based on slang terminology has not been attempted before, and the authors' approach is an attempt to analyze the Twitter landscape in the language of the audience.",
        "The authors have identified that tobacco and related drugs are tweeted about and spoken of frequently, but the linguistic cues common among these tweets were not considered until now.",
        "The authors have found that using common slang as a basis for dataset creation and filtration for this task has not been attempted before, and their approach is an attempt to analyze the Twitter landscape in the language of the audience.",
        "The authors have identified that the inclusion of tweets into the corpus based on slang terminology is an attempt to analyze the Twitter landscape in the language of the audience, which most highly correlates with the demographic of consumers for the aforementioned products.",
        "The authors have found that the linguistic cues common among these tweets were not considered until now, and their approach is an attempt to analyze the Twitter landscape in the language of the audience.\"]"
    ],
    "5987": [
        "The use of a polyglot source model helps in four out of five cases using single treebank target models.",
        "The two source languages that have lowest LAS when using monolingual parsers, namely Danish and Swedish, see significant improvements when switching to a polyglot model.",
        "The best performing single target model is trained on Faroese trees projected from Norwegian Bokm\u00e5l trees produced by a polyglot model.",
        "The use of a polyglot model introduces a new interaction point for cross-lingual features via the feature extractor of the polyglot parser.",
        "Multi-treebank training for the POS tagger and parser improves results for all settings.",
        "Using multiple pre-trained embeddings in a model with a shared vocabulary can be effective, and using available cross-lingual word embedding tools may provide better representations.",
        "The use of contextual embeddings such as ELMo or multilingual BERT would likely provide better annotations for the target language.",
        "Recent work has already shown promising work in using cross-lingual word embeddings, and more sophisticated approaches could be used to select better training instances.",
        "The findings may change when the number of source languages or treebanks is changed, and the observations carry over to other languages than Faroese.']"
    ],
    "6007": [
        "The proposed framework can recover missing or degraded parts of time-frequency representations of speech.",
        "Employing deep feature losses in training leads to better results compared to conventional methods.",
        "The speech-specific feature extractor speechVGG applied to obtain deep feature losses led to improved performance.",
        "The proposed system can simultaneously identify degraded parts of the input speech and recover them.",
        "The system for blind speech inpainting improved STOI and PESQ scores of degraded speech.",
        "The approach could further benefit from incorporating phase information as an additional input feature for the DNN.",
        "The limited phase reconstruction capacity may be underlying the only case when the LPC-based algorithm yielded higher PESQ score than the proposed approach.']"
    ],
    "6055": [
        "The proposed method of multi-tasking with a dependency analysis task improves the performance of the NMT model, especially in low-resource conditions.",
        "The simple data manipulation technique of alternating translation and linearized parsing is not practical, but learning to translate and parse improves over comparable multi-task setups with uninformative secondary tasks.",
        "Re-interpreting one of the self-attention heads in the Transformer model as the dependency analysis of the sentence is surprisingly effective and can be done at little or no cost in training time.",
        "The proposed method of multi-tasking with a dependency analysis task can improve the translation performance, and the gain from the multi-tasking may be useful in low-resource conditions.",
        "The observation that a similar gain can be achieved using either true syntactic trees or dummy diagonal parse raises doubts about the utility of explicit linguistic information for Transformer models.",
        "The results suggest that the NMT model can learn to translate and parse at the same time, and the parse accuracy is reasonable, but the translation performance is significantly better than the baseline.']"
    ],
    "6092": [
        "a novel pipeline to predict three types of time series using exclusively a textual source",
        "able to predict the electricity consumption with less than 5% of MAPE for both France and the United-Kingdom",
        "sufficient accuracy to be used to replace missing data or as first approximation in traditional models",
        "the texts were encoded numerically using either TF-IDF or our own neural word embedding",
        "a plethora of machine learning algorithms such as random forests or neural networks were applied on top of those representations",
        "the intrinsic value of the textual sources for the three considered time series",
        "all the algorithms naturally extract calendar and meteorological information from the texts",
        "words impact the time series in the expected way (e.g. winter words increase the consumption and summer ones decrease it)",
        "the norm of a word vector reflects its significance",
        "the words are also grouped by topic with for example winter, summer or day of the week clusters"
    ],
    "6108": [
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "In this paper, we propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We have performed extensive experiments on three public datasets of machine reading comprehension: CoQA, QuAC, and TriviaQA. Our approach outperforms benchmark models across different datasets.",
        "Interlocutor prediction and RGMPC are researchable tasks.",
        "The responding speaker and the target addressee contribute to generating better responses.",
        "The overall performance on predicted interlocutors is slightly worse than the one with gold interlocutors, but still outperforms the strongest baseline.",
        "The correctness of the responding speaker and target addressee has a significant impact on response generation performance.",
        "Interlocutor-aware Contexts into Recurrent Encoder-Decoder frameworks (ICRED) for RGMPC remarkably outperforms strong baselines on automatic and manual evaluation metrics.\"']"
    ],
    "6134": [
        "The beam problem in neural machine translation can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Our approach to section title generation uses an efficient sentence compression model and performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks, as shown by a human evaluation.",
        "The current analysis focuses on whether the full reasoning paths can help the machine learning models to improve their performance on those 80% of the questions that do not require the multi-hop ability, and cover the left 20% of questions that indeed require the multi-hop ability.",
        "The aforementioned biases make the full reasoning paths less useful for a large portion of data, therefore making it more challenging for reader models to improve with full reasoning paths.",
        "Our co-matching methods can indeed benefit from the reasoning paths, confirming the effectiveness of our proposed dataset and settings for the analysis purpose.",
        "The less biased a dataset is, the more likely a model can easily benefit from the availability of reasoning paths.",
        "To encourage model design towards real reasoning instead of fitting the data biases, we believe that an improved evaluation is necessary.']"
    ],
    "6160": [
        "Our results based on source of the news (Section 4) show that number of authors of the news is a strong indicator of credibility.",
        "We found that when the news article has no authors, it is more likely to be fake news.",
        "Our findings on collaboration of authors suggest that authors who are engaged in true credible news are less likely to collaborate with authors who are associated with fake news.",
        "The results also suggest that credibility history of authors can provide insights on credibility of other articles from the same author.",
        "Furthermore, we found that authors\\' affiliations with well-recognized organizations can be a signal for credibility.",
        "The use of numbers in true news articles occurred more often than in fake news, perhaps because true news is supported with facts that include numbers.",
        "Comparing the number of words and sentences in true news and fake news showed that on average, true news had more words and sentences than fake news.",
        "Surprisingly, we observed more typos in true news than in fake news.",
        "Our analyses also showed that domain expertise on topics discussed in news can enhance fake news detection.",
        "The F 1 -score of 0.80 obtained by predictive models built with source-credibility features show that with a small number of features, one can still detect fake news reasonably well.\"']"
    ],
    "6161": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR), with performance not too far from a rule-based language-specific (in this case Arabic) root extractor.",
        "Automated discriminators transfer poorly between decoding strategies, but training on a mix of data from methods can help.",
        "The accuracy of human raters varies wildly, but has a median of 74%, which is less than the accuracy of our best-performing discriminator.",
        "Human raters and discriminators make decisions based on different qualities, with humans more easily noticing semantic errors and discriminators picking up on statistical artifacts.",
        "As the p in nucleus sampling is set increasingly lower to achieve more fluent text, the distributional deviations that plague top-k text will surface in nucleus sampling as well.",
        "We suggest three prongs for future research: 1. Identifying ways to improve the language models and decoding strategies we use in order to generate text that is both exciting (ie. unlikely) and semantically plausible.",
        "Developing tools and educational materials to improve humans\\' ability to detect machine-generated text.\"']"
    ],
    "6166": [
        "The proposed task of controlling complexity in machine translation output is a new and important one.",
        "The multitask model improves performance over translation and simplification pipelines, according to both machine translation and simplification metrics.",
        "The reading grade level of the multi-task outputs correlates better with target grade levels than with pipeline outputs.",
        "The ability to combine larger training data from different tasks is beneficial for improving performance.",
        "Simplifying translations using the multitask model results in better correlation with target grade levels compared to previous approaches.",
        "However, even when simplifying translations, the multitask models are not yet able to exactly match the desired complexity level.",
        "The gap between the complexity achieved and the target complexity increases with the amount of simplification required.",
        "The federal government conducted a contract with the youth detention center at Vicennes, Indiana, from 2004 to 2010, to host immigrant children considered as the most dangerous ones.",
        "The federal government made a contract with the youth detention center in Vice-Year, which is in the United States since 2004 to 2010.",
        "The government made a deal with the youth detention center in Vice-Year, which hosted those immigrant children considered as the most dangerous ones in 2010.']"
    ],
    "6175": [
        "The FEVER dataset is the most comprehensive dataset introduced so far, but our corpus is different and more challenging for fact-checking.",
        "The claim validation problem for our corpus is more difficult than for the FEVER dataset because of the way the evidence was constructed.",
        "The stance of ETSs is not necessarily indicative of the veracity of the claim, and the correlation between the two is weak (Pearson correlation coefficient of 0.16).",
        "Our corpus includes validated claims along with related documents, evidence of two granularity levels, the sources of the evidence, and the stance of the evidence towards the claim, which allows for training machine learning systems for document retrieval, stance detection, evidence extraction, and claim validation.",
        "The fact-checking problem defined by our corpus is more difficult than for other datasets due to heterogeneous data and evidence from unreliable sources.",
        "More elaborate approaches are required to achieve higher performance in this challenging setting.']"
    ],
    "6177": [
        "Our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequenceto-sequence approaches in low-resource domains.",
        "A human evaluation showed that our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "We demonstrated that our approach can automatically verify the presence of therapeutic factors, which allows us to analyze larger amounts of conversational data.",
        "Our analysis indicates that OSG conversations satisfy higher number of conditions approximating therapeutic factors than Twitter conversations.",
        "The proposed approach is an approximation of the tedious tasks of annotation of conversations by experts versed in the therapeutic factors and their associated theories.",
        "The method can serve as a tool for general practitioners and psychologists who can use it as an additional source of information regarding their patients' condition and, in turn, offer a more personalized support that is better tailored to individual therapeutic needs.",
        "Our analysis focuses on the structure of conversations, being agnostic to the content.",
        "NLP allows us to strengthen our approximations even further.",
        "The further extension of our work is also augmentation of our study with other language analysis metrics and their correlation with human annotation.\"]"
    ],
    "6229": [
        "The proposed model significantly outperforms previous neural generation as well as strong retrieval baselines in both automatic and human metrics.",
        "The novel coordinator model generates questions that are more grounded on documents of interest, and this is achieved through reinforcement learning and an effective auxiliary.",
        "The model's ability to generate questions that are specific to the relevant documents is improved by accounting for their semantic similarity.",
        "The proposed framework has promising comprehensive results and can be extended with appropriate modifications and training approach.",
        "The use of a publicly available state-of-the-art pre-trained ranker for computing retrieval statistics is effective in evaluating the models.",
        "The model's ability to retrieve relevant documents based on the input questions is improved by using a larger pool of relevant documents gathered via BM25.",
        "The use of imitation learning algorithms for training the model is left for future work.\"]"
    ],
    "6243": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "PLMs largely do not learn the meaning of negation, and instead, they mostly seem to predict fillers based on co-occurrence of subject and filler.",
        "The PLM objective encourages the model to predict fillers based on similar sentences in the training corpus, which can lead to incorrect predictions.",
        "BERT is able to memorize negative facts that occur in the corpus, but it is not able to classify truth/falseness correctly without supervised training.",
        "The mispriming experiment shows that BERT often handles random misprimes correctly, but it is highly sensitive to misleading context that would not change human behavior in QA.",
        "It is especially striking that a single word suffices to distract BERT, which suggests that it is not knowledge that is learned by BERT, but rather similarity matching between the current context and sentences in its training corpus or recent context.",
        "Our results suggest that pretrained language models address open domain QA in datasets like LAMA by mechanisms that are more akin to relatively shallow pattern matching than the recall of learned factual knowledge and inference.\"]"
    ],
    "6247": [
        "The text-level method should be the preferred method for policy learning due to its simplicity and cheap computational cost.",
        "Future work should focus on identification of types of errors that are recoverable, better detection of out-of-domain and out-of-vocabulary requests, and investigation of correlations between different sources of noise and the domain of user\\'s requests.",
        "We plan to improve the existing audio-level method by inserting different levels and types of noise based on the audio content instead of uniformly randomly.",
        "We plan to further improve the existing text-level method by leveraging phoneme/semantic similarity when mapping out-of-vocabulary words or trimming the n-gram confusion matrix.",
        "We can also explore generative adversarial networks [12] for simulating more realistic ASR output.",
        "The discriminator learned the following rule - \\'if the score of U is in bin 8, U is simulated, and otherwise not.\\'",
        "The discriminator would then be able to correctly predict 100% of U in the simulated data and 70% of U in the real data, and achieve (70 + 100)/200 = 85% training set accuracy.\"']"
    ],
    "6250": [
        "The proposed framework, SMART, significantly improves the performance of fine-tuning large scale pre-trained natural language models on many NLP benchmarks.",
        "The framework effectively alleviates the overfitting and aggressive updating issues in the fine-tuning stage.",
        "The proposed framework is applicable to domain adaptation and results in a significant performance improvement.",
        "The proposed finetuning framework can be generalized to solve other transfer learning problems.",
        "The use of smooth-inducing adversarial regularization and Bregman proximal point optimization improves the performance of fine-tuning large scale pre-trained natural language models.",
        "The state-of-the-art pre-trained models (e.g., BERT, MT-DNN, RoBERTa) are significantly improved by the proposed framework.",
        "The proposed framework improves the performance on a variety of NLP benchmarks, including GLUE, SNLI, SciTail, and ANLI.']"
    ],
    "6252": [
        "The Graph2Graph Transformer architecture outperforms previous state-of-the-art models for transition-based dependency parsing.",
        "The use of graph inputs and outputs in the Graph2Graph Transformer architecture leads to significant improvements in performance.",
        "The incorporation of BERT pre-training results in substantial improvements in traditional transition-based dependency parsing.",
        "The graph input mechanism is an effective replacement for composition models.",
        "The graph output mechanism allows for the integration of a wide variety of decoding algorithms, including non-autoregressive decoding.",
        "The Graph2Graph Transformer architecture can be easily applied to a wide variety of NLP tasks, such as semantic parsing tasks.']"
    ],
    "6253": [
        "Our approach can achieve competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our approach enables easy extension to add new classes, striking the balance between simplicity and extendability.",
        "Our modeling choices enable the system to perform zero-shot generalization to unseen classification targets and questions.",
        "Our method uses information gain to select the best question to ask at every turn, and a lightweight policy to efficiently control the interaction.",
        "The system can be bootstrapped without any interaction data and show effectiveness on two tasks.",
        "The approach demonstrates the potential for bridging the gap between simple bootstrapping paradigms and incorporating user free-form responses.",
        "The initial query collection process is challenging, and we set up a two-stage process to ensure the quality of the initial queries.",
        "The tag association task is designed to associate tags with classification labels, and we train a model on the collected initial queries to rank tags for each classification target.']"
    ],
    "6284": [
        "Sublayer reordering can improve the performance of transformer models.",
        "The sandwich pattern (reordering the sublayers of a transformer model) can significantly improve performance over the baseline at no cost in parameters, memory, or runtime.",
        "The sandwich ordering improves language modeling performance on a different word-level language modeling benchmark and achieves state-of-the-art results on character-level language modeling.",
        "Sublayer reordering can improve the performance of transformer models on one group of tasks (word/character-level language modeling) but may not improve the performance on another task (machine translation).",
        "The baseline interleaved transformer model is robust to layer order changes, and even extreme reorderings (all attention sublayers at the bottom, and all the feedforward sublayers at the top) perform as well as the baseline.']"
    ],
    "6287": [
        "The proposed neuro-symbolic approach using neural representations of large-scale commonsense knowledge graphs (COMET) can generate contextual knowledge graphs on demand for zero-shot question answering, outperforming zero-shot pretrained language models.",
        "The approach dynamically constructs a knowledge graph of commonsense inferences related to a presented context and uses it to evaluate answer options for a posed question, showing promising results in answering questions without training on the end task.",
        "The use of a novel inference algorithm that reasons over the constructed graph to select the most likely answer to a question, leading to better performance compared to using vanilla knowledge models (COMET-Direct) to directly answer questions.",
        "The approach dynamically generating a contextualized commonsense knowledge graph for inference performs better than using vanilla knowledge models, and there is minimal opportunity for data leakage between the resources used.']"
    ],
    "6346": [
        "The proposed end-to-end neural architecture for jointly modeling NLU tasks achieves notably better performance than word-level models for all NLU tasks and all data sizes.",
        "The character-level model performs better than the corresponding word-level models for all NLU tasks, and for all data sizes.",
        "The multi-task models obtain higher F1 scores than the singletask models for all NLU tasks.",
        "Information sharing among related tasks is extremely valuable irrespective of available training data.",
        "Pre-trained word embeddings initialized the model perform better than randomly initialized models for all classification tasks.",
        "The simple technique to transfer the encoded semantic knowledge of pre-trained embeddings to the compositional layers of the character-level model is effective.",
        "The proposed neural architecture is an efficient approach for bootstrapping the NLU module with minimal training data.']"
    ],
    "6354": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Our approach can assist the sexual violence supporting coalitions across the world in identifying the victims.",
        "An automated tracker can find the self-reported victims in social networks and connect with the authorities.",
        "We plan to continue working along this line in future.",
        "Our contributions in this paper, the annotated data, models, and insights, can be useful in designing an automated, scalable, and reliable sexual violence reports tracking system.",
        "Our analysis finds some interesting insights, such as the relationship between violence severity and perpetrator category.",
        "We argue that our contributions in this paper, the annotated data, models, and insights, can be useful in designing an automated, scalable, and reliable sexual violence reports tracking system.\"']"
    ],
    "6356": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "We contrast the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "The latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning, compared to the raw features themselves.",
        "The derivatives of the mean and variance are important to the success of LayerNorm by re-centering and re-scaling backward gradients.",
        "Experiments show that the bias and gain increase the risk of over-fitting and do not work in most cases.",
        "The number of warmup steps is 8K.",
        "We use \\'transformer_wmt_en_de\\' as our basic model.",
        "The setting of PreNorm is adopted.",
        "We average the last 10 checkpoints for evaluation and set the beam size to 5.\"']"
    ],
    "6395": [
        "Our unsupervised method for discovering root-and-pattern morphology in Semitic languages is effective, as validated by intrinsic and extrinsic evaluations.",
        "Our method for extracting Semitic roots using discovered rules is competitive with a rule-based language-specific (Arabic) root extractor.",
        "We propose two self-supervised tasks to tackle the training data bottleneck, which significantly outperforms previous methods by reducing the error by 21% on English Switchboard.",
        "Our model trained on the full dataset outperforms previous systems trained using the full dataset.",
        "Our attention scores can capture mention importance, as shown by the analysis of attention scores assigned to mentions in different positions in the cluster.",
        "Our cluster ranking approach is linguistically more appealing and has practical restrictions compared to mention ranking models.",
        "Using oracle clusters instead of system clusters reduces training time, making our model 5 times faster to train.']"
    ],
    "6411": [
        "We contrast the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "We show that the latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "We have tried to improve on the issue of unbalanced datasets, but further work is needed to completely mitigate it.",
        "The dataset can be improved by adding more categories."
    ],
    "6414": [
        "We presented differentiable data selection, an efficient RL framework for optimizing training data usage.",
        "Our approach outperforms benchmark models across different datasets.",
        "We have shown that using cosine distance is more stable than simply taking dot product between the gradients.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "Our formulation of the machine translation algorithm uses cos J(\u03b8 t , D dev ) \u2022 \u2207 \u03b8 (x, y; \u03b8 t-1 ) as the reward signal.",
        "We use a slightly modified update rule based on Adam.",
        "Our approach leads to significant BLEU gains.",
        "Solving the brevity problem leads to significant BLEU gains; how much, if any, improvement remains to be gained by solving label bias in general?\"']"
    ],
    "6444": [
        "The FairyTED setup is applicable to any dataset with sensitive attributes that can cause biased predictions, and it can be used to identify and remove bias/unfairness in rating prediction.",
        "The FairyTED setup successfully identified the necessary properties (1) through (3) in the TED talk dataset and removed bias/unfairness in rating prediction.",
        "The impact of this work is many-fold, including the identification of the necessity of applying counterfactual fairness on the rich and influential database of TED talk videos, and the ability to correct for resulting bias.",
        "The FairyTED setup can deal with situations where it is hard to know the true causal model for the observed dataset, and it can handle unobserved attributes by inferring a posterior distribution over them.",
        "The FairyTED setup can be extended to other domains, such as job interview datasets, and it can include better encoding schemes or multi-modal information to obtain rich representation and test how counterfactual fairness measure generalizes across various modes.",
        "The proposed intuitive novel metric quantifies the degree of fairness employed by the FairyTED setup, and information from temporal evolution can also be used to improve the framework in future work.",
        "The FairyTED setup has three important social impacts: ensuring that speakers get fair feedback and can improve only based on fair fallacies, allowing organizers to employ diverse speakers without worrying about degradation in rating, and preventing propagation of unfairness over time.']"
    ],
    "6451": [
        "The proposed method achieves competitive performance compared to previous systems using less than 1% of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The proposed approach determines the translation latency adaptively based on the input observations.",
        "The method uses a meta token <wait> to wait until the observation of the next input token.",
        "The experimental results suggest that the proposed method determines when to translate or when to wait in an adaptive manner.",
        "Future work includes further analyses on translation accuracy in different latency conditions and time-based latency evaluation instead of the token-based one.']"
    ],
    "6457": [
        "The proposed novel constituent hierarchy predictor based on recurrent neural networks outperforms the state-of-the-art baseline parser by achieving higher F1 scores on standard WSJ and CTB evaluations.",
        "The resulting fully-supervised parser achieves 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation, outperforming the state-of-the-art baseline parser.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains, and the solution is a very limited form of globally-normalized models for NMT.",
        "The proposed method for alleviating or eliminating the beam problem involves learning the parameters of corrections to the model, and this method is helpful and easy to implement.",
        "The brevity problem is an example of label bias, and the solution is a very limited form of globally-normalized models for NMT.",
        "Fixing one of the two nouns in the pattern, as in strict bootstrapping and NC-only strict bootstrapping, yields significantly higher accuracy for both NC and NC-pattern pair extraction compared to loose bootstrapping.",
        "The number of extracted NCs is much higher with the strict methods because of the higher number of possible instantiations of the generalized query patterns.",
        "Using the latter (i.e., N = 10) yields a sizable drop in the number of extracted NCs and NC-pattern pairs, and it also tends to yield a slightly improved accuracy.",
        "The method of Kim and Baldwin (2007) generates new semantically interpreted NCs by replacing either the head or the modifier of a seed NC with suitable synonyms, hypernyms, and sister words from WordNet, followed by similarity filtering using WordNet::Similarity."
    ],
    "6460": [
        "We introduce a new dataset for abstractive summarization of dialogues.",
        "The current metrics used to evaluate text summarization, such as ROUGE, may not be suitable for dialogues.",
        "Our models benefit from the introduction of separators that mark utterances for each person.",
        "Dedicated models with architectural changes, taking into account the assignation of a person to an utterance in a systematic manner, could improve the quality of dialogue summarization.",
        "The ROUGE metrics do not reflect the quality of a summary for dialogues.",
        "A new metric should be designed to measure the quality of abstractive dialogue summaries.\"']"
    ],
    "6479": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "This work presents an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR).",
        "The quality of sentences is good, with few false positives mostly in High German or German dialects.",
        "The presence of specific structures in the sentences are often the cause of such mistakes.",
        "Typical segmentation mistakes come from the use of ASCII emojis as punctuation marks.",
        "Splitting text into sentences is not a trivial task.",
        "We have demonstrated how this new resource can significantly improve Swiss German language modeling.",
        "Our experiments support the reasoning that Swiss German is still scarce and very hard to find online.\"']"
    ],
    "6487": [
        "The proposed AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The ability to predict the ranking of educational organizations might seem trivial, but it is actually valuable because USE scores are measured only once per cohort, making it extremely hard to estimate any added value provided by an educational organization.",
        "Domain-specific unsupervised learning of word embeddings allows predictive models to be trained using relatively small labeled data sets.",
        "The use of continuous word representation is preferable to common approaches relying on counting word frequencies.",
        "Models trained on text data could be successfully transferred from one data source to another.",
        "The potential risk to users' privacy exists if users' attributes can be predicted from digital traces on one platform but not on another.\"]"
    ],
    "6507": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Using a deep multimodal classifier is beneficial for the task of screening depression.",
        "The feature engineering models (our baseline), on the other hand, yields competitive results when considering text or image separately; however, when using concatenated features, the results are worse.",
        "Previous studies have pointed the same direction for the screening depression task: simply concatenating engineered features makes the model focus on unimodal features instead of paying attention to both.",
        "Our results also support this finding, for the feature engineering models, that concatenating visual and textual features do not improve model accuracy.",
        "Detecting depression using image features should lead to improved results compared to textual features, but our findings suggest that this is not the case.",
        "The feature engineering models yield competitive performance compared to the deep learning methods.",
        "However, we lose interpretability when using deep learning, which is important for trusting issues in AI-based systems.\"']"
    ],
    "6528": [
        "The proposed approach achieves state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses a combination of Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The use of pseudo labels improves the domain adaptation performance.",
        "The approach reduces the noise of pseudo labels to improve the domain adaption performance in the future.",
        "The method uses less than 1% of the training data to achieve competitive performance compared to previous systems.",
        "The approach significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "The task has attracted a total of 41 teams, with subtask B being the most popular.",
        "The approach is part of a larger Sentiment Track, together with three other closely-related tasks.",
        "The task focuses on sentiment analysis and has five subtasks: detecting sentiment of terms in context, classifying the sentiment of an entire tweet, predicting polarity towards a topic, quantifying polarity towards a topic, and proposing real-valued prior sentiment scores for Twitter terms.",
        "The approach will be used for trend detection on a five-point scale in the future, which is closer to what businesses and researchers want.']"
    ],
    "6541": [
        "The proposed method achieves high word accuracy (%) compared to the state of the art with a limited-size ensemble and low complexity.",
        "The method uses five networks, yielding comparable or better results than numerous networks (up to 118 or 2100 network instances) required in the ensemble.",
        "The proposed method does not use feature descriptors such as histogram of oriented gradients (HOG), and the process starts with a pixel image and is trained end-to-end.",
        "The average performances of the two coding schemes (Plain and Extra separator) differ significantly if the dual-state word-beam search is used for CTC decoding.",
        "Using the decoder with dictionary boosts the performance of the model.",
        "Ensemble voting clearly improves the word accuracy (%); its effect is stronger on weaker recognizers.",
        "The method recognized the common length OOV and INV words with a high accuracy.",
        "Increasing the size of difficult in-vocabulary word classes yields superior results, while the performance on easy in-vocabulary word classes is high even for a limited number of samples.",
        "The goal of this research is not a record attempt towards maximized accuracy on the RIMES and the KdK datasets.",
        "Providing a more than 30 times larger dictionary results in a slight drop in performance, but the relative improvement is present for the dictionary-free approach."
    ],
    "6551": [
        "The development of the AI2D-RST corpus showed that exploiting readily available annotations can increase the volume of richly annotated multimodal corpora, but this comes at a cost, particularly for annotating discourse structure.",
        "The level of detail needed for decomposing diagrams to achieve a sufficient inventory of elementary discourse units varies from one diagram to another, and the AI2D layout segmentation does not provide discourse-driven decomposition at various levels of detail.",
        "The AI2D-RST annotation schema had to make compromises in the description of discourse structure due to the lack of sufficient expressive resources for identifying elementary discourse units, particularly for diagrams.",
        "Crowd-sourcing annotations for the diagrammatic mode in any domain is unlikely to work due to the issue of defining crowd-sourcing tasks developed for the annotation of photographic images, and the discourse structure determines to what extent the diagram must be decomposed.",
        "The development of AI2D-RST revealed various challenges, but the corpus is still a valuable resource for studying how the diagrammatic mode is used in the domain of primary school natural sciences and beyond.']"
    ],
    "6564": [
        "The proposed model (RAIT) maintains state-of-the-art accuracy while significantly reducing model size and computation load.",
        "The full window-size model behaves like a standard transformer, virtually eliminating the \"see more\" action when the window size is at its maximum.",
        "The proposed method differs from the BERT model only in its reinforcement learning loss instead of its multiclass cross-entropy loss, but it is more accurate when the window size is limited to a value less than the maximum possible size.",
        "The proposed method accurately and efficiently classifies partially observed input data based on the model\\'s representation using a dynamic selection of observed windows.",
        "The proposed method adapts to the smaller window size by increasing the number of observed windows, minimizing the negative impact of the limited self-attention perception range.",
        "The reinforcement learning-based global solution aggregation described in Equations 5 and 6 minimizes the negative impact when compared to state-of-the-art models.']"
    ],
    "6566": [
        "The proposed framework outperforms existing HRED models and attention variants in multi-turn dialogue generation.",
        "The model utilizes self-attention to effectively capture long-distance dependency relations, leading to improved performance.",
        "The detected relevant contexts are significantly coherent with human judgments.",
        "The use of proper detection methods, such as self-attention, can improve the quality of generated responses.",
        "The model can be further improved by synthesizing more realistic user queries and considering topical information and content information in the relevant contexts.",
        "The current tagging method is not accurate, and the entities are not always tagged at the correct boundary.",
        "The segmentation of entities is also not correct, and the entity \"extremities\" was missing in some cases.",
        "The lack of ground truth hinders the evaluation of the framework\\'s performance.",
        "The use of BioBERT embeddings or query expansion techniques could improve the model\\'s performance in the future.']"
    ],
    "6569": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "The proposed model can be used in any context/domain because it only uses the inherent structure of the persuasion tactics.",
        "Our model's accuracy is highest for the following persuasion tactics: reasoning, deontic/moral appeal, outcome, empathy.",
        "The performance of the proposed model with synthesized prototype strings is better than that of vector-embedding models, such as Doc2Vec, which uses deep learning.",
        "Our model runs faster than Doc2Vec by a factor of almost 1.5.",
        "The baseline SVM on lexical features had considerably high precision but it fell short on recall.",
        "Our model performs better at both binary and multi-class classification than the approach used in [6].\"]"
    ],
    "6570": [
        "Most participants restricted themselves to the provided data and submitted constrained systems.",
        "The best systems for each of the two subtasks and for each of the two testing datasets were constrained systems.",
        "Additional data would be useful for the task.",
        "Most systems were supervised, with only five semi-supervised systems and one unsupervised system.",
        "The majority of participants predicted all three labels (positive, negative, and neutral).",
        "The most popular classifiers included SVM, Max-Ent, linear classifier, Naive Bayes.",
        "A variety of features were used, including word-related, word-shape, syntactic, Twitter-specific, and sentiment-related features.",
        "Almost all participants relied heavily on various sentiment lexicons, such as MPQA and SentiWordNet.",
        "Given that Twitter messages are noisy, most participants did some preprocessing, including tokenization, stemming, lemmatization, stopword removal, normalization/removal of URLs, hashtags, users, slang, emoticons, repeated vowels, punctuation.']"
    ],
    "6580": [
        "The algorithm presented in (Momeni et al. 2018) can be improved for better automatic topics modeling and further processing.",
        "The existing algorithm resulted in too many clusters that needed to be filtered, and the words in each topic needed filtering and ranking to make sense of them.",
        "We implemented a network analyzing component to obtain characteristics of the topics and filtered topics based on known network analysis metrics such as centrality, density, and cluster frequency value.",
        "We added the k-core decomposition method to obtain the main words that are in the conversation of a topic and ranked words inside a cluster to identify the most meaningful and novel words.",
        "The final algorithm provides a method to filter clusters by their centrality and cluster frequency values and filter and rank words to keep only essential words.",
        "Our enhancement modules for the whole system were effective and efficient, as shown by both subjective user evaluation and objective topic coherency evaluation.",
        "The proposed topics were interesting and usable for further processing or work, and the ranking of words in a topic was helpful for making a future automatic analysis possible.",
        "We had a definite improvement concerning the pairwise mutual information score compared to the score in the state-of-the-art work in (Momeni et al. 2018) .']"
    ],
    "6590": [
        "We introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our model outperforms the simple projection baseline using fast-align on most languages, and achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We propose two self-supervised tasks to tackle the training data bottleneck.",
        "Our method trained on the full dataset significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Attention over Lexicon Memory Figure 4 illustrates which words will be given more weights computed by the attention operations over the lexicon memory.",
        "The model can learn to assign more weights on the key words of named entities, and the attentions are sharp for those words particularly informative for NER.",
        "Taking the entities of \\'ORG\\' (organization) as examples, more weights are placed to the last two characters, such as \\'\u4e2d\u5fc3\\' (center), \\'\u653f\u5e9c\\' (government), \\'\u5b66\u6821\\' (school), \\'\u7ec4 \u7ec7\\' (organization), etc.",
        "We found the similar phenomenon when recognizing person names. For instance, famous names such as \\'\u6c5f\u6cfd\u6c11\\' (Zemin Jiang) can be matched exactly and recognized as a person name, while for names of less well-known persons, the first character (i.e. surname) tends to be given more attention.",
        "Decoding Threshold Settings We reported the F1-scores for different settings of LEMON on the development set of Weibo NER in Figure 6 .\"']"
    ],
    "6629": [
        "The proposed model, MSIN, can learn to focus on a small subset of textual data that best aligns with the time series, significantly improving the performance of the model.",
        "The MSIN model discovers relevant news stories to the stocks that do not even explicitly mention the company name, but include highly relevant events that may influence or reflect their performance in the market.",
        "The model outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The adaptive search method can find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The use of pre-trained GloVe embedding and the choice of hyperparameters (such as dropout rate and number of timesteps) can significantly improve the performance of the model.']"
    ],
    "6652": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "the neural attention mechanism not only provided a performance boost over a mean pooled neural architecture, but also enabled greater model explainability.",
        "The HEA models performed 7% higher in F1-macro compared to the HE models.",
        "our raters achieved an average of 94% agreement across criteria.",
        "the HON organization reported an average percent agreement of 85% on their criteria, and this drops to 81% when only considering criteria that are shared with Brief DISCERN.",
        "the HEA BioBERT model provided reasonable context sentences (i.e. sentences supporting a prediction).",
        "our study demonstrates that neural models are able to perform online health information quality assessment in accordance with an existing quality criteria (Brief DISCERN) with a performance above 80% accuracy.\"']"
    ],
    "6684": [
        "The proposed approach, Attention over Parameters, learns how to softly combine independent sets of specialized parameters into a single set of parameters, achieving compositionality and interpretability.",
        "By organizing a multi-domain task-oriented datasets into end-to-end trainable formats and combining it with a conversational dataset, the model can learn to consider each task and domain as a separate skill that can be composed with each other or used independently.",
        "The model achieves effective compositionality and interpretability, as demonstrated by competitive experimental results and thorough analysis.",
        "The approach allows for incremental learning and zero-shot skill composition, enabling the addition of new skills over time and the ability to apply existing skills in new contexts.",
        "The model can learn more general skills, such as Machine Translation or emotional responses, and combine them with existing skills to obtain compositional responses without labeled data.']"
    ],
    "6717": [
        "Prior work on deep neural networks for clinical outcomes has focused mostly on end-to-end prediction models and large datasets.",
        "Our work demonstrates the benefits of learning general purpose representations using the entire EHR dataset, which can then be re-used to train better prediction models.",
        "Language model-based representations (such as DoctorAI and CLMBR) are significantly better than alternative representations for training clinical prediction models across a range of training set sizes.",
        "The choice of language model objective matters, with the more expansive language model, CLMBR, providing better representations.",
        "Language model-based representations are superior to end-to-end trained neural nets in the large sample regime.",
        "Other learned representations proved to be of little value relative to simpler count-based representations when enough data was available.",
        "Clinical prediction models using simple count-based representations can perform very well, but it is important to use a model type with sufficient expressive power.",
        "The upfront computation costs of training and tuning language model-based representations can be significant, but this process is a one-time cost per institution that can be amortized over many clinical outcomes.",
        "Recent language model work has demonstrated considerable reduction in training costs.",
        "Our findings are limited to the five clinical outcomes used in this work and may not generalize to all other possible EHR-based model types.']"
    ],
    "6718": [
        "The proposed AdaBERT model achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The adaptiveness of the proposed Ad-aBERT model allows it to find different models varying in model efficiencies and architectures that are suitable for different downstream tasks.",
        "The method proposed in this paper can potentially be extended to linguistic groups of similar geographic spread and time depth.",
        "Much work remains to be done in order to understand the complex history of the Iranian languages, and larger data resources are needed to achieve this goal.",
        "The posterior distributions of \u03b8, \u03c6 can be used to reconstruct the probability that a given data point is associated with a given dialect component.",
        "The use of uninformative Gamma(1, 1) priors over \u03b4 and \u03b3 allows for the estimation of the posterior distributions of \u03b8, \u03c6 without prior knowledge of the degree to which data points within a given language should be dispersed across components.']"
    ],
    "6724": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "We present an unsupervised method for the discovery of root-and-pattern morphology in Semitic languages, which can be used to extract Semitic roots.",
        "Our methodology enables developers to iteratively refine both the generic question templates and domain-specific annotations.",
        "By capturing the variety of natural language with a common set of generic question templates, we eliminate the need for manually annotated training data and minimize the reliance on paraphrasing.",
        "Our BERT-LSTM model outperforms the MQAN model by 4.6% to 12.9%.",
        "The Q&A agents produced by Schema2QA can answer crowdsourced complex queries with 70% average accuracy, significantly improving over Alexa, Google, and Siri.']"
    ],
    "6747": [
        "The proposed interaction quality estimator based on BiLSTMs with attention mechanism outperforms the baseline in learning all temporal dependencies implicitly.",
        "The use of an IQ reward estimator leads to improved performance in terms of both task success rate and estimated user satisfaction.",
        "The dialogues learned with the proposed interaction quality estimator show a slightly higher robustness towards noise and shorter dialogues, while still yielding good performance.",
        "The improvement in performance is demonstrated through training the reward estimator on a bus information domain and applying it to learn dialogue policies in five different domains.",
        "The use of a user satisfaction-based reward estimator has the potential to improve the estimation performance further by incorporating domain-independent linguistic data.",
        "The proposed interaction quality estimator is effective in improving the robustness of the dialogues towards noise and shortening the dialogues, while still maintaining good performance.']"
    ],
    "6760": [
        "The use of value-aligned priors, such as normative text classifiers, can effectively reduce the likelihood of generating non-normative sentences by approximately 27-61%.",
        "Applying reward-based fine-tuning techniques using a normative text classifier can be an effective means for reducing the generation of non-normative descriptions.",
        "Our approach of using a normative text classifier to fine-tune a language model can be used as an alternative or complement to debiasing techniques.",
        "The policy-gradient based reinforcement learning technique using a value-aligned prior can be effective in reducing non-normative generation, and is roughly equivalent to teaching a language model to censor itself.",
        "Our approach can be limited by the context-dependent nature of normativity, and the possibility of overlooking non-normative sentences that may appear normative out-of-context.",
        "Finetuning GPT-2 on Plotto, ROCstories, and sci-fi datasets can lead to it generating both neutral and normative sentences, and a model that generates solely normative sentences may be desired.']"
    ],
    "6879": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Using natural language descriptions for object selection eases overall effort.",
        "The greatest limitation of our current system is that the range of regions that could be selected is bounded by VI-SION ENGINE capabilities.",
        "As shown in Figure 4, users typically relent after paraphrasing REFER multiple times.",
        "A possible solution is to include selection tools and the option to modify system\\'s inferred MASK; region selection is a very delicate procedure.",
        "Users should be able to enjoy the convenience of using referring expressions and fall back to manual selection whenever needed.",
        "It could be useful to have a learning system that tries to relate region selection to language.",
        "For future work, we plan to incorporate additional edit types and develop a multimodal setting that includes gesture manipulations; we expect that language will provide an easy-to-learn interface for users.\"']"
    ],
    "6896": [
        "The release of a native French Reading Comprehension dataset is motivated by the release of recent French monolingual models and industrial opportunities.",
        "While it is generally accepted that monolingual models perform better than multilingual models, we find that the gap is narrower than expected for the Reading Comprehension task.",
        "To fine-tune a model on a target language, translated datasets have been extensively used, but the lack of native data to evaluate the approach, at least in French, makes it difficult to evaluate it.",
        "Apart from Question Answering models for French applications, cross-lingual applications have found significant interest recently with [Artetxe et al., 2019] and [Lewis et al., 2019].",
        "The FQuAD dataset is the result of two different annotation processes, and it is the first dataset for native French Reading Comprehension.",
        "The Human performances on FQuAD1.1 reach comparable scores to SQuAD1.1.",
        "Various experiments were carried out to evaluate the performances of fine-tuned monolingual and multilingual language models, and our best model, CamemBERT LARGE, achieves a F1-score and an Exact Match of respectively 92.2% and 82.1%, surpassing the established Human performance in terms of F1-Score and Exact Match."
    ],
    "6903": [
        "Our results demonstrate the utility of using deeper task-specificity in models for joint NER and RE.",
        "We conclude that prior work on joint NER and RE undervalues the importance of task-specificity.",
        "Other approaches that employ a single model architecture across different datasets are laudable, but our results suggest that carefully balancing the number of shared and task-specific parameters is important for MTL.",
        "The similarity between the NER and RE tasks varies across domains, and improved performance can be achieved by tuning the number of shared and task-specific parameters.",
        "In our work, we treated the number of shared and task-specific layers as a hyperparameter to be tuned for each dataset, but future work may explore ways to select this aspect of the model architecture in a more principled way.",
        "Finetuning the model used to obtain contextual word embeddings, e.g. ELMo or BERT, during training could lead to improved performance relative to our results.",
        "An extension of this work to other related NLP tasks, such as coreference resolution and cross-sentential relation extraction, could be an opportunity for future work.\"']"
    ],
    "6932": [
        "Existing VQA models are not robust to questions composed with logical connectives.",
        "Humans are able to answer complex logical compositions, such as \\'Is every boy who is holding an apple or a banana, not wearing a hat?\\'",
        "Our work focuses on predicting a single binary answer for VQA questions.",
        "We have shown how connectives in a question can be identified by enhancing LXMERT encoders with dedicated attention modules and loss functions.",
        "Our models show improvements in terms of answering logically composed questions, while retaining performance on the original VQA test-set.",
        "The ability to answer questions about an image must be extendable to a logical composition of two such questions.",
        "State-of-the-art models trained on VQA dataset lack this ability.",
        "Our solution involves the \\'Lens of Logic\\' model architecture that learns to answer questions with negation, conjunction, and disjunction.\"']"
    ],
    "6999": [
        "STEM-ECR v1.0 corpus offers multidisciplinary PROCESS, METHOD, MATERIAL, and DATA entities that are disambiguated using Wiki-based encyclopedic and lexicographic sources.",
        "Our corpus can be leveraged for machine learning experiments in several settings: as a vital active-learning test-bed for curating more varied entity representations; to explore domain-independence versus domain-dependence aspects in scientific IE; for EL and WSD extensions to other ontologies or lexicographic sources; and as a knowledge resource to train a reading machine.",
        "Our corpus is particularly unique w.r.t. the investigation of their scientific entities, notably, the inclusion of understudied domains like Mathematics, Astronomy, Earth Science, and Material Science.",
        "We plan to extend this corpus with relations to enable building knowledge representation models such as knowledge graphs in a domain-independent manner.",
        "Given the stage 1 scientific entities, the annotators could make one of two decisions: a) an entity is linkable; or b) an entity is unlinkable.",
        "The \u03ba scores were then computed on this data representation.\"']"
    ],
    "7052": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Our baselines perform homogeneously over all existing datasets for relation prediction in AM while using generic features.",
        "As it may be noticed in the examples provided in Section 2., the datasets differ at granularity: some consist of pairs of sentences (e.g., IBM) whereas others include pair of multiple-sentence arguments (e.g., Nixon-Kennedy debate).",
        "Other works that address the task of relation prediction make use of features specific to the single dataset of interest, making it difficult to test those models on the other datasets.\"']"
    ],
    "7055": [
        "The proposed method for assessing the stability of LDA can increase the similarity of topic structures across replicated runs.",
        "The ideal situation is when clustering topics from different runs leads to groups of topics that consist of exactly one topic per run.",
        "Two random selected models are likely to produce considerably different topic structures.",
        "A method for increasing similarity by selecting prototypes from repeated LDA runs can be used to overcome this issue.",
        "The improvement is reached by repeating the modeling procedure and selecting the prototype model that is on average most similar to all other runs.",
        "The proposed method is usable to compare replications by matching topics of other topic models than LDA.",
        "The new stability measure S-CLOP can be used to compare replications by matching topics of other topic models than LDA.",
        "The idea to use replicated LDA runs and select a prototype can be transferred to other similarity measures than the modified Jaccard coefficient Jm that was used here.",
        "The method is also usable to compare replications by matching topics of other topic models than LDA.",
        "The proposed similarity measure for models based on different text corpora can open various fields of other applications, such as quantifying similarities in reporting of newspaper offices or differences in coverage on various media channels like twitter, online and print.']"
    ],
    "7099": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "We contrast the performance of these widely-known baselines to conditional language models where we simply initialize the decoder either with an aggregate action feature vector obtained through maxpooling or the output of a multi-label classifier trained to predict visual entities from the action features.",
        "The latter performs substantially better than the former, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Our experiments show that M-BERT achieves a reasonable accuracy on X-stance, outperforming majority class baselines and a fastText classifier.",
        "The supervised part of X-stance has a similar difficulty as the SemEval-2016 (Mohammad et al., 2016a) or MPCHI (Sen et al., 2018) datasets on which BERT has previously been evaluated.",
        "The mean score drops by 6-8 percentage points compared to the supervised setting; while zero-shot transfer is possible to a degree, it can still be improved.",
        "The additional experiments (Table 5 ) validate the results and show that the sequence-pair classification approach to stance detection is justified.",
        "Manual annotation could eliminate very implicit samples in a future version of the dataset.']"
    ],
    "7109": [
        "Our approach involves crucial information from the decoder (encoder-decoder attention from all decoder layers).",
        "Probe training requires supervision.",
        "The use of attention matrices is only to bring supervision (word translations) from the target side to the source side, which is inevitable.",
        "Decoder representations cannot flow back to the frozen encoder.",
        "Our approach trains a linear projection layer that bridges representations from the frozen pre-trained analyzed layer and the frozen pre-trained classifier.",
        "Our analysis is the first to reveal the existence of target translations performed by encoder layers (including the source embedding layer).",
        "Increasing encoder depth while removing decoder layers can lead to significant BLEU improvements while boosting the decoding speed.\"']"
    ],
    "7123": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "Developing annotated datasets from multiple sources based on existing datasets and the proposed causality tagging scheme can improve the performance of SCITE.",
        "Combining the proposed method with distant supervision and reinforcement learning can achieve better performance without requiring a large amount of high-quality annotated data.",
        "The proposed novel constituent hierarchy predictor based on recurrent neural networks outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "The proposed method can enhance results significantly under highly unbalanced training sets by composing a classification component to locate whether a sentence involves weak classes.",
        "Investigating prior knowledge into the classification model and jointly training two models under specific constraints can make the method more robust under noise and missing data."
    ],
    "7132": [
        "Existing datasets for medical term similarity are too small to detect significant differences between embeddings and similarity metrics.",
        "Our new large-scale datasets reveal significant differences in the performance of different embeddings.",
        "The new datasets expose the enormous difficulty of current embeddings in predicting the similarity of non-obvious term pairs.",
        "The standard cosine similarity measure yields poor results for most medical word embeddings, and the recently introduced Fuzzy Jaccard similarity measure for multi-word strings (Zhelezniak et al. 2019b ) yields better results.",
        "Available embeddings are unable to adequately represent medical terminology at scale.",
        "Embeddings based on explicit knowledge (MeSH thesaurus) yield the best representations, which is a promising direction for future research.",
        "The vocabulary of the different embeddings covers the existing datasets poorly, illustrating that our new datasets encode much more of the enormous medical terminology.",
        "The AUEB embeddings have a better coverage than BioNLP PM and LTL, despite using the same corpus and method.",
        "The embeddings can be split into three groups with large vocabulary overlaps: 1) the different BioNLP embeddings and LTL, 2) BioASQ, AUEB, MeSH and MIMIC, and 3) the non-medical GloVe and Fasttext embeddings.",
        "The embedding with the largest vocabulary is MIMIC, which also has the best dataset coverage.']"
    ],
    "7134": [
        "HardWEAT success factors:",
        "SoftWEAT advantages:",
        "Debiasing leads to substantially decreased variance in the predicted polarity:",
        "Contribution to ongoing efforts towards providing more unbiased neural representations of textual data:"
    ],
    "7135": [
        "The proposed novel constituent hierarchy predictor outperforms the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The joint model improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "The approach allows lexicality and syntax to interact with each other in the joint search process, leading to improved performance.",
        "The use of a five-action transition system and three classifiers to resolve structural, tagging, and labeling conflicts improves the accuracy of the model.",
        "The model achieves high accuracy given the input set and even matches actual drug use trends in society.",
        "The approach can be used as a surveying tool to obtain metrics and real behavioral trends, and can be further improved by considering subword embedding and real-time data processing.",
        "The methodology can be used to classify drug-related tweets and extract feature-related information from the results, leading to a high classification rate.']"
    ],
    "7146": [
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\')",
        "We further introduce a novel end-to-end model for joint slot label alignment and recognition that requires no external label projection.",
        "Experiments show that multilingual BERT brings substantial improvements on multilingual training and cross-lingual transfer tasks.",
        "Our analysis also points out several challenges of this dataset, including the need to integrate OCR tokens and the requirement for generating long sentences.",
        "Current state-of-the-art image captioning models cannot read when trained on existing captioning datasets, but adapting a recent M4C VQA model to our task and training it on our TextCaps dataset allows us to generate impressive captions.",
        "Our human evaluation confirms the result of the automatic metrics with very high correlation, and also shows that human captions are still significantly better than automatically generated ones, leaving room for many advances in future work.\"']"
    ],
    "7159": [
        "The proposed architecture, RNG Transformer, can iteratively refine arbitrary graphs with non-autoregressive fashion.",
        "The RNG Transformer model can capture complex patterns of interdependencies between graph edges and exploit BERT pre-training.",
        "The SynTr model, which is one iteration of the RNG Transformer model without graph inputs, significantly outperforms previous state-of-the-art models on all treebanks.",
        "The RNG Transformer refinement almost always improves accuracies, setting new state-of-the-art accuracies for all treebanks.",
        "RNG Transformer consistently results in improvement regardless of the initial parser, reaching around the same level of accuracy even when it is given an empty initial parse.",
        "The RNG Transformer architecture is a very general and powerful method for structured prediction, which could easily be applied to other NLP tasks.']"
    ],
    "7171": [
        "The beam problem in transition-based parsing can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "Dynamic Memory Induction Networks (DMIN) for few-shot text classification achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The correlation between an algorithm's biased latent dependency displacement distribution and the target treebank being parsed is statistically significant for the sentence lengths that comprise most of the sentences found in actual corpora.",
        "The effect of displacement distributions on parsing accuracy is independent of the effect of transition sequence length.",
        "Algorithms tend to be more accurate on corpora that are closer to their inherent distribution, which could be used to guide parsing algorithm design.",
        "Linguistic considerations can have an impact in natural language processing systems.",
        "The concept of an algorithm's inherent displacement distribution captures the algorithm's bias towards implicitly preferring certain dependency lengths and directions to others.\"]"
    ],
    "7174": [
        "Our BMC-trained models improve over previous work for all four tasks evaluated.",
        "BERTeus outperforms every other model by a wide margin.",
        "The POS tagging results on UD 1.2 are especially interesting, as they show improvement over previous work.",
        "Our objective was to provide a head-to-head comparison between analogous models.",
        "The FastText-BMC embeddings used to train the Flair document classifier outperform multilingual BERT for the Topic Detection and Sentiment Analysis tasks.",
        "The FastText-BMC embeddings are robust and do not fluctuate as much across tasks.",
        "Our Flair-BMC model outperforms both Flair-official and multilingual BERT in every setting.",
        "The good performance of BERTeus is due to the combination of a larger and better corpus, as well as the use of language-specific subword tokenization.",
        "The improvements provided by BERTeus give a huge boost in performance, which helps to correctly classify difficult NER examples.']"
    ],
    "7180": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "Our approach to joint POS tagging and dependency parsing improves over previous work on three treebanks across a variety of natural languages.",
        "Our models can effectively preserve salient source relations in summaries.",
        "The structural models are on-par with or surpass state-of-theart published systems.",
        "We introduced the first Swiss German dictionary, which is useful for automated speech recognition and can replace the previous state of the art.",
        "The written form to SAMPA is promising and has applications in areas like text-to-speech.",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, leading to improved results.']"
    ],
    "7195": [
        "The importance of linguistic context in human evaluation of MT is emphasized, as raters assessing documents as a whole show a significant preference for human translation, but when assessing single sentences in random order, they show no significant preference for human translation.",
        "Document-level evaluation exposes errors to raters which are hard or impossible to spot in a sentence-level evaluation, such as coherent translation of named entities.",
        "The choice of raters, the availability of linguistic context, and the creation of reference 8 have a significant impact on perceived translation quality.",
        "Aggressive editing of human reference translations for target language fluency can decrease adequacy to the point that they become indistinguishable from machine translation.",
        "The design of MT quality assessments with human raters should be revisited to reveal errors in the output of strong MT systems.",
        "Alternative evaluation protocols that can demonstrate their validity at a lower cost are needed for assessing human-machine parity.']"
    ],
    "7203": [
        "Our new technique for generating adversarial examples (BAE) through contextual perturbations based on the BERT Masked Language Model achieves strength and effectiveness in attacking text classification models.",
        "We propose inserting and/or replacing tokens from a sentence, in their order of importance for the text classification task, using a BERT-MLM.",
        "Our automatic and human evaluation on several datasets demonstrates the effectiveness of our attack.",
        "We use a pre-trained BERT Base-uncased MLM to predict the masked tokens for our BAE attacks.",
        "We only consider the top K=50 synonyms from the BERT-MLM predictions and set a threshold of 0.8 for the cosine similarity between USE based embeddings of the adversarial and input text.",
        "For R operations, we filter out predicted tokens which form a different POS than the original token in the sentence.",
        "For both R and I operations, we filter out stop words using NLTK from the set of predicted tokens.",
        "Additionally, we filter out antonyms using synonym embeddings for sentiment analysis tasks.\"']"
    ],
    "7219": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "The beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "LPA creates signatures that are meaningful in several domains.",
        "LPA has limitations when used in social media and short text.",
        "LPA offers an unsupervised yet quick and practical method for determining vector representations for users.']"
    ],
    "7237": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "Our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing.",
        "We use the observation that biased representations lead to biased inferences to construct a systematic probe for measuring biases in word representations using the task of natural language inference.",
        "Our experiments reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion, and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases and show that it works for the static GloVe embeddings.",
        "Our approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Our work initiates a study of action prediction in a single one-shot game, focusing on the transductive learning setup where the learning algorithm has access to the texts written by all the participants.",
        "A larger data set would allow us to better understand the extent to which the success of our approach relies on the transductive setup and whether modern inductive algorithms can compensate for the lack of textual data from the agents whose behavior is to be predicted.']"
    ],
    "7252": [
        "The coverage of first-order links is high for Orphanet and Disease Ontology, with English labels being the most prevalent.",
        "The issue with labels is that the information is mainly in English and French, resulting in poorer results for other languages.",
        "All entities with links have English labels, and more than half have French labels, but only around 20% of entities have at least one label in German.",
        "Second-order links help a lot for languages other than English.",
        "The quality of the label translations has improved compared to previous work by Silva et al. (2015).",
        "Translations obtained through first-order links are not so distant from Google Cloud Translation.",
        "The quality of the translations obtained through second-order links is substantially different from the translation coming from first-order links.",
        "Google Cloud Translation has an advantage as Orphanet is primarily maintained in English and French, and translated by experts to other languages.",
        "Translating all the labels from English to the different languages allows having more synonyms than Orphanet in other languages.",
        "Wikidata is poorer in terms of synonyms than Orphanet except for English.']"
    ],
    "7282": [
        "The proposed ReCoSa model significantly outperforms existing HRED models and their attention variants.",
        "The relevant contexts detected by the ReCoSa model are coherent with human judgments.",
        "The beam problem in HRED models can be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "The optimal number of layers for each task can be found using a dev set and verified on a hold-out set.",
        "The pruned models achieved on par with distilled models building using knowledge distillation, but without requiring retraining.",
        "The lower layers are most critical to maintain downstream task performance.",
        "Certain downstream tasks require as few as only 3 layers out of 12 layers to maintain within 1% performance threshold.",
        "Networks trained using different objective functions have different learning patterns."
    ],
    "7299": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "The automative domain (D2) is the best source domain for clothing (D6), both being unrelated domains in terms of the products they discuss.",
        "LM4 predicts the best source domain correctly for D2 and D4, which all other metrics fail to do.",
        "For labelled data, we observe that LM2 (Symmetric KL-Divergence) and LM3 (Chameleon Words Similarity) perform better than other metrics.",
        "Amongst the metrics which utilize word embeddings, ULM1 (Word2Vec) outperforms all other metrics for all values of K.",
        "We also observe that word embeddings-based metrics perform better than sentence embeddings-based metrics.",
        "The reported NRA is low for all the values of K across all metrics.",
        "If a considerably larger amount of data would be used, the NRA should improve.",
        "We suspect that the use of ELMo and Universal Sentence Encoder to train models for contextualized embeddings on review data in individual domains should improve the precision for ULM6 (ELMo) and ULM7 (Universal Sentence Encoder).\"']"
    ],
    "7302": [
        "The cross-lingual transfer gap on XNLI was overestimated.",
        "Our work shows that there is another factor that can have a much larger impact on performance when generalizing from English to other languages: the loss of performance when generalizing from original to translated data.",
        "The real cross-lingual generalization ability of XLM-R is considerably better than what the accuracy numbers in XNLI reflect.",
        "Overcoming the cross-lingual gap is not what makes TRANSLATE-TRAIN work.",
        "The original motivation for TRANSLATE-TRAIN was to train the model on the same language it is tested on, but we show that it is training on translated data that is key for this approach to outperform ZERO-SHOT as reported by previous authors.",
        "Improvements previously attributed to data augmentation should be reconsidered.",
        "The method by Singh et al. (2019) combines machine translated premises and hypotheses in different languages, resulting in an effect similar to BT-XX and MT-XX.",
        "The potential of TRANSLATE-TEST was underestimated.",
        "Our work reduces the gap between TRANSLATE-TEST and the state-of-the-art by addressing the underlying translation artifacts.']"
    ],
    "7369": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Eliminating posterior collapse is necessary to get useful VAE models, but not sufficient.",
        "Depending on the data, these local features are sometimes not very correlated with global aspects like topic or sentiment.",
        "To learn to infer more global features, we explored alternative architectures based on bag-of-word assumptions on the encoder or decoder side.",
        "The latent variable is more predictive of global features and memorization of the first words and sentence length is decreased.",
        "These models are more suitable for diverse and controllable generation.",
        "A simple reason for the memorization of the first few words could be that, in the beginning of training, the reconstruction loss is higher on these words.",
        "More powerful decoders with alternative factorizations could avoid this issue.",
        "VAEs operate on uncorrupted inputs and learn a corruption process in the latent space.",
        "In contrast, models in the BERT family are given corrupted inputs and are penalized only on these corrupted inputs, thereby avoiding memorization altogether.\"']"
    ],
    "7371": [
        "The proposed approach outperforms several baselines, including a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "The proposed approach demonstrates almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks, as demonstrated by a human evaluation.",
        "The use of OCaml metaprogramming via PPX allows for eliding explicit references to the internal structure of Coq datatypes, making it easier to maintain the toolchain as Coq evolves.",
        "Our models and toolchain are applicable to Coq projects unrelated to the MathComp family of projects, but may require more effort to maintain as Coq evolves.",
        "The current version of Lean does not provide serialization of internal data structures as SerAPI does for Coq, which prevents direct application of our toolchain.",
        "Our models are applicable to proof assistants with different foundations and proof-checking toolchains, such as Isabelle/HOL, but the Archive of Formal Proofs (AFP) contains many projects with high-quality lemma names.",
        "We presented novel techniques for learning and suggesting lemma names in Coq verification projects, based on neural networks.",
        "Our results show that the multi-input models, which use internal data structures, substantially outperform several baselines, and the model that uses the lemma statement tokens and the chopped kernel tree with attention and copy mechanism performs the best.']"
    ],
    "7394": [
        "The taxonomical hierarchy is repeatedly evident from individual resource availabilities (LDC, LRE, Wikipedia, Web), entropy calculations for conferences, and the embeddings analysis.",
        "LREC and Workshops(WS) have been more inclusive across different classes of languages, seen through the inverse MRR statistics, entropy plots and the embeddings projection.",
        "There are typological features (such as 144E), existing in languages over spread out regions, represented in many resource-poor languages but not sufficiently in resource-rich languages.",
        "Newer conferences have been more language-inclusive, whereas older ones have maintained interests in certain themes of research which don't necessarily favour multilingual systems.",
        "There is a possible indication of a time progression or even a technological shift in NLP, which can be visualized in the embeddings projection.",
        "There is hope for low-resource languages, with MRR figures indicating that there are focused communities working on these languages and publishing works on them, but there are still plenty of languages, such as Javanese and Igbo, which do not have any such support.\"]"
    ],
    "7457": [
        "The proposed method outperforms the state-of-the-art baseline parser by achieving high F1 scores on standard WSJ and CTB evaluations.",
        "The use of a novel constituent hierarchy predictor based on recurrent neural networks improves the performance of a shift-reduce parser.",
        "The resulting fully-supervised parser has better performance than the state-of-the-art baseline parser.",
        "The interventions used in this study have a negative impact on transfer learning, but they may help reduce the prevalence of artifacts in generated hypotheses.",
        "The use of human-annotated data like MNLI has already proven to be a valuable tool in teaching machines general-purpose skills for language understanding.",
        "The need and opportunity to discover ways to more effectively build and use such data could further accelerate the field's progress toward robust, general-purpose language understanding technologies.",
        "The study highlights the importance of bias mitigation in models and datasets for widely deploying systems based on datasets like the ones studied.\"]"
    ],
    "7458": [
        "Our approach achieves new state-of-the-art on FewRel 2.0 dataset.",
        "We first train a representation extractor with the Clustering Promotion Mechanism.",
        "Two methods are combined by the proposed Cosine Annealing Strategy.",
        "The representation extractor is used to encode unlabeled target-domain data into features.",
        "Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.",
        "Imatinib induces a durable response in most patients with Philadelphia chromosome-positive chronic myeloid leukemia.",
        "The only hypothesis consistent with current data on ... gradual decrease in the BCR-ABL levels seen in most patients is that these patients exhibit a continual, gradual reduction of the LSCs.",
        "Germany defeated Argentina to win the 1990 FIFA World Cup.",
        "The River Thames is the longest river entirely in England and the second-longest in the United Kingdom, after the River Severn.",
        "The Isis is an alternative name for the River Thames, used from its source in the Cotswolds until it is joined by the Thame at Dorchester in Oxfordshire.\"']"
    ],
    "7477": [
        "The brevity problem in neural machine translation (NMT) can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F 1 on standard WSJ evaluation and 85.5% F 1 on standard CTB evaluation.",
        "Masking is an efficient alternative to finetuning for utilizing pretrained language models like BERT/RoBERTa.",
        "Extensive experiments show that masking yields performance comparable to finetuning on a series of NLP tasks.",
        "Leaving the pretrained parameters unchanged, masking is much more memory efficient when several tasks need to be solved.",
        "Intrinsic evaluations show that binary masked models extract valid and generalizable representations for downstream tasks.",
        "The minima obtained by finetuning and masking can be easily connected by a line segment, confirming the effectiveness of applying masking to pretrained language models.']"
    ],
    "7486": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "Methods that rely on pre-trained language models to produce sentence embedding outperform traditional methods of the word representation.",
        "Our model with 110M parameters and 8 layers was able to outperform XLNet with 340M parameters and 24 layers.",
        "Utilizing the linguistic structure of humor in designing the proposed model is important for achieving high accuracy in the current task.",
        "The usage of sentence embeddings is a crucial factor in achieving high accuracy in the current task.",
        "Our hypothesis on the structure of humor is valid and can be utilized to create very accurate systems of humor detection.\"']"
    ],
    "7489": [
        "Modern word embeddings contain information that allows distinguishing synonyms and antonyms.",
        "The approach could possibly be scaled to other semantic aspects of the words.",
        "The proposed methodology allows for revisiting the questions of language acquisition in the context of the distributional hypothesis.",
        "The spaces of modern word embeddings could be profoundly nonlinear concerning a given semantic attribute of the word.",
        "A deeper understanding of the geometric properties of these spaces could significantly improve the quality of the resulting models.",
        "The very assumption that semantic similarity could be captured with cosine distance in Euclidian space is debatable.",
        "The proposed methodology demonstrates that contrary to a widely spread opinion, modern word embeddings contain information that allows distinguishing synonyms from antonyms.",
        "The new embeddings, in which the information on synonyms and antonyms is disentangled, improve the performance on the downstream tasks.\"']"
    ],
    "7500": [
        "UXLA outperforms other zero-resource cross-lingual task adaptation methods.",
        "UXLA achieves a new SoTA for all tested languages on the zero-resource XNER task.",
        "UXLA performs well on both XNLI and PAWS-X tasks, even with only 5% labeled data in the source.",
        "The confidence penalty helps to separate clean samples from noisy ones.",
        "Training with confidence penalty improves pseudo labeling accuracy and helps distillation methods perform better noise filtering.",
        "Using a confidence penalty in training makes the model more robust.",
        "Without confidence penalty, there are many noisy samples with a small loss which is not desired.",
        "The length distribution of the samples affects the performance of the lower length samples.",
        "Selecting sentences from the lower length distribution while still covering the entire lengths can improve the performance.\"']"
    ],
    "7525": [
        "We propose a generative adversarial imitation learning framework for text generation - TextGAIL.",
        "Experiment shows that TextGAIL can generate not only more diverse but more accurate and reasonable outputs.",
        "The discriminator can provide meaningful reward signals in various unconditional and conditional text generation tasks.",
        "TextGAIL outperforms the simple projection baseline using fast-align on most languages.",
        "Our model achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "We extend the exploration of adversarial training on text generation by incorporating large scale pre-trained models.",
        "We use a contrastive discriminator and proximal policy optimization to improve the stability of the generator\\'s training.",
        "We incorporate a large-scale pre-trained language model, GPT2 in our framework as the generator.",
        "We also use a pre-trained RoBERTa model to initialize the discriminator to provide reliable rewards to the imitation learning framework.\"']"
    ],
    "7530": [
        "We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The current work only uses subtitle block level translations to make a decision and ignores temporal aspect.",
        "Temporal structure can bring significant information to make a better judgment particularly on Loose translations.",
        "Training one model per language pair requires considerable operational load.",
        "A multilingual model can reduce this load while helping resource-starved languages.",
        "Exploiting temporal information and learning a common space for multiple languages are future directions we are considering for this work.\"']"
    ],
    "7563": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "When only allowing adversarial perturbations that preserve semantics and grammaticality, NLP models are relatively robust to current synonym substitution attacks.",
        "As models improve at measuring semantic similarity, we will be able to more rigorously ensure that adversarial perturbations preserve semantics.",
        "It remains to be seen how robust BERT is when subject to paraphrase attacks that rigorously preserve semantics and grammaticality.",
        "The lack of a shared vocabulary for discussing NLP attacks makes it unclear what improvements in attack success rate between TEXTFOOLER and GENETICATTACK are due to.",
        "With a shared framework for defining and applying constraints, future research can focus on developing better search methods and better constraint application techniques for preserving semantics and grammaticality.\"']"
    ],
    "7574": [
        "The proposed UniConv model achieves promising performance on the MultiWOZ benchmark, validating its efficacy.",
        "Jointly training a Bi-level State Tracker and Joint Dialogue Act and Response Generator improves the performance of conversational agents for Multi-domain Task-oriented Dialogues.",
        "Delexicalizing each target system response sequence by replacing matched entity attributes with canonical tags improves the performance of the model.",
        "Using a separate embedding matrix to encode target system responses improves the performance of the model.",
        "The proposed method can handle multi-domain dialogues and involve parts of the dialogue in each domain.",
        "The model achieves competitive performance with the previous systems, using less than 1% of the training data.",
        "Late gaze features emerge as the most discriminative ones for disambiguating categories.",
        "The proposed method does not require text processing and can handle a wide range of gaze and linguistic features.\"']"
    ],
    "7582": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but the results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The methodology used in this paper can be used to attenuate biases in word representations using a projection-based method.",
        "The approach can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "The experiments show that the static GloVe embeddings can be attenuated using the projection-based method.",
        "The approach can also be extended to contextualized embeddings (ELMo, BERT) by debiasing the first non-contextual layer alone.",
        "There is a need to move beyond model evaluation in the standard, i.i.d. setting, and to explicitly incorporate distribution shifts into evaluation.",
        "The new test sets offer a helpful starting point for future work.",
        "The direction of constructing metrics for comparing datasets that can explain the performance differences observed is promising.",
        "The interplay between additional data and model robustness is an important direction for future research.']"
    ],
    "7592": [
        "Our models improve the current state-of-the-art.",
        "A cross-segment BERT model is extremely competitive with hierarchical models.",
        "Local context is sufficient in many cases.",
        "Deep transformer encoders are useful for encoding long and complex inputs.",
        "Bi-LSTMs proved useful for discourse segmentation.",
        "RNNs in general may also be useful for very long documents.",
        "Distillation is an effective technique to build much more compact models.",
        "We plan to further investigate how different techniques apply to the problem of text segmentation."
    ],
    "7602": [
        "Our results re-emphasize the gap between BLEU scores and translation quality at the discourse level.",
        "The overall BLEU scores for Fr-En are higher than the BLEU scores for De-En; however, we see that both the lexical consistency and the discourse connective accuracies are higher for De-En.",
        "We reveal a gap in performance consistency across language pairs.",
        "Models may be tuned for a particular language pair, such as ANAPH which was trained for En-Ru.",
        "Our findings match the conclusions from Kim et al. (2019) regarding the lack of satisfactory performance gains in context-aware models.",
        "Given no comprehensive evaluation across language pairs, the best bet for training an MT model is to use the baseline SEN2SEN and CON-CAT models, which perform more or less reliably across different tasks.",
        "Our results emphasize the need for standard benchmarking datasets and evaluation measures across language pairs, that will provide a better picture of MT system performance.\"']"
    ],
    "7604": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "The human baseline for FigureQA was exceeded by PReFIL, but the results were more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "Better OCR methods led to better results for DVQA.",
        "Future developments in OCR technology would likely improve PReFIL further.",
        "The strong results in this paper suggest that the community is ready for more difficult CQA datasets.",
        "Document-level CQA is needed to understand charts in documents, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments.']"
    ],
    "7605": [
        "The traditional global discrimination task for evaluating coherence is simpler than downstream tasks such as extractive and abstractive summarization, and the models may not learn useful features for these tasks due to the difference between positive and negative documents being a permutation/re-ordering of sentences.",
        "The models fail to generalize and perform poorly on downstream applications, despite task-specific re-training.",
        "The training procedures may not provide the right setting to learn features that are generic enough to apply to tasks in a harder setup.",
        "The models' self-supervision from distinguishing an original coherent document from its incoherent renderings generated by random permutations of its sentences is a poor approximation of real-world coherence problems.",
        "Models that perform well on the permutation task may be overfitting and failing to find coherence issues that are more subtle than shuffled text.",
        "The current self-supervision for coherence modeling is not suitable for downstream coherence problems, and a different training setting may be required to make the models applicable to actual downstream tasks.\"]"
    ],
    "7608": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "NUBIA achieves state-of-the-art results across evaluation of machine translation and image captioning strongly building on the successes of recent NLP architectures such as RoBERTa and GPT-2.",
        "These strong results suggest that using a neural networks to extract features and combine them will be a key component of building future automatic scoring metrics for text generation with the promise of unifying evaluation of image caption, machine translation and potentially other text generation tasks.",
        "NUBIA can be improved through three axis: (1) the efforts of the wider NLP community at creating models achieving strong results on the NLU benchmarks like GLUE; (2) the addition of better features capturing aspects of human quality assessment; and (3) better aggregator design.",
        "Current limitations include the language, with existing NUBIA models only working for English sentence pairs, and potential biased training data leading to underscoring or over scoring valid translations.\"']"
    ],
    "7612": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The resulting fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "Neural methods have shown great promises for unsupervised multi-document abstractive summarization, overcoming the lack of fluency of extractive models.",
        "The proposed models aim to overcome the problems of complexity in training and the tendency to generate incorrect statements in unsupervised settings.",
        "The use of control codes makes it easy to extend for other multi-document summarization use-cases.",
        "The generated reviews are more factual than those generated by other models, but inaccuracies can still appear.",
        "The models learn the conjugations from the input, which is mostly in first persons, and such summaries might be misleading as it could lead to believe that an actual human wrote those.",
        "Any use of such algorithms should be accompanied by a clear disclaimer on its true nature.']"
    ],
    "7613": [
        "The proposed method surpasses prior state-of-the-art methods for both DVQA and FigureQA.",
        "The model achieves accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features are the most discriminative ones for disambiguating categories of it.",
        "The method is robust to exposure bias and can improve generation quality.",
        "The proposed loss minimizes the na\u00efve upper bound of the KL divergence of the KDE nonparametric distribution from the model.",
        "The method does not suffer from the posterior collapse issue.",
        "The model generates sequences at train time, and there is no such mismatch between test and train regimes.",
        "The method is simple to use and efficient, and it does not suffer from the exposure bias issue.",
        "The proposed method can improve representation learning and language modelling.",
        "The method has the potential to improve retrieval of information from charts, which has numerous applications, including automatic information retrieval, table reconstruction, and enabling better understanding of charts by people with visual impairments.']"
    ],
    "7623": [
        "The quality of crowdsourced annotations is a major factor contributing to the overall error rate of models on TACRED.",
        "The large number of label corrections and the improved average model performance show that the quality of crowdsourced annotations is a major factor contributing to the overall error rate of models on TACRED.",
        "The low quality of crowd-generated labels in the Challenging group may be due to their complexity, or due to other reasons, such as lack of detailed annotation guidelines, lack of training, etc.",
        "Crowdsourcing, even with crowd worker quality checks, may not be sufficient to produce high-quality evaluation data.",
        "Models may be able to adequately utilize noisily labeled data for training, but measuring model performance and comparing progress in the field may require an investment in carefully labeled evaluation datasets.",
        "The relation label predicted most frequently across the 49 models disagreed with the ground truth label of the re-annotated Challenging and Control Test groups in 1017 (43.3%) of the cases.",
        "The inter-annotator agreement of error categories assigned to these examples is high at \u03ba T est = 0.83 (\u03ba T est = 0.67 if the category No Relation is excluded).",
        "Argument errors accounted for only 43 (4.2%) misclassifications, since the entities seem to be mostly correctly assigned in the dataset.",
        "In all entity type misclassification cases except one, the errors originate from false annotations in the dataset itself.",
        "Context misinterpretation caused 974 (95.8%) false predictions.",
        "No relation is incorrectly assigned in 646 (63.6%) of misclassified instances, even though the correct relation is often explicitly and unambiguously stated.",
        "In 134 (13.2%) of the erroneous instances, the misclassification resulted from inverted or wrong argument assignment, i.e., the predicted relation is stated, however, the arguments are inverted or the predicted relation involves an entity other than the annotated one.",
        "In 96 (9.4%) instances, the error results from TAC KBP guidelines prohibiting specific inferences, affecting most often the classification of the relations per:cities of residence and org:top member/employee.']"
    ],
    "7630": [
        "The proposed methodology for clustering word embeddings provides a viable alternative to traditional topic modeling at lower complexity and runtime.",
        "Pretrained word embeddings, combined with tf-weighted k-means and tf-based reranking, achieve better results than traditional topic modeling.",
        "The use of pretrained word embeddings and clustering algorithms leads to a significant increase in the quality of unsupervised document analysis.",
        "The proposed method outperforms traditional topic modeling in terms of complexity and runtime.",
        "The systematic comparison of various influential embedding methods and clustering algorithms shows that the proposed method is a viable alternative to traditional topic modeling.']"
    ],
    "7655": [
        "The proposed model achieves state-of-the-art results on two benchmark datasets.",
        "The model's performance is better than previous methods, despite being simpler.",
        "The model uses a link predictor and EType+, which are key contributors to its success.",
        "The silver relations are helpful in improving the model's performance.",
        "The model outperforms previous state-of-the-art methods, including feature engineering and deep learning models.",
        "Context information is crucial for distinguishing the relation between two entities.",
        "Combining entity types with other features does not improve the model's performance.",
        "The BOW feature has negative effects on the RE performance.",
        "The proposed method uses only entity types, yet it yields higher performance than previous work on both NYT-FB and TACRED.",
        "A strong inductive bias is required to train a relation extraction model without labelled data.\"]"
    ],
    "7667": [
        "The proposed model, HERO, achieves significant improvements in performance compared to the state of the art when transferred to multiple video-and-language tasks.",
        "The novel pre-training tasks proposed in this work capture temporal alignment both locally and globally, leading to improved performance on downstream tasks.",
        "The extension of the model to other video-and-language tasks is a potential direction for future work.",
        "Pre-training greatly lifts HERO performance on VIOLIN by approximately +2.9%.",
        "The use of global frame-level features in HERO may limit its ability to capture inconsistencies between hypothesis and video content, such as changes in region-level attributes.",
        "Extending HERO for region-level video representations could be an interesting future direction.",
        "HERO is also extensible to generation tasks, such as multi-modal video captioning.",
        "Pre-training only applied to the encoder significantly improves HERO performance on TVC across all metrics.']"
    ],
    "7679": [
        "The proposed sentence simplification approach does not rely on labeled parallel simplification data and achieves language-agnostic performance, outperforming previous state-of-the-art results.",
        "The approach can be scaled to more languages and types of simplification, and applied to paraphrase generation.",
        "The method shows promise in improving factual consistency, particularly in named entity hallucination or disappearance.",
        "The use of controllable generation, pretraining, and large-scale mining of paraphrases from the web is effective in simplifying sentences.",
        "The approach can be applied to three languages: English, French, and Spanish.",
        "The method outperforms previous state-of-the-art results, even those using labeled simplification data.",
        "The use of mined data from the web is beneficial for improving the quality of the simplified sentences.",
        "The approach can be used for paraphrase generation and other types of simplification.']"
    ],
    "7685": [
        "We have demonstrated that semantic type filtering is a valuable addition to NLP pipelines for broad-coverage biomedical information extraction.",
        "Our novel WikiMed and PubMedDS datasets for biomedical concept normalization research are significantly larger than any previous resources for medical entity linking research.",
        "We demonstrated that semantic type prediction measurably improves information extraction performance on four benchmark datasets from different genres of text and types of information.",
        "The choice of information extraction tool for a given setting is an important consideration.",
        "Opportunities for further research synthesizing semantic type prediction and disambiguation are present.",
        "Broad-coverage information extraction from biomedical text is an important application area for biomedical NLP tools, and one which poses significant challenges in the scale and diversity of information to extract.\"']"
    ],
    "7710": [
        "Our approach for learning robust and interpretable models for handling spatial references in text achieves up to 16.7% improvement in goal localization error compared to existing state-of-the-art systems.",
        "Our model is more robust to noise compared to baselines, with improvements of up to 16.7% in terms of both unseen objects (observational noise) and randomly injected words (textual noise).",
        "Our approach uses a cross-modal attention layer to capture fine-grained spatial concepts between entity pairs, with dynamically computed weights.",
        "Our model's intermediate representations provide a way to interpret its predictions, allowing for future research on more types of spatial relations and larger observation spaces.",
        "Our approach can be readily extended to more scenarios by considering longer-range cells and incorporating the object detector to extract the object in the scene.\"]"
    ],
    "7729": [
        "The performance of SCITE is limited by the insufficiency of high-quality annotated data.",
        "The proposed method achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what we have used here for few-shot learning.",
        "Our attempts to predict good vector weights using a simple regression model yielded encouraging results.",
        "Testing on PUD languages, we match the performance of using the best fixed treebank embedding vector in nine of ten cases within the bounds of statistical significance and in five cases exactly match it.",
        "The LAS landscape in the weight vector space is not noisy.",
        "There is room for improvement for the predictor to find interpolated vectors which are better than the fixed ones.",
        "Exploring other methods to predict treebank vectors, such as neural sequence modeling, may be beneficial.",
        "The previous work on the use of treebank vectors in mono-and multi-lingual parsing suggests that treebank vectors encode information that enables the parser to select treebank-specific information where needed while also taking advantage of treebank-independent information available in the training data.",
        "Interpolating treebank vectors adds a layer of opacity, and experiments with synthetic data may provide a better understanding of what they may be capturing.']"
    ],
    "7730": [
        "Our approach outperforms benchmark models across different datasets.",
        "We propose a chunking policy network for machine reading comprehension, which enables a model to learn to chunk lengthy documents in a more flexible way via reinforcement learning.",
        "We add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers.",
        "Our approach has practical value for tasks like SQuAD, where annotators are shown only context and are required to provide (q, a) pairs.",
        "Our method is flexible and can be extended to other datasets/tasks.",
        "We propose to teach extractive MRC with explanations, with a focus on annotation efficiency.",
        "Our system demonstrates the efficiency of our approach for MRC.",
        "We believe that explanations stating \"why\" and justifying \"deduction process\" opens up a new way to communicate human\\'s generalization abilities to MRC model training.']"
    ],
    "7745": [
        "We propose a novel Transformer-based table-to-text generation framework to address the faithful text-generation problem.",
        "To enforce faithful generation, we propose a new table-text optimal transport matching loss and a table-text embedding similarity loss.",
        "Our framework can significantly outperform the state-of-the-art methods.",
        "We further propose a new automatic evaluation metric specialized to the table-to-text generation problem.",
        "Extensive experiments are conducted to verify the proposed method, and both automatic and human evaluations show that our framework can significantly outperform the state-of-the-art methods.",
        "The PARENT-T score for one instance is computed as follows: EQUATION ...",
        "The system-level PARENT-T score for a model M is the average of instance-level PARENT-T scores across the evaluation set.\"']"
    ],
    "7772": [
        "The proposed method has four notable benefits: it does not construct an explicit contextual language model, it does not require external meta-data or class annotations, it pre-computes the bias score for every word during training, and it prevents over-biasing.",
        "The method is effective in attenuating biases, as shown by the experiments using a probe for measuring biases in word representations.",
        "The method does not require explicit class labels for the contextual phrases or the train corpus, making it applicable to a wider range of contexts.",
        "The proposed method uses unsupervised clustering and therefore does not require explicit class labels for the contextual phrases or the train corpus, opening the door to a wider range of contexts.",
        "The method pre-computes the bias score for every word during training, so there is minimal overhead during inference.",
        "The proposed method prevents over-biasing by imposing an upper bound on the boost score such that it does not alter its n-gram nature within the corpus.']"
    ],
    "7777": [
        "Forward Tests Stretch User Memory: The authors claim that users may have difficulty retaining insights from the learning phase during later prediction rounds, as they only show 16 examples during learning phases and do not allow users to reference the learning data during prediction phases.",
        "Generating Counterfactual Inputs: The authors claim that generating counterfactual inputs can be challenging, especially when seeking to change the model's prediction, and that their text counterfactuals may be out of the data distribution.",
        "Fair Comparison of Explanation Methods: The authors claim that they provide a fair comparison of explanation methods by controlling for the number of data points between methods, but not for user exposure time or computation time of explanation generation.",
        "Improvements in Simulatability: The authors claim that their explanation methods improve simulatability, as measured by simulation tests with text and tabular data, and that subjective user ratings of explanation quality are not predictive of explanation effectiveness in simulation tests.",
        "Greedy Editing: The authors claim that their greedy editing method improves the model's evidence margin by selecting the single edit from the remaining edits that least changes the model's evidence margin.",
        "Distance Function: The authors claim that their distance function, which is the count of different features between inputs plus the squared Euclidean distance between latent representations, serves as a tie-breaker in their greedy editing method.\"]"
    ],
    "7786": [
        "The brevity problem can largely be explained by the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The discovered rules are used to extract Semitic roots, which are the basic units of these languages.",
        "Intrinsic and extrinsic evaluations of these rules allow us to validate our pattern discovery method as well as our root extraction method (JZR).",
        "The annotation sessions were introduced by a lecture addressing the issue of cyber-bullying, followed by a role-playing game using WhatsApp chats.",
        "The CREENDER tool exposed students to real images available online, giving them the possibility to reflect on the content of the pictures and on the reasons why abusive comments are written.",
        "The participatory analysis of the outcome of the annotation during the last session allowed students to exchange ideas, among others, on the low agreement scores, on the content of their comments, and on the subject of the most criticised pictures.",
        "The validity of the corpus created with the experimentation is an open issue, as it is difficult to perform a comparison to assess the plausibility of the messages collected during the annotation sessions.",
        "Only a thorough methodology could respond to the question of the validity of the corpus gathered through the platform, but these preliminary findings suggest that participants may act in a way that resembles real life.']"
    ],
    "7794": [],
    "7801": [
        "supervised learning can achieve strong results in identifying politically-relevant events within unstructured text.",
        "the addition of an attention layer to the input sequences would allow researchers to identify those input tokens (i.e. words) that contribute the most (or least) to a given prediction.",
        "the ease with which neural network models like those used here can overfit to the training data means that care must be taken to ensure that the models continue to perform well on out-of-sample data.",
        "when sufficient training data are available (perhaps only a few thousand examples), supervised learning can play an important role in generating political event data.",
        "the use of sub-word (i.e. character) information but only in the construction of word vectors from a pre-trained FastText model may be beneficial.",
        "distributed character n-gram vectors could form the input sequences to a neural network classifier like those discussed above.",
        "the CLEF 2019 ProtestNews Lab has provided the research community with a valuable \\'ground truth\\' data set on protest (and non-protest) events.\"']"
    ],
    "7811": [
        "PReFIL surpassed prior state-of-the-art methods for both DVQA and FigureQA.",
        "While PReFIL exceeded the human baseline for FigureQA, results are more nuanced for DVQA due to OCR model variations.",
        "All OCR versions exceeded the human baseline for structure questions, but only PRe-FIL using oracle OCR exceeded humans across all question types.",
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Our work is aimed at leveraging abundant monolingual data and syntactic analyses provided by analyzers so that the pre-training stage becomes aware of language structure.",
        "JASS, which leverages syntactic parsing knowledge from the KNP parser, outperforms MASS in many low-resource settings.",
        "The combination of MASS and JASS yields significantly better results than the individual pretraining methods.",
        "Our positive results show that the pre-training step is an appropriate place to provide linguistic hints to a NMT system.",
        "We are now working on several directions for improving and broadening our approach, including domain adaptation and multi-task pre-training.\"']"
    ],
    "7814": [
        "The proposed methodology provides easy access to different levels of analysis, allowing for the investigation of various well-defined CLMDs.",
        "The approach abstracts away from local-syntactic detail, focusing on content words, while still providing means to investigate any kind of well-defined CLMD.",
        "The methodology and datasets support the validation of UD annotation and the development of crosslingual transfer learning.",
        "The analysis reveals inconsistencies in the treatment of multi-word expressions across languages, and can be used to detect and bridge such inconsistencies.",
        "Removing inconsistencies in the application of UD and refining UD with semantic distinctions can reduce entropy in the mapping between the syntactic relations of the source and target sides, and advance UD's stated goal of bringing out cross-linguistic parallelism across languages.\"]"
    ],
    "7825": [
        "The proposed acceptability detection framework is effective for three downstream tasks, including named entity recognition.",
        "The named entity recognition task has a special structure that requires specific decision-making for what constitutes f T (t) = f T (r) in Equation (2).",
        "The framework allows for combining multiple downstream tasks by taking f T to perform multiple tasks together and return a tuple of task outputs.",
        "The framework is not without limitations, as not every downstream task can fit into it, especially if the input to the task is more than one sentence or the output is complex.",
        "Our framework can be extended to detect acceptability for NLP tasks with unstructured output, such as machine translation and semantic parsing, but abundant parallel data are required.",
        "The proposed framework can also build acceptability detectors for automatic speech recognition and semantic parsing systems, as long as abundant parallel data are available.']"
    ],
    "7838": [
        "The embeddings of GloVe, ELMo, and BERT encode gender, religion, and nationality biases.",
        "A projection-based method can be used to attenuate these biases.",
        "The method works for static GloVe embeddiments, but not for contextualized embeddings.",
        "Debiasing the first non-contextual layer alone can effectively attenuate bias in both contextualized embeddings without loss of entailment accuracy.",
        "Previous work has found that dependency label probes can be used to recover dependency labels from mBERT embeddings.",
        "However, this does not provide full insight into the nature of latent dependency label structure in mBERT.",
        "Our method provides a view into mBERT\\'s \"own\" dependency label representation.",
        "mBERT shares portions of its representation space between languages, and there is evidence for fine-grained, cross-lingual syntactic distinctions in these representations.",
        "The UUAS metric alone is insufficient for evaluating the accuracy of the structural probe.",
        "Our methods are unable to tease apart whether transfer performance is caused by subword overlap or by a more fundamental sharing of parameters.']"
    ],
    "7841": [
        "Our findings suggest that it is important for robust exploitation of retrieved context for unsupervised tasks.",
        "Basing design decisions with a limited set of downstream uses when designing general purpose pre-trained models may well us lead to less flexible models.",
        "BERT with retrieved context and no fine-tuning performs on par with DrQA on the LAMA probe.",
        "An unsupervised system performs just as well as a system that requires significant supervision such as DrQA.",
        "LMs are abstractive models, whereas DrQA is extractive, confined to returning answers that are spans of retrieved context.",
        "Generating an arbitrarily long sequence of contiguous tokens from bidirectional LMs like BERT and RoBERTa is not trivial.",
        "Unsupervised question answering is not only possible, but beginning to reach and even outperform some standard supervised baselines.",
        "Powerful and flexible unsupervised QA systems could soon be a reality, bringing with them many advantages including avoiding biases that often plague smaller datasets by incorporating knowledge from much larger corpora and greater abilities to combine and abstract pieces of information from different sources.\"']"
    ],
    "7853": [
        "The proposed reinforcement learning-based model achieves new state-of-the-art results on benchmark datasets for few-shot text classification.",
        "The model is able to preserve content and adapt to support sets, as evidenced by its performance on the miniRCV1 and ODIC datasets.",
        "The use of dynamic memory in the model allows it to learn more general learning mechanisms than previous models.",
        "The model is able to generate sentences in target style while preserving content, as shown in the sample outputs in Table 4 and 5.",
        "The model outperforms other baselines, including Cross-Aligned and CopyNMT, on various transfer tasks such as formal to informal and exciting to non-exciting.",
        "The model is able to retain entities and generate sentences with correct target style, as shown in the sample outputs in Table 4 and 5.",
        "The use of lexical and structural level changes in the model allows it to learn differences in various transfer tasks, such as formality, beyond formality, and beyond affective dimensions.']"
    ],
    "7874": [
        "We presented a novel multi-grained MRC framework based on graph attention networks and BERT.",
        "Our proposed method outperforms previously existing methods by a large margin.",
        "Improving our graph structure of representing the document as well as the document-level pretraining tasks is our future research goals.",
        "Currently existing methods cannot process long documents without truncating or slicing them into fragments.",
        "Our approach helps the model do well in cases that have a short answer.",
        "The significant improvement of our model lies in the long answer on Case 1.",
        "For Case 2 and Case 5, our Model-III does not have significant improvement compared to Model-I.",
        "The reason is that, for instances with no answer or no apparent answers, fine-grained information Question: when is a spearman correlation meant to be used instead of a pearson correlation."
    ],
    "7938": [
        "NMT encoders learn similar source syntactic information regardless of target language, consistently outperforming RNNs trained specifically for the constituent label prediction task.",
        "The performance of SCITE is still limited to some extent by the insufficiency of high-quality annotated data.",
        "NMT encoders specifically have been found not to encode fine-grained syntactic information.",
        "RNNs learn limited syntax and rely on explicit morphosyntactic cues to extract syntactic information from sentences.",
        "The successes of bidirectional and Transformer models may be due partially to their ability to combine later information with representations of earlier words.",
        "NMT encoder representations across target languages encode similar source syntax, but explicit syntactic architectures may be necessary for tasks requiring fine-tuned syntactic parses.']"
    ],
    "7960": [
        "Fine-tuned GPT-2 can generate not only fluent but also adequate text.",
        "The proposed system shows a larger improvement over previous systems in terms of semantic similarity and human evaluation.",
        "Leveraging pre-trained transformers greatly expands the vocabulary available on AMR-to-text systems.",
        "The single reference text used in current strong language models may not properly reflect AMR-to-text quality.",
        "The proposed system can generate diverse text data for AMR parser training or other applications where diversity plays a role."
    ],
    "7968": [
        "Our model shows interesting performance on the MIMIC-III dataset, with consistent results across validation folds, showing evidence of the potential of using free-text nursing notes for mortality prediction.",
        "Our results are not directly comparable to those published by Grnarova et al. [8] given that we restricted our input window to the first 48 hours of patient stay, instead of using all available notes up until the time of discharge.",
        "Our model is able to offer performance competitive with that of much more complex models with little text pre-processing, while at the same time providing visual explanations of feature importance based on coalitional game theory.",
        "Our visualizations also provide a way to annotate freetext medical notes with markers to flag parts correlated with predictions of survival and death.",
        "We have shown that nursing notes could be rich enough to capture the concepts needed for mortality prediction at a level of accuracy far higher than what is currently possible with traditional statistical techniques.']"
    ],
    "8005": [
        "Our automated system for general-purpose daily dosage calculation can handle a wide range of medications and achieve high performance.",
        "The proposed approach is sufficient to cover most cases, but there is room for improvement with different normalization modules for medications with different routes of administration.",
        "There is a need for future work to understand Sig information beyond daily dosage, such as PRN, time-limited dosing, specific indications, and cases where daily dosage is not meaningful.",
        "Understanding a patient's medication history is essential for physicians to provide appropriate treatment recommendations.",
        "A medication's prescribed daily dosage is a key element of the medication history and used in many medication timeline designs, but it is generally not provided as a discrete quantity and needs to be derived from free text medication instructions.",
        "This work is the first to generalize the task of daily dosage extraction on all medications.\"]"
    ],
    "8036": [
        "The proposed approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses a combination of Similarity Entropy Minimization and Adversarial Distribution Alignment to promote clustering and align similar class distributions across domains.",
        "The use of pseudo labels improves the domain adaptation performance.",
        "The model based on gaze features and part-of-speech information achieves accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features are the most discriminative ones for disambiguating categories.",
        "The paraphrase relationship should be symmetrical, and a clear notion of what a paraphrase is is important for both constructing test datasets and determining how a given application can use a paraphrase detection model.",
        "Selection bias in the pointwise setting can cause inconsistent results and affect the performance of the model.",
        "The state-of-the-art paraphrase identification models may not handle real-world problems and unseen data well, and may have worse results than a BOW model on simple tasks.']"
    ],
    "8047": [
        "ParsBERT achieves state-of-the-art performance on all downstream tasks, indicating that monolingual language models outperform multilingual ones.",
        "The improvement in performance by ParsBERT is due to several factors, including the standardization and pre-processing of the Persian corpora, the diverse range of topics and writing styles in the pre-training dataset, and the larger size of the corpus used for training.",
        "Compared to multilingual BERT, ParsBERT has a more diverse vocabulary and is able to perceive and understand the Persian language better.",
        "ParsBERT outperforms other Persian NER competitor models in terms of F1 score, achieving 93% and 98% scores for PEYMA and ARMAN datasets, respectively.",
        "ParsBERT also outperforms multilingual BERT and other suggestion networks in downstream tasks such as Sentiment Analysis, Text Classification, and Named Entity Recognition.",
        "The number of datasets for downstream tasks in Persian is limited, but ParsBERT still achieves state-of-the-art performance on a considerable set of datasets.",
        "ParsBERT can be synchronized with Huggingface Transformers for public use and serves as a new baseline for numerous Persian NLP use cases.']"
    ],
    "8064": [
        "The proposed approach (ConCET) achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Entity-aware classifiers are prone to overfitting to the majority entity type.",
        "Adding sparse and dense representations of the entity types helps in smoothing the entity representation and reducing bias towards certain entity types.",
        "The ConCET model avoids errors due to entity-type biases in two different ways: by using an additional network and separately training the entities, and by using two joint deep network layers.",
        "The ConCET model captures the likelihood of the appearance of a specific entity type in an utterance text to learn a specific topic label.",
        "The ConCET model enriches the textual representation of an utterance with entity information for topic classification.",
        "The interactions between entity types can be inferred, and the system can jointly learn a semantic (dense) representation and the distribution of entity types with textual information to represent an utterance.",
        "ConCET outperforms all of the state-of-the-art baselines with either entity linker, especially when using the PMI-EL domain-specific linker.",
        "The classification latency for the proposed approach is not substantially higher compared to the baseline classifier that operates on an utterance text alone.",
        "The main reason for this is that all 4 stages of the ConCET can be run in parallel, and entity linking requires only a knowledge base lookup, which does not introduce perceptible increases to response latency.']"
    ],
    "8065": [
        "The proposed fully-supervised parser outperforms the state-of-the-art baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation.",
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The proposed joint model improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages.",
        "The strengths and potential limitations of the proposed CTS models on different topics at different stages in the conversation.",
        "Analyzing CF contribution to RNN-based models shows that the results are statistically significant with p-value < 0.05.",
        "Performance for different conversation stages, where the average accuracy of the suggested topic drops as the number of suggestions in a conversation increases.",
        "The limitations of the experimental evaluation using off-line analysis, and the potential limitation of the form of the recommendations themselves.']"
    ],
    "8069": [
        "VoxClamantis V1.0 is the first large-scale corpus for phonetic typology, with extracted phonetic features for 635 typologically diverse languages.",
        "The corpus has the potential to enable further developments in both phonetic typology and methodology for working with cross-linguistic speech corpora.",
        "The corpus includes extracted phonetic features for 635 languages, allowing for investigation of phonetic typology at a large scale.",
        "The corpus has several caveats and limitations, including the need for further improvements.",
        "The corpus includes alignments and token-level features to enable greater research accessibility in the area of phonetic typology.",
        "The corpus motivates and enables further developments in both phonetic typology and methodology for working with cross-linguistic speech corpora.",
        "The corpus has the potential to be used for investigation of phonetic typology at a large scale, allowing for the exploration of both the research potential and limitations of this corpus.']"
    ],
    "8096": [
        "The proposed approach uses multi-head self-attention weights from transformer models to detect causal relations for counterfactual detection in text.",
        "RoBERTa overall shows the best performance for the counterfactual detection task.",
        "BERT performs the best for the antecedent-consequent detection task.",
        "DistilBERT, a considerably smaller model, shows comparable performance with the rest of the transformer models for both sub-tasks.",
        "The attention-heads in BERT assign linguistically selective-attention with respect to the conditional nature of the counterfactual statements.",
        "The proposed system detects antecedent-consequent spans in counterfactual statements with good efficiency using a simple regression over the spans.",
        "Post-inference processing on the predicted spans or replacing the regression module with a token-level sequence labelling module can further improve the system's performance.\"]"
    ],
    "8101": [
        "Fine-tuning models for codeswitching can lead to a drop in performance in monolingual models.",
        "A pooled model performs well on both monolingual and code-switched test sets, but fine-tuning this model with less code-switched data and regularization leads to best performance.",
        "Building and Fine-tuning a pooled model relies on the availability of monolingual data that the original ASR was trained on.",
        "The LWF framework can be used to adapt speech recognition models to specific domains without forgetting the distribution of the original data they were trained on.",
        "The loss in performance on monolingual datasets is mitigated by using the LWF technique.",
        "Models built using the LWF framework outperform monolingual models fine-tuned with code-switched data.",
        "Adding BLSTM layers as task specific layers may improve the performance of the model.",
        "Adversarial training procedure can be used to make the shared layer parameters agnostic to the specific task while encouraging the model to learn discriminative parameters at the task specific layers.']"
    ],
    "8109": [
        "providing lexical information to parsing (\\'tag \u2192 parse\\') leads to more benefits than providing syntactic information to tagging (\\'tag \u2190 parse\\')",
        "removing the interaction between tagging and parsing significantly deteriorates both tagging and parsing quality",
        "our approach allows lexicality and syntax to interact with each other in the joint search process, improving over previous work on joint POS tagging and dependency parsing",
        "we remove only linearly-present information, and that the classifiers can rely on arbitrary features that happen to correlate with the gold label",
        "the information we remove in practice should be seen only as an approximation for the abstract information we are interested in",
        "one has to be cautious of causal interpretations of the results",
        "creating algorithms for non-linear information removal is important",
        "it is not clear how to detect disentangled representations of properties such as POS, which are independent of other properties of the text",
        "there is an ongoing debate on the assertion that certain information is \\'encoded\\' in the representation"
    ],
    "8146": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "Extensive experiments demonstrate that AdaBERT achieves comparable performance while significantly improving the efficiency by 12.7x to 29.3x speedup in inference time and 11.5x to 17.0x compression ratio in parameter size.",
        "The word loneliness tends to be used in more negative contexts than the word lonely.",
        "There were 12% more tweets with the word lonely written by female users than tweets written by males, even though in the General Tweet Corpus (used as control) there were 10% more tweets written by male users.",
        "The emotional content in the SOLO tweets written by male and female users was strikingly similar.",
        "We found more words associated with the adolescent age group (especially, teenagers) and less words associated with the adult age group in the lonely corpus as compared to the solitude corpus, which suggests a higher vulnerability of teenagers to the negative experiences of feeling lonely.",
        "We make SOLO and other resources created in this project freely available to encourage further research on health, economical, and other issues related to people\\'s experiences of being alone and how these issues affect the population\\'s well-being.\"']"
    ],
    "8153": [
        "Providing lexical information to parsing leads to more benefits than providing syntactic information to tagging.",
        "The approach of joint part-of-speech tagging and dependency parsing using transition-based neural networks improves over previous work on joint POS tagging and dependency parsing.",
        "The use of a projection-based method for attenuating biases in word representations can be effective.",
        "The classification-aware topics provide class-related topics, which are a efficient way to discover the class of (pre-defined) related topics.",
        "The automatically assigned category labels of debunked COVID-19 disinformation can be used for statistical analysis.",
        "The main findings of the statistical analysis of COVID-19 disinformation include claims about community spread and impact, medical advice and virus effects, prominent actors, conspiracies, virus transmission, and vaccines, medical treatments, and tests.']"
    ],
    "8183": [
        "Human brain activity appears to help machine attention in attending toward the most crucial words in a sentence.",
        "There are differences between the exploited EEG attention scores, dependent on both frequency domain and reading task of the EEG signals.",
        "We observe smaller performance gains for NER compared to Relation Extraction.",
        "Brain activity extracted from sentences read in NR appear to be more useful to detect named entities compared to EEG signals from AR.",
        "Frequency domains Performance enhancement differed depending on the frequency domain from which EEG attention scores were extracted.",
        "Overall, the attention scores distilled from 15-and 30dimensional embeddings carry slightly more informative signals than attention scores extracted from 5-dimensional embeddings.",
        "The fact that we exploited little EEG data to create the attention scores -merely 300 sentences for NR and 407 sentences for AR, averaged over 12 participants.",
        "Useful signals could be extracted that help neural networks to understand language in a similar manner as the human brain does.\"']"
    ],
    "8192": [
        "Our experiments reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion, and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "The proposed frameworks could be conveniently adapted to other text classification tasks or extended to multiple target domains setting.",
        "The effectiveness of separating ability is shown in the training process for experiments on Amazon.",
        "The validation of weighting scheme Table 3 reveals the effectiveness of the weighting scheme in our frameworks.",
        "The discriminator as a probability distribution estimator is a key feature of our frameworks, which can be further studied in the future in other application scenarios.",
        "Experimental results on two SA datasets demonstrate the promising performance of our frameworks.\"']"
    ],
    "8194": [
        "Our proposed method for data augmentation improves the performance of downstream dialog models in the presence of ASR errors.",
        "Our approach does not require any modification to the ASR models or downstream dialog models, and it does not introduce any additional latency during inference time.",
        "Our data augmentation approach is effective in low-resource situations where the model size needs to be small or the training data is scarce.",
        "Our approach can be combined with other methods, such as acoustic embeddings and multi-task training, for further performance gains.",
        "Adjusting the word-error-rate (WER) of individual words can improve the overall WER of the system.",
        "The target overall WER can be adjusted by changing the frequency of correct recognition based on the confusion matrix.",
        "The adjustment of individual WER can be done using a simple heuristic that treats each n-gram in the confusion matrix as a single word.']"
    ],
    "8228": [
        "We presented a fine-grained manual evaluation for English\u2192Chinese on the two mainstream architectures of NMT: RNN and Transformer.",
        "The error taxonomy was developed from the MQM core taxonomy for MT evaluation.",
        "Transporter-based systems generate significantly more accurate, fluent, and comprehensible translations with less westernized Chinese expressions.",
        "However, Transformer systems do not handle typography as well as RNN.",
        "None of the MT systems produced any errors related to unpaired marks, and only one system produced errors related to classifiers, which were very rare (0.16% of the tokens).",
        "We can conclude that Transporter systems produce an overall better translation compared to RNN when translating from English to Chinese.",
        "Our taxonomy could be of use for further error analysis on Chinese MT quality.",
        "Future research could include a larger annotation sample to investigate if punctuation is a common issue in NMT systems based on Transformer and to verify that NMT is able to produce correct classifiers.\"']"
    ],
    "8238": [
        "Increasing the number of training examples for 'standard' models such as LSTM leads to performance gains that are within 1 percent of their massively pretrained counterparts.",
        "The findings of this work have significant implications on both the practical aspect as well as the research on pretraining.",
        "For industrial applications where there is a trade-off typically between accuracy and latency, our findings suggest it might be feasible to gain accuracy for faster models by collecting more training examples.",
        "Pretrained LSTMs have kept up very well with transformer-based counterparts on many tasks.",
        "Finetuning BERT-style models on resource-rich downstream tasks is not well studied, but our findings suggest that it is possible to achieve competitive accuracy results by training a simple LSTM.",
        "Reusing the token embeddings learned during BERT pretraining in an LSTM model leads to significant improvements.\"]"
    ],
    "8255": [
        "We have shown that word embeddings are able to capture the meaning of human defined word lists.",
        "Our method differs in that it looks at an embedding\\'s general ability to understand human-defined concepts.",
        "We have shown the ability of embedding algorithms in learning concepts from word lists.",
        "GloVe outperforms word2vec generally, but fastText performs better in the majority of situations for all word lists we have tested from LIWC.",
        "Our method can also be used as a measure of the quality of word lists and their ability to accurately describe a concept.",
        "Future work with this method would involve extensive testing of the method using with varying differing hyperparameters to see the optimal performance of these embedding algorithms.",
        "There is potential to compare these embeddings by testing the extracted embedding with a linear classifier, or fine-tuning their full model to our task.",
        "The benefit of this could be to validate word lists that are not as carefully curated as LIWC word lists.",
        "Different source corpora may also change the performance of these word lists due to the meaning of some words changing from domain to domain.\"']"
    ],
    "8277": [
        "The approach achieves new state-of-the-art performance on the FewRel 2.0 dataset.",
        "The representation extractor uses Similarity Entropy Minimization and Adversarial Distribution Alignment to align similar class distributions across domains.",
        "The Cosine Annealing Strategy combines the two methods.",
        "The pseudo-labeled target-domain data is used to train the few-shot classifier.",
        "The approach reduces the noise of pseudo labels to improve domain adaption performance in the future.",
        "The neural models performed well even in relatively low-resource cases.",
        "Multilingual training can take advantage of commonalities across languages in the dataset.",
        "Data augmentation techniques such as hallucination helped fill in the gaps and allow networks to generalize to unseen inputs.",
        "The task's focus on typological diversity revealed that some morphology types and language families remain a challenge for even the best systems.",
        "Cross-linguistic transfer of similarities by multilingual training is less viable for some languages due to their low resource and unique morphology.",
        "Hand-encoding linguist knowledge in finite state grammars is necessary for optimal performance for some languages.\"]"
    ],
    "8284": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Our framework outperforms baseline classifiers by a significant margin.",
        "Knowledge distillation is not only a tool for model compression, but can also be used to reduce overfitting.",
        "Our technical solution can be easily applied to other clinical datasets.",
        "The message triage system is of real impact to healthcare providers and potentially save valuable working hours by freeing them from reading patient portal messages in chronological order.",
        "After the message triage classifier produces the priority of messages, healthcare providers can take further actions based on their existing workflow.",
        "The message triage system is limited to only utilizing the content of portal messages.",
        "Incorporating negation detection into our classification framework could improve the system\\'s accuracy.",
        "Our current message triage system has a few limitations from the clinical perspective, such as the need for more granular categories and the incorporation of patient demographic information or comorbidities.\"']"
    ],
    "8317": [
        "There have been various efforts being made from multiple angles in order to handle\" the COVID-19 pandemic.",
        "Research focused on building effective question answering (QA) and information retrieval (IR) systems has been especially promising.",
        "Many IR models for COVID-19 perform in an unsupervised or zero-shot setting due to the lack of datasets for the pandemic.",
        "QA models rely on a large amount of rich annotations, unlike IR where unsupervised models and algorithms often produce satisfying results.",
        "The COVIDASK system uses QA models trained on SQuAD and Natural Questions and returns zeroshot results on COVID-19 questions.",
        "COVIDASK could benefit from recent discoveries in unsupervised QA, zeroshot QA, and question generation.",
        "The development of COVIDASK faced several challenges, including answerability of questions for a large amount of unstructured text and leveraging domain-adapted datasets.",
        "These challenges are actively studied in many NLP researches and will be tackled in the near future.']"
    ],
    "8326": [
        "GShard operates with lightweight sharding annotations required in the user model code only and delivers an easy to use and flexible API for scaling giant neural networks.",
        "We applied GShard to scale up Transformer architecture with Sparsely-Gated Mixture-of-Experts layers (MoE Transformer) and demonstrated a 600B parameter multilingual neural machine translation model can efficiently be trained in 4 days achieving superior performance and quality compared to prior art when translating 100 languages to English with a single model.",
        "Our proposed method presents a favorable scalability/cost trade-off and alleviates the need for model-specific frameworks or tools for scaling giant neural networks.",
        "Progressive scaling of neural networks yields consistent quality gains, validating that the quality improvements have not yet plateaued as we scale up our models.",
        "We also urge practitioners to strive for training efficiency and identified factors that affect the training efficiency and showed their implications on downstream task quality.",
        "Generating a single program that is general enough to express computation on all underlying parallel devices is the key to compile scalably.",
        "Our experimental results empirically support that, mere parameter counting does not always correlate with the effective capacity of the models at scale [85, 86] .\"']"
    ],
    "8336": [
        "Our model significantly outperforms existing HRED models and its attention variants.",
        "The relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Our model achieves competitive performance to the state-of-the-art label projection approach with only half of the training time.",
        "The upper section of Table 3 suggests that iteration number, paraphrase model score, and aligner model score each have slightly different filtering characteristics.",
        "A higher recall subset may be obtained with the R-Classifier, which retains 96.99% of acceptable outputs with a precision of 81.19%.\")",
        "Our framework can be used for iterative construction of semantic resources via automatic paraphrasing.",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our model generalizes well to unseen words, even when the source-side spans in the test set were guaranteed to not have been observed at training time.",
        "A simple conjunction of criteria achieves higher precision than any condition alone.",
        "The P-Classifier, optimized to select a high-precision subset of the data, achieves higher precision than any of the heuristic methods, and higher recall than the highest-precision heuristic method.\"']"
    ],
    "8344": [
        "Providing lexical information to parsing (\"tag \u2192 parse\") leads to more benefits than providing syntactic information to tagging (\"tag \u2190 parse\").",
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans\\' judgements.",
        "Pitch and additive noise are the most powerful data augmentation techniques for unsupervised representation learning in English, Mandarin, and French.",
        "The two most popular data augmentation techniques that are typically done in the spectral domain do not work very well for CPC training.",
        "Time-domain noise augmentation is more effective than spectral data augmentation for CPC training.",
        "With data augmentation, CPC can take good advantage of relatively short (around 100 hours) clean and well segmented speech, although it is currently insufficient to learn competitively with very small amounts of data (between 2.5 and 50 hours).']"
    ],
    "8348": [
        "The model achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "The model based on gaze features and part-of-speech information achieved accuracy similar to that of the linguistic-based model and state-of-the-art systems, without the need for text processing.",
        "Late gaze features emerged as the most discriminative ones, with disambiguation effort indicators as late as third pass revisits.",
        "Quality-diversity evaluation under conditional text generation settings is getting great attentions.",
        "Our work provides a paradigm for future theoretical analysis, including metric definition, Pareto-optimality analysis, and divergence-compatibility judgement.",
        "The goal of unconditional text generation is to design models that better fit the text distribution, while in conditional text generation, better human evaluation results are viewed as final goal.",
        "Using properly paired quality-diversity metrics can be used to evaluate the relation between quality-diversity evaluation and distribution-fitting goal.",
        "The commonly used BLEU and Self-BLEU metric pair fails to reflect the distribution-fitting goal, and a substitute can be used instead, such as CR-NRR.\"']"
    ],
    "8367": [
        "We explore a set of interventions to the Transformer-XL\\'s architecture that are very simple to implement, but shed light on the fundamental workings of the model when modelling long sequences of text.",
        "Our finding is that we do not need long-range memories at every layer of the network. Comparable performance can be obtained with a fraction (1/6th) of long-range memories if they are spaced equally across the network, or in the latter layers.",
        "We hypothesise this is because modelling long-range correlations is best done when representations are first formed from short-range correlations.",
        "A real performance drop using a single long-range memory, proving long-range dependency is not superfluous to the task.",
        "This study has implications for practitioners interested in speeding up deep Transformer-XL models.",
        "Long-range attention does not need to be scaled for every layer, and thus these architectures can be further sped-up with this observation.",
        "The practice of storing deep long-range memories is not scalable if we wish for neural networks to have the kinds of large-horizon reasoning that humans possess.",
        "Maintaining a small number of long-range memories is a step towards tractable lifelong memory.\"']"
    ],
    "8368": [
        "The online training ability of Continual BERT enables adaptive learning on new data flowing in a time-sequential manner, especially fitting to the overwhelming amount of COVID-19 literature published on a daily basis.",
        "In contrast to the provided abstracts, extractive summarization of those literature can provide not only a sound, original summary of the article but also indications of where the interesting sentences and ideas lies within the text.",
        "The scalable architecture of Continual BERT also enables continually learning over longer time and in more frequency to digest new research data and information faster.",
        "The evaluations listed on Table 2 shows that Continual BERT performs much better on articles with extensive medical terms (PubMed) compared to the generic scientific articles (ScisummNet).",
        "The model performs better with more availability of historical knowledge, even if the COVID-19 itself is a new disease.",
        "The difficulty that Continual BERT experienced during the progress step can be justified with the fact that CORD-19 contains publications dating back to the 20th century, which present radically different information to the more modern publications.']"
    ],
    "8369": [
        "Our approach can achieve competitive performance compared to the previous systems (trained using the full dataset) by using less than 1% (1000 sentences) of the training data.",
        "Our method significantly outperforms previous methods, reducing the error by 21% on English Switchboard.",
        "Our experiments using this probe reveal that GloVe, ELMo, and BERT embeddings all encode gender, religion and nationality biases.",
        "We explore the use of a projection-based method for attenuating biases.",
        "Our model allows a direct extension to other commonly-used languages.",
        "Our chain-like model exploits learnings from one language for detecting offensive language on another unrelated language.",
        "We further performed experiments for detecting offensive language in a language different from the ones that we have trained on without supervision.",
        "Our model achieves competitive performance with a difference of 1.3% in Accuracy, further showing an increase in the metric of Precision by a margin of 4.06%.",
        "This work presents a cross-lingual inductive transfer learning approach to detect the offensive language in tweets across five languages: English, Turkish, Greek, Arabic, and Danish.",
        "Our proposed model can be easily extended to other languages with minimum training efforts and resources, performing competitively even with significantly imbalanced data.\"']"
    ],
    "8376": [
        "The experimental results show that our model significantly outperforms existing HRED models and its attention variants.",
        "Our further analysis shows that the relevant contexts detected by our model are significantly coherent with humans' judgments.",
        "The utility of the complementary information encoded by visual self-supervision is demonstrated, and the potential of multimodal self-supervision as a useful tool in speech representation learning is highlighted.",
        "Our method significantly outperforms fully supervised training from scratch, which further motivates the utility of self-supervised pretraining for speech.",
        "The results show that our model can learn with fewer labels, which is very relevant to low resource domains.",
        "Our method has the potential to improve low resource language ASR, emotion recognition, and cross-cultural ASR.",
        "Future work includes exploring other speech-related applications, improving the method, and utilizing cross-modal information for visual representation learning.",
        "Multimodal contrastive self-supervised learning is an interesting direction to explore for improving the method.\"]"
    ],
    "8384": [
        "Our proposed dataset, CompRes, is the first for narrative structure in news media and has the potential to be used for a wide range of applications.",
        "We have adapted two elements from the theory of Labov and Waletzky (1967) and added a new element, Success, to create a better-suited narrative structure annotation scheme for informational text.",
        "Our newly created dataset contains 29 articles with 1,099 sentences, which were collected from news and partisan websites.",
        "We have tested two supervised models on the dataset, a linear SVM over bag-of-words baseline classifier and a fine-tuned pre-trained RoBERTa-base transformer, and found that they demonstrate the potential of supervised learning methods in inferring narrative information from raw news text.",
        "Our preliminary results show an average F1 score of up to 0.7, indicating the potential of supervised learning methods for inferring narrative information.",
        "We are currently engaged in ongoing efforts to improve the annotation quality of the dataset and increase its size.",
        "Future work includes exploring the incorporation of additional elements from the narrative theory, enriching the scheme with clause-level annotation, and introducing additional layers of information to encode more global narrative structures.']"
    ],
    "8407": [
        "Our approach significantly outperforms existing HRED models and its attention variants.",
        "Our section titles lead to strong improvements across multiple reading comprehension tasks.",
        "We demonstrated that our approach performs almost as well as sequence-to-sequence approaches with unlimited training data while outperforming sequence-to-sequence approaches in low-resource domains.",
        "The state-of-the-art geoparser, DM_NLP can achieve over 0.91 in precision, recall, and F1 score, and a relatively low toponym resolution error using a simple population heuristic.",
        "Geoparsing without population information is possible, but it requires the use of alternative methods that do not rely on population information.",
        "Geoparsing fine-grained locations within a city or a specific area is possible by using a geoparser based on a large and general gazetteer.",
        "Using gazetteers beyond GeoNames can improve the performance of geoparsing, especially for historical texts in the context of digital humanities applications.\"']"
    ],
    "8451": [
        "The proposed method achieves new state-of-the-art results on the miniRCV1 and ODIC datasets.",
        "Dynamic memory can be a learning mechanism more general than what has been used here for few-shot learning.",
        "The model performs substantially better than widely-known baselines, and only marginally worse than a much more sophisticated GRU-based sequence-to-sequence baseline.",
        "Learning a mapping from action-oriented features to visual entities may provide a more expressive signal for captioning compared to the raw features themselves.",
        "The full potential of automatic summarization is unlocked when the document sets are so large that the average person would not be able to digest them.",
        "The proposed schema can handle large product review sets in a weakly supervised manner.",
        "The initial summarization system based on the proposed schema shows promising results.",
        "The framework sparks interest and subsequent research on MMDS.",
        "Alternative ways of clustering reviews and choosing their weak-references in order to improve training quality may be explored.",
        "A hierarchy of reference summaries, ending with a single reference summary or a handful of high-quality summaries, may be implemented.']"
    ],
    "8464": [
        "ConMask outperforms other KGC models on metrics such as Mean Rank and MRR.",
        "The Duluth system used a cutoff of .8, which resulted in a smaller number of tweets being considered offensive or targeted.",
        "The choice of .5 as the boundary between offensive or not, targeted or not, etc. may have been too lenient.",
        "Tweets that scored above .8 tended to be somewhat harsher and more offensive than those with lower scores.",
        "There appear to be some limitations in the gold standard annotations, with quite a few false positives in the gold data.",
        "It is virtually impossible to reliably annotate data without some background knowledge about the participants in the dialogue and larger cultural contexts.",
        "Annotated corpora is an important resource for this problem, and it is necessary to continue to refine the processes for the creation of the same.']"
    ],
    "8471": [
        "The approach of combining linear word embeddings with neural networks is capable of capturing the information of grammatical gender in Swedish with an accuracy of (92.02%).",
        "The artificial neural network encounters difficulties in cases of polysemy, where a linguistic form may link to different referents which belong to different part of speech categories.",
        "Additional tuning of the computational model is expected to improve the performance, specifically in cases of polysemy.",
        "The study demonstrates the potential of using word embeddings and neural networks to answer research questions of linguistic nature.",
        "The linguistic analysis targeting errors of the model is equivalently beneficial to enhance the computational model.",
        "The study is limited in terms of broadness, as it only considers a simple model and does not account for other factors such as syntax, semantics, morphological associations, etc.",
        "Further testing is required to compare the contribution of different factors with regard to gender classification.",
        "Different combinations of word embedding models and neural network classifiers should be investigated to verify which type of model provides the most precision with regard to the task of grammatical gender assignment.",
        "The study only involved one language, Swedish, which has an unbalanced distribution of gender among the lexicon, so future research should include a phylogenetically weighted sample of languages to scrutinize if word embedding and neural network can reach the same level of accuracy cross-linguistically.']"
    ],
    "8510": [
        "The beam problem in NMT can largely be explained by the brevity problem, which results from the locally-normalized structure of the model.",
        "Solving the brevity problem leads to significant BLEU gains.",
        "Our solution to the brevity problem requires globally-normalized training on only a small dataset.",
        "The discovery of root-and-pattern morphology in Semitic languages using an unsupervised method can be used to extract Semitic roots, which are the basic units of these languages.",
        "Our self-trained model, belabBERT, reaches a higher classification accuracy than the best performing RobBERT model.",
        "A smaller chunk size of 220 tokens leads to a significant accuracy gain for both base models.",
        "The addition of an audio classification network next to a strong standalone text classification model leads to an overall better precision for all labels on top of the higher classification accuracy.",
        "Our new model, belabBERT, outperforms the current state-of-the-art RobBERT model.",
        "We hypothesized that we could increase the size of our dataset by splitting the samples up into chunks of a fixed length without losing classification accuracy, and our results support this approach.']"
    ],
    "8511": [
        "Our proposed GenMA model outperforms other models as it is capable of generating new morphemes out of neighbor characters and it identifies the essential morphemes to classify a sentence.",
        "The two main advantages of our model are: \u2022 The model can construct sentence embeddings based on the new generative morphemes which are created artificially in combination of both the languages Hindi and English. These morphemes carry the features of both the languages.",
        "The model is able to correctly identify the co-occurring character sets with highest importance in sentiment analysis.",
        "Our model is capable of classifying the sentiment of the sentences without considering language difference between words in the sentences with an F1-score of 0.68 on the test data.",
        "Future work may reveal how to capture sentiment based on emojis that are widely used in tweets.",
        "One of our settings is artificial morpheme generation for the Hindi and English dataset, but we have not explored this method in the context of morphologically complex code-mixed datasets.",
        "We will aim to implement the model in the complex code-mixed dataset in the future.",
        "We will also try to capture word level information of code-mixed sentences without language identity to understand what the important key words are to classify sentences.']"
    ]
}